input:
1. FAST_2 conference:
Participate | Call for Papers 
  Double-Blind Guidance 
  Author Response Advice 
    Call for Posters and WiPs 
  Instructions for Presenters 
  Questions 
 FAST '23 Call for Papers  
 Sponsored by USENIX  in cooperation with ACM SIGOPS.   
 The 21st USENIX Conference on File and Storage Technologies (FAST '23)  will take place on February 21–23, 2023, at the Hyatt Regency Santa Clara in Santa Clara, CA, USA.  
 Important Dates  
  Author response period begins: | Monday, November 28, 2022 
  Notification to authors: | Friday, December 9, 2022 
  Final paper files due: | Tuesday, January 24, 2023 
  Download Call for Papers PDF    
 Conference Organizers  
 Overview  
 The 21st USENIX Conference on File and Storage Technologies (FAST '23) brings together storage-system researchers and practitioners to explore new directions in the design, implementation, evaluation, and deployment of storage systems. The program committee interprets "storage systems" broadly: submissions on low-level storage devices, distributed storage systems, and information management are all of interest. The conference will consist of technical presentations including refereed papers, and poster sessions.  
 Topics  
 Topics of interest to FAST should include files and/or storage, and may overlap with other topics including, but not limited to:  
 Archival systems 
  Auditing and provenance 
  Search and data retrieval 
  Security 
  In evaluating the fit of a paper for FAST, a key ingredient is the design of storage software. A paper with only hardware-level contributions will be out-of-scope; a paper could be brought into scope for FAST by demonstrating for example how software can leverage novel hardware.  
 Submission Instructions  
 The complete submission must be no longer than 12 pages excluding references. | There is no short-paper category. | The program committee values conciseness: if you can express an idea in fewer pages than the limit, do so. Supplemental material may be added as a single separate file without page limits. However, the reviewers are not required to read or consider such material. Content that should be considered to judge the paper is not supplemental and counts toward the page limit. 
  Papers must be typeset on U.S. letter-sized pages in two columns using 10-point Times Roman font on 12-point leading (single-spaced), within a text block | 7" wide by 9" deep | . 
  Labels, captions, and other text in figures, graphs, and tables must use font sizes that, when printed, do not require magnification to be legible. References must not be set in a smaller font. Submissions that violate these requirements will not be reviewed. Limits will be interpreted strictly. No extensions will be given for reformatting. 
  A LaTeX template and style file are available on the | USENIX templates page | . 
  Double-blind policy: | Authors must not be identified in the submissions, either explicitly or by implication. To refer to your previous work, consider it as written by a third party. Do not say "reference removed for blind review." Supplemental material must be anonymized. Submissions violating anonymization rules will not be considered for review. If you are uncertain about how to anonymize your submission, please contact the program co-chairs, | fast23chairs@usenix.org | , well in advance of the submission deadline. 
  Prior Workshop Paper Policy: | If a submission extends a prior workshop paper, please include an anonymized copy of the workshop paper in the submission field. This should be the same as the published version, with any identifying information removed. 
  Simultaneous submission of the same work to multiple venues, submission of previously published work, or plagiarism constitutes dishonesty or fraud. USENIX, like other scientific and technical conferences and journals, prohibits these practices and may take action against authors who have committed them. See the | USENIX Conference Submissions Policy | for details. 
  If you are uncertain whether your submission meets USENIX's guidelines, contact the program co-chairs, | fast23chairs@usenix.org | , or the USENIX office, | submissionspolicy@usenix.org | . 
  Papers accompanied by nondisclosure agreement forms will not be considered. 
  Submissions should abide by the Conflict Identification guidelines (see below). 
  The program committee and external reviewers will judge papers on technical merit, significance, relevance, and presentation. Research papers on new and unexplored problems are encouraged. A good research paper:  
 addresses a significant problem; 
  presents an interesting, compelling solution; 
  clearly describes what the authors have done; and 
  clearly articulates the advances beyond previous work. 
  Program committee members, USENIX, and the broader community generally value a paper more highly if it clearly defines and is accompanied by artifacts not previously available. These artifacts may include traces, original data, source code, or tools developed as part of the submitted work.  
 Blind reviewing of all papers will be done by the program committee, assisted by outside referees when necessary. Accepted papers will be shepherded by a member of the program committee.  
  Deployed-Systems Papers  
 In addition to papers that describe original research, FAST '23 also solicits papers that describe real operational systems, including systems currently in production. Such papers should address experience with the practical design, implementation, analysis, deployment, or operation of such systems. We encourage submission of papers that disprove or strengthen existing assumptions, deepen the understanding of existing problems, and validate known techniques in environments in which they were never before used or tested, with preference given to experimental results based on production data. Deployed-system papers will be treated similarly to other papers for publication purposes; they need not present new ideas or results to be accepted, but should offer useful guidance to practitioners.  
 A good deployed-system paper:  
 clearly articulates lessons learned from deploying in production; 
  describes an operational system of broad interest; 
  discusses practical problems encountered in production; and 
  supports the lessons with appropriate evidence, potentially including statistical data from the deployment, empirical evaluation of the system, and anecdotes. 
  For deployed systems papers, the title should be prefixed with "Deployed System: "  , followed by the title. Authors must also indicate in the submission form that they are submitting a deployed-system paper.  
 Double-blind Policy for Deployed-system Paper:  All submissions for FAST '23 are required to follow the double-blind policy (see above). However, for only deployed-system papers, the product or company described in the paper need not be anonymized (authors still need to be anonymized).  
 Author Response Period  
 Conflict Identification  
 Upon submitting your paper, authors must indicate conflicts with PC members. A conflict exists in one of the following cases:  
 Institution:  You are currently employed at the same institution, have been previously employed at the same institution within the past two years, or are going to begin employment at the same institution. A completed internship does not constitute an institutional conflict.  
 Advisor/Advisee:  Doctoral thesis advisor and post-doctoral advisor (if relevant) are conflicts for life.  
 Collaboration:  You have a collaboration on a project, publication, grant proposal, or editorship within the past two years.  
 Close friends and family:  Close family relations (e.g., spouse, parent/child, sibling) and close friends are conflicts forever, if they are potential reviewers.  
 The PC will review paper conflicts to ensure the integrity of the reviewing process, adding conflicts if necessary. If there is no basis for conflicts indicated by authors, such conflicts will be removed. Do not identify PC members as a conflict solely to avoid having them as reviewers. If you have any questions about conflicts, contact the program co-chairs.  
 Author Notification and Beyond  
 Authors will be notified of paper acceptance or rejection according to the schedule above. If your paper is accepted and you need an invitation letter to apply for a visa to attend the conference, contact conference@usenix.org  as soon as possible. Visa applications can take at least 30 working days to process. Identify yourself as a presenter and include your mailing address in your email.  
 Early Rejection Notification.  This year, we will notify authors of papers that are rejected early in the process, prior to the author response period. The goal is to allow authors of early rejected papers to use reviewer feedback earlier and resubmit to another conference as soon as possible. Early rejected papers will no longer be considered under submission (for the purposes of multiple submission policies) upon receipt of a rejection notification.  
 All papers will be available online to registered attendees no earlier than Thursday, January 26, 2023. If your accepted paper should not be published prior to the event, please notify production@usenix.org  . The papers will be available online to everyone beginning on the first day of the main conference, February 21, 2023. Accepted submissions will be treated as confidential prior to publication on the USENIX FAST '23 website; rejected submissions will be permanently treated as confidential.  
 By submitting a paper, you agree that at least one of the authors will attend the conference to present it. If the conference registration fee will pose a hardship for the presenter of the accepted paper, please contact conference@usenix.org  .  
 SUBMIT YOUR WORK    
  Participate | Call for Papers 
  Double-Blind Guidance 
  Author Response Advice 
  Call for Posters and WiPs 
  Instructions for Presenters
2. FAST_3 conference:
Participate | Call for Papers 
  Double-Blind Guidance 
  Author Response Advice 
    Call for Posters and WiPs 
  Instructions for Presenters 
  Questions 
 FAST '23 Technical Sessions  
 All sessions will be held in Santa Clara Ballroom unless otherwise noted.   
 Papers are available for download below to registered attendees now and to everyone beginning Tuesday, February 21. Paper abstracts are available to everyone now. Copyright to the individual works is retained by the author[s].  
 Proceedings Front Matter  
 Proceedings Cover  | Title Page and List of Organizers  | Message from the Program Co-Chairs  | Table of Contents   
 Papers and Proceedings  
 The full Proceedings published by USENIX for the conference are available for download below. Individual papers can also be downloaded from their respective presentation pages. Copyright to the individual works is retained by the author[s].  
 Full Proceedings PDF Files   
  FAST '23 Full Proceedings (PDF)     
  FAST '23 Full Proceedings Interior (PDF, Best for Mobile Devices)     
 Attendee Files   
 (Registered attendees: Sign in  to your USENIX account to download these files.)  
  FAST '23 Attendee List (PDF)    
  FAST '23 Proceedings Web Archive (ZIP)    
 View mode:  condensed 
  Expanded 
 Tuesday, February 21, 2023   
 Continental Breakfast  
 Perseus: A Fail-Slow Detection Framework for Cloud Storage Systems   
 Ruiming Lu, Shanghai Jiao Tong University;  Erci Xu, Alibaba Inc. and Shanghai Jiao Tong University;  Yiming Zhang, Xiamen University;  Fengyi Zhu, Zhaosheng Zhu, Mengtian Wang, and Zongpeng Zhu, Alibaba Inc.;  Guangtao Xue, Shanghai Jiao Tong University;  Jiwu Shu, Xiamen University;  Minglu Li, Shanghai Jiao Tong University and Zhejiang Normal University;  Jiesheng Wu, Alibaba Inc.   
 Awarded Best Paper!    
 Deployed-Systems Paper   
 Available Media   
 Available Media   
 Distributed in-memory key-value (KV) stores are embracing the disaggregated memory (DM) architecture for higher resource utilization. However, existing KV stores on DM employ a \emph{semi-disaggregated} design that stores KV pairs on DM but manages metadata with monolithic metadata servers, hence still suffering from low resource efficiency on metadata servers. To address this issue, this paper proposes FUSEE, a FUlly memory-diSaggrEgated KV StorE that brings disaggregation to metadata management. FUSEE replicates metadata, i.e., the index and memory management information, on memory nodes, manages them directly on the client side, and handles complex failures under the DM architecture. To scalably replicate the index on clients, FUSEE proposes a client-centric replication protocol that allows clients to concurrently access and modify the replicated index. To efficiently manage disaggregated memory, FUSEE adopts a two-level memory management scheme that splits the memory management duty among clients and memory nodes. Finally, to handle the metadata corruption under client failures, FUSEE leverages an embedded operation log scheme to repair metadata with low log maintenance overhead. We evaluate FUSEE with both micro and YCSB hybrid benchmarks. The experimental results show that FUSEE outperforms the state-of-the-art KV stores on DM by up to 4.5 times with less resource consumption.  
 ROLEX: A Scalable RDMA-oriented Learned Key-Value Store for Disaggregated Memory Systems   
 Pengfei Li, Yu Hua, Pengfei Zuo, Zhangyu Chen, and Jiajie Sheng, Huazhong University of Science and Technology   
 Awarded Best Paper!    
 Available Media   
 Disaggregated memory systems separate monolithic servers into different components, including compute and memory nodes, to enjoy the benefits of high resource utilization, flexible hardware scalability, and efficient data sharing. By exploiting the high-performance RDMA (Remote Direct Memory Access), the compute nodes directly access the remote memory pool without involving remote CPUs. Hence, the ordered key-value (KV) stores (e.g., B-trees and learned indexes) keep all data sorted to provide rang query service via the high-performance network. However, existing ordered KVs fail to work well on the disaggregated memory systems, due to either consuming multiple network roundtrips to search the remote data or heavily relying on the memory nodes equipped with insufficient computing resources to process data modifications. In this paper, we propose a scalable RDMA-oriented KV store with learned indexes, called ROLEX, to coalesce the ordered KV store in the disaggregated systems for efficient data storage and retrieval. ROLEX leverages a retraining-decoupled learned index scheme to dissociate the model retraining from data modification operations via adding a bias and some data-movement constraints to learned models. Based on the operation decoupling, data modifications are directly executed in compute nodes via one-sided RDMA verbs with high scalability. The model retraining is hence removed from the critical path of data modification and asynchronously executed in memory nodes by using dedicated computing resources. Our experimental results on YCSB and real-world workloads demonstrate that ROLEX achieves competitive performance on the static workloads, as well as significantly improving the performance on dynamic workloads by up to 2.2 times than state-of-the-art schemes on the disaggregated memory systems. We have released the open-source codes for public use in GitHub.  
 FAST '23 Poster Session and Reception  
 Mezzanine East/West  
 Check out the cool new ideas and the latest preliminary research on display at the Poster Session and Reception. Take part in discussions with your colleagues over complimentary food and drinks. View the complete list of accepted posters  .  
 Wednesday, February 22, 2023   
 Continental Breakfast  
 In this paper, we make a first effort to investigate how and where the lack of any uniform approach to handling name collisions leads to a diffusion of responsibility and resultant vulnerabilities. Interestingly, we demonstrate the existence of a range of novel security challenges arising from name collisions and their inconsistent handling by low-level utilities and applications. Specifically, our experiments show that utilities handle many name collision scenarios unsafely, leaving the responsibility to applications whose developers are unfortunately not yet aware of the threats. We examine three case studies as a first step towards systematically understanding the emerging type of name collision vulnerability.  
 ConfD: Analyzing Configuration Dependencies of File Systems for Fun and Profit   
 Xiaobin He, National Supercomputing Center in Wuxi;  Bin Yang, Tsinghua University, Dept. of C.S; National Supercomputer Center in Wuxi;  Jie Gao and Wei Xiao, National Supercomputing Center in Wuxi;  Qi Chen, Tsinghua University, Dept. of C.S;  Shupeng Shi and Dexun Chen, National Supercomputing Center in Wuxi;  Weiguo Liu, Shandong University;  Wei Xue, Tsinghua University, Dept. of C.S; Tsinghua University, BNRist.; National Supercomputer Center in Wuxi;  Zuo-ning Chen, Chinese Academy of Engineering   
 Deployed-Systems Paper   
 Available Media   
 Available Media   
 Byte-addressable Non-Volatile Memory (NVM) allows programs to directly access storage using memory interface without going through the expensive conventional storage stack. However, direct access to NVM makes the NVM data vulnerable to software memory bugs (memory safety) and hardware errors (fault tolerance). This issue is critical because, unlike DRAM, corrupted data can persist forever, even after the system restart. Albeit the plethora of research on NVM programs and systems, there is little attention protecting NVM data from software bugs and hardware errors. In this paper, we propose TENET, a new NVM programming framework, which guarantees memory safety and fault-tolerance to protect NVM data against software bugs and hardware errors. TENET provides the most popular Persistent Transactional Memory (PTM) programming model. TENET leverages the concurrency and commit-time guarantees of a PTM to provide performant and cost-efficient memory safety and fault tolerance. Our evaluations shows that TENET offers the protection for NVM data at a modest performance overhead and storage cost, as compared to other PTMs with partial or no memory safety and fault-tolerance support.  
 MadFS: Per-File Virtualization for Userspace Persistent Memory Filesystems   
 Available Media   
 In this work, we design and implement a Stackable Persistent memory File System (SPFS), which serves NVMM as a persistent writeback cache to NVMM-oblivious filesystems. SPFS can be stacked on a disk-optimized file system to improve I/O performance by absorbing frequent order-preserving small synchronous writes in NVMM while also exploiting the VFS cache of the underlying disk-optimized file system for non-synchronous writes. A stackable file system must be lightweight in that it manages only NVMM and not the disk or VFS cache. Therefore, SPFS manages all file system metadata including extents using simple but highly efficient dynamic hash tables. To manage extents using hash tables, we design a novel Extent Hashing algorithm that exhibits fast insertion as well as fast scan performance. Our performance study shows that SPFS effectively improves I/O performance of the lower file system by up to 9.9×.  
 Available Media   
 RDMA-enabled remote memory (RM) systems are gaining popularity with improved memory utilization and elasticity. However, since it is commonly believed that fine-grained RDMA permission management is impractical, existing RM systems forgo memory protection, an indispensable property in a real-world deployment. In this paper, we propose PATRONUS, an RM system that can simultaneously offer protection and high performance. PATRONUS introduces a fast permission management mechanism by exploiting advanced RDMA hardware features with a set of elaborate software techniques. Moreover, to retain the high performance under exception scenarios (e.g., client failures, illegal access), PATRONUS attaches microsecond-scaled leases to permission and reserves spare RDMA resources for fast recovery. We evaluate PATRONUS over two one-sided data structures and two function-as-a-service (FaaS) applications. The experiment shows that the protection only brings 2.4% to 27.7% overhead among all the workloads, and our system performs at most ×5.2 than the best competitor.  
 More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba   
 Qiang Li, Alibaba Group;  Qiao Xiang, Xiamen University;  Yuxin Wang, Haohao Song, and Ridi Wen, Xiamen University and Alibaba Group;  Wenhui Yao, Yuanyuan Dong, Shuqi Zhao, Shuo Huang, Zhaosheng Zhu, Huayong Wang, Shanyang Liu, Lulu Chen, Zhiwu Wu, Haonan Qiu, Derui Liu, Gexiao Tian, Chao Han, Shaozong Liu, Yaohui Wu, Zicheng Luo, Yuchao Shao, Junping Wu, Zheng Cao, Zhongjie Wu, Jiaji Zhu, and Jinbo Wu, Alibaba Group;  Jiwu Shu, Xiamen University;  Jiesheng Wu, Alibaba Group   
 Deployed-Systems Paper   
 Available Media   
 FAST '23 Test of Time Award Presentation  
 A Study of Practical Deduplication   
  Dutch T. Meyer and William J. Bolosky  
 Mezzanine East/West  
 Thursday, February 23, 2023   
 Continental Breakfast  
 Available Media   
 The emerging computational storage device offers an opportunity for in-storage computing. It alleviates the overhead of data movement between the host and the device, and thus accelerates data-intensive applications. In this paper, we present λ-IO, a unified IO stack managing both computation and storage resources across the host and the device. We propose a set of designs – interface, runtime, and scheduling – to tackle three critical issues. We implement λ-IO in full-stack software and hardware environment, and evaluate it with synthetic and real applications against Linux IO, showing up to 5.12× performance improvement.  
 Revitalizing the Forgotten On-Chip DMA to Expedite Data Movement in NVM-based Storage Systems   
 There have been drastic changes in the storage device landscape recently. At the center of the diverse storage landscape lies the NVMe interface, which allows high-performance and flexible communication models required by these next-generation device types. However, its hardware-oriented definition and specification are bottlenecking the development and evaluation cycle for new revolutionary storage devices.  
 In this paper, we present NVMeVirt, a novel approach to facilitate software-defined NVMe devices. A user can define any NVMe device type with custom features, and NVMeVirt allows it to bridge the gap between the host I/O stack and the virtual NVMe device in software. We demonstrate the advantages and features of NVMeVirt by realizing various storage types and configurations, such as conventional SSDs, low-latency high-bandwidth NVM SSDs, zoned namespace SSDs, and key-value SSDs with the support of PCI peer-to-peer DMA and NVMe-oF target offloading. We also make cases for storage research with NVMeVirt, such as studying the performance characteristics of database engines and extending the NVMe specification for the improved key-value SSD performance.  
 SMRSTORE: A Storage Engine for Cloud Object Storage on HM-SMR Drives   
 Su Zhou, Erci Xu, Hao Wu, Yu Du, Jiacheng Cui, Wanyu Fu, Chang Liu, Yingni Wang, Wenbo Wang, Shouqu Sun, Xianfei Wang, Bo Feng, Biyun Zhu, Xin Tong, Weikang Kong, Linyan Liu, Zhongjie Wu, Jinbo Wu, Qingchao Luo, and Jiesheng Wu, Alibaba Group   
 Deployed-Systems Paper   
 Available Media   
 Cloud object storage vendors are always in pursuit of better cost efficiency. Emerging Shingled Magnetic Recording (SMR) drives are becoming economically favorable in archival storage systems due to significantly improved areal density. However, for standard-class object storage, previous studies and our preliminary exploration revealed that the existing SMR drive solutions can experience severe throughput variations due to garbage collection (GC).  
 In this paper, we introduce SMRSTORE, an SMR-based storage engine for standard-class object storage without compromising performance or durability. The key features of SMRSTORE include directly implementing chunk store interfaces over SMR drives, using a complete log-structured design, and applying guided data placement to reduce GC for consistent performance. The evaluation shows that SMRSTORE delivers comparable performance as Ext4 on the Conventional Magnetic Recording (CMR) drives, and can be up to 2.16x faster than F2FS on SMR drives. By switching to SMR drives, we have decreased the total cost by up to 15% and provided performance on par with the prior system for customers. Currently, we have deployed SMRSTORE in standard-class Alibaba Cloud Object Storage Service (OSS) to store hundreds of PBs of data. We plan to use SMR drives for all classes of OSS in the near future.  
 Fast Application Launch on Personal Computing/Communication Devices   
 Junhee Ryu, SK hynix;  Dongeun Lee, Texas A&M University - Commerce;  Kang G. Shin, University of Michigan;  Kyungtae Kang, Hanyang University   
 Available Media   
 We present Paralfetch, a novel prefetcher to speed up app launches on personal computing/communication devices by: 1) accurate collection of launch-related disk read requests, 2) pre-scheduling of these requests to improve I/O throughput during prefetching, and 3) overlapping app execution with disk prefetching for hiding disk access time from the app execution. We have implemented Paralfetch under Linux kernels on a desktop/laptop PC, a Raspberry Pi 3 board, and an Android smartphone. Tests with popular apps show that Paralfetch significantly reduces app launch times on flash-based drives, and outperforms GSoC Prefetch and FAST, which are representative app prefetchers available for Linux-based systems.  
 Integrated Host-SSD Mapping Table Management for Improving User Experience of Smartphones   
  Participate | Call for Papers 
  Double-Blind Guidance 
  Author Response Advice 
  Call for Posters and WiPs 
  Instructions for Presenters
3. FAW_0 conference:
International Joint Conference on Theoretical Computer Science – Frontier of Algorithmic Wisdom        
 August 14 - 18, 2023,  University of Macau, Macau SAR, China   
 Overview  
   International Joint Conference on Theoretical Computer Science – Frontier of Algorithmic Wisdom (IJTCS-FAW 2023)  will be held on August 14 - 18, 2023, jointly hosted by CCF and University of Macau. We will cooperate with CSIAM and ACM China Council to provide you with a feast of theoretical computer science. It is the 4th IJTCS and 17th FAW (originally run as a workshop). FAW is joining IJTCS as the main track selecting high quality submissions together with several new tracks. This year we are going to host the CCF 2023 Annual Conference on Computational Economics (CCF CE 2023)  as part of IJTCS-FAW 2023 conference.   
 IJTCS-FAW 2023   is calling for papers concerning any branch of theoretical computer science, together with focus tracks in Algorithmic Game Theory, Blockchain, Multi-agent Reinforcement Learning, Quantum Computation, Theory of Machine Learning, Machine Learning, Formal Method, Algorithm and Complexity, Computational and Network Economics. Furthermore, we will also host a series of featured forums including Female Forum, Youth PhD Forum, and Undergraduate Research Forum.   
 The invitation letter for attending the IJTCS-FAW 2023 can be found here  .   
 The invitation letter for attending the CCF CE 2023 can be found here  .   
 The proceedings of IJTCS-FAW 2023 can be found here  .  
 Important Dates    
  Notification: May 31, 2023 
  Camera Ready: June 09, 2023 
  Conference: August 14 - 18, 2023 
 Advisor Committee     
  Registration Fee: Macau Expo Group Limited at ijtcs2023@iccmacao.com 
 IJTCS-FAW 2023  
 Overview 
  Chairs/Committees
4. FAW_1 conference:
Registration Portal  
 International Joint Conference on Theoretical Computer Science – Frontier of Algorithmic Wisdom (IJTCS-FAW 2023)  will be held on August 14 - 18, 2023, jointly hosted by CCF and University of Macau. We will cooperate with CSIAM and ACM China Council to provide you with a feast of theoretical computer science. It is the 4th IJTCS and 17th FAW (originally run as a workshop). FAW is joining IJTCS as the main track selecting high quality submissions together with several new tracks. This is the IJTCS-FAW 2023 and Annual Meeting of CCF Computational Economics 2023 registration portal. To participant the conference in Macao, please register here!   
 IJTCS-FAW 2023 & Annual Meeting of CCF Computational Economics 2023   
 August 14-18, 2023    
 University of Macau, Macao SAR, China
5. FCCM_1 conference:
Menu  Home 
  Call for Contributions | Author Guidelines 
  Call for Papers 
  Call for Workshops 
  Diversity 
  Past FCCMs 
  FCCM 2024 Event Photos 
 FCCM Main Page  
 The IEEE Symposium on Field-Programmable Custom Computing Machines is the original and premier forum for presenting and discussing new research related to computing that exploits the unique features and capabilities of FPGAs and other reconfigurable hardware. Over the past two decades, FCCM has been the place to present papers on architectures, tools, and programming models for field-programmable custom computing machines as well as applications that use such systems.  
 News
6. FCCM_2 conference:
Call for Demos 
  Ph.D. Forum 
  Author Guidelines 
  Organization 
  Past FCCMs 
  Diversity 
 Program 2023  
 FCCM 2023 Preliminary  Program   
 * All times shown in the Pacific Time Zone (UTC-8)   
 Monday (May 8) and Thursday (May 11): Workshops and Tutorials    
 Tuesday – May 9   
 ★  indicates best paper candidate   
 Open Research Objects (ORO)  Research Objects Reviewed (ROR)  Results Reproduced (ROR-R)    
 SCCL: An open-source SystemC to RTL translator      
  Zhuanhao Wu (University of Waterloo); Maya Gokhale (Lawrence Livermore National Laboratory); Scott Lloyd (Brigham Young University); Hiren Patel (University of Waterloo) 
 Lasa: Abstraction and Specialization for Productive and Performant Linear Algebra on FPGAs  (short paper)  
  Xiaochen Hao (Peking University); Hongbo Rong (Intel Labs); Mingzhe Zhang (Tsinghua University); Ce Sun (University of Science and Technology of China); Zhuofu Tao (University of California, Los Angeles); Yu Zhang (University of Science and Technology of China); Lei He (University of California, Los Angeles); Eric Petit (Intel); Wenguang Chen (Tsinghua University); Yun Liang (Peking University) 
 A Machine Learning Approach for Predicting the Difficulty of FPGA Routing Problems  ★   
  Andrew David Gunter and Steven Wilton (University of British Columbia) 
 CXL over Ethernet: A Novel FPGA-based Memory Disaggregation Design in Data Centers  (short paper)  
  Chenjiu Wang (SKLP, Institute of Computing Technology, CAS; University of Chinese Academy of Sciences); Ke He (unaffiliated); Ruiqi Fan (SKLP, Institute of Computing Technology, CAS; University of Chinese Academy of Sciences); Xiaonan Wang (WUXI Institute of Interconnect Technology); Wei Wang (unaffiliated); Qinfen Hao (SKLP, Institute of Computing Technology, CAS; University of Chinese Academy of Sciences) 
 Optimizing Hybrid Binary-Unary Hardware Accelerators Using Self-Similarity Measures   
  Alireza Khataei, Gaurav Singh, and Kia Bazargan (University of Minnesota) 
 Efficient Implementation of Ring-Binary-LWE-based Lightweight PQC Accelerator on the FPGA Platform  (short paper)  
  Pengzhou He, Tianyou Bao, Yazheng Tu, and Jiafeng Xie (Villanova University) 
  Wednesday – May 10   
 SQL2FPGA: Automatic Acceleration of SQL Query Processing on Modern CPU-FPGA Platforms   
  Alec Lu and Zhenman Fang (Simon Fraser University) 
 DGNN-Booster: A Generic FPGA Accelerator Framework For Dynamic Graph Neural Network Inference  (short paper)  
  Hanqiu Chen and Cong Hao (Georgia Institute of Technology)
7. FCT_0 conference:
History    
 2023/24    
 2022/23    
  Publications    
 2023    
 2022    
 CMSC 2024    
 SOFSEM 2024    
 FCT 2023    
 GLiCS 2023    
 IWOCA 2022    
 Research     
 Conferences and Workshops     
 FCT 2023    
  24th International Symposium on Fundamentals of Computation Theory, FCT 2023  
 September 18-21, 2023, Trier, Germany  
 News  
 Venue: All talks will be in room C10 on Campus 1, Univ. Trier. Rooms C2, C3 and C4 will be also available for common research during the conference and the subsequent workshop. All the rooms are in the C building (see | Campus 1 map | ). A more detailed conference program is available below. 
  Registration is | now open | . For the authors of accepted papers, we request that at least one author will register with "full registration". Non-full online registrations are for free. Only to fully-registered participants, printed proceedings can be delivered. We will close the registration server on THU, Sept. 14th, 2023. 
  List of accepted papers appears below. 
  Titles of the invited talks have been added below to the speakers. 
  The | submission server | is closed now. We are currently evaluating the submissions. As planned, notifications will be sent out by mid-July. 
  Thanks for submitting original research in Theoretical Computer Science, formatted using LNCS style: at most 14 pages (including references) plus a clearly marked appendix. Firm abstract deadline: May 26th, 2023; full papers must be submitted until June 1st, 2023. The reviewing is single-blind. 
  We plan to have a 1-day informal workshop on “Graph Labelling in Computer Science” (GLiCS) after the conference, on September, 22nd. Details will be fixed soon. 
  Special Issue for the best papers of FCT 2023 approved with the prestigious | Journal of Computer and System Sciences | . 
  Four invited speakers have confirmed their participation, details below. 
  Further algorithmic papers will be invited for another Special Issue with | Algorithms | . 
  There will be a best paper and a best student paper award sponsored by the publisher Springer. 
  We are grateful for a further special sponsorship of the conference from the publisher MDPI. 
  Detailed Program  
 SUN, Sept. 17th:  
  MON, Sept. 18th, in C10:  
  TUE, Sept. 19th, in C10:  
  Uéverton Souza, Dieter Rautenbach, | Johannes Rauch | : Exact and Parameterized Algorithms for the Independent Cutset Problem 
  WED, Sept. 20th, in C10:  
  Antonio Al Serhali, | Joachim Niehren | : Subtree Projection for Stepwise Hedge Automata 
  THU, Sept. 21st, in C10:  
  Matyáš Křišťan | , Jakub Svoboda: Shortest Dominating Set Reconfiguration under Token Sliding 
  FRI, Sept. 22nd, in C10, C2, C3, C4: Common Research (GLiCS)  
 Invited Speakers  
 Karl Bringmann, MPI Saarbrücken, Germany: Fine-Grained Complexity of Knapsack Problems 
  Kodai Tanaka, Takaaki Mizuki: Two UNO Decks Efficiently Perform Zero-Knowledge Proof for Sudoku 
  Tomoyuki Yamakami: Power of Counting by Nonuniform Families of Polynomial-Size Finite Automata 
  In the list above, the selected best papers  are highlighted in bold, while the best student paper  is underlined.  
 Important Dates  
 May 26th, 2023, AOE: Abstract submission deadline (firm) 
  June 1st, 2023, AOE: Paper submission deadline (firm) 
  July 17th, 2023: Author notifications 
  July 24th, 2023, AOE: Final papers due 
  September 18th-21st, 2023: FCT conference dates 
  Venue  
 University of Trier, Campus 1
8. FCT_1 conference:
Edition     1st ed. 2023 
  Publication date     22 September 2023 
  Language     English
9. FCT_2 conference:
Shop Our Holiday Gift Guide Find a Gift    
 Fundamentals of Computation Theory: 24th International Symposium, FCT 2023, Trier, Germany, September 18-21, 2023, Proceedings  This book constitutes the proceedings of the 24th International Symposium on Fundamentals of Computation Theory, FCT 2023, held in Trier, Germany, in September 2023.  
  Books | 2 
    Fundamentals of Computation Theory: 24th International Symposium, FCT 2023, Trier, Germany, September 18-21, 2023, Proceedings  
 View More    
  Add to Wishlist     
    Fundamentals of Computation Theory: 24th International Symposium, FCT 2023, Trier, Germany, September 18-21, 2023, Proceedings  
 View More    
 Paperback (1st ed. 2023)   
 Paperback (1st ed. 2023) 
 SHIP THIS ITEM  Ships in 1-2 days | Instant Purchase 
  PICK UP IN STORE  Your local store may have stock of this item. | Available within 2 business hours 
  Want it Today?  
  Check Store Availability 
    Overview  
 This book constitutes the proceedings of the 24th International Symposium on Fundamentals of Computation Theory, FCT 2023, held in Trier, Germany, in September 2023.  
 Publisher: | Springer Nature Switzerland 
 Publication date: | 09/21/2023 
 Edition description: | 1st ed. 2023 
 Product dimensions: | 6.10(w) x 9.25(h) x (d)
10. FC_0 conference:
Silver Sponsors   
  Sponsors in Kind | Financial Cryptography and Data Security 2023  
 Twenty-Seventh International Conference  
  May 1–5, 2023  
  Bluesun Hotel Elaphusa   
  Bol, Brač, Croatia  
 Financial Cryptography and Data Security is a major international forum for research, advanced development, education, exploration, and debate regarding information assurance, with a specific focus on commercial contexts. The conference covers all aspects of securing transactions and systems. Original works focusing on both fundamental and applied real-world deployments on all aspects surrounding commerce security are solicited. Submissions need not be exclusively concerned with cryptography. Systems security and interdisciplinary works are particularly encouraged.  
 The goal of the conference is to bring security and cryptography researchers and practitioners together with economists, bankers implementers and policy-makers. Intimate and colourful by tradition the FC program features invited talks, academic presentations technical demonstrations and panel discussions. In addition, several workshops  will be held in conjunction with the FC conference.  
 This conference is organized annually by the International Financial Cryptography Association  . | Program Chairs | Foteini Baldimtsi

output:1. FAST_2 information:
2. FAST_3 information:
3. FAW_0 information:
4. FAW_1 information:
5. FCCM_1 information:
6. FCCM_2 information:
7. FCT_0 information:
8. FCT_1 information:
9. FCT_2 information:
10. FC_0 information:
