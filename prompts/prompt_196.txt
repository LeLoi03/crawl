input:
1. KEOD_0 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
 Information  Conference Details  Technical Program  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Doctoral Consortium  Partners  Academic Partners  Industrial Partners  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Abstracts  Awards    
 Sponsored by:    
 Logistics:    
 KEOD is part of IC3K    , the 15th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management.  
  Registration to KEOD allows free access to all other IC3K conferences.   
  IC3K 2023 will be held in conjunction with WEBIST 2023    , IN4PL 2023    and IJCCI 2023    .   
  Registration to IC3K allows free access to the WEBIST, IN4PL and IJCCI conferences (as a non-speaker).  
  New registrations are now only available at the conference welcome desk    
  Although the conference is back to the normal mode (i.e., in-person) speakers are allowed to present remotely if unable to travel to the venue (hybrid support).    
 Please visit the KEOD 2024 website   
 Knowledge Engineering (KE) refers to all technical, scientific and social aspects involved in building, maintaining and using knowledge-based systems. KE is a multidisciplinary field, bringing in concepts and methods from several computer science domains such as artificial intelligence, databases, expert systems, decision support systems, information systems and software engineering. KE is also related to logic (both in mathematics and philosophy domains) and to the cognitive and social sciences and socio-cognitive engineering.  
  Ontology Development (OD) aims at building reusable semantic structures that can be informal vocabularies, catalogs, glossaries as well as more complex finite formal structures specifying types of entities and types of relationships relevant within a certain domain. Ontologies have been gaining interest and acceptance in computational audiences. For example, formal ontologies are increasingly used as one of the main sources of software development and methodologies for this end can be adapted to include ontology development. A wide range of applications is emerging, including library science, ontology-enhanced search, e-commerce, business process management and enterprise engineering, among others.  
  KEOD aims at becoming a major meeting point for researchers and practitioners interested in the study and development of methodologies and technologies for Knowledge Engineering and Ontology Development.  
    Conference Chair    
 Publications:     
  All papers presented at the conference venue  
  will be available at the SCITEPRESS Digital Library   
  ( consult SCITEPRESS  Ethics of Publication  )
2. KEOD_1 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
 Documents  Actions  On-line Registration  Registration Fees  Deadlines and Policies  Submit Paper  Guidelines  Preparing your Presentation  Templates  Glossary  Author's Login  Reviewer's Login  Ethics of Review  Information  Conference Details  Important Dates  Technical Program  Social Event  Call for Papers  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Doctoral Consortium  Travel and Accommodation  Conference Venue  About the Region  Reaching the City  Visa Information  Hotel Reservation  Partners  Academic Partners  Industrial Partners  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Websites  Abstracts  Invited Speakers  Awards  Books Published    
 Sponsored by:    
 Logistics:    
 KEOD is part of IC3K    , the 16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management.  
  Registration to KEOD allows free access to all other IC3K conferences.   
  IC3K 2024 will be held in conjunction with ICINCO 2024    , WEBIST 2024    and CoopIS 2024    .   
  Registration to IC3K allows free access to the ICINCO, WEBIST and CoopIS conferences (as a non-speaker).  
  New registrations are now only available at the conference welcome desk    
  Although the conference is back to the normal mode (i.e., in-person) speakers are allowed to present remotely if unable to travel to the venue (hybrid support).    
 Knowledge Engineering (KE) refers to all technical, scientific and social aspects involved in building, maintaining and using knowledge-based systems. KE is a multidisciplinary field, bringing in concepts and methods from several computer science domains such as artificial intelligence, databases, expert systems, decision support systems, information systems and software engineering. KE is also related to logic (both in mathematics and philosophy domains) and to the cognitive and social sciences and socio-cognitive engineering.  
  Ontology Development (OD) aims at building reusable semantic structures that can be informal vocabularies, catalogs, glossaries as well as more complex finite formal structures specifying types of entities and types of relationships relevant within a certain domain. Ontologies have been gaining interest and acceptance in computational audiences. For example, formal ontologies are increasingly used as one of the main sources of software development and methodologies for this end can be adapted to include ontology development. A wide range of applications is emerging, including library science, ontology-enhanced search, e-commerce, business process management and enterprise engineering, among others.  
  KEOD aims at becoming a major meeting point for researchers and practitioners interested in the study and development of methodologies and technologies for Knowledge Engineering and Ontology Development.  
    Conference Chair    
 Publications:     
  All papers presented at the conference venue  
  will be available at the SCITEPRESS Digital Library   
  ( consult SCITEPRESS  Ethics of Publication  )
3. KEOD_2 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
 Documents  Actions  On-line Registration  Registration Fees  Deadlines and Policies  Submit Paper  Guidelines  Preparing your Presentation  Templates  Glossary  Author's Login  Reviewer's Login  Ethics of Review  Information  Conference Details  Important Dates  Technical Program  Social Event  Call for Papers  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Doctoral Consortium  Travel and Accommodation  Conference Venue  About the Region  Reaching the City  Visa Information  Hotel Reservation  Partners  Academic Partners  Industrial Partners  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Websites  Abstracts  Invited Speakers  Awards  Books Published    
 Sponsored by:    
  2023  
  Ana Fred, Frans Coenen, Jorge Bernardino (Eds.)  
  2023  
  David Aveiro, Jan Dietz, Antonella Poggi, Jorge Bernardino (Eds.)  
  2023  
  Le Gruenwald, Elio Masciari, Colette Rolland, Jorge Bernardino (Eds.)
4. KEOD_3 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
 Documents  Actions  On-line Registration  Registration Fees  Deadlines and Policies  Submit Paper  Guidelines  Preparing your Presentation  Templates  Glossary  Author's Login  Reviewer's Login  Ethics of Review  Information  Conference Details  Important Dates  Technical Program  Social Event  Call for Papers  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Doctoral Consortium  Travel and Accommodation  Conference Venue  About the Region  Reaching the City  Visa Information  Hotel Reservation  Partners  Academic Partners  Industrial Partners  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Websites  Abstracts  Invited Speakers  Awards  Books Published    
 Sponsored by:    
 Call for Papers  
 Scope  | Conference Areas  | Keynote Lectures  | Paper Submission   
  Publications  | Important Dates  |  Secretariat  | Venue    
  Conference Chair  | Program Chair  | Program Committee    
 KEOD is sponsored by INSTICC  – Institute for Systems and Technologies of Information, Control and Communication  
  SCOPE  
 Knowledge Engineering (KE) refers to all technical, scientific and social aspects involved in building, maintaining and using knowledge-based systems. KE is a multidisciplinary field, bringing in concepts and methods from several computer science domains such as artificial intelligence, databases, expert systems, decision support systems and information systems. From the software development point of view, KE uses principles that are strongly related to software engineering. KE is also related to logic (both in mathematics and philosophy domains) and to the cognitive and social sciences and socio-cognitive engineering. Knowledge is considered to be produced by humans and structured according to our understanding of how human reasoning and logic works. Currently, KE is mostly related with the construction of shared conceptual frameworks, often designated as ontologies, hence the relevance given to this term in our conference.  
  Ontology Development (OD) aims at building reusable semantic structures that can be informal vocabularies, catalogs, glossaries as well as more complex finite formal structures specifying types of entities and types of relationships relevant within a certain domain. Ontologies have been gaining interest and acceptance in computational audiences. For example, formal ontologies are increasingly used as one of the main sources of software development and methodologies for this end can be adapted to include ontology development. A wide range of applications is emerging, especially given the current web emphasis, including library science, ontology-enhanced search, e-commerce and business process design. Of particular interest, is the development of ontologies and/or methods in the fields of enterprise engineering and enterprise architecture as it has been proven that many problems in software development occur due to incomplete specifications of an enterprise's needs, leading to a huge lack of success in software projects. It is then natural that one should strive to more fully capture and specify enterprise knowledge.  
  KEOD aims at becoming a major meeting point for researchers and practitioners interested in the study and development of methodologies and technologies for Knowledge Engineering and Ontology Development.  
  CONFERENCE TOPICS  
  João Gama  ,  University of Porto, Portugal   
  PAPER SUBMISSION  
 Authors can submit their work in the form of a complete paper or an abstract, but please note that accepted abstracts are presented but not published in the proceedings of the conference. Complete papers can be submitted as a Regular Paper, representing completed and validated research, or as a Position Paper, portraying a short report of work in progress or an arguable opinion about an issue discussing ideas, facts, situations, methods, procedures or results of scientific research focused on one of the conference topic areas.  
  Authors should submit a paper in English, carefully checked for correct grammar and spelling, addressing one or several of the conference areas or topics. Each paper should clearly indicate the nature of its technical/scientific contribution, and the problems, domains or environments to which it is applicable. To facilitate the double-blind paper evaluation method, authors are kindly requested to produce and provide the paper WITHOUT any reference to any of the authors, including the authors’ personal details, the acknowledgments section of the paper and any other reference that may disclose the authors’ identity.  
  When submitting a complete paper please note that only original papers should be submitted. Authors are advised to read INSTICC's ethical norms regarding plagiarism and self-plagiarism  thoroughly before submitting and must make sure that their submissions do not substantially overlap work which has been published elsewhere or simultaneously submitted to a journal or another conference with proceedings. Papers that contain any form of plagiarism will be rejected without reviews.  
  All papers must be submitted through the online submission platform PRIMORIS  and should follow the instructions and templates that can be found under Guidelines and Templates  . After the paper submission has been successfully completed, authors will receive an automatic confirmation e-mail.  
  PUBLICATIONS  
 All accepted complete papers will be published in the conference proceedings, under an ISBN reference, on paper and on digital support.  
  The proceedings will be submitted for indexation by SCOPUS, Google Scholar, DBLP, Semantic Scholar, EI and Web of Science / Conference Proceedings Citation Index  .  
  IMPORTANT DATES  
 Conference Date:  17 - 19 November, 2024   
 Regular Papers   
 Paper Submission:  June 24, 2024 (expired)   
  Authors Notification:  July 31, 2024 (expired)   
  Camera Ready and Registration:  September 13, 2024 (expired)   
  Position Papers /Regular Papers   
 Paper Submission:  July 24, 2024 (expired)   
  Authors Notification:  September 12, 2024 (expired)   
  Camera Ready and Registration:  September 25, 2024 (expired)    
  Late-Breaking   
 Paper Submission:  September 6, 2024 (expired)   
  Authors Notification:  September 26, 2024 (expired)   
  Camera Ready and Registration:  October 7, 2024 (expired)    
  Workshops    
 Workshop Proposal:  June 26, 2024 (expired)    
  Special Sessions    
 Special Session Proposal:  June 26, 2024 (expired)    
  Tutorials    
 Tutorial Proposal:  October 9, 2024 (expired)    
  Demos    
 Demo Proposal:  October 9, 2024 (expired)    
  Panels    
 Panel Proposal:  October 9, 2024 (expired)    
  SECRETARIAT  
 KEOD Secretariat    
  Address: Avenida de S. Francisco Xavier, Lote 7 Cv. C   
  e-mail:  keod.secretariat@insticc.org   
  VENUE  
 The conference will take place at the Vila Galé Porto hotel which is located in the beautiful and historical city of Porto in Portugal. Porto is considered to be one of the most beautiful historical towns in Portugal, with its remarkable architectural heritage shaped by history.  
   IC3K CONFERENCE CHAIR
5. KES AMSTA_1 conference:
Read instantly on your browser with Kindle for Web.  
  Using your mobile phone camera - scan the code below and download the Kindle app.  
 Image Unavailable  
 Read sample     
 Agents and Multi-agent Systems: Technologies and Applications 2023 : Proceedings of 17th KES International Conference, KES-AMSTA 2023, June 2023  Paperback – 28 May 2023   
 by Gordan Jezic  (Editor),    J. Chen-Burger  (Editor),    M. Kusek  (Editor)    & 0  more     
 Purchase options and add-ons  
 This book highlights new trends and challenges in research on agents and the new digital and knowledge economy. It includes papers on business process management, agent-based modeling and simulation and anthropic-oriented computing that were originally presented at the 17th International KES Conference on Agents and Multi-Agent Systems: Technologies and Applications (KES-AMSTA 2023), held in Rome, Italy, in June 14-16, 2023. The respective papers cover topics such as software agents, multi-agent systems, agent modeling, mobile and cloud computing, big data analysis, business intelligence, artificial intelligence, social systems, computer embedded systems and nature-inspired manufacturing, all of which contribute to the modern digital economy.  
  Read more     
  Publisher     Springer 
  Publication date     28 May 2023 
  Language     English 
  Product details  
 Publisher ‏ : ‎  Springer (28 May 2023) 
  Language ‏ : ‎  English
6. KES AMSTA_2 conference:
This book constitutes the refereed proceedings of 17th International Conference, AC 2023, held as part of the 25th Inter  
 This book constitutes the refereed proceedings of 17th International Conference, AC 2023, held as part of the 25th Inter  
 Author / Uploaded 
  Gordan Jezic 
  J. Chen-Burger 
 Table of contents :  
  KES-AMSTA-2023 Conference Organization  
  Preface  
  Contents  
  1 Introduction  
  1.1 Contribution  
  1.2 Organization of the Paper  
  2 Problem Definition  
  2.1 Robot Positioning Relative to a Single Person  
  6 Conclusions  
  References  
  Author Index   
 Citation preview   
  Gordan Jezic · J. Chen-Burger · M. Kusek · R. Sperka · R. J. Howlett · Lakhmi C. Jain Editors  
  Agents and Multi-agent Systems: Technologies and Applications 2023 Proceedings of 17th KES International Conference, KES-AMSTA 2023, June 2023  
  Gordan Jezic · J. Chen-Burger · M. Kusek · R. Sperka · R. J. Howlett · Lakhmi C. Jain Editors  
  Agents and Multi-agent Systems: Technologies and Applications 2023 Proceedings of 17th KES International Conference, KES-AMSTA 2023, June 2023  
  Editors Gordan Jezic Faculty of Electrical Engineering and Computing University of Zagreb Zagreb, Croatia  
  KES-AMSTA-2023 Conference Organization  
  KES-AMSTA-2023 was organized by KES International—Innovation in Knowledge-Based and Intelligent Engineering Systems.  
  Honorary Co-chairs I. Lovrek, University of Zagreb, Croatia Lakhmi C. Jain, KES International, Selby, UK  
  vi  
  KES-AMSTA-2023 Conference Organization  
  Publicity Co-chairs P. Skocir, University of Zagreb, Croatia M. Halaska, Silesian University in Opava, Czech Republic  
  International Programme Committee Assoc. Prof. Patricia Anthony, Lincoln University, New Zealand Dr. Alanis Arnulfo G., Instituto Tecnologico de Tijuana, Mexico Prof. Ahmad Taher Azar, Prince Sultan University, Saudi Arabia Assoc. Prof. Marina Bagi´c Babac, University of Zagreb, Croatia Prof. Dariusz Barbucha, Gdynia Maritime University, Poland Dr. Olfa Belkahla Driss, ESC University of Manouba, Tunisia Prof. Monica Bianchini, University di Siena, Italy Assoc. Prof. Bruno Blaskovic, University of Zagreb, Croatia Assoc. Prof. Frantisek Capkovic, Slovak Academy of Sciences, Slovakia Prof. Zeljka Car, University of Zagreb, Croatia Prof. Matteo Cristani, University of Verona, Italy Dr. Houssem Eddine Nouri, University of Gabes, Tunisia Prof. Margarita N. Favorskaya, Reshetnev Siberian State University of Science and Technology, Russia dipl.ing. Frano Skopljanac-Macina, University of Zagreb, Croatia Prof. Paulina Golinska-Dawson, Poznan University of Technology, Poland Dr. Michal Halaska, Silesian University in Opava, Czech Republic Dr. Madiha Harrabi, Esprit School of Engineering, Tunisia Prof. Tzung-Pei Hong, National University of Kaohsiung, Taiwan Prof. Dragan Jevtic, University of Zagreb, Croatia Prof. Arkadiusz Kawa, Pozna´n Institute of Technology, Poland Prof. Petros Kefalas, CITY College, University of York Europe Campus, Greece Prof. Zdenko Kovacic, University of Zagreb, Croatia Prof. Adrianna Kozierkiewicz, Wrocław University of Science and Technology, Poland Assoc. Prof. Konrad Kulakowski, AGH University of Science and Technology, Poland Prof. Setsuya Kurahashi, University of Tsukuba, Japan Prof. Mario Kusek, University of Zagreb, Croatia Prof. Kazuhiro Kuwabara, Ritsumeikan University, Japan Dr. Marin Lujak, University Rey Juan Carlos, Spain Prof. Rene Mandiau, University Polytechnique Hauts-de-France, France Dr. Bilel Marzouki, Esprit School of Business, Tunisia Prof. Manuel Mazzara, Innopolis University, Russia Prof. Jose M. Molina, Universidad Carlos III de Madrid, Spain  
  KES-AMSTA-2023 Conference Organization  
  vii  
  viii  
  KES-AMSTA-2023 Conference Organization  
  Intelligent Agents in Health, Wellness and Human Development Environments Applied to Health and Medicine Prof. Rosario Baltazar Flores, Tecnologico Nacional de Mexico—Campus Leon, Mexico Prof. Arnulfo Alanis Garza, Tecnologico Nacional de Mexico—Campus Tijuana, Mexico  
  Preface  
  The Programme Committee defined the main track multi-agent systems and the following invited sessions: agent-based modelling and transportation, business economics, intelligent agents in health, wellness and human development environments applied to health and medicine, business economics and education in software engineering in the post-COVID era. Accepted and presented papers highlight new trends and challenges in agent and multi-agent research. We hope that these results will be of value to the research community working in the fields of artificial intelligence, collective computational intelligence, health, robotics, smart systems and, in particular, agent and multi-agent systems, technologies, tools and applications. The chairs’ special thanks go to the following special session organizers: Prof. Rosario Baltazar Flores, Tecnologico Nacional de Mexico/Campus Leon, Mexico, Prof. Arnulfo Alanis Garza, Tecnologico Nacional de Mexico/Campus Tijuana, Mexico, Prof. Hiroshi Takahashi, Keio University, Japan, Prof. Setsuya Kurahashi, University of Tsukuba, Japan, Prof. Takao Terano, Chiba University of Commerce, Japan, Prof. Jean-Michel Bruel, Toulouse University, France, Assoc. Prof. Salvatore Distefano, University of Messina, Italy, Prof. Manuel Mazzara, Prof. Evgenii Bobrov, Mr. Petr Zhdanov and Mr. Nursultan Askarbekuly from Innopolis University, Russia, and Mr. Muhammad Ahmad, National University of Computer and Emerging Sciences, Pakistan, for their excellent work. Thanks are due to the programme co-chairs, all the programme and reviewer committee members, and all the additional reviewers for their valuable efforts in the review process, which helped us to guarantee the highest quality of selected papers for the conference. We cordially thank all the authors for their valuable contributions and all of the other participants in this conference. The conference would not be possible without their support. Zagreb, Croatia Edinburgh, UK Zagreb, Croatia Karvina, Czech Republic Selby, UK/Arad, Romania/Poole, UK Selby, UK April 2023  
  Gordan Jezic J. Chen-Burger M. Kusek R. Sperka R. J. Howlett Lakhmi C. Jain  
  Agent-Based Medical Image Processing Using Multi-stage Distributed Neural Network ˇ Armin Stranjak, Swen Campagna, and Igor Cavrak  
  Abstract In this paper, we describe possible applications of early exit deep neural networks in magnetic resonance imaging, aiming to improve patient scan times and reduce processing costs. The solutions rely on deep neural network layers deployed over a hierarchical processing environment of embedded, edge-based and shared cloud-based resources. Coordination among processing environment layers is achieved by representing different processing resources as agents, effectively forming a multi-agent system. The described approach combines the least-required processing property of the early exit deep neural network and the resource usage optimisation of the multi-agent system in the dynamic and open environment.  
  A. Stranjak et al.  
  two-fold; the central aspect is medical diagnostics, while the second is improving the utilisation of high-value medical equipment by maximising patient throughput. Using complex AI algorithms, such as DNN, on massive amounts of data generated by MRI workflow is difficult if the processing is limited to locally available computing power. Offloading processing to a remote, more powerful edge and cloudbased computing infrastructure necessitates high throughput connectivity and introduces network-related delays. Adding more computing power to the baseline MRI equipment would reduce local processing times but raise the cost of MRI equipment and introduce additional computing resources that would be unused most of the time. However, completely offloading the processing to a cloud-based infrastructure would result in longer wait times, lower patient throughput, and underutilisation of expensive MRI equipment. One of the solutions lies in the distribution of required processing over different layers of the scalable distributed computing hierarchy [5], where different segments of a DNN are distributed over heterogeneous processing nodes. In addition, the early exit technique can be used [14], where the early exit distributed neural network (eeDNN) processing is propagated to higher eeDNN layers only if non-satisfactory results are achieved in lower eeDNN layers. A multi-agent system is employed to select the optimal processing resources in distributed computing hierarchy. Through negotiation among agents, the system effectively maps one or more stages of an eeDNN to available computing resources in the cloud layer. Thus, the system tries to achieve the lowest possible processing times that satisfy the defined result quality, facilitating, in most cases, faster patient processing and increasing MRI equipment utilisation while requiring only modest local computing resources on the MRI equipment. The remainder of the paper is organised as follows. Section 2 describes the specifics of the MR Imaging workflow, and Sect. 3 presents a brief overview of the usage of multi-agent systems in the MR Imaging domain. Section 4 introduces the proposed MAS model combining eeDNN and multi-agent system into a coherent resource-negotiation framework. Section 5 defines the MAS negotiation protocol and award function. Finally, Sect. 6 concludes the paper.  
  2 Application in Magnetic Resonance Imaging Workflow The workflow of Magnetic Resonance (MR) Imaging systems is typically relatively complex compared to other medical imaging technologies due to the high flexibility and some intrinsic properties of Magnetic Resonance Imaging principles. Additionally, Magnetic Resonance Imaging systems have significant computing power requirements when using state-of-the-art algorithms, both for executing single measurements to compose a complete MR examination and to support and ease the workflow of an entire examination for the operator of the system. In this section, we will highlight some use cases where we envision that our proposed strategy of eeDNN can be applied directly to the major processing problem of Magnetic Resonance Imaging:  
  Agent-Based Medical Image Processing Using Multi-stage …  
  A. Stranjak et al.  
  started to make significant gains in the disciplines like medical sciences and AI-based diagnostics. Medical Image Segmentation and image reading [3] utilised a multiagent approach to image segmentation to extract objects of interest and coordinate the outcome of the agents’ work to construct an anatomical structure. Similarly, a significant set of e-Health initiatives and projects [2] are emersed into content-rich, decentralised, interdependent dynamic environments, which are found appropriate for using multi-agent systems. An image interpretation difficulty, especially a sheer number of image readings required by the radiology department, is addressed through the usage of multi-agent systems [1]. We have also addressed in Chap. 2 several use cases within the MR image analysis and processing. These activities rarely demand only simplified, centralised processing where the image analysis is non-time critical and quality tolerant. However, a significant improvement in image quality and more complex algorithms for image-related task processing required an increased computing power and innovative approach. We have addressed this challenge in our previous work [10]. Such a demanding environment led further to the consideration of neural network processing. If fused with the higher demand for computational processing, it provides a contextual suitability for multi-agent systems and cloud-based computing. This paper addresses such fusion.  
  4 Model In our processing model, we adopt the baseline eeDNN model [14], with deep neural network layers partitioned into three stages—local, edge and cloud (Fig. 1, left). Early  
  7  
  exit layers (local and edge) perform final processing at the corresponding stage and permit quality assessment of the achieved result (classification, segmentation etc.), allowing the system to decide whether to conclude the processing or advance it to the upper DNN stage. As a rule of thumb, the number of DNN layers in a stage corresponds to the expected processing power in the corresponding layer of the hierarchical computing environment, with the local stage expected to be the least powerful one. Result quality assessment is based on the entropy of the achieved result [13], where entropy lower than the threshold value implies early exit result being treated as the final processing result and returned to the user. In the opposite case, the processing results of the uppermost DNN layer of the current stage are transferred to the input of the first DNN layer of the higher stage, where the processing is resumed. Benefits of the early exit mechanism are three-fold: (i) eeDNNs inherently possess the least-required-processing property: data transfer to and higher-stage processing is performed only if required result quality cannot be achieved at the lower stage; (ii) significant data compression is achieved when transferring processing results between DNN stages, as opposed to the raw MRI input; and (iii) system resilience—unavailability of higher DNN stages does not automatically imply service unavailability—but may yield lower quality results. Different static or dynamic placement strategies are possible concerning the threestaged structure of the eeDNN (Fig. 1, right). Scenario #1 placement implies mapping each eeDNN stage to a separate distributed hierarchical computing layer, where the local stage is being allocated to the locally available computing power, the more demanding stage is deployed at the edge layer, and the highest stage is deployed at the cloud layer. The second scenario implies co-location of edge and cloud stages in case of usage scenario where edge layer resources are not available. Finally, the third scenario implies the availability of cloud-only processing resources, where raw input data is transferred to the cloud, averaging the highest communication delays and dependence on stable data links. The model proposed in this paper focuses only on Scenario #1, where there is a 1:1 mapping between an edge resource and the local resource, and there exists a dynamic marketplace of cloud-based resources. The placement of three eeDNN stages in this scenario implies fixed mapping of local and edge stages to known computing resources, but the cloud stage-to-resource mapping is done dynamically, using a multi-agent system and a bidding process. Fusion between eeDNNs and the open environment of multi-agent systems proved to be a challenging task. Tight coupling of disparate layers of DNN networks requires strong binding between bordering neurons to encompass a holistic and consistent execution environment for neural processing. On the other hand, market-based systems enable a flexible and open environment where offered services are validated through negotiation and competition mechanisms while service agents arbitrarily choose when and how to provide their processing activities. Therefore, we propose to distribute three distinct layers of processing (Fig. 2), determined and shaped by the heterogeneous character of every layer found in the hierarchical distributed computing model [5].  
  8  
  Fig. 2 The contextual differences of hierarchical neural network layers  
  The first level of processing, normally associated with the embedded or limited local power, is organised as a fixed, predefined network layer operating on locally sourced data. Neural network inputs are fed directly from the local imaging process and desired entropy level is targeted by best-effort approach. Typical local systems would be low-power computing nodes of a mobile MR scanner or an embedded camera with a localised embedded processing device. Had desired entropy of the first network level not been achieved, a dedicated proxy device is defined in the next, higher hierarchy layer. The second layer determines the role of a dedicated proxy device associated with the network nodes from the first layer. Since such device is dedicated to its own pairing device from the local environment, its network configuration is fixed and determined in advance. In case of an unsatisfactory entropy level achieved by the first layer, a vector of weight values is transported as an input to the second layer. Since the achieved entropy level in the first layer is dependent on several factors, including input data complexity, desired quality output, appropriateness of chosen network configuration and so on, the data transferred with weight vector also includes a desired entropy level. This value will be used in comparison with the entropy level of the second layer output of the neural network. In case that desired entropy level is achieved, the results will be provided back to the first layer without further involvement of the third layer.  
  Agent-Based Medical Image Processing Using Multi-stage …  
  9  
  The third layer brings wider flexibility in terms of connectivity and processing possibilities. Instead of adopting another peer-to-peer connection characteristics, like a link between the first and second layer, a more fitting solution would be a market-based negotiation environment. Processing nodes are represented by cloudbased agents as autonomous entities. They would engage in an open negotiation with the Proxy devices, who are also equipped with a multi-agent communication stack, while competing with other cloud agents for processing services. Proxy device would collaborate with the cloud agents in order to agree and achieve fast execution of the final network layer. Unlike in the case between the first and second layer, where expected entropy is imposed on Proxy device, cloud agents are not tasked with such demand as they will inherently aim for the best possible result, as agreed during the negotiation conversation. However, our approach enables independent, commercially cloud-based entities to participate in such negotiation by offering their computer resources with the possibility to dynamically adopt a given network topology during negotiation by accessing such a topology configuration remotely.  
  5 Market-Based Environment for Multi-agent Systems Goal autonomy and independent decision-making are the fundamental characteristics of any multi-agent system. In order to ensure optimal or, in given circumstances, the best possible outcome, mutual competition is encouraged among participating agents. Since their own goals stay private, the establishment of design mechanisms to encourage agent to reveal their true intentions or estimations is important for robust and stable dynamic systems. It is, therefore, crucial to ensure an appropriate mechanism design to reveal the agent’s true preferences. Since the agents do not know their preferences in advance, they would require to gather the information before deliberation about the allocation of their resources and the next actions. This approach is especially beneficial, as will be shown later when the design of the negotiation protocol needs to encompass both: the task description for the agent to execute and the corresponding award as compensation for the successful computational performance. Various incentivisation strategies were developed to encourage the deliberative agents to reveal their true intentions. A conversation protocol needs to be designed in such a way that the deliberate untruthfulness of a particular participant would be fundamentally against his own interest or goals, and such behaviour would be seen as irrational. A reasonably simple solution would be based on Vickrey auction [15] where truthful revealment of the agent’s private intentions is achieved by committing the winning agent with the second-best offer. In such a way, the agents are incentivised not to speculate on the offers of other agents, but their truthfulness in their negotiation bids is their best possible strategy. For this reason, we would base our negotiation design on such an approach.  
  11  
  negotiation by sending a message with CFP performative to all Processor agents using declaration. The content of this “call for proposal” message contains the vector of weight values transported from the second layer, guaranteed value A0 if a task is executed at the time estimated by the Processor agent, and maximum award value Am in case that the processing result is provided earlier. The details about an award function are provided in Sect. 5.2. Based on these values, Processor agents reply with their proposals (message PROPOSE) providing estimated time Te when the processing will be completed. Alternatively, they can simply terminate the conversation by sending NO_PROPOSAL message. Within a given timeout, the Proxy agent collects all received offers and, based on the principles of the Vickrey auction, selects the best offer, but it offers the second-best estimated time back to the winning Processor agent. This OFFER message also contains the additional parameters Td , k and q that completes the definition of the award function. Td parameter defines the deadline for the reception of the processing results, after which no award will be provided. The remaining k and q parameters are used to define the “curvature” of the award function, giving a chance to the Proxy agent to shape the award function based on the urgency of processing. The winning Processor agent receives a complete set of parameters to define the award function. If it is satisfactory, it has a chance to accept or reject the offer (corresponding ACCEPT and REJECT messages are used) based on its estimation of award function and its cost-effectiveness in exchange for the processing time and result. If the offer is satisfactory, the Proxy agent receives the acceptance of the offer, and it still has a final chance to commit or decline the acceptance using COMMIT or DECLINE performatives. After the reception of the task commitment, the Processor agent performs the agreed task. The time difference between the transmission of the COMMIT message and the reception of the INFORM message by Proxy agent is used as reception time Tr which is used to calculate an award At that will be transferred to Processor agent by AWARD message. Otherwise, if the result is provided after the deadline time Td , no award is given, and NO_AWARD message is sent. This protocol does not dictate the exact award character, financial or symbolic, and one possible implementation could include some sort of cryptocurrency or locally sourced award tokens.  
  5.2 Award Function The award function and corresponding function definition are depicted at Fig. 4. As explained above, the winning agent is awarded based on an agreed award function shown above. Its parameters A0 , Am , Te and Td define the functional dependency between the time of delivered results and award amount. The choice of the exponential function (top function definition in Fig. 4) seems reasonable as most natural and market-based processes and bidding mechanisms could be accurately explained with natural exponential functions. In addition, k and q exponents that define the “curvature” of the award function are also agreed upon as a part of the negotia-  
  tion conversation, as explained in 5.1. If we want to transform given coefficients into negotiation parameters mentioned above, we get the second form for the award function, shown as a bottom function definition on right in Fig. 4. Since two different exponential functions describe the award given before and after estimated time Te , the condition was that they provide the same value at the point (Te , A0 ) with the additional condition that the first function contains point (0, Am ) and the second one contains (Td , 0). Based on these conditions, the bottom award function was deducted from the top one.  
  6 Conclusion and Future Work In this paper, we proposed a multi-agent based framework for advanced data processing in Magnetic Resonance Image systems using early exit distributed neural networks. To coordinate the usage of available computing resources hosting layers of eeDNN within distributed computing hierarchy, we employ a multi-agent system, with self-interested agents representing computing resources in different layers. Through negotiation among agents, based on a reward mechanism, currently optimal computing resources are selected in different layers for processing data within eeDNN, effectively composing the deep neural network processing infrastructure on-demand. The future work will include the detailed study of optimal k and q parameters of the award function for various bidding strategies. Further interest will include a careful analysis of received results and the establishment of the trust mechanisms in the open, market environment where, in addition to agent’s processing capability, a trust parameter will be included in the negotiation protocols.  
  Agent-Based Medical Image Processing Using Multi-stage …  
  F. Balbo et al.  
  1 Introduction A challenge facing Industry 4.0 systems is to manage heterogeneous robots in a factory. It requires tools that are independent of the manufacturers for supervising and coordinating robots at runtime, especially when these entities have a high level of autonomy, as Autonomous Mobile Robots (AMR) [5]. Individual robots are designed to achieve their tasks, usually without exhibiting an explicit description of their behavior in different situations, i.e. what autonomous decision might it make corresponding to some event. To use them in different contexts and, even better, to improve the global system, it becomes necessary to model the way a given robot will act in a given environment. Designed by private manufacturers, these robots are often black-boxes and non intrusive solutions have to be found for achieving control. In this work, we propose a framework, called RoboTwin, consisting in a methodology and its tools combining Digital Twin (DT) and Artificial Intelligence (MultiAgent System (MAS) and Machine Learning (ML)) models and techniques for controlling a robot into an existing real system. We reproduce the action model of a robot as an autonomous agent behavior in order to control the robot. We aim to achieve this objective by applying existing machine learning techniques to predict the robot actions in the near future, given the logs of the actions performed in various situations collected by simulation of the robot digital twin. This paper is structured as follows: Sect. 2 overviews the background and related works; Sect. 3 presents our hypothesis; Sect. 4 details our methodology and tools; Sect. 5 illustrates our proposal with a proof of concept; Last section concludes and proposes research perspectives.  
  2 Background Our approach involves observing the behavior of a robot’s digital twin, learning its action model, abstracting this digital twin as an autonomous agent that adopts the learned action model, and using it to control the real robot. In this section, we provide an overview of the models and technologies used in RoboTwin.  
  various approaches can be used. For instance, if we have an offline dataset with a sequence of previous actions as input and a discrete last action output, the ML model used can consist of RNNs. If we acquire data online, we can use Reinforcement Learning techniques.  
  2.3 MultiAgent System (MAS) Prototyping and abstracting Multiagent Systems (MAS) paradigm can be a suitable approach to addressing the challenges of prototyping and abstracting Multi-Robot Systems (MRS). According to [8], the main differences between the techniques used in coordinating MAS and those used in MRS are actually very few when the coordination mechanism is explicit (e.g. coordination protocols). So far, a number of review, taxonomy and survey analysis for MRS and MAS coordination has been published, some examples are the works of [4, 13] for classifying similarly MAS and MRS according to for instance communication, cooperativeness or decision autonomy. However, it is important to note that there are challenges and limitations to using MAS in robotics that must be carefully considered. One of the main challenges is that existing MAS coordination approaches may not be sufficiently adapted to handle uncertainty, acquire information from the environment, and model the incompleteness of robotics [16]. Therefore, it is important to identify a common framework that can effectively address these challenges. In addition, coordinating different types of robots can present an interesting challenge that requires careful consideration of multiagent coordination mechanisms. In this work, we develop the role of the agent embedding the learned action model in the multiagent part and present its place and use within the framework.  
  3 Hypothesis We consider a robot R and a factory F, with their physical and digital twins. The physical twin of R is noted R pt (resp. F pt ) and Rdt is its digital twin (resp. Fdt ). In the following, we use R or F to refer to both dimensions (see Fig. 1).  
  F. Balbo et al.  
  4.1 Observation During the observation phase, we aim to generate logs that will be used in subsequent phases. We use three tools to manipulate scenarios (see Fig. 2): Builder. This tool is used to create a scenario, which sets the parameters for the Simulator and Logger. Simulator. This tool simulates the behavior of the robot R pt following the scenario. Rdt is connected to Simulator using the publish/subscribe protocol on the same topics as R pt . When Rdt makes a decision it publishes it. On reception, Simulator executes it, and publishes the new computed robot status with observations on the system (observation topic). Observations are related to all information that might influence Rd t decision. This last one being a black-box, we do not know what and how it is proceed but we know what are the input of the decision. Indeed to be included in the system, Rd t has subscribed to topics in the broker and has been connected to the factory information system. An observation is a collection of all the Rd t incoming information. Logger. This tool records data about the simulation by subscribing to the topics about R pt status (computed by the Simulator) and actions (computed by Rdt ) and obser vation. The result of this phase is the execution log, which is structured as a list of entries (CSV file format). Illustrative example. The specifications of the tools are: • Builder. Duration, robot locations (origin, destination), map definition, observation range are the simulation parameters. Observation range corresponds to the size of the discrete environment that is observed. This value may be different of the real size of the environment that is perceived by Rdt . In addition it supports the design of the related map (limits, obstacles (shape, location)). • Simulator. It updates the simulated robot location based on the movement published by Rdt and publishes the direction and location (status topic) and the content of the part of the environment that is in the observation range (obser vation topic). The simulation ends when R reaches its destination. • Logger. Figure 3 is a state of the system example that represents the 3r d row of Table 1. The x and y coordinates represent the location of the robot relative to the goal position, while orientation denotes the direction in which the robot is oriented. The neighborhood of the robot, where a cell (i, j) may have a numerical value vi, j defining the nature/identifier of an obstacle located at the position (i, j). If the cell is empty, it has a value of 0. Finally, decision denotes the action performed by the robot at time t.  
  RoboTwin: Combining Digital Twin and Artificial Intelligence …  
  stop  
  4.2 Learning During this phase, the R’s action model must be learned. To achieve this, we propose a tool called Learner, which aims to construct an action model by learning the pattern of taking an action for each specific context present in the log generated by the Logger. The choice of the learning model among learning models depends on the problem and is independent of the methodology. The output of this phase is a trained predictor (action model) that we define as a function that outputs an action (the predicted or most likely one) to be performed given a state (or a sequence of states). Illustrative example. The observable state of the system at time t is the input and we expect a movement as output. We have a limited, discrete output domain, which is the movement space. Therefore, the problem can be seen as a classification problem, where states are classified based on the movement they are expected to lead to. The logs of the robot can be seen as a labeled dataset to train algorithms to recognize specific patterns within the dataset and draw conclusions on how the states should be labeled or defined. Our problem can be seen as a supervised learning problem [3]. We can apply a feedforward machine learning algorithm, assuming that data points are independently and identically distributed. However, as an action performed at a step t may affect the state at every step t  > t, a data item can be dependent on those  
  22  
  that come before it. This type of information is also known as sequence data, and we therefore choose to apply LSTM. To construct the action model, we consider two spaces: the state space (location, orientation, neighborhood) and the movement space (decision) (see Table 1).  
  4.3 Integration In order to solve problems using the learned action model, it must be integrated into the loop between F pt and Fdt . The tool Decision, based on a MAS model, is responsible for anticipating potential problems and solving them through control of the robots. Decision subscribes to topics of the status of the robots and has the same level of information as Rdt , with access to the digital information in Fdt (Fig. 4). Each robot R is associated with an agent Ra that embeds the learned robot prediction model. Ra is able to anticipate the robot’s next states and make decisions accordingly. Decision for a problem may be used according to three automation levels: • without automation: Decision detects for the decision maker the future instances of the problem and supports the co-construction of the solution by computing the consequences of the modifications done on the system. The decision maker executes the solution. • semi-automation: Decision computes alternatives for solving the detected future problems, the decision maker chooses the best one and Decision executes it. • complete automation: Decision computes the best solutions to the detected future problems and executes it. The decision maker monitors the system. Illustrative example. Let us consider the without automation level. Decision presents to the decision maker the future locations of the agents computed using the learned action models. If a problem arises, the operator can add virtual obstacles on the map (Decision contains a copy of the Fdt map) to observe the modifications of the  
  RoboTwin: Combining Digital Twin and Artificial Intelligence …  
  A Method of Positioning a Humanoid Robot Relative to the Center of a Group of People—Analysis and Implementation Marko Kovaˇcevi´c and Zdenko Kovaˇci´c  
  Abstract Here we describe the mathematical model of the humanoid robot positioning strategy guided by the idea of adopting a new position with respect to the center of a group of people. We show that such a type of positioning changes for different numbers of people, where the cases studied include one, two, three, and four or more people in a group. We conclude the paper with the implementation of the group center positioning method on the humanoid robot Pepper and the tests under laboratory conditions showed the naturalness and effectiveness of the chosen positioning strategy.  
  can interact with humans as their companions, talking to them, instructing or guiding them while navigating in closed or open space [22–24]. From the human point of view, the robot’s navigation behaviour must look “natural”. The movements must be soft and adaptable, in a word “human-like” [25, 26]. Just as humans learn how to communicate/interact with other humans from an early age, the robot can do the same. In [27, 28] learning techniques such as reinforcement learning are used to achieve socially adaptive path planning for approaching humans. Adaptation of robot behaviour has become an important research topic in the field of human-robot interaction [29, 30]. The problem of approaching a group of people involved in a discussion or other type of interaction was treated in [31] as part of a mission consisting of entering a room, approaching the people, being in the group and leaving the group, and continuing to navigate the environment. The described approach technique aligns the robot’s direction of travel with the direction of the detected person. In case of approach of more than one person, the robot has been guided so that it faces the center of the group. This was achieved by defining the direction of travel of the robot by averaging the directions of travel of each individual person.  
  1.1 Contribution Although it seems that the positioning of the robot next to a group of humans during establishing contact is a well studied problem, we have found that it can still be addressed in a deeper elaborated analytical way. A chosen robot positioning strategy implies that the robot positions itself at the edge of the best-fitting circle of the group (with the center in a mean circle center point). Therefore, the main contribution of this paper is the mathematical formulation of this strategy that allows implementation on a real humanoid robot in such a way that the robot can naturally and comfortably initiate interaction with individuals or groups of humans. The mathematical study also considers proxemics theory to ensure that the robot stays within the social distance of the people: not so close as to be uncomfortable, and not so far as to impede communication. The recorded experiments compiled in the video supplement show the cases of effective robot positioning for a group of one/two/three/four people implemented on the humanoid robot Pepper within the Robot Operating System (ROS) environment.  
  1.2 Organization of the Paper The paper is structured as follows. First, we analyze the requirements that a robot must satisfy in order to position itself near humans. Then, we define the mathematical model of a chosen relative positioning strategy. Furthermore, we describe experimental results obtained on the humanoid robot Pepper. We conclude the paper with comments on the obtained results and present some ideas for future work.  
  A Method of Positioning a Humanoid Robot Relative to the Center …  
  29  
  2 Problem Definition We address the problem of relative positioning the robot with respect to the group of people {H1 , . . . , Hn }, while allowing other robots {R1 , . . . , Rn } to be there. Humans and robots are in open unconstrained two-dimensional space. The positions of the people are expressed in the local coordinate system of the robot in the form Ri [x HRij , y HRij , ϕ H j ], j = 1, . . . , n. Similarly, the position of the robot Ri is expressed Ri Ri as [x Ri , y Ri , ϕ Ri Ri ], i ∈ [1, m]. We assume that the position of the robot in space is determined. The following assumptions must be considered: • A group of people is standing in position and slightly moving their feet. An example might be a reception with drinks and snacks, where people stand in groups of somewhat circular shape or in the arc formation facing the center of the room. People interact with each other, talk to each other, move their heads, and interact with other members of the group. • The robot has access to accurate data [x HRij , y HRij , ϕ H j ] about the positions and directions of the human heads. All coordinates are in the local robot system whose origin is between the legs of the robot. • We consider here the case of a single robot that is positioning itself with respect to a human, i.e., m = 1. • The robot has unobstructed access to the group, i.e., the space consists only of the people in the group and the robot. • When interacting with humans, robots must respect people’s personal space and be generally mindful of their comfort [7]. Nowadays, perhaps the most acceptable approach is to respect the four proxemic zones [2, 3] shown in Table 1. • We assume that the persons and the robot have not met and are not yet familiar with each other up to this point. The method described in the paper determines the distance between robots and people based on the formation in which the people are standing. The robot mimics the human in the sense that it positions itself as close as possible to the group members, and the upper boundary of the personal zone is taken as the minimum distance. The designed method would have a wider scope if it allowed an additional degree of freedom, namely the distance to the persons. Therefore, the desired distance is implemented as parameter d ∗ .  
  Table 1 Proxemic zones defined by Hall [2] Zone Distance Intimate Personal Social Public  
  4 Conclusion In the field of robot-human interaction, there are a number of properties that a robot must have in order to achieve a successful interaction. This requires sufficient physical proximity to humans and awareness of the presence of humans in its vicinity. Physical proximity is achieved by positioning near humans, and the way in which a robot performs this is very important. In this work, we have performed the analysis of a strategy for positioning near the people with respect to the mean circle center of the group. For this strategy, we derived a mathematical model in a systematic way, based on the typical sensory apparatus of contemporary robots. We then used this mathematical model to validate the method in the simulation environment. We found that the mean circle center method is robust enough to be implemented on the Pepper humanoid robot. Although the Pepper robots have many advanced features that aid in the implementation of the positioning strategy presented in this paper, there is clearly much room for further improvement and development of new features. In particular, this highlights the need for a larger field of view and faster image processing and interpretation of dynamic changes due to unpredictable movements of people, occlusions, position changes, and much more. We plan to continue our work on improving localization and path planning algorithms. We also plan to improve the cognitive capabilities of the Pepper robots using state-of-the-art artificial intelligence and machine learning techniques.  
  K. Abe et al.  
  1 Introduction With the rapid development of AI and machines, various human jobs are being automated. Various studies have been conducted on whether AI and machines will replace human jobs, with both pessimistic and optimistic predictions [1–9]. In any case, working people now need to continue studying more than ever. Several issues must be considered when working people attend school. Employees may find it difficult to make time to study while working; for employers, filling vacancies and managing costs can be challenging. Taking Japan as an example, the Ministry of Health, Labor and Welfare has established an “education and training leave system” [10]. Under certain conditions, the government provides subsidies to companies that provide their employees paid leave for education and training. This program is noteworthy for employees who wish to acquire new skills. However, few companies have adopted the program [11], primarily owing to the difficulty in securing replacement personnel [12]. In this study, we propose an intercompany employee exchange method as a sociotechnological system to solve these problems and examined its effectiveness using simulations. Specifically, (1) employees in occupations highly substitutable for assistance by AI or machines are allowed to take long-term paid leave to attend school, and (2) a company with vacancies caused by long-term paid leave temporarily replaces members of its workforce through an intercompany employee exchange. For (1), we verified the effectiveness of this approach by comparing the number of students who complete their studies with and without introducing the company employee exchange program. Regarding (2), we examine the effectiveness of the employee replacement program by comparing the number of employees who completed the study period with the number of employees who did not. Several researchers have argued that AI, machines, and other technologies may replace human workers, even though their estimates differ. While some studies predict that a certain number of replacements will occur [1–5], others predict that replacements will occur owing to factors other than the introduction of AI and machines [6, 7]. Certain studies have also pointed to the importance of reshaping the competencies of employees who may be replaced by AI [8], while others have noted that AI will allow them to focus on strategic tasks [9]. In other words, regardless of whether AI and machines are implemented to perform human jobs, humans must continue to learn. Many previous studies have been conducted on learning and associated difficulties for working people [13–16] as well as on staffing [17–25], in particular, [19– 25] discuss proposed algorithms for staffing, while [21–25] validated the developed algorithms in a software or a simulator. However, none of these studies discuss how working adults can find time to study or how to fill in for employees who have taken time off to attend school. In addition, as pointed out in [23–26], tasks and resources can be distributed in either a centralized or decentralized fashion. In the centralized approach, a control center is responsible for task and resource distribution for each agent. In contrast,  
  How Can Adults Make Time to Study: A System for Employee Sharing …  
  in the decentralized approach, each agent acts cooperatively or independently to achieve an optimal task and resource distribution. In this study, we adopt a centralized approach because the proposed system is assumed to be a government project, in which the government coordinates the requirements of the various companies. Furthermore, as this project involves government subsidies, it is more realistic to assume that would have to be under government control. The remainder of this study is organized as follows. In Sect. 2, we explain the proposed employee exchange system using virtual tokens to allow for educational leave. In Sect. 3, we describe the configuration of the experimental simulation. In Sect. 4, we detail the method used to match companies that need to borrow employees and those that can lend them. In Sect. 5, completion of an educational leave is defined. In Sect. 6, the results of the experiment are discussed. In Sect. 7, we provide some concluding remarks and suggest some possible directions for future research.  
  2 Token/Employee Exchange System to Allow Study Time In this study, we propose a method to fill in for employees who take full-year paid leave to attend school. Specifically, we propose an intercompany employee exchange system based on virtual token exchange. We evaluated the effectiveness of the proposed approach in a simulation. This system is inspired by the work of Saito et al. [23], which assumes that the number of employees of all participating companies does not increase or decrease. We assume a certain number of vacancies due to employees attending school each year.  
  2.1 System Overview The purpose of this system is to provide several employees with paid time off for attending school each year. Furthermore, this system is designed to ensure the availability of labor through an intercompany employee exchange. The government functions as the administrator of the system and manages it on an annual basis. Each company provides paid study leave to all employees (candidate workers) whose jobs may be replaced by AI or machines in the near future. Upon completion of the study, employees return to their original company and begin working again, which means that each company participating would have several vacancies each year. To fill these vacancies, companies send employees to each other during busy and slow periods.  
  6.2 Effectiveness of Business Execution Figure 2a–c) each shows the daily workload over the 5 years. One of the ten simulations is shown as an example; the other nine showed similar trends. In Fig. 2c, the workload remains flat because there is no excess or shortage of employees. Similarly, Fig. 2a shows a relatively flat trend. This result indicates that no extreme workload carryover occurred. In contrast, Fig. 2b shows the result when there were vacancies that were not filled by employee exchanges. It may be observed that the workload accumulates until the number of candidate workers whose motivation value exceeds the threshold is reduced. The results show that our proposed token/employee exchange system allows workers to perform their daily tasks properly.  
  How Can Adults Make Time to Study: A System for Employee Sharing …  
  Data Discretization for Data Stream Mining Anis Cherfi and Kaouther Nouira  
  Abstract Data discretization has become essential task to cope with streaming data. However, this field still requires particular attention. Indeed, various constraints should be taken into consideration to cope with discretization in data stream scenarios. In this paper, we discuss the discretization process for data stream scenarios. In this setting, we examine the proposed algorithms in the literature, their limitations and the new challenges. Also, we propose a novel discretization method so called Hybrid Online Discretization (HOD). Based on the well-known Minimum Description Length Principle (MDLP) discretization combined with the Online ChiMerge Algorithm, the proposed method guarantees an efficient and effective discretization. Empirical trials show that in most cases the proposed algorithm outperforms its competitors in terms of accuracy level and reduction rate. Finally, we experimentally demonstrate that coupled with HOD, Naive Bayes classifier shows high classification accuracy.  
  A. Cherfi and K. Nouira  
  information bias and maximize the data quality which improve the performances and effectiveness of data mining algorithms [5–7]. Moreover, discretization is used to reduce irrelevant, noisy and redundant instances. Thus, redundant values are combined in an interval. Discretization corrects inconsistencies in the data. By minimizing the number of intervals, discretization reduces the amount of data with high noise content which contains conflicting instances. The benefits of feature discretization include a reduction in inconsistency level, discarding the noisy, irrelevant and redundant information present in data, while improving the generalization power of data [8–10]. Reducing the cardinality of the attribute improve both data quality and complexity. Even for most of data mining algorithms that work directly with numerical attributes, discretization as a preprocessing task can improve the model performances. The achievement of algorithms belonging to the set of statistical learning and those that use information measures ( i.e., Support Vector Machine (SVM), K-nearest Neighbor (KNN)...) is dramatically affected by attribute discretization. Good quality solutions and higher performances are observed in the results obtained on discretized data [11–14]. Many researchers prove that discretization improves the performances of data mining algorithms since it gives a learning algorithm a smaller space to reason about, with more examples in each part of the space [15, 16]. Also discretization performs features selection by mapping the feature values into a single interval [3, 17]. Nowadays, it is surprising that discretization methods for data streams remain fairly unexplored and have received little attention [7, 18]. The main reason may be that many constraints should be taken into consideration while discretizing data to guarantee positive results. First of all, handling the appearance of concept drifts is one of the major keys for discretization success. That means that at some point the relation between the input and the target variable or the input data statistical characteristics may change [19]. The nature of data streams is based on the continuous change of attribute values over time. Therefore, in discretization process, the number of intervals and the definition of boundary points may change as the stream progress. Thus, results follow shifts in data characteristics which results to discretized values whose meaning changes over time. While some authors argue that this can bias users against using discretization. Supported by [18], we agree that change in data values is one of the basics meanings of data streams. Therefore, to maintain the relevant meaning of input data, discretization methods might smoothly adapt intervals and so change values meaning. Data streams discretization in the literature requires various level of refinement where sophisticated iterations are performed using several data structure to smoothly adapt the discretization to concept drift. Here we outline that one of the main challenges is to improve intervals and adapt discretization scheme quality without involving increased computational cost [7]. Moreover, performances of classification and prediction models are directly influenced by the number of intervals produced by discretization methods. Discretization methods reduce quantitative data space considering a tradeoff between minimizing the number of intervals and maximizing data consistency. Also, by reducing the number of intervals, discretization methods can eliminate some irrelevant attributes. It performs feature selection by discretizing a continuous attribute into one interval.  
  Data Discretization for Data Stream Mining  
  This major advantage in traditional discretization methods presents a new challenge in data stream scenarios. Actually, the proposed discretization methods for data streams require a predefined number of intervals which omit the possibility of eliminating irrelevant attributes. In the next section we discuss with more details the advantages and challenges of discretizers for data streams from a theoretical and practical point of view. This paper is organized as follows: The related and advanced works on discretization for data streams are provided in Sect. 2. In Sect. 3, we extend more details about the new discretization algorithms we implement. Section 4 presents experiment evaluation of the algorithm in real-world problem and we discuss the results. Finally Last section concludes the paper and presents some future research lines.  
  2 Related Works There are rare examples of discretizers were designed specifically to operate on data streams environments for machine learning purposes. This is explained by the fact that many requirements must be taken into consideration while designing a discretization method for data stream scenarios. Table 1 lists the most relevant discretization methods for data streams. Partition Incremental Discretization algorithm (PiD) is one of the first attempts to deal with data discretization for streaming environments [20]. According to [21, 22], equal-frequency can be used as an alternative strategy to improve this discretizer. Whereas Incremental Flexible Frequency Discretization (IFFD) requires a random ordered streaming records arriving, which is impossible in many learning problems [7]. When it comes to Optimal Flexible Frequency Discretization (OFFD) [23] , authors report that is not optimal in all the cases [24]. In the case of Online ChiMerge based algorithm (OCM) [25], it does not require a user-provided number of intervals to initialize the discretization process, yet using several data structures may prevent OCM algorithm to be used in limited computational resources problems [7, 25]. According to a recent study, Incremental Discretization Algorithm (IDA) is the agilest and the most effective discretizer [7]. To conclude, in this paper we propose a supervised discretization algorithm based on OCM discretizer. As discussed previously, OCM is the only supervised algorithm proposed in the literature. Based on critics summarized in this section, our objective is to avoid user defined parameters related to number of intervals, and to propose a data driven discretization algorithm. We aim to improve the trade-off between accuracy level and reduction rate. The proposed algorithm is discussed with more details in the next section.  
  Fig. 2 Intervals update process  
  Definition 1 A value of the random examples is defined by the attribute value Vi and a class value Ci , belong to I j , where I j is an interval defined by the majority class C j and two Cut Points (CP) C P j and C P j+1 . Vi is selected as a CCP for interval I j if C P j ≤ Vi < C P j+1 and Ci = C j . Theorem 1 If Vi belongs to the minority class of an interval, then Vi is a CCP. Proof As described in Fig. 2, assume we have an interval I2 with a majority class C2 and two born B1 and B2 . The information entropy Ent(I2 ) is minimal. If a new value Vi with a class value Ci belong to the interval I2 (B1 ≤ Vi < B2 ) and Ci = C2 then the information entropy Ent(I2 ) will remain minimal if we insert the new value in I2 . Else, if Ci = C j then adding the new value in interval I2 will increase the information entropy Ent (I2 ). Thus a binary discretization for I2 defined by the new value Vi may reduce the interval impurity and information entropy. If the new arriving value is classified in the right interval and has the same label as this interval, then, no changes will be performed. Else the arriving value will be selected as a CCP. The second layer starts by initializing a balanced binary search tree using results from MDLP discretizer and the discretization scheme is updated only periodically. The algorithm constructs the initial intervals set which depends on the data distribution. Then, the algorithm performs merging process based on initialized statistics and the set of intervals from layer 1. Interval merging is performed Algorithm 1 Layer 1(n, CC P, I nter vals) 1: Inputs: n: the number of instances to initialize the model; CC P: the candidate cut point list; I nter vals: the curent list of intervals; 2: Outputs: Layer : the index of the current Layer; CC P: the updated candidate cut point list; 3: While E ← DataSour ce() = null do 4: if (count < n) then Data ← E; 5: if (count == n) then 6: CC P ← M DL P(Data); 7: Layer ← 2; 8: end 9: if (count > n) then 10: r es ← I nser t (E, I nter vals); 11: if Not (r es) then 12: Layer ← 2; 13: CC P ← E; 14: end 15: end 16: count ← count + 1; 17: end  
  explain the results obtained in Fig. 3a,b. IDA discretizer always performs better than others. Its results are pretty close to those obtained by NB. While detailed results show that memory usage of HOD algorithm is too far than OC and PiD. Also, for CPU usage, HOD is more competitive, it performs faster than OC and PiD, and slower than IDA and the base solution, but it still acceptable.  
  6 Conclusion In this paper we have summarized the key issues that surround data discretization for data stream scenarios. Most proposed algorithms require various level of refinement and sophisticated iterations to perform discretization. The high number of parameters that should be tuned to guarantee better discretization scheme represent the major disadvantage of proposed discretization algorithms. The new algorithm proposed in this paper, aims to detect the number of intervals based on MDLP criterion. HOD compute the initial number of intervals and performs discretization based on MDLP discretizer and OCM algorithm. Results show that NB classifier combined with HOD discretizer generates efficient and effective models. In many cases, HOD outperforms his competitors in both accuracy level and reduction rate. We conducted our experiments using NB as a base solution. And we have shown that HOD algorithm improves the results of this classifier. Future research could explore the benefits of the proposed discretization algorithm in contexts of decision trees and clustering for data stream scenarios.  
  Data Discretization for Data Stream Mining  
  (1)  
  Equation 1 is closely related to the GLM, which function mathematically as a weighted sum of features, where the mean of the distribution is assumed using the link function, which can be chosen flexibly depending on the type of outcome. The formula for the GLM can be written as shown in 2, where α0 , α1 , etc. are the weighted parameters, x1 , x2 , etc. are the input variables, and g −1 is the inverse of the link function. The link function g may be different for different types of GLMs, e.g., the logit function (logistic regression) function for logistic regression, the identity function for linear regression, or the log function for Poisson regression. The inverse of the link function is the inverse of the function, that is, the function that cancels  
  Linear Machine Learning Algorithm for Early Annual …  
  Multi-agent Modal Logic Evaluating Implicit Information Vladimir V. Rybakov  
  Abstract This paper studies relational Kripke models for computation truth values in some extended logical language. The multi-agent modal language is extended by introduction a new modal operation D(α, β). The truth of this operation is determined in a way by implicit information: it is true only if the number of states where the formula α is true is strictly less than the number of states where the formula β is true. From mathematical point of view, the paper investigates satisfiability problem for formulas in such language, we construct a mathematical algorithm verifying satisfiability the formulas.  
  2 Denotation, Definitions, Preliminary Facts We first recall necessary for reading the paper definitions and facts concerning modal multi-agent logics. Following modern trends by a logic we understand the set of all theorems provable in a given axiomatic system, or the set of valid formulas for a certain class of Kripke frames. In particular, a normal modal logic λ is a set of modal formulas which is closed under substitution, modus ponens {α, α → β/β} and necessitation rule {α /α}, and contains all theorems of the minimal propositional modal logic K .  
  Multi-agent Modal Logic Evaluating Implicit Information  
  V. V. Rybakov  
  or (it might be) F F P = [ci , ci1 ], ≤ or (even - for infinite time interval) F F P = [ci , ∞], ≤ , where ≤ is the standard linear order relation on natural numbers. The language of our logic is the standard language of modal propositional logic extended by a binary logical operation D(x, y), which may be applied to any (arbitrary) formulas—cf.— D(α, β). The valuations V of a set Pr op of propositional variables in frames F F P from  FP ∈ Pr op, V ( p) ⊆ i [ci , ci1 ]. models M defined as follows, ∀ p  The basis of any such model is i [ci , ci1 ]—the set of natural numbers N . If a ∈ M F P , and a ∈ V ( p) then write (M F P , a) |=V p and we say that p is true on state (element) a. Since we are interested in multi-agent logics, we will keep in the language several modal operations ♦1 , . . . , ♦n and accept several different valuations V1 , . . . , Vn of letters—each one for each supposed agent. This one will give an unusual and big distinction from standard approach and makes problems and considerations interesting. So we switch indexes for modal operations and valuations. The essence of this distinction in immediately visible in the definition of truth valuers for formulas below. Valuations of variables Pr op may be extended to formulas as follows: • Truth of Boolean connectives ¬, ∧, ∨, → is defined in the standard way; • (M F P , x) |=V j i α ⇐⇒ ∃i ∈ N : x ∈ [ci , ci+1 ] =⇒ ∀ y ∈ [ci , ci+1 ] (x ≤ y ≤ ci+1 =⇒ y |=Vi α); • (M F P , x) |=V j ♦i α ⇐⇒ ∃i ∈ N : x ∈ [ci , ci+1 ] =⇒ (∃y ∈ [ci , ci+1 ] (x ≤ y ≤ ci+1 & (M F P , y) |=Vi α)); • (M F P , x) |=V j D(α, β) ⇐⇒ ∃i ∈ N : x ∈ [ci , ci+1 ] and the number of states on the segment [ci , ci+1 ] in which the formula α is true w.r.t. valuation V j , is strictly less then the number of states of this segment at which the formula β is true. The binary logical operation D(α, β) will also be used in an unusual way. We may interpret it as an expert assessments; that is a sort of comparison statements in an implicit situation—when precise amount of states where the statements are true is not known in precise numerical value. The introduced constraint on the ♦ modal operator causes, leads, for example, to the implementation on the introduced M F P models of the following properties’: (1)  p ∧ ♦♦¬ p, and as consequence  p ∧ ♦k ¬ p, k > 1. (2) ¬ p ∧ ♦ p. (3) D(α, β) ∧ ♦¬D(α, β).  
  Multi-agent Modal Logic Evaluating Implicit Information  
  We also admit a big restriction in use of the language. In what follows we will consider only formulas where the operation D(α, β) may occur only once and not may occur inside subformulas of any modal operation as an internal logical operation. The reason for that is a technical one (but important) which we will comment after all main proofs. Definition 3 The logic L F P is the set of all such special formulas that are true on all multi-modal models M F P defined above.  
  3 Formulation of Main Results, Satisfiability Problem For some class of models K, a formula α is said to be satisfiable (in a given class K) if at some element of some model from this class this formula is true. In our case for logics L F P the class of models is given by the set of models of the form M F P . Thus, a formula is satisfiable in logic L F P if there exists a specific model M F P with some valuation V , such that, for some element x ∈ M F P of this model, the statement (M F P , x) |=V φ holds. Satisfiability is directly related to decidability: the logic is decidable if there is an algorithm that verifies, for any given formula φ, whether φ ∈ L F P is true. It is clear that φ ∈ L F P if and only if the formula ¬φ is not satisfiable. And vice versa, φ is satisfiable if and only if ¬φ ∈ / LFP. Thus, it is necessary to find an algorithm that checks the satisfiability of formulas at such finite models, where the size of models is limited by the value of some computable function from the length of formulas φ. In this our paper we use some similar ideas as we used in our recently submitted paper. But in those paper we considered only simply modal (single modal) logic but not an multi-agent logic. This makes very essential difference in proofs and makes new obtained results appreciably more strong. So let we directly start from Theorem 1 A formula f is satisfiable in a model introduced earlier (cf. M F P =   i∈I n [ci , ci1 ]) iff it is satisfiable in a special model based at a frame of kind F F P in its interval [c1 , c2 ] size of which is computable from the size of f . Proof Here as in those mentioned paper, we consider a modal formula f with propositional letters Pr op( f ) = { p1 , p2 , . . . , pk }. Assume that this formula be satisfiable at some element x of the model M F P , e.g. (M F P , x) |=V f . We need to show that in this case the formula f also holds at some finite model MM c , whose structure and size to be determined later in the course of the proof. Without loose of the generality, we can assume that x ∈ [1, c1 ]. We denote by Sub( f ) the set of all subformulas of the formula f , so Sub( f ) is closed w.r.t subformulas. Let S R := {A1 , A2 , . . . Ak } ⊆ Sub( f )} the set of all sets of all subformulas of the formula f , which are true at some elements of the interval [1, c1 ]. That is, for each subset Ai , all formulas of Ai are true on some element xi of [1, c1 ], and for any subformula of f if it is true at this element xi then it belongs to  
  V. V. Rybakov  
  Ai . If the initial model is finite we act as earlier in our mentioned paper, so for each such subset Ai , we choose and fix the ≤–maximal element xi with this property from the given segment [1; c1 ]. Let’s arrange all these elements xi in ≤-increasing order: ti0 < ti1 < · · · < tik . It is clear that the number of such elements is not greater than 2|Sub( f )| . Let M1 be the set of all shown states ti0 < ti1 < · · · < tik . Since that the proof may follow as before for single-agent case. But if the initial model is infinite the proof will be very different. Here is the main branching point, we need to consider stabilization points—states after which the appropriate sets Ai , will appear infinitely many times, and as soon as it appears a time, it will happen in future infinitely many times. After that we will need to collapse line on states after stabilization point in an appropriate loop. After what we do a proof for the loop part proceeding the all loops. The possible size of this our paper does not allow to include more useful details or to give all complete proof, but the main idea is to concentrate on stabilization states and to recover the appearing loop problem . Using this theorem, as a consequence we obtain Theorem 2 The problem of satisfiability for formulas in the logic L F P is decidable.  
  S. Gorecki et al.  
  The SIMUTEC platform can simulate a variety of sectoral policies, including the pricing for transportation, new transportation infrastructure, renovating housing stock for energy efficiency, establishing a green belt, as well as scenarios of rising energy prices and population growth in the area under consideration. Moreover, SIMUTEC can analyze the energy and socio-spatial issues of the proposed policy as well as the constrained household expenses (home-work transportation, housing, etc.) for each of the simulated policies. In this article we will focus on two laws under discussion at both national and European levels, and propose modifications to standard LUTI model in order to apply them. • Low Emission Zone (LEZ). A LEZ is a territory in which an access ban has been established for certain categories of polluting vehicles which do not meet some emission standards. The main objective of the LEZ is to improve air quality and life quality in a concerned zone. This law also aims at climate action: accelerate vehicle fleet renewal. To circulate in a LEZ, an “air quality” certificate is mandatory and the most polluting vehicles (identified by a “Crit’Air” stickers 5, 4, and 3) will be prohibited from circulation. • Withdrawal Energy-intensive Housing (WEH). As part of the “Climate and Resilience” law, high energy consumption housing, called “thermal sieves”, has been prohibited for rental since January 1, 2023. The objective of this law is twofold: protect tenants against too high energy bills; reduce greenhouse gas emissions. In this article, we will first present a brief state of the art of LUTI (Land Use—Transport Interaction) simulation tool. In a second time, we will present the SIMUTEC agent-based platform in both transport part and housing location parts. This will be done in order to demonstrate what extension to LUTI model can be made on both sides to integrate new tested scenarios. Then, we will illustrate our proposition by studying a simulation ok one of the two European laws before ending with a brief conclusion.  
  2 State of Art Urban planners are aware of the intimate relationship between land use and transportation. The fundamental tenet of transportation analysis and forecasting is that human activities are spatially separated, which implies the necessity for transportation of people. Hence the need to design and use modeling and simulation tools that consider the interactions between citizens’ transportation and their residential choices: LUTI models. In some cases, LUTI are just mathematical models, in other cases, these models are executable and can be simulated. Thus, we can observe several uses or models, or implementations of LUTI tools in the state of the art.  
  (1)  
  The outcome of a calculation performed for each agent is Utility (Uh,d,z ) [10]. This function’s goal is to depict the “performance” of a workplace or the “wellbeing” of a household, or profitability of a workplace. The utility is measured in monetary terms (e) and is composed of (1) accessibility to employment (liked with the transport model), (2) notoriety of an area, due to its proximity to shops and services, (3) desired surface of the current accommodation, (4) Energy bill of the dwelling (directly related to the energy class of housing in Fig. 2) and (5) Price per square meter of the zone. Each agent (household or workplace) in the simulation has a single primary goal: to maximize his own Utility value, which depends on several parameters: h for a certain Housing, d, for a Desired surface of housing, and z, because this housing is located in a Zone. These parameters introduce the notion of housing as a full-fledged agent in the simulation which is the main proposition of this chapter. This contribution allows locating households and workplaces agents in housing and premises agents. In the same time, it allows us to attach energetic data to buildings. Based on open national statistical data (INSEE [11]), the statistical thermal capacity of the dwellings, as well as the age of the dwellings are added to the data of the new agent population. Moreover, statistics on the size of households can also be added to the simulation.  
  Multisectoral Household Location Agent-Based Simulation for Testing …  
  (2)  
  In this step, a Cost is calculated for both Private Vehicle travels (C V P ) and Public Vehicle travels (C T C ). These costs are injected in the above (2) formula, and allow to split flows from the F M M matrix into two separate flow matrixes: Private Vehicle flow matrix (F V P ) and Public Vehicle flow matrix (F T C ). Once flows are generated, there are simulated on real roads network in step (4): congestion simulation, traffic, and travel time along the real roads extracted from OpenStreetMap database [13]. Step (5) update the cost of travels of private and public vehicles travels. These 3 steps (3, 4, and 5) are repeated until each simulated agent has determined its optimal means of transport. The final step (6) converts costs and flow matrixes into an accessibility value, injected in each zone of the land-use model. Our proposition in this part is to integrate the management of the Low Emission Zone (LEZ) in the transport model. As a reminder, a LEZ is à zone where an access ban has been established for certain categories of polluting vehicles. Thus, we can list 3 situations that will be affected by this measure: • people who live and work in the LEZ zone (internal traffic) • people who live in the LEZ and work outside (exchange traffic)  
  Multisectoral Household Location Agent-Based Simulation for Testing …  
  DaFne: Data Fusion Generator and Synthetic Data Generation for Cities Ayse Glass, Kübra Tokuç, Jörg Rainer Noennig, Ulrike Steffens, and Burak Bek  
  Abstract In the planning of smart cities, machine learning models can support decision-making with intelligent insights. But what data sets should training processes be based on if there is not yet a city from which to collect data, or if data is not usable due to privacy issues? Synthetic data can provide a realistic representation of conditions in the city not only for machine learning experts, but also for smart city experts. For data-affine users, there are machine learning-based methods for generating synthetic data, but these have limited accessibility to data amateurs. The Platform Data Fusion Generator (DaFne) project aims to improve the usability of data generation methods for various professions. The platform with its generic functionalities should appeal to users from all domains. This paper refers to the research on how smart-city use cases can be addressed and how complex machine learning based methods can be made accessible through the platform to urban professions. Based on results from user interviews and experiments of a smart city case study, the need for a non-generic platform feature emerges. The Use Case Explorer feature provides users with a simple interface to query pre-trained machine learning models to generate data for a specific use case.  
  A. Glass et al.  
  1 Introduction Data-driven cities are growing with data challenges. The main challenges are; security and privacy [1–5], data quality [6], accuracy and representation [3], accessibility [7, 8], validity [9, 10], scalability [8] and accuracy of representations [3], management [11] and costs. Due to challenges, when the real data is not available, not in suitable form or limited, the generation of synthetic data is a promising approach. Modelling, analysing or simulating without having the mentioned challenges has an importance for the development of the design, research and application of data-related urban design projects. Synthetic data, also known as simulated data, enables the rapid prototyping of services without real data and it is researched and applied to many scientific fields [4, 5, 8, 12]. This research aims to improve the usability of synthetic data generation with a focus on the smart cities. The Platform Data Fusion Generator (DaFne) is a multidisciplinary research project that targets the domains of artificial intelligence and smart cities. It aims to integrate different approaches for data generation on a digital platform in the form of services [13]. In addition to the infrastructural provision of services, one goal is to improve the usability of these methods through a welldesigned user interface. The functionalities can be divided into generic methods and use-case-specific functions. The generic functionalities are divided into data-related, generation-related and evaluation-related services. Synthetic data can support the development and operation of smart cities, by providing data that can be used for simulations and tests without having privacy issues [4, 8, 14]. That can assist the decision-making process and improve the smart city operations. For example, for optimizing energy consumption or improving transportation, health care, light and mobility, for making predictions about urban civil protection, hazard prediction, green space irrigation [12, 15–17]. This allows city officials, planners and researchers to identify potential issues and improve the sustainability, efficiency, and livability of urban environments. DaFne includes different generic data generation techniques as outlined in Kunert’s work [13]. One example of a use-case specific DaFne functionality is the generation of synthetic citizen movement data by utilizing an agent-based simulation of pedestrian paths. This paper takes a closer look at underscoring the need for a use-case specific approach for the smart city context besides the generic techniques.  
  2 Methodology This paper is investigating the effectiveness of the generic platform functionalities in addressing the problems faced by urban experts. To achieve this, an experimental research methodology is employed, which is supported by user interviews. The methodology consists of several stages. Firstly, the general platform functionalities are introduced, which have been pre-set by prior research. Secondly, the platform is  
  DaFne: Data Fusion Generator and Synthetic Data Generation . . .  
  characterized as a product at the layers of AIaaS. Next, a user research is conducted to understand the context of use and the problem space of the users. Ultimately, the objective of the methodology is to evaluate the ability of the platform to address the needs and requirements of urban experts, and to provide insights into potential areas for improvement. The user research was also used to create a solution space in the form of a user interface design, which is not in the scope of this paper. In order to find users, urban planners from Hamburg were contacted directly. To also find interested parties independent of domains, the project was presented at different online and offline events and exhibitions related to artificial intelligence and architecture. The presumption of the user types—(1) Domain experts for smart cities, (2) data scientists, (3) data engineers, (4) developers of generative ML models, (5) developers of evaluation methods [13]—did not represent an exclusion of other interested parties. Thus, 4 people were interviewed: Two of them are urban planner and designer, one product manager for a Data Analytics SaaS and a PhD student researching data literacy in academia. The user research helps to identify the context of use and derive user requirements on the platform. The context of use analysis was carried out based on the master-student model by conducting informal contextual interviews with the recruited users [18]. The user was treated as a master by the interviewer, who is the student, because the interviewer wants to understand in detail the user’s goals regarding synthetic data and the problems related to data in smart cities. This resulted in an output of qualitative information, which was then transformed into personas. A persona is a fictional character and detailed user model that represent archetypal users and helps to identify different user needs [19]. The interviews revealed important pain points and user needs concerning synthetic data, which are briefly summarized in Sect. 4. The following case study of the agent-based pedestrian movement simulation in Sect. 6 then compares the generic platform functionality with a use-case specific approach by presenting the experiment setups and their output. In order to understand the experiment setup, the logic behind the use case development is examined in advance in Sect. 5. Finally, the paper concludes with a summary of the conducted work and gained insights followed by further work.  
  4 User Research Through the interviews, four personas were created. Two of them are specifically relevant to the smart city domain. User 1 is an urban planner without programming skills. He can use GIS systems and Excel at an intermediate level. He is working more on strategic aspects of urban planning in inter-governmental structures, aiming to develop ideas and strategies to increase quality of life in cities through the use of technology. This is mainly done by up-scaling already existing approaches and use cases to other places, especially in the global south. One of the biggest obstacles is the lack of citizen and movement data, as collecting it requires technological know-how from stakeholders in the respective areas. According to the user, this is a particularly big problem in the global south, where surveying and infrastructure deficiencies are more prevalent than in more developed countries. For example, projects are underway to rebuild war-damaged cities in Ukraine, where participatory data collection would provide a basis for decision-making in form of citizen mobility data. The absence of the citizens themselves is the biggest obstacle in this case. An explicit user gain would be way to access, share and extend algorithms and use cases for generating urban data, especially in abandoned areas such as war zones or informal settlements. As an example, he cited a project in South Africa in which roofs in informal settlements were identified through image recognition, since there are no maps of these regions. Extending the algorithm to include other urban elements such as streets or parks could be a great help. User 2 is an urban planner and designer working on the data collection for ML models in smart city research. The data is provided to developers of ML models to gain new insights for decision making, e.g., social service planning. She can use advanced Excel and GIS, has a very good understanding of data structures and statistical methods, and intermediate Python scripting skills. Her goal is to open new perspectives for urban planning by combining different data sets such as geospatial data and socioeconomic data. The work consists of finding different interesting and useful data sets to draw inspiration from in order to expand the scope of the research. When data is available, other problems arise that are related to the quality and usability of urban data portals. The bad interoperability and comparability of these different data sets (due to differences in e.g., quantity, units, attributes, scales, etc.) is a major barrier and often leads to loss of value of the information.  
  5 Use Case Design Smart cities encounter various challenges in utilizing synthetic data, which necessitate addressing the quality and validity of the produced data, integrating it with real-world data, and optimizing the generation process for efficiency. To solve these challenges, algorithms and models leverage machine learning and data mining tech-  
  6 Experiment: Pedestrian Path Generation Urban designers can consider agent-based models as an option, even if there is no learning function involved. An experiment was conducted for DaFne, where a synthetic data set from “Grasbrook CityScope” [22] simulation tool was used to determine if generic data reproduction methods would be suitable for the designer’s specific needs. Can insights from a simulated model of Hamburg be applicable to Berlin? In cases where data is unavailable, how can existing data sets be leveraged through the DaFne platform? However, this initial experiment was unsuccessful due to various data issues, including inconsistent location points, missing values, and an unbalanced distribution of pedestrians [12]. Thus, obtaining a new data set without modelling a new area may be necessary, and a new simulation tool is required to apply the agent-based modelling approach to different cities or neighbourhoods. Therefore, path generator agent base modelling tool is designed and developed to learn about citizen behavior in urban environments to allow urban designers and decision makers to identify potential issues and take actions to improve pedestrian safety, mobility, and accessibility. The deep reinforcement learning used to solve simulation issues to understand trends on the generated paths by artificial intelligence. First prototype and the reward system to understand behavior was explained in KES conference in Verona [12]. After that, we used a flow of possible game play to model the behavior of citizens in the real world and implemented it in Python with using real data set. The agent (citizen) is placed at a randomly defined point. The movements of the agent are defined programmatic. The novel approach here is to enhance a simple randomized walk through the simulated environment by the agent which can learn. The agent is in the simulation several times and improves its life by adjusting the flow. The outcome of actions is measured by a global score, judging the behavior. For example, actions that make an actor unhappy, unsafe, e.g. a situation with long hours at the office stuck in a loop with home, will leave a minuscule score, to be heightened by walks in the park, visits to the bar etc. The agent optimizes the score not to be achieved by an exhausting trial of all possibilities, but will use an artificial intelligence algorithms e.g. gradient descent. While the agent uses the run time program each step is recorded in an output file which will be used for further iterations. The first prototype can be played by the human agents too and seen in the presented paper [12]. For the second prototype the real data extracted from HafenCity Hamburg. Duration on the road, time spent on the location and types of locations are rated by the score system. Q-learning algorithm was added to the implementation and the tests were repeated. In a later stage a longer time will be taking to generate a robust model. The results shows that the agent can learn and deep reinforcement approach is a promising method in this use case to simulate complex behavior of the pedestrians without having complex modelling time or extreme computational power. Happiness score will be validated by scientific literature, real data from survey and interviews with the citizens living in the area. The algorithm currently runs and the trends from the outcomes will be compared with the citizen interviews.  
  and Roman Šperka  
  Abstract Business process simulations (BPS) are considered a relevant and highly applicable method of analysis. BPS allow analysis of business processes under different conditions. There are several approaches towards simulation of business processes such as discrete event simulations (DES), agent-based modelling and simulation (ABMS) or system dynamics (SD). This research focuses on ABMS approach towards simulation of business processes and its importance for simulation of costs using time-driven activity-based costing (TDABC). Using ABMS approach, active elements of the system are represented by software agents which are programmed to follow some behavioral rules and autonomously interact with each other. This is especially important for TADBC approach to simulation of costs as simulation of resources has a direct impact on activity durations which drives the costs in TDABC approach. We present a case study, where is shown, how important is a process flow and organizational perspective for simulation of process costs using TDABC approach. The required detail of business process simulation can be achieved through the use of ABMS approach. After that, we discuss other shortcomings of traditional business process simulation techniques which might have a significant impact on allocation of costs using TDABC and its simulations.  
  M. Halaška and R. Šperka  
  [2], direct costs are easily linked to cost objects such as direct labor, direct expenses, equipment rental, etc. Meanwhile, indirect costs like office equipment, maintenance, utilities, etc. cannot be easily traced to cost objects. To determine the costs of each cost unit, it is crucial to allocate the total costs incurred to each individual unit. With changes in the cost structure of organizations, including an increase in overhead costs and the adoption of different technologies [3], it is necessary to examine the processes and activities involved, cost factors, and process costs through analysis of business processes at an operational level. This is especially important in today’s highly competitive markets, where organizations must utilize every advantage to succeed. It is widely acknowledged that organizations relying on older functional paradigms struggle to compete in current market conditions [4]. Contemporary cost accounting systems should be equipped to calculate various cost components, possess adaptability, and take into account the diversity and intricacy of business processes. Business process management aims to improve business processes by employing a range of techniques, such as statistical analysis, mathematical methods, queueing theory, and optimization, to meet established key performance indicators (KPIs) at the operational, tactical, and strategic levels of management. Business process simulation is a tool used to model and analyze the performance of a business process. It helps organizations understand how changes to the process, such as changes to the control flow, data flow, or organizational structure, will impact overall efficiency and costs [5]. By using simulations, organizations can make informed decisions about process improvements before implementing them in real life, leading to a more sustainable and adaptive business. Business process simulations can be conducted through three different methods [6]: (1) DES, which uses entities resources and block charts to depict entity flow and resources allocation. (2) SD, which portrays processes as “stocks”. (3) Agent-based modelling and simulation, where active elements of the modeled system are represented by software agents that follow predetermined behavioral rules and interact with each other to make decisions. DES and ABMS are bottom-up methods, while SD is a top-down approach. According to van der Aalst [7], business process is a sequence of activities which execution results in a specific outcome. TDABC assigns business’s costs to individual activities which are building components in analysis and modelling of business processes. Costs of particular activities are furthermore aggregated into the cost of the whole process. The advantage of this method compared to others is the effort of evaluating each single activity instead of evaluation based on allocation bases. This is especially important for TDABC approach and ABMS allows to simulate business process models at much lower level of abstraction. Research in this paper explores the necessity and advantages of use of ABMS approach to business process simulations for the purpose of cost allocation using TDABC. In this paper we postulated following research questions (RQ): (1) Why an ABMS approach should be used when simulating costs via TDABC approach? (2) What are the advantages and disadvantages of application of ABMS approach when simulating costs via TDABC? The remainder of this paper is as follows. Section 2 presents an introduction to TDABC approach. Section 3 is devoted to  
  Application of TDABC Systems and Their Support with ABMS Approach  
  2 Time-Driven Activity-Based Costing Time-driven activity-based costing was specifically introduced to solve implementation problems related to activity-based costing approach. It was specifically designed to simplify the implementation and maintenance of the activity-based costing systems [8]. TDABC provides a simplified method of identifying and reporting complex transactions through the use of time-based equations that use multiple drivers. Several researchers have shown that TDABC can produce improved cost representations and positive results. According to Everaert et al., the key lies in time estimations, where the time required for each activity is calculated [9]. TDABC only requires two parameters to estimate: the unit cost of resource supply and the time required for the resource group to perform activity [10]. While time drivers might offer greater accuracy in cost allocation, they can also be more expensive to measure. But this comes with the advantage of reducing the need for time consuming interviews and surveys, which were a hindrance in traditional activity-based costing. With TDABC, the first step of activity-based costing system implementation, defining resource pools, is eliminated, allowing for a more streamlined implementation process and quicker, more cost-effective integration with software. The second step of assigning costs remains, but time is used to directly link costs from resources to cost objects, making the design of the costing system easier and faster to implement [11]. TDABC also provides a more comprehensive accounting of business transactions by utilizing time-based equations that consider the time involved in a particular process [12]. The use of time drivers makes the system easier to maintain compared to the transaction drivers used in traditional activity-based costing systems. TDABC eliminates the need for activity pools and the use of quantity-based cost drivers, simplifying the cost allocation process [8].  
  3 Business Process Simulation and ABMS Approach Modelling and simulation are valuable methods of comprehending real-world systems through imitation at varying levels of abstraction. This approach has become widely accepted as a research methodology, similar to other established methods such as induction or deduction. According to Axelrod [13], simulation is highly valued due to its versatility and the diverse range of purposes it can serve, including prediction, performance evaluation, discovery, and more. These purposes are of great significance in business, as they aid in the improvement of business  
  M. Halaška and R. Šperka  
  processes by enabling the analysis of behaviour, evaluating decision-making strategies, reengineering existing processes, and designing new ones. Correctly designing and analysing business processes is crucial to prevent negative outcomes such as dissatisfied customers and poor performance, such as long response times or low service levels. It is essential to not only examine and understand processes before implementation, but also after to ensure they are functioning optimally. The reality that business processes within organizations are not static, but constantly adapting to meet the demands of a constantly evolving market, highlights the importance of regular analysis. Management often has to make decisions about processes without fully understanding their potential outcomes, particularly for organizations that strive for continuous improvement. According to van der Aalst [14], there are specific risks to business process simulations: instance context, process context, social context. However, a major challenge in current business simulation approaches is the accurate representation of resources, as noted by van der Aalst et al. [15] and Martin et al. [16]. People are often involved in multiple processes and allocate only a portion of their time to each process based on priorities and workload, making it difficult to model accurately. Performance is also influenced by workload and can vary accordingly, as demonstrated by studies such as Wickens et al. [17]. In addition, people often work part-time and in batches, causing work items to accumulate and be processed together. Simulation tools assume usually a stable process and organization, but if the flow of work becomes too prolonged, resources may choose to skip certain activities or additional resources may be mobilized. Another problem with business process simulation is that they treat resources as undifferentiated entities grouped into resource pools [18]. This leads to issues such as pooled resource allocation and undifferentiated performance and availability. This is a limitation as each resource has its own capabilities, performance, and availability. Moreover, many existing techniques adopt a resource allocation model based on First-In-First-Out allocation approach, while not taking into account range of possible existing resource patterns [19].  
  3.1 Agent-Based Modelling and Simulation ABMS approach is relatively new compared to other simulation methods, such as discrete event simulation and system dynamics. ABMS allows for more detailed representation of a system by using software agents as active elements, which follow behavioural rules and interact autonomously to make decisions. These agents can represent entities such as products, organizations, departments, people, etc. [20], in simulation of for example manufacturing, logistics, operational and management science, internet of things, cyber physical systems, and other complex heterogenous systems. In recent years, there has been a growing interest in using ABMS for business process execution. Some authors proposed an automated method to map BPMN (business process modelling notation) diagrams to beliefs-desires-intentions agents. They used a model transformation technique to convert BPMN models to  
  Application of TDABC Systems and Their Support with ABMS Approach  
  target models. However, it is not possible to provide a direct mapping from business languages to an agent execution platform due to significant difference in the models and levels of abstraction between them as BPMN notation does not have a formal behavioural semantics while implementation of agents requires it [21]. The ability for agents in an ABMS to change their behaviour without external control based on changes in their environment and operating conditions is known as self-organization [22]. This feature is important for business process modelling and simulation. In the digital and automated business world, sophisticated interactions between robots and humans, or robots and robots, are necessary and ABMS are well-suited to model and simulate these interactions. For ABMS to attain autonomy and self-organization, the agents must be able to coordinate their actions, learn, and more. The implementation of ABMS involves creating realistic scenarios using a group of self-governed agents, either as simple entities within the computer code or as highly intelligent objects. This approach is similar to a human’s ability to solve problems with multiple states, beliefs, trusts, decisions, actions, and responses. The most challenging aspect of ABMS is developing comprehensive and logical model to accurately represent the system being tested in the simulation. Disadvantages of ABMS Validation and verification are critical challenges in ABMS research and attract a lot of attention from researches. The difficulty in managing ABMS models arises as they become more complex. However, similar challenges have been encountered in the system dynamics approach, but it has not proven to be a substantial obstacle [23]. Another drawback of ABMS is that it requires the modeler to be familiar with object-oriented programming and a programming language, such as Java. Although this has been partially addressed through the use of graphical approaches, such as drag-and-drop techniques, specialized tools, toolkits, or development environments are still required for modelling the behaviour of typical software agents [20]. Currently, there is no established modelling notation for ABMS. It is more timeconsuming than discrete event simulation or system dynamics. This is due to the lack of a general framework to guide both academics and practitioners during the modelling and simulation process. Despite this, once the model is established, ABMS becomes very flexible and reusable [24]. The last significant barrier is the reluctance of managers to embrace new techniques. However, the growing influence of datadriven approaches and the increasing demand for informatics literacy are driving organizations to continually improve their practices. Advantages of ABMS One of the most notable advantages of ABMS is its capacity to model highly complicated systems, something that traditional business process management simulations struggle with. This is especially relevant in today’s organizations, which are becoming increasingly complex due to trends such as globalization and horizontal integration. The vast majority of organizations are characterized by uncertainty and complexity that go beyond intuition and traditional analytical methods [24]. Regarding the intricacy of simulated systems, ABMS provides the capacity  
  (1)  
  where x1 = 1 if event E with assigned activity name ‘W_Afhandelen leads’ occurred a second time or more in the trace. Otherwise, if it is the first occurrence of activity in the trace then x1 = 0. Equation 2 is then used for estimation of time required to process all events with assigned activity name ‘W_Afhandelen leads’: TW _A f handelen_leads =  
  6 Conclusion The demand for precise and reliable cost information systems is forecasted to rise in the coming years, due to the increasing competitiveness and cost effectiveness of companies’ operations, changes in structures of organizations and further automation. Thus, there will be greater emphasis on simulation techniques used for simulation of business processes. In this research, we had following research questions: (1) Why an ABMS approach should be used when simulating costs via TDABC approach? (2) What are the advantages and disadvantages of application of ABMS approach when simulating costs via TDABC?  
  Application of TDABC Systems and Their Support with ABMS Approach  
  With regards to RQ1, we showed that it is necessary to go into lower level of abstraction to be able to properly simulate cost allocation using TDABC based on discovered time driver. In this case, it is necessary to go into more detail with regards to process flow and organizational perspective. In such case, the difference in cost allocation can be of significant difference with regards to individual activities with regards to both case level and trace level of cost allocation. When it comes to RQ2, there are other issues with traditional simulation techniques which can be addressed through sufficient amount of detail in simulation model provided by ABMS approach which can have a significant impact when allocating costs. Manual design of process models for documentation and communication purposes is a common practice, but it does not always capture all the nuances of how the process is executed in reality. These models tend to focus on the most common pathways, and may not consider exceptions or infrequent scenarios. Yet, in many cases, exceptions occur in a non-negligible percentage of instances of a process [26]. For example, these techniques may assume that resources only perform one task at a time and are always available. People are usually involved in multiple processes. People also do not work at a constant speed. Moreover, people tend to work part-time and in batches. Another problem facing business process simulation the fact that they treat resources as undifferentiated entities grouped into resource pools. Moreover, many of the existing techniques adopt a resource allocation model based on a First-In-First-Out allocation approach In many of the processes supported by information systems and other forms of automation, human resources are the limiting factor. Moreover, nowadays it is necessary to also consider a worker-robot cooperation. Acknowledgements This paper was supported by the Ministry of Education, Youth and Sports Czech Republic within the Institutional Support for Long-term Development of a Research Organization in 2023.  
  Abstract The low car occupancy and the great demand for automobile transportation lead to traffic congestion in many urban areas. In large-scale networks with high shareability (opportunity for sharing the trips), a successful taxi-sharing program that increases vehicle occupancy may significantly save the roadway system’s driving costs and alleviate traffic congestion. Pricing plays an essential role in this system, as the taxi provider always seeks to maximize his benefits, and the passenger prefers a cheaper fare if he is going to share his taxi. So pricing can impact the performance of a shared-mobility system and, consequently, the network traffic. In this research, we define a pricing scheme based on the shareability concept to consider the impact of trip fare on the traffic situation. To model the passengers’ and taxi providers’ behavior, we use an agent-based approach to model the taxi-sharing service. We use real-world data from the city of Lyon in France to assess the behavior of the proposed taxi-sharing system under different pricing conditions. We implement two scenarios with different maximum fares acceptable by the passengers to see the impact of pricing on congestion.  
  1 Introduction The significant travel demand for personal car transportation and low occupancy lead to traffic congestion, an increasingly important issue in many urban areas with rapid population and economic growth [1]. In [2], the authors show that in large-scale networks where the opportunity for sharing passengers’ trips is high, a successful ride-sharing program that increases vehicle occupancy may significantly save the roadway system’s driving costs and alleviate traffic congestion. Taxi-sharing is a type of ride-sharing where the driver is just a professional taxi driver. Recently, ridesharing is expanding from traditional private car ride-sharing to taxi-sharing [3], and  
  N. Alisoltani and M. Zargayouna  
  soon, to autonomous vehicle taxi-sharing [4] and big taxi providers around the world are becoming reputed for providing shared services. In a taxi-sharing system, the passenger, the ride provider, and the dispatcher are the main parts. The passenger seeks a ride to pick her/him up at the origin point and drop her/him off at the desired destination within a time interval. The ride provider has a fleet of taxis ready to serve the passengers’ requests. The dispatcher receives the requests and the fleet information and tries to find the best matches on short notice. In such a system, provider and passenger criteria determine service efficiency and impact the service capability to reduce congestion. The impact of taxi-sharing and Uber-like services on traffic congestion has been studied in many pieces of research in different urban networks and contradictory conclusions have been claimed. The authors, in [5], define the concept of shareability and show that the ability of shared services to reduce congestion highly depends on this concept. Shareability is the potential for sharing trips, and it is different for different networks with different service demand conditions. They consider both passengers’ and providers’ objectives and constraints to model the service and cluster the ride requests based on the concept of shareability. However, they do not consider pricing a necessary criterion for both passengers and providers. In the current research, we define a pricing scheme based on the shareability concept defined in [5] to consider the impact of trip fare on the traffic situation. The passengers expect to pay less when they share their ride, as their travel time would increase. So if the taxi-sharing fare is higher than a specific price, they will not be willing to use the service and may reject the offer. This behavior can have major impacts on the shareability in the system. In [6], passengers can communicate with multiple vehicles and choose the offer according to their individual preferences, such as the earliest starting time, lowest trip time, and lowest cost. So if the offered fare exceeds the lowest cost, they may reject it. In [7], the authors propose monetary constraints to model a more realistic taxi ride-sharing system. These constraints provide incentives for both passengers and taxi drivers. Passengers will not pay more compared to no ride-sharing and get compensated if their travel time is lengthened, and taxis will make money for all the rerouting distance due to ride-sharing. In this paper, we consider the maximum trip fare for the passengers. If the price of the shared taxi is higher than this maximum fare, the passenger may reject the offer. This maximum fare can depend on different parameters, such as the price of public transportation and traditional taxi services, the passenger’s profile, and priorities. Defining a pricing scheme that ensures both passengers’ and providers’ benefits is essential to consider these monetary constraints in modeling a taxi-sharing service. Various pricing schemes have been proposed for the taxi-sharing systems [8–10]. In a recent survey on taxi-sharing in [11], the authors classify the proposed pricing schemes in the literature into four categories: number of passengers-based, travel distance-based, trip urgency-based, and hybrid pricing schemes, which integrates the three previous categories. However, an important factor in this classification is missing. The time taken to cover the same distance can be different based on the traffic situation in the network. Gurumurthy et al. [12] consider a travel time-based road-pricing policy where all the major network links carry a toll based on travel time  
  Assessing the Impact of Shared-Taxi Pricing on Congestion Using . . .  
  on the link for all road users during the morning and evening peak periods and assess the impact of this pricing on the mode share. Xu et al. [13] presents a dynamic pricing strategy with a time-varying commission rate. Wong et al. [14] propose to impose a surcharge on taxi customers who take taxis during peak hours and/or travel towards congested areas. In our method, we propose a new pricing scheme to consider the travel distance and the number of passengers, considering the traffic situation in the taxi dispatching computations. We use an agent-based approach to model passenger and taxi provider behavior. The agent-based approach offers a way to capture both supply and demand at a microscopic level, considering individual accessibility, available choices, and personal tastes and needs [15]. Therefore, this approach can make the possibility to evaluate our taxi-sharing system from the passengers’ (transport cost, satisfaction, and service quality) and the providers’ (operational cost, incomes, and fleet configuration) point of view [16, 17]. In addition, it can easily assess the impacts on the transportation network criteria such as energy consumption and emissions, shifts between transport modes, network demand satisfaction level, and network congestion [18]. This paper uses an agent-based approach to model the taxi-sharing system, considering passengers, taxis, and dispatchers as agents. Our previous study proposed a solution for the dynamic traffic conditions for a real-time ride-sharing service [19]. We use the same approach in this paper to consider large-scale network traffic. We define two models to deal with dynamic traffic conditions: the plant model and the prediction model. The current mean speed in the network will be used over the next 10 min to predict travel times for the dispatcher’s calculations. Then, taxis and personal vehicle travel are simulated. The traffic situation is updated every 10 s using a trip-based MFD model as the plant model to represent the traffic dynamics. The remainder of the paper is organized as follows. First, Sect. 2 proposes a multi-agent model for the taxi-sharing model. Then Sect. 3 presents a pricing scheme for this system. Section 4 explains how we solve the dispatching problem. Section 5 presents the numerical experiment, and finally, Sect. 6 concludes the paper.  
  2 Multi-agent System for the Shared Taxi Service The multi-agent system designed for the taxi-sharing service is shown in Fig. 1. The main components of this model are the passenger agents, the taxi agents, and the dispatching system. Each component is described as follows. The passenger sends her/his request for a trip, defining the number of demanded seats, the time window (earliest pick-up time and latest drop-off time) Passenger agent: The passenger sends his request for a trip via an application. This request contains the origin location, the destination point, the desired time window (earliest pick-up time and latest arrival time), and the number of demanded seats. Then he receives the dispatcher’s choices, with different prices and waiting/travel times.  
  Distributed, Classical and Flexible Job Shop Scheduling Problem with Transportation Times: A State-of-the-Art Bilel Marzouki, Olfa Belkahla Driss, and Khaled Ghedira  
  Abstract Today, several companies have successfully distributed their planning machines in different locations or factories, which facilitates the execution of tasks, the rapidity with which tasks are performed, and decreases the delay time. The Distributed Job shop Scheduling Problem with Transportation time (DJSPT) is one of the scheduling problems, where each operation must be processed on one or different machines and its processing time depends on the used machine, and this is in a set of geographically distributed factories. Each factory contains m machines, on which n jobs must be processed. The transport of jobs between machines is made by one or several transport robots. The DJSPT, with two versions; centralized and no-centralized; combines three Np-Hard problems: The assignment problem of jobs to machines, the distribution problem of jobs to factories, and the robot routing problem. In this paper, we present the state of the art of job shop scheduling problem with transportation times in three versions: classical, flexible, and distributed according to different criteria such as the used approach, the number of transport robots (SR to single/MR to transport robots), the flexibility of job shop environment (Yes or No) and the optimization criterion.  
  B. Marzouki et al.  
  1 Introduction Today, several companies have succeeded to distribute their scheduling machines in different locations or factories, which facilitates the execution of the tasks, the speed of completing the execution of the tasks, and decreases the delay time. The Distributed Job shop Scheduling Problem (DJSP) is considered among the most studied scheduling problems in the literature due to its importance. This type of scheduling can be found in several areas (textile factory, automobile factories, cable assembly factories ). In the DJSP, there is a set of distributed factories composed of m machines, n jobs, and a distance between each job and each factory. The Distributed scheduling problems with transportation time and more specifically the Distributed Job shop Scheduling Problem with Transportation time with a single robot (DJSPT-SR) and many transport robots (DJSPT-MR) where the jobs are transported between machines by one or more transport robots are much more complicated than standard problems. The Distributed Job shop Scheduling Problem with transportation Time (DJSPT) combines three NP-Hard Problems: 1. The assignment problem of jobs to machines [1]. 2. The problem of distribution of jobs in different factories [2]. 3. Robot routing problem or pickup and delivery problem [3]. Therefore, the DJSPT is much more complex than standard problems. The majority of existing works are limited to the standard job shop problem with transportation time or distributed and flexible job shop scheduling problem without transportation robots. The Distributed Job shop Scheduling Problem with Transportation robots is composed of a set of factories (l = 1, 2, . . ., F ), a set of jobs (i = 1, 2, . . ., N ) and they are geographically distributed with a travel time (T Tli ). Each factory has a set of machines (Ml ) and each job i is composed of a set of Ni operations (Oi j , i = 1, 2, . . ., N; j = 1, 2, . . ., Ni ). Each job Ji is composed by (n i−1 ) transport operations (Ti,1 , Ti,2 , . . ., Ti,ni−1 ) to be made by a single robot r or by a set of r robots (r = r1 , r2 , . . ., Rl ) between two machines for each transport operation Ti, j , we find two types of movements: full transport operation and empty transport operation. Our objective is : • Affect the jobs in factories. • Determine the scheduling of jobs in each factory. In DJSPT, we chose the following hypotheses and constraints: • • • •  
  Each machine executes at most one operation at each time. Precedence constraint between the operations of the same job. The execution of the operation cannot be divided into several steps. Once a job is allocated to a factory, all of its operations will be processed in that factory. • Each operation is transported by a single transport robot. • Each robot must move at most one operation at each time.  
  2.1 Exact Methods We start with the exact methods: [4] proposed a formulation using mixed linear programming for job shop scheduling problem with transportation time with many transport robots to minimize the makespan. Pundit and Palekar [5] developed an algorithm of branch and bound for JSPT-MR to minimize the makespan. Deliktas et al. [6] proposed novel mathematical models to single and bi-objective functions for the flexible job shop scheduling problem in a cellular manufacturing environment by taking into consideration exceptional parts, intercellular moves, intercellular transportation time, sequence-dependent family setup times, and recirculation. The authors used the scalarization method, the weighted sum method, -constraint method, and conic scalarization method. Ham [7] proposed a novel application of constraint programming for the job shop scheduling problem with transportation time using instances in the literature and proving the optimality of the instances of Bilge and ulusoy, and he proposed also a new benchmark instance.  
  2.2 Approximate Methods The approximate methods such as heuristics and metaheuristics have also been used to solve the classical and flexible job shop scheduling problem with transportation time due to its ability to solve these problems in a reasonable resolution time. Anwar and Nagi [8] treated the multi-objective job shop scheduling problem with transportation time with many transport robots using a forward propagation heuristic to optimize the makespan, Work in process and inventory holding costs. Authors [9, 10] proposed an approach based on tabu search to job shop scheduling problem with a single robot to minimize the maximum completion time (Cmax). Tamer et al. [11] proposed a hybrid approach based on genetic algorithm with a heuristic to solve the job shop scheduling problem with transportation time with many transport robots (JSPT-MR) to minimize the maximum completion time and they tested their approach on instances of Bilge and ulusoy. Deroussi and Norre [12] proposed to solve the flexible job shop problem with transportation time with many transport robots approach based on a simulated annealing metaheuristic with a phase of local search  
  algorithms. We can note that the most used works have not studied the flexibility of jobs because the difficulties of this constraint. We can also note that the most used methods a mono objective as an optimization criterion, see Fig. 2. Also, the most studied approach used is not hybrid whatever the categorie of used method, see Fig. 3.  
  4 Conclusion and Perspectives In this paper, we present the different works proposed for the distributed job shop scheduling problem where we present a classification of the works according to four criteria which: The used approach, the number of transport robots (SR to single/MR to transport robots), the flexibility of job shop environment (Yes or No), the optimization criterion and the Method type. As perspectives, we propose to treat the distributed real-time job scheduling problem with transportation robots to schedule, the operations and take into consideration the new operations that arrive to execute and the cases where the machines break down and transport them from one resource to another and from stock radius to machine and finally from machine to an deposit and we need to move the operation many robots and Automated Guided Vehicles (AGV).  
  Yes  
  Makespan, robot travel distance, time difference with due date and critical waiting time  
  [19]  
  2.2 Transfer Reinforcement Learning Transfer learning is the reuse of knowledge learned in one domain for learning in a different domain [2]. Transfer learning in reinforcement learning is expected to have the following effects: jump-start, learning speed improvement, and asymptotic improvement [3]. On the other hand, if the relevance between the source and target domains is low, negative transfer may worsen learning performance [4]. Various methods of knowledge reuse among agents using transfer learning have been discussed in MARL, where multiple reinforcement learning agents learn simultaneously [5].  
  Proposal of Bicycle Sharing Operation System …  
  4.5 Discussion In this study, we considered that negative transfer could be avoided by assigning Low and High labels to stations based on whether the number of remaining stations reaches 0 or not, and using a combination of these labels to determine the transfer target. In Experiment 3, we confirmed that transitions between homogeneous labels were better than those between heterogeneous labels. On the other hand, in Experiment 2, we confirmed that transfers between heterogeneous labels were also effective, indicating that negative transfers did not occur. Although the label combinations were different, the large demand trends were similar, resulting in effective transfers. In addition, the demand for each station is complicated, and due to the difference in the time period when the number of remaining stations is zero, there are cases where the similarity of transfer targets cannot be sufficiently defined by the two types of demand, low and high, and the number of remaining stations is remains zero, and so on. It is necessary to consider how to express the index for selecting the transfer targets as an important issue to be solved in the future. In Experiment 4, each DQN agent learns cooperative behavior and is able to achieve an operation in which the number of remaining bicycles does not reach 0. However, learning is unstable, and the reward design and the input and output of each agent also need to be considered.  
  5 Conclusion In this study, we confirmed the possibility of a bicycle sharing operation system by local residents using the MARL model, which employs DQNs with four stations as  
  2 Previous Research 2.1 Social Network Analysis Social network analysis is an approach that measures interactions and the social structures using graph theory. Relationships typically comprise nodes and edges that represent the connections between nodes. When V and E respectively refer to a set of nodes and edges, such network G is defined as G = (V, E) [5]. We examine the relationship between centrality and performance using social network analysis.  
  2.2 Performance and Centrality in Companies There have been many studies regarding the relationship between centrality of leaders/managers and performance (Ref. [6–8] etc.). For example, degree centrality, which counts the number of paths that emanate from an organizational actor [9], is generally considered to indicate a leader’s popularity. It is said to have a positive effect on team leader’s degree centrality and team performance [6]. A study of 19 teams in a manufacturing organization showed that teams with leaders whom many subordinates sought advice from had lower conflict within the team and high viability [10]. On the other hand, some studies have pointed out the disadvantages of high degree centrality. As the direct linkages increase, maintaining them may consume more personal resources [11]. Furthermore, having strong connections with others may lead to individual behavior being restricted by roles defined by those connections [12]. Additionally, betweenness centrality, which represents the degree of mediating between individuals, is a strong predictor of leadership and being perceived as a leader. Leaders with high betweenness centrality serve as intermediaries between different teams and coordinate work and information flow within organizations [6]. On the other hand, a high level of betweenness centrality may also result in more conflicting demands from teams and individuals not connected to them [6]. The eigenvector centrality evaluates nodes with more connections [5]. It is said to help avoid the cost associated with high degree centrality and betweenness centrality positions [6]. A study using network data from a financial services firm’s sales  
  Change in Centrality and Team Performance …  
  4 Results 4.1 Centrality and Performance First, we show the mean and standard deviation of team performance, network indicators (centrality), and control variables in Table 1. We then performed a multiple regression analysis using the ordinary least squares (OLS) method, with team performance as the dependent variable. To explore the relationship between centrality and team performance, we calculated three models. Model 1 used the centrality values calculated at one point in time, specifically the final month of the year, as the independent variable. Model 2 used the median centrality values calculated over the course of the year as the independent variable. Lastly, Model 3 included both the median and standard deviation of centrality over the course of a year as independent variables. We used these three models to determine the extent to which centrality was associated with team performance, while controlling for other variables. The results are shown in Table 2. The best-fitting model was the one that included the standard deviation of centrality over 12 months (Model 3). The results showed that eigenvector centrality had the highest positive impact on performance among the independent variables. The standard deviation of betweenness centrality was also found to have a positive impact on team performance in Model 3.  
  4 Case Study This sample case study bases on student feedback and student profile. Assume a decision matrix bases on student’s feet back and we analyze bases on that matrix. Here CR is very important. If and only if CR is less than 10% then we accept the result. So it helps to minimize the risk of student priority.  
  4.1 Background of the Problem We analyze AHP methods bases on Sect. 3.1. In this case our bases on student feedback and student profile. That why in every case it may vary. We consider 4 criteria and 9 alternatives here. Here we use AHP online software1 to calculate AHP value.  
  1  
  R. Islam et al.  
  Limitation Online ed. (COVID-19 era) can be beneficial, but there are limitations. Selfmotivation and organization is key. Interaction with teachers and other students is limited, and tech issues may arise. Written comm. is a crucial aspect and can be difficult for some. Furthermore, our focus is on providing an overview based on the changing values. It is possible that different weights may be assigned to the alternatives.  
  Training Students as Agile Developers: Team and Role Building Games Paolo Ciancarini and Marcello Missiroli  
  Abstract Computational Thinking is a skill related to problem solving: it is the competence necessary for applying, assessing, producing an algorithmic solution, and implementing it. Agile values and principles are both an ethic framework and a practical reference for teams of contemporary software developers. We study the combination of Computational Thinking competence with Agile values and principles: we have called the result of this combination Cooperative Thinking. Our hypothesis is that the practice of Cooperative Thinking is especially important for students and practitioners who will work in software development teams. Individual productivity in agile software projects is less important than team productivity, which in turn is influenced by team dynamics. In this paper we describe an approach to practicing Cooperative Thinking that we are experimenting, consisting in coaching groups of developers using team-building games, some well known, some invented ad hoc. We discuss the topic of team building by serious games; we show how to evaluate the performance of a team engaged in playing games to train themselves as agile software developers. We will distinguish games playable online from other games, as during the pandemic we were compelled to organize team building tasks online only. We have developed a GQM schema to evaluate the teamwork during a game involving cooperative thinking.  
  Impact of Covid-19 on Employee Satisfaction and Trust with Focus on Working from Home Miriam Gazem, Ralf Härting, Anna Schneider, and Christopher Reichstein  
  Abstract Due to the Covid-19 pandemic, companies were forced to send employees into home office. This presented major challenges for many companies. In addition, this meant a change of habits, both for employees and managers. The paper focuses on the question how this circumstance has an influence on employee satisfaction. Therefore, a theoretically based empirical study was conducted. A conceptual model was created, and hypotheses were formulated for this purpose based on a structured literature review. Isolation, communication, and trust (from supervisors’ and employees’ perspective) were identified as influencing factors that were examined in this study. It could be shown that the biggest influencing factors are isolation and supervisor trust. The influence of communication on employee satisfaction was confirmed, too. For an outlook, linear regression was conducted.  
  study will focus on four main influencing factors “isolation”, “communication”, “supervisor trust” and “employee trust”.  
  2 Theoretical Background The present study was preceded by a literature review. None of the available studies refer explicitly to how the Covid-19 pandemic and the resulting increased occurrence of home offices affect employee satisfaction. In the present study the already known influential constructs “communication” and “isolation” are applied to the challenging pandemic situation and are supplemented by the construct “trust in home office”. Employee satisfaction. It has already been proven that home offices have an impact on employee satisfaction [3]. In the past there has already been many studies about the influence of teleworking on employee satisfaction [4]. In this context employee satisfaction is considered one of the most frequently studied topics [5–9]. To investigate the impact of home office on employee satisfaction this research assumes that employee satisfaction is conditioned by employee motivation. Motivation was operationalized by Herzberg’s hygiene factors as well as by intrinsic and extrinsic motivation [10]. For employees to be motivated the hygiene factors must be fulfilled. Intrinsic and extrinsic motivation derive from the self-determination theory of Deci and Ryan [11]. Interest-driven actions are perceived as intrinsically motivated behaviors [11, 12]. Additionally, there is extrinsic motivation. This is often based on a request that leads to the expectation of a consequence [11]. The extrinsic motivation was divided into external, introjected, identified and the integrated regulation [13–15]. Isolation. Home office employees are less likely to identify with their team, less likely to share experiences, less likely to receive feedback and more likely to be ostracized [16, 17]. They are also less able to relate to others. This leads to a change in affective relationships and can have negative effects on motivation, engagement, and work performance [18, 29]. The reduced emotional involvement in home office can be both an advantage and a disadvantage [19]. Social isolation in home office can be reduced by receiving more support from colleagues and superiors [20]. Separation from colleagues can also lead to dissatisfaction [21]. Constant work in the home office can result in a reduction of opportunities for further development [22]. The negative effects are intensified if the manager is also working from home [23]. The most cited study focuses on the impact of isolation on work performance and job change intentions [24]. The constructs of home office and isolation are present in all the cited references, but the circumstances may change significantly in the context of a global pandemic. This paper attempts to relate the construct of isolation to employee satisfaction much more clearly than in previous work rather than focusing on pure job performance  
  Impact of Covid-19 on Employee Satisfaction and Trust with Focus …  
  M. Gazem et al.  
  H4: “Communication in the home office has an influence on employee satisfaction.” The present study applies the presented constructs to the completely changed situation due to the global pandemic in Germany.  
  According to the calculations the biggest factors influencing employee satisfaction are the constructs “supervisor trust” and “isolation” (Fig. 2). The tested null hypotheses can all be rejected. All examined constructs have an influence on employee satisfaction in the home office during the Covid-19 pandemic.  
  5 Discussion The present results confirm that the constructs “isolation”, “trust” and “communication” have an influence on employee satisfaction. This study demonstrates the positive influence of the reviewed constructs “communication” and “isolation” on employee satisfaction in the home office during the Covid-19 pandemic. These results support the findings of other studies conducted without the influence of the crisis event. An important positive influencing factor was isolation where a rather negative influence was expected during the current situation. Satisfaction is reinforced by the feeling of being able to work in isolation and alone. The initial perspective is that isolation has a positive effect on satisfaction because it leads to fewer interruptions to work due to disturbances from colleagues and superiors. The greatest positive influence on communication satisfaction was the availability of the manager and personal identification with the company. The availability of the manager should therefore be ensured as a matter of principle. Personal identification with the company also addresses an important motivational factor. The goal should  
  Impact of Covid-19 on Employee Satisfaction and Trust with Focus …  
  M. Gazem et al.  
  on employee satisfaction. However, this does not claim to be exhaustive. It is likely that there are other constructs that have an impact on home office satisfaction during the Covid-19 crisis. For example, the constructs “stress” and “anxiety” could be mentioned which should be investigated in further studies. Examples of fears that could arise in conjunction with the Covid-19 pandemic are basically fear of contracting the virus or fear of losing a job due to the worsened economic situation. Acknowledgements We thank Alina Gehrig, Demian Deffner and Chiara Frank for supporting our research. Thank you for your valuable and helpful contributions.  
  An Approach for Organising and Managing an Academic Year Using Online Tools and Techniques Liviu-Andrei Scutelnicu and Marius Ciprian Ceobanu  
  Abstract E-learning is a concept in continuous research, which provides for the instruction of pupils/students/trainees in a dynamic way, incorporating certain aspects from all forms of modern education. It is desired both to increase the knowledge of the individual and to increase the efficiency of interpersonal relationships, so much affected by SarsCov2. In the context of the pandemic, in order to continue working with students, we chose to organise and manage the academic year in a more special way, namely, to use one of the free tools available on the Internet, but with the possibility to customise it according to our needs and requirements. In this paper we will present a way in which we managed the academic year, applied to the Faculty of Computer Science, from our university and what additional development we took into account to succeed in adapting the program to our needs. We took into consideration the Discord platform, first because it is an open-source (free) platform and second because it has the possibility and offers the opportunity to implement socalled bots, which are in fact programs implemented by developers, which perform certain processes automatically or semi-automatically, with certain pre-established conditions at the time of development. The purpose of this study is to help nontechnical people as well to use this kind of platform for organising and managing their classes, to have an active interaction with their students, even if such events occur that force us to socially distance ourselves.  
  L.-A. Scutelnicu and M. C. Ceobanu  
  1 Introduction In the field of artificial intelligence and machine learning [1], we noticed an accelerated evolution that has behind it the huge investments made by states in research, but especially the investments of the big international corporations Google, Apple, Microsoft. These companies have at least three competitive advantages. First advantage is related to the ability to attract the best minds from the academy to their research centres. Second advantage is the availability of model drive data. Another advantage is the easy and sometimes virtually free access to cloud resources for model training and experimentation due to the fact that they can use the resources they do not rent at a given time. Other important advantage of this approach is that these large companies seek to make scientific results available to the whole world (for pragmatic reasons related to the recruitment capacity of researchers) but at the same time on the operating side they are only interested in results that impact billions of people. E-learning [2] is a concept in continuous research, which provides for the instruction of pupils/students/trainees in a dynamic way, incorporating certain aspects from all forms of modern education. It is desired both to increase the knowledge of the individual and to increase the efficiency of interpersonal relationships, so much affected by SarsCov2 [3]. The aim is to move from the first e-learning concepts that aimed only at the use of new technologies to the humanization of the learning process. In the context of the pandemic, in order to continue working with students, we chose to organise and manage the academic year in a more special way, namely, to use one of the free tools available on the Internet, but with the possibility to customise it according to our needs and requirements. In this paper we will present a way in which we managed the academic year, applied to the Faculty of Computer Science, from our university and what additional development we took into account to succeed in adapting the program to our needs. In this study, we will present the development and the applicability of such a program which interacts with the Discord platform and how this program can be reused, to generate such a session, regardless of specialisation, language or even without the need for certain studies in the field of computer science. The user interaction being minimal only by specifying certain information about the session (name of the session, number of groups, session etc.). The program automatically configures and initialises a session on Discord, based on certain criteria such as the user roles (teacher, student, staff etc.) the year of study or by the master’s specialisation (1st year, 2nd year, 3rd year, 1st master—1st specialisation, 1st master—2nd specialisation a.s.o.).  
  An Approach for Organising and Managing an Academic Year Using …  
  2.6 Comparisons Between the Mentioned Platforms Based on the short brief description of the text and video streaming platforms, we made a short comparison between them (see Table 1). As can be seen from Table 1, all of the platforms do approximately the same thing, but have certain limitations, especially on the number of meeting participants. But this issue, as we mentioned in the beginning of this chapter, can be removed with a paid subscription.  
  3 The Main System—Algorithmic Proposal Due to the many platforms that are available both free and paid, we decided that the platform that can fulfill our problems is based on Discord. The decision was made because, Discord is the only platform that allows external users and developers to integrate additional programs or code snippets or APIs, that can cover almost the use-cases that we took into consideration. Another aspect that attracted us to use this platform to took it into consideration, was that it is an open-source (free) platform and it has the possibility and offers the opportunity to implement so-called bots, which are in fact programs implemented by developers, which perform certain processes automatically or semi-automatically, with certain pre-established conditions at the time of development. Discord uses servers and channels similar to Internet Relay Chat (IRC) [9], even though these servers do not emulate traditional hardware architecture due to its distributed nature. A user can create a server on Discord, manage their public visibility and access, and create one or more channels within this service. Within a server, depending  
  An Approach for Organising and Managing an Academic Year Using …  
  Exploring the Impact of COVID-19 on Education: A Study on Challenges and Opportunities in Online Learning Ananga Thapaliya and Yury Hrytsuk  
  Abstract The COVID-19 pandemic has brought about numerous changes in the education sector, including the transition to online learning. This literature review explored the impact of the pandemic on higher education and the challenges and opportunities associated with online learning. Results indicate that students and teachers in developing nations faced significant difficulties due to the lack of proper digital infrastructure. Additionally, a dearth of information was found regarding the impact of COVID-19 on academic performance. Nevertheless, the pandemic also brought about opportunities for innovation and capacity building in the education sector. The study concluded that neither teachers nor students were fully prepared for the shift to online learning during the pandemic. This literature review highlights the need for future research to investigate the effect of COVID-19 on academic performance and to develop strategies to improve online learning experiences for all students.  
  A. Thapaliya and Y. Hrytsuk  
  they lack resources including infrastructure, technology, and healthcare facilities [5]. The inequality gap across nations, regions, and communities would enlarge as a result [5, 6]. The United States, China, Europe, Iran, South Korea, and other regions were among those first severely hit by significant pandemics. Authorities in several nations have put processes and rules into place to try and stop the spread of COVID-19. This includes putting in place measures like restricting social gatherings and encouraging social segregation through widespread lockdowns [7]. In order to combat the virus, a number of social and commercial activities, including gyms, museums, movie theaters, swimming pools, and educational institutions, had to be suspended [8]. The increasing expansion of COVID-19 has also provided a significant issue for the educational field, as educational institutions at all levels were compelled to close and discover new means of instructing and learning [9, 10]. The COVID-19 laws have forced educational institutions all around the world to adopt online learning because traditional classroom-based learning is no longer practicable [11, 12]. It is clear that COVID-19 has seriously disrupted the educational system, much of which is still being comprehended as a result of its extensive ramifications [13, 14]. The transition from in-person to online learning has many stakeholders, including government officials, academic staff, students, and parents, worried about the possible results [15]. Although the increased use of online learning brings with it new difficulties, the potential for innovation in the education field should not be disregarded. We seek to answer the following research questions from our study: 1. What are the challenges and opportunities associated with the transition to online learning due to COVID-19 in the higher education sector? 2. How has the COVID-19 pandemic affected the education sector in developing nations in terms of digital infrastructure and online learning capabilities? 3. What is the impact of the COVID-19 pandemic on academic performance in the higher education sector, and what evidence supports this impact? 4. What are the gaps in the current literature on the impact of COVID-19 on higher education and online learning, and what areas require further research and investigation? This study seeks to offer a comprehensive assessment of the available literature on the subject in light of the numerous concerns that have been voiced regarding the quality of online teaching and learning. During the shutdown and COVID-19 pandemic, it is important to understand the practices, difficulties, and opportunities related to online teaching and learning. It outlines the difficulties and possibilities of online learning and continuing education throughout the pandemic and offers suggestions for the future. This paper is organized as follows. Section 2 describes the methods we used for the literature review, Sect. 3 explains the results we obtained for the above research questions by grouping our findings into methods for the continuation of online education, obstacles for learning and teaching and opportunities for teaching and learning, Sect. 4 discusses the findings and Sect. 5 concludes the study and provides some insights for future research.  
  Exploring the Impact of COVID-19 on Education …  
  3 Results This conclusion was reached after reviewing the literature on online learning during the COVID-19 pandemic. Few of the studies that were reviewed emphasized the opportunities and academic outcomes that the pandemic presented, with the majority of them concentrating on how educational institutions responded to it and the challenges of online learning. The findings are divided into three groups: methods for continuing education online, obstacles for teaching and learning, and opportunities for teaching and learning.  
  3.1 Methods for the Continuation of Online Education Worldwide, educational establishments, including schools and higher education institutions, have been forced to close due to the COVID-19 pandemic and the ensuing lockdown and social isolation measures [17]. As a result, there has been a change in how educators deliver education, with an increase in the usage of different online platforms. Despite the difficulties that both teachers and students face, online learning, distance learning, and continuing education have emerged as the answers to this worldwide dilemma. Both students and teachers may find the switch from in-person to online learning to be novel and unfamiliar, but it is a need they must adapt to with few options available [11, 18]. In spite of their lack of readiness, the educational system and educators are currently implementing online platforms.  
  A. Thapaliya and Y. Hrytsuk  
  Despite the shutdown of educational facilities, online education technology has played a significant role in helping schools and universities continue to educate students during the pandemic [19, 20]. In order to make the transfer to this new method of learning successful, it is imperative to evaluate and support staff and student readiness. While some students with a fixed perspective can find it difficult to adjust, those with a growth mindset typically find it easier to adjust to different learning situations. There is no one-size-fits-all strategy for online learning because the needs of various disciplines and age groups vary [21]. Online education also reduces the requirement for physical mobility by giving students with physical disabilities more independence and the opportunity to participate in virtual learning activities [12, 22]. Globally, the COVID-19 pandemic has had a profound influence on education, resulting in the closure of several schools. This unforeseen circumstance has an impact on students, parents, and teachers. Education systems work to guarantee pupils continue to receive a high-quality education despite efforts by governments and health officials to stop the virus’s spread. However, many students find it challenging to concentrate on their academics when they are at home and under psychological and emotional stress [23]. The knowledge and expertise of both instructors and students with information and communications technology (ICT) may impact different approaches to online education. Many platforms have been used to deliver education, training, and skill development programs, including Google Classroom, Microsoft Teams, Canvas, and Blackboard [19]. These platforms make it simpler to organize classes and interact by providing tools like chat, video conferencing, and file storage. They facilitate the exchange of a variety of material formats, including Word documents, PDFs, Excel files, audio, and video, among others. Additionally, these platforms give users the option to analyze and monitor student learning through quizzes and the grading of submitted work using a predefined rubric [24]. A session of in-depth conversation and participation with instructors and classmates is followed by the distribution of learning materials, such as articles, videos, or links before the class even starts. The promotion of critical thinking, problemsolving, and self-directed learning is greatly enhanced by this approach. The usage of online learning management systems (LMSs) in the cloud, such as Elias, Moodle, BigBlueButton, and Skype, is becoming more common [25].  
  3.2 Obstacles for Learning and Teaching The COVID-19 pandemic has significantly disrupted the educational systems around the world, which has resulted in a wide acceptance of distance learning as a method of continuing education. While there are numerous benefits to remote learning, there are also new difficulties that have emerged, notably in terms of barriers to teaching and learning. Numerous challenges have arisen in education as a result of the abrupt change to online learning brought on by COVID-19 and lockdowns [26]. While some institutions that already had established online systems saw success, many  
  Exploring the Impact of COVID-19 on Education …  
  Memes as a Memorization Technique in Education Hamza Salem and Siham Siham Hattab  
  Abstract This paper explores the use of memes as a memorization technique in education. The study aims to define the concept of using memes as a memorization tool and to investigate its effectiveness in enhancing student learning outcomes. A review of literature is conducted to gather information on the current state of research in this area and to identify the key factors that contribute to the success of using memes as a memorization technique. The results of the study suggest that using memes as a memorization tool can be an effective way to promote student engagement and improve learning outcomes, particularly in subjects that are difficult to learn. Overall, this paper provides a comprehensive overview of the definition, benefits, and challenges of using memes as a memorization technique in education.  
  H. Salem and S. Siham Hattab  
  mental image of information in order to remember it. Elaboration involves actively thinking about the information and connecting it to prior knowledge in order to reinforce memory. In comparison to these traditional memorization techniques, using memes as a memorization tool is a relatively new concept. The unique combination of humor and visual representation in memes has the potential to make information more memorable and engaging for students. This paper aims to provide a comprehensive overview of the definition and benefits of using memes as a memorization technique in education [2]. The goal of this study is to define the concept of using memes as a memorization tool, to review current research on the topic, and to examine the pedagogical and ethical considerations involved in using this technique in the classroom.  
  2 Literature Review The use of memes as a memorization technique in education is a relatively new concept and as such, there is limited research in this area. However, a review of the existing literature suggests that the use of humor and visual aids in memorization can be effective in enhancing student engagement and learning outcomes. One study found that the use of humor in educational settings can have a positive impact on students’ motivation and can increase their engagement in the material being taught [3]. The use of humor can also serve as a stress reliever, which can improve students’ overall well-being and ability to focus [4]. Additionally, research has shown that visual aids, such as images and videos, can be effective in enhancing memory retention and recall [5]. The use of visual aids in education has been shown to increase students’ ability to retain information, particularly in subjects that are difficult to learn [6]. The unique combination of humor and visual representation in memes makes them well suited for use as a memorization tool. A study [7] found that the use of memes in the classroom can increase student engagement and improve learning outcomes, particularly in subjects that are traditionally considered dry or boring. The study also found that students who used memes as a memorization tool reported higher levels of enjoyment and motivation in their coursework. In conclusion, the existing literature suggests that the use of humor and visual aids in education can be effective in enhancing student engagement and learning outcomes. The use of memes as a memorization tool has the potential to provide these benefits and more, given the unique combination of humor and visual representation that is inherent in memes. Further research is needed to fully understand the potential of memes as a memorization technique in education and to determine the best practices for their use in the classroom.  
  Quantifying Education in the Post-COVID Era: An Engineering Approach Concept Gerald B. Imbugwa and Tom Gilb  
  Abstract The outbreak of Covid-19 has had a catastrophic impact on the education sector, leading to an unprecedented number of students struggling to keep up with their curriculum. This paper proposes an engineering approach concept that incorporates stakeholder analysis, quantification, and planning to ensure that students receive a high-quality education. Through stakeholder analysis, educators can gain a better understanding of the needs and perspectives of the stakeholders and involve them in the development of effective educational strategies. The quantification component involves using data and metrics to objectively measure and evaluate educational outcomes, thereby supporting student learning and tracking progress. Planning, which is a critical component of the engineering approach, helps ensure that education remains resilient and adaptable.  
  1 Background 1.1 Problem The COVID-19 pandemic has had a profound impact on the traditional education system, forcing schools to close and sending students of all ages home. This has resulted in a shift to online learning platforms, which have presented multiple challenges to both students and educators. Students have found it difficult to adapt to the new platform, with its lack of direct instructional guidance hindering their ability to fully engage in the learning process. Teachers and parents have also had to grapple with the unpredictable quality and level of engagement, as well as the lack of ways G. B. Imbugwa (B) Innopolis University, Innopolis, Norway e-mail: [email protected]  T. Gilb Independent Researcher, Kolbotn, Norway  
  G. B. T. Imbugwa Gilb  
  to measure student output due to the inability to directly observe progress. Furthermore, the lack of physical proximity has impeded meaningful conversations, robbing students of the opportunity for stimulating discourse.  
  1.2 Solution The use of an engineering approach concept, which would incorporate stakeholder, quantification, and planning. Stakeholder analysis involves identifying and engaging key groups who have a vested interest in the success of education, including parents, educators, administrators, policymakers, and community organizations [1]. By incorporating this aspect of the engineering approach, educators can better understand the needs and perspectives of the stakeholders and involve them in the development of effective education strategies [2]. Quantification, another key component of the engineering approach concept, involves the use of data and metrics to objectively measure and evaluate educational outcomes [3–5]. This helps educators track student progress, identify areas of weakness, and implement targeted interventions to support student learning [3]. With the use of technology and data, educators can ensure that students receive the education they need to succeed, even in a remote learning environment and post-COVID era [6]. Better planning is also an important part of the engineering approach concept [7, 8]. By developing detailed and flexible plans, educators can quickly pivot and adjust their approaches to meet the changing needs of students and communities [5]. This helps ensure that education remains resilient and adaptive in the face of change [8]. Considering the points above, a set of principles for the engineering approach concept can be established. This concept can guarantee high-quality education for students when confronted with difficulties [4, 5], through the understanding of stakeholders, establishment of quantified goals, and planning to fulfill these goals. This paper will be structured as follows: First, a literature review will be presented, followed by an application of the engineering approach concept through an example [9]. Then, a discussion of the methodology that will be employed to test this concept will be provided. Lastly, the conclusion will provide a general overview and plans for the next steps.  
  2 Literature Review The conventional approaches to teaching and learning have been severely disrupted by the COVID-19 pandemic. To maintain educational continuity, education systems around the world have been compelled to swiftly modify their approaches to teaching and learning [10]. Even though the pandemic has been difficult for both students and  
  teachers, research indicates that new methods have been developed. This article will go over some of the ways that the pandemic is altering how educational institutions teach, support learning, and assess academic achievement [11]. The increased use of technology to support learning in the classroom and at home is one benefit of the pandemic. For instance, online meetings, streaming, and virtual classrooms have become popular tools for teaching and parent-teacher communication in some nations [12]. With the aid of technology, students can interact with instructors and get feedback while relaxing in their own homes. In comparison to the conventional method of in-class instruction, this change has also made it challenging for teachers to monitor students’ progress [13]. The pandemic has promoted the creation of online learning platforms and virtual classrooms, among other remote learning technologies. The ability to access course materials, video lectures, and exam results from home is made possible by virtual classrooms. Additionally, online learning options for students all over the world have significantly increased thanks to virtual learning platforms like Khan Academy and Coursera [14]. The pandemic has compelled educators to consider novel ways to measure and evaluate learning. For instance, teachers are using online quizzes and rubrics more and more to keep track of their student’s progress. To evaluate student learning, schools also had to create alternative assessment techniques. For instance, some schools use project-based assessment to gauge how well students comprehend a subject or idea [13, 15]. In conclusion, the COVID-19 pandemic has forced academic institutions to fundamentally change how they influence knowledge and foster learning. By experimenting with new methods and applying an engineering mindset, the education system during this time can be enhanced even further. Our proposed concept can assist in ensuring that all stakeholders’ needs are met, in setting measurements for each goal or requirement, and in analyzing the results. More information on the idea and applications is provided in the following section.  
  3 Engineering Approach Concept The engineering approach is a systematic methodology for problem-solving that prioritizes rigour, well-defined processes, and quantification. The approach focuses on extreme clarity and heuristic-driven decision-making informed by a comprehensive systems overview. The engineering approach enables the identification, analysis, and solution of complex problems through objective data and evidence-based analysis. In contrast to subjective opinions or ambiguous assessments. The approach employs a structured and systematic approach to arrive at optimized solutions that are reliable, safe, meet the needs and expectations of stakeholders. The engineering approach is a disciplined and methodical problem-solving approach. Quantification and subsequent measurement are critical initial stages in achieving practical objectives. Education institution may suffer as a result of a lack of clear  
  G. B. T. Imbugwa Gilb  
  targets to strive for and precise criteria for evaluating success or failure. As a result, quantifying objectives is an important tool for motivating all the stakeholders in the academia in optimizing their efforts to achieve desired results [4]. Scale can be defined as central to the definition of all scalar attributes, that is, to all the performance and resource attributes [4]. A scale template for quantification is provided below. This scale can be used to assess and measure a variety of factors. The proper quantification of these factors can be critical to achieving set goals. The template allows for accurate progress measurement and the identification of areas that may require additional attention. Individuals and organizations can develop a comprehensive understanding of their performance and make informed decisions to improve their operations by implementing a clear and precise scale. In this sense, the use of a scale for quantification is an essential component of effective management and decision-making [4]. Tag: < Assign a tag to this Scale >. Type: Scale. Version: < Date of the latest version or change >. Owner: < Role/email of person responsible for updates/changes >. Status: < Draft, SQC Exited, Approved >. Scale: < Specify the Scale with defined [qualifiers] >. Alternative Scales: < Reference by tag or define other Scales of interest as alternatives and supplements >. In this example [9], a series of questions are posed to assess their difficulty and the effectiveness of the proposed approach in addressing them. A thorough understanding of the challenges faced and the identification of potential solutions can be facilitated by a systematic evaluation of the proposed approach in the context of the questions posed. As a result, the proposed method can be refined and optimized to achieve the desired result. The careful consideration of such questions is an essential component of effective problem-solving, and the proposed approach is intended to aid in this process. “It is important to use evidence for their appropriate creation and development so that action has the greatest impact possible.” From the excerpt, we can deduce the following questions: 1. 2. 3. 4. 5. 6. 7.  
  What is the significance of utilizing evidence in the context of the excerpt? What type of evidence is being referred to? What is the required quality of the evidence? Can you define the term ‘appropriate creation and appropriate developmentâŁ™? What specific actions are being referred to? How do we assess the impact of these actions? Are there any limitations, such as costs or long-term harm to students, that may limit the attainment of the greatest possible impact?  
  Teaching Object-Oriented Requirements Techniques: An Experiment Maria Naumcheva  
  Abstract Scenario-based software requirements specifications, due to limitations of natural language and scenarios, lack precision and abstraction. Formal methods address this problem, but are rarely used. A Unified Object-Oriented (OO) approach complements the simplicity and appeal of scenario techniques with the rigor and clarity of software contracts. In this study we conduct a teaching experiment to evaluate the perception of usefulness and difficulty of the approach. The obtained results demonstrate that the unified OO requirements approach has a potential to be adopted by requirements engineering practitioners.  
  1 Introduction In current practice, software requirements specifications heavily rely on natural language, scenarios [4, 9, 10] and UML diagrams [6]. Such specifications, due to limitations of natural language and scenarios, lack precision and abstraction. Formal methods address these limitations, yet their use is very limited outside of missioncritical systems [1, 6]. A Unified Object-Oriented (OO) approach [17] unites both worlds: it complements the simplicity and appeal of scenario techniques with the rigor of software contracts. In this approach, requirements, elicited as scenarios, are further analyzed to formulate abstract properties related to a system and its environment. The elements of a system and its environment are modeled as software classes, and requirements are formalized as software contracts of the features of those classes. The practical applicability of an approach depends on the required efforts for its learning. In this study we evaluate whether our proposed approach could be fitted into current university curriculum as a part of “OO Analysis and Design” course at the University of Toulouse.  
  M. Naumcheva  
  Modifying university curriculum is a sensitive topic. Since Covid19 outbreak, university programs experience tightened competition for students with massive open online courses (MOOCs) and online degree programs [16]. Students are focused on obtaining knowledge and skills that are relevant for their future jobs [11, 16]. As we aim to teach a new approach, which is not used in the industry yet, we should evaluate whether its learning helps to produce better specifications within commonly used approaches, such as use-case driven UML specifications. To address the concerns, discussed above, we formulate the following research questions for our study: (i) is limited training sufficient for learning OO contractbased requirements? (ii) does learning contract-based OO requirements techniques help to produce better UML specifications? The article is structured as follows. Section 2 describes the contract-based OO approach to requirements. Section 3 describes study design and results. In Sect. 4 we discuss related work, limitations and future work directions. Finally, we summarize the outcomes in Sect. 5 and conclude the paper.  
  2 Contract-Based OO Requirements In the unified OO approach to requirements, scenarios serve as an elicitation tool. In addition to commonly practiced requirements analysis techniques, such as UML diagrams [5], it involves formulating requirements as software contracts (preconditions, postconditions and class invariants). The contracts are formulated in a programming language which leads to seamless software development. The Eiffel language is a natural fit due to its readability and support of Design by Contract, yet the approach is applicable to any other OO language, such as Java or C++.  
  2.1 OO Requirements Fundamentals The basis of OO requirements is the concept of class, that serves as a unit of abstraction. The elements of the system and its environment are modeled as classes populated with relevant features (queries and operations). Those features are abstract (not implemented), as the classes describe system’s requirements, not implementation. The semantic specification of classes and their features is formulated with assertions, known as contracts [14]: preconditions express the properties that must hold when calling the routine, postconditions express the properties that must hold on the routine’s exit, class invariant expresses the properties that must be preserved by all methods of a class. [15] describes OO requirements fundamentals in details.  
  4.3 Future Work The experiment, although limited in scope, provides valuable inputs for future empirical studies of the unified OO approach to requirements, such as the difficulties that study participants faced when applying the approach, listed in Sect. 3.2. We plan to conduct another experiment to evaluate whether these difficulties can be eliminated with improving the supporting material by including more examples and illustrations, and making explicit the course prerequisites (such as Design by Contract knowledge).  
  5 Conclusion Producing unambiguous requirements is an important problem in software engineering. To be embraced by the industry practitioners, the solution should satisfy two criteria: (i) it should not require substantial training, (ii) it should be perceived as useful for current practices. In the experiment, presented in this paper, we evaluated the unified OO approach to requirements against these two criteria. With respect to the study participants, both of the criteria were satisfied: (i) most of the participants were able to learn the approach in a limited time, (ii) all participants were able to list the advantages of using the approach; applying the approach helped to improve UML-based specifications. Due to the limited scope of the experiment (a single course with a limited number of participants), we plan to conduct further empirical studies to generalize the outcomes of the experiment. Nevertheless, the results demonstrate that the unified OO requirements approach has a potential to be adopted by requirements engineering practitioners.  
  Onlife Education: Beyond Distance Learning by Intelligent Tutoring Systems Salvatore Distefano  
  Abstract In the pandemic scenario, characterized by lockdowns and service interruptions, Distance Learning allowed educational pathways remotely, proving to be a reliable solution, resilient to the emergency. However, issues related to its effectiveness as well as social and psychological implications, remain unresolved. From many sides, there is a desire to go beyond DL, towards a more engaging and efficient experience that takes advantage of the limitless knowledge resources offered by the infosphere. This new type of DL or DL 2.0 integrates different educational modes, beyond the blended approach, following the path of onlife education, without distinction between online and offline. In this context, tools such as Intelligent Tutoring Systems (ITS) could prove to be useful and effective as a complement to DL, a significant step towards onlife learning. In this paper, an ITS solution that led to the implementation of Virtual Study Buddy is proposed, primarily designed to support students in their individual studies, acting as a digital learning assistant. Virtual Study Buddy brings together concepts of machine learning and gamification with cloud technologies, leveraging personal and mobile devices, and can integrate with traditional DL processes for a new and more complete form of onlife learning.  
  25% technology.” From a psychological point of view [8], it is possible to identify three fundamental phases to effectively involve game participants: • Motivating. The starting point of every gamification activity is to give people a reason to participate. The game and challenge mechanism are deeply rooted in the human mind and are a powerful stimulus, but to make it work best, it is essential that players have a reward, a goal, an objective that attracts attention and increases determination. The choice of benefits and rewards is essential because the more accurate it is, the greater the competitive drive that will be generated in the group. • Provide tools to participate. For gamification to work, it is necessary to provide participants with the tools to participate. The interface must be intuitive and easy to use, allowing everyone to interact and perform the tasks required to achieve the objectives, achieving a tradeoff between simplicity and effectiveness. • Kickoff. Gamification activities require a moment to establish a starting point, such as an event, team building activity, or official communication. Long-term competitions should have intermediate milestones to monitor progress, reward participants, celebrate successes, and motivate those who are struggling. Timing is crucial, as activating game mechanics simultaneously and in a coordinated manner is essential to retain the participant interest.  
  2.2 Related Work Edutainment is a form of entertainment that is designed to educate its audience about specific subjects or concepts while engaging them in an entertaining and interactive manner. The concept of edutainment has been applied to a wide range of mediums, including video games, television shows, movies, and live events. Edutainment can be particularly effective in the areas of science, technology, engineering, and mathematics (STEM) education, as well as for promoting health behaviors and environmental awareness [9–11]. One of the key elements of effective edutainment is the incorporation of game-like elements, such as points, rewards, and levels, into the learning experience [12]. This approach is based on the principles of gamification, which involves the application of game design elements to non-game contexts to motivate and engage individuals in specific behaviors [13]. With specific regard to the ITS, the use of gamification has been explored in several studies. In [14], the authors present empirical results on teaching basic Mandarin as a second language to university students using a gamification approach. In [15], the authors describe how the game “Musou Roman” can help Japanese culture enthusiasts learn the more complex aspects of the language through gamification. In [16], the authors explore key elements that can lead to successful gamification in the context of history as a learning context, guided by student motivation and based on the Octalysis framework. In [17], the authors describe and analyze some gamification methods used by the Zagreb School of Economics and Management in various technology and legal discipline courses. In [18], primary school students used the Octalysis structure proposed in [19] for educational practices. In [20], the authors analyze the application  
  Fig. 1 Virtual study buddy ITS high-level scheme  
  of gamification strategies in MOOCs. In [21], the authors present a didactic strategy that integrates gamification with traditional teaching methods for final-year students in Computer Science and Engineering. In [22], the authors successfully applied gamification to the study of the Quran to improve learning. In [23], the authors conducted an exploratory study to evaluate the effect of using gamification through interactive digital storytelling on classroom dynamics and student interaction.  
  The system analyzes every part of speech, detecting the morphology, dependency on other words present, and the taxonomy of the text to match the topics, concepts, and words present in both texts. At the end of the analysis, the app stores the metrics related to this training session in the Firebase DB. Then, it compares text and metrics of the basic knowledge with text and metrics of the current training session. The results are stored in the DB and shown to the user on the app (Fig. 6). The score is expressed in hundredths, taking into account: (i) oral exposition; (ii) equivalence of texts; (iii) similarity of the topic; (iv) percentage of acquired text knowledge; (v) time spent to rehearsal.  
  4 Conclusions This paper introduces an Intelligent Tutoring System based on machine learning and gamification concepts, with the main goal of supporting students in the educational process. Virtual Study Buddy has been released to support Distance Learning. By integrating VSB into a DL tool a significant step towards onlife education, blending online and offline education practices, can be implemented. Future framework implementations will move in this direction, increasing integration with current DL tools on one hand, and further automating interaction with students on the other, for example by automatically generating questions to evaluate their preparation. To support such DL 2.0, augmented, virtual extended and mixed reality technologies, the metaverse and social networks can be exploited to implement the onlife, phygital dimension of a brand new generation of students.  
  A Personal View on Past and Future Higher Education Nikola Zlatanov  
  Abstract In this paper, I first provide a brief overview on the history of institutional educational systems and show that universities had to evolve in order to cope with the challenges of the time. One such major challenge, which requires another round of evolution of the university, is online education. To understand the gravity of this challenge to the university, I discuss the advantages and the disadvantages of online education from the university’s and students’ perspective. Finally, I provide some predictions for the future of online higher education in the area of information technology (IT).  
  N. Zlatanov  
  that students had to dedicate their leisure time to attend schooling [1]. Consequently, those that could not afford any leisure time, could not join the schooling process. The phenomenon of dedicating one’s leisure time to schooling is present even today. Later in Roman times, the three-phased educational system was invented, comprised of an early forms of primary, secondary, and tertiary education [4]. Primary schools taught literacy and basic mathematics, secondary taught literature and arts. Finally, tertiary education in Rome, accessible only to the elites, was concerned with teaching oratory, law, and philosophy with the aim of preparing the students to become the future political elites that would run the county. The elitism of tertiary education prevailed, in a modified form, almost to the 16th century and beyond. In the begining of the middle ages, formal education was mostly performed by religious organizations [7]. As a result, in Europe, the main educational organization, in terms of quantity, was the Church. Students attended religious educational organizations in order to join the clergy. As time progressed, due to necessity, other educational organizations appeared aimed at educating those students that were not able to join the religious educational organizations [5]. These non-religous educational organizations quickly grew in numbers and attracted high numbers of students and teachers. At some point, the teachers working in these educational organizations formed unions in order to maintain high standards of teaching, as well as to preserve their interests against outside forces. One such union was the universitas magistrorum et scholarium, which means union of teachers and scholars, which is the origin of the name university as well as the origin of today’s university administrative structure. Even in the middle ages, the universities had different faculties teaching Arts, Medicine, Law, and Theology. Moreover, similar to today’s university rankings, in the middle ages different universities in Europe were famous for different faculties [6]. For example, Paris was famous for Arts and Bologna was famous for Law. The curriculum of a university course was composed of a list of texts and books [6]. Every faculty used books of authors with well established competences in the area from which they taught the courses. The university professors had to convey to the students the contents of those books without any alteration. The lectures were divided into two parts. In the first part, the professor would read the text to the students and in the second part the students would discuss the content of what was read by the professor. This teaching method invented in the medieval ages continued up to this date, with some modifications. Graduating from these universities was not easy nor cheap [6]. For example, a Law student at the University of Bologna could reach the final examination for graduation only after 8 years of study. The first phase of the final examination would involve an interview with a university professor, where the student would be questioned on the entire subject. If this stage is passed, in the next and even harder phase, the student will face the entire faculty. Only if the student passes the second phase, he would go to the third and final phase, the public phase, which was very costly. Of course, this lengthy and costly university educational process created elitism, since only a few students could attend and graduate successfully. The elitism in the tertiary education, which started as early as Roman times and continued through the middle ages, started to fade out in the 16th century, when the colonial European states required higher  
  A Personal View on Past and Future Higher Education  
  numbers of university graduates that could be sent to administer state functions in the newly acquired colonies. The above brief overview of institutional education shows how educational institutions evolved over time. This evolution was due to different challenges that educational institutions faced, such as unionizing in order to protect their interests, to restructuring in order to increase the number of graduates when that was required by the colonial states. Each time, the educational institutions adapted and reorganized in order to keep with the modern trends of the time. In fact, the current tertiary educational system is simply an adaptation of the middle age universities to modern times. At this point in time, the university is again facing a major challenge due to the penetration of the Internet and its consequences.  
  2 Current and Future Education The university educational system that we had until the wide penetration of the Internet was very similar to the university educational system established in the middle ages. One of the main differences between the two is that many more students could attend university, hence, the amount of elitism compared to the middle ages is significantly reduced. In terms of the teaching method, similar to the middle ages, each student had to come to the university campus, physically attend the lectures of the professor, physically attend labs/tutorials based on the lectures, and at the end pass the exam. Hence, that main constraint of the university educational system, up to recently, was the compulsory university onsite attendance by the students. This constraint was not forced by the university, instead, it was due to the natural order of how a student could hear and watch the lectures of a professor. With the penetration of the Internet, this constraint become obsolete, since now a student could hear and watch the lectures of a professor online [8]. Although online teaching is welcomed by the university, removing the students’ compulsory university onsite attendance, is not. This is because almost all universities have invested huge amounts of funds into building glamorous university campuses that would attract higher number of students. Hence, having a purely online university education would mean that these campuses would become vacant and thereby obsolete [9]. As a result, the huge amounts of funds invested into the campuses become would become a malinvestment. Another reason why universities resist complete online higher education is increased competition. Specifically, up to recently, the university campus enabled a university to cover a specific geographical area from which it could recruit students. Competition with other universities in that geographical area, to a large extend, was significantly reduced. Moving to a completely online education would result in the universities loosing the geographical edge. In that case, students would not be compelled to go to the closest university and could chose any online university independent of the location. Hence, completely online education would create a fierce  
  N. Zlatanov  
  competition among the universities leaving the low ranked universities in a highly disadvantaged position compared to the high ranking universities. From the student’s perspective, complete online education is highly valued since each student can study at their own pace without leaving their home [10]. However, the main disadvantage of complete online education is the inability of students to make connections with other students [11]. During onsite campus presence at faculty buildings or at student dorms, students are able to interact physically and thereby create connections that would last and benefit them for years. Such strong connections are almost impossible in the virtual reality. The future of completely online university was experienced during the government imposed lockdowns as their response to the Covid-19 pandemic. During the lockdowns, the university campuses had to be emptied and therefore the entire educational process had to move strictly online. Due to the empty campuses, the universities saw their revenues decline and their expenses either stayed the same or increased. As a result, many of the universities faced bankruptcies and turned to laying off staff in order to cope with their budget deficits. Difficulties were especially experienced by universities that did not have any financial support from governmental tax revenues, and were low ranked. However, from the lockdown experience, both the students and the teaching staff realized that complete online learning/teaching is possible for most subjects, at least in the information technology (IT) area. In fact, students and their professors in the IT area adapted to strictly online teaching to such an extent that the university administration had to force both the professors and the students to compulsory onsite attendance of courses. Currently there is an ongoing discussion among university administrators on how the university should adapt to the online learning challenges and their consequences. Many solutions are proposed and discussed. Finally, some solutions will emerge and with their implementation the current higher educational system would be adapted to a one that is suitable for the needs of the future. It is very hard to provide predictions of what these solutions would be. However, it is clear that for areas such as IT, where all of the lectures, labs, and tutorials can be completely provided online, the future is completely online education with some form of a reach in-person experience.  
  3 Conclusion Universities have adapted to challenging situations and evolved accordingly. The post lockdown period showed that completely online university education is not only doable, especcialy in the IT area, but much more efficient in terms of learning than onsite university education. However, the main disadvantages are empty campuses, the lack of strong connection between students (and staff) that are formed during onsite campus attendances. Hence, universities have to adapt and find solutions that would enable them to jointly provide online learning and reach in-person experiences at their campuses.  
  N. V. Shilov et al.  
  minds of our colleagues was more important for ruSTEP than COVID-19 story. Steering Committee is very sure that moves like these of our international colleagues are absolutely wrong and counterproductive: researchers should work together for peace and sustainability together worldwide. At least, ruSTEP is intended to continue to running regular research seminar in hybrid mode for Russian and International audience in year 2023. Later we plan to classify by topics all ruSTEP’ talks, meetings, and events, and to survey selected talks and events in more details. But in this conference paper (because of the space limitations) let us survey (in the next section) just talks in the field of program/system analysis, semantics, specification, and verification.  
  2 Selected Contributions on Analysis, Semantics, Specification, and Verification Talks on program/system analysis, semantics, specification, and verification presented on ruSTEP may be grouped (preliminary) into the following topics: • • • • • • • • • • • •  
  specification approaches, ways to define semantics of programming languages, verification conditions simplification, model reduction for efficient model checking, type-checking and inference, static analysis approaches and systems, verification of safety properties, verification of concurrent systems, verification of cyber-physical systems, verification of multi-agent system, proving verification conditions, error localization using formal methods.  
  Different approaches to formal specifications were demonstrated in talks by Aleksandr Naumchev, by Dmitry Kondratyev, and by Natalia Garanina. Aleksandr Naumchev suggested to use design-by-contract method to solve this problem. This approach is based on importance of defining program contracts during software development life cycle. The approach is to define seamless requirements and specifications for program components and its interfaces. This approach allows developers to implement more reliable and self-documented programs [22]. Talk by Dmitry Kondratyev contains an approach to loop invariants generation for loops with finite iterations over data sequences. This approach is based on replacement of variables from definite iterations by special recursive functions in verification conditions [23]. This replacement allows to avoid user-defined loop invariants in the case of finite iterations [14]. Natalia Garanina presented approach for defining Event-Driven Temporal Logic requirements. This logic allows verifying temporal and timed Properties of reactive  
  Running Regular Research Seminar Online  
  N. V. Shilov et al.  
  Talk of Nataliya Garanina contains suggestions of using bounded model checking. Proposed constraints allows checking less states [31]. Type checking and type inference may be considered as important part of formal verification in some cases. Type-based approaches were considered in talks by Andrei Klimov, by Vitaly Romanov, and by of Nikolai Kudasov. Andrei Klimov described a dependent type theory with abstractable names proposed by Pitts [24]. Vitaly Romanov presented combination of CodeBERT and Graph Neural Networks for type prediction in the case of dynamically typed programming language Python [25]. Nikolai Kudasov presented the possibility of representing various higher-order unification problems as a special case of E-unification for second-order algebra of terms. This allows presenting beta-reduction rules for various application terms, and some other eliminators as equations, and reformulate higher-order unification problems as E-unification problems with second-order equations [16]. Nikolai Kudasov described also a procedure for deciding such problems by introducing second-order mutate rule (inspired by one-sided paramodulation) and generic versions of imitate and project rules. He demonstrated a prototype Haskell implementation for syntax-generic higher-order unification based on these ideas in Haskell programming language. Verification of safety properties was considered in talks by Natalia Garanina and by Alexander Kogtenkov. Talk of Natalia Garanina contains approaches to verification safety properties in Event-Driven Temporal Logic [31]. Alexander Kogtenkov presented approach of verification such safety property as void-safety which provides a guarantee of absence of dereferencing null-pointer (or void-reference) [10]. He presented checking correctness of one of the published approaches to guarantee of void-safety (Freedom before commitment by Summers and Müller). Alexander Kogtenkov had shown through a formalization in Isabelle/HOL that this approach contains errors and then has corrected them. Formal verification of concurrent systems is an important problem because of complexity of testing of such systems. The problem was discussed in talks by Natalia Garanina, by Julio Cesar Carrasquel, and by Artem Burmyakov. As was mentioned in the above, talk by Natalia Garanina contains suggestions of verification of concurrent systems using Event-Driven Temporal Logic. Julio Cesar Carrasquel proposed using nested Petri and classic Petri nets to verify concurrent systems [5]. Artem Burmyakov described consensus problem and available solutions [4]. In particular, Artem Burmyakov demonstrated applications of the consensus problem to compare the synchronization power of various concurrent data structures (e.g., a FIFO queue) and CPU synchronization primitives for the development of concurrent non-blocking algorithms, by means of consensus numbers. Verification of cyber-physical systems was discussed in talks by Natalia Garanina and by Sergey Staroletov. Sergey Staroletov presented use of Promela language for encoding tasks and SPIN verifier for checking obtained models [29].  
  Running Regular Research Seminar Online  
  Multi-agent system verification has been considered in talks by Natalia Garanina and by Nikolay Shilov. Natalia Garanina has proposed to use ontological approach for solving this task [8]. Talk of Nikolay Shilov contains description of Mars robot puzzle solution (a multiagent approach to the Dijkstra problem) [3]. Proving verification conditions is an important problem for practical application of formal verification. This problem was presented in talks by Dmitry Kondratyev and by Kirill Ziborov. Talk of Dmitry Kondratyev contains strategies for proving automation using ACL2 theorem prover [11, 13]. The main ACL2 feature is automation of proving by rewriting (based on using lemmas) and automation of proving by induction [20]. Kirill Ziborov reported experience of use of HOL4 theorem prover [28] and auxiliary lemmas for proving smart contracts [17]. Problem of error localization using formal methods was presented in talks by Dmitry Kondratyev, by Andrey Belevantsev, and by Nikolay Shilov. Dmitry Kondratyev proposed method of error localization and its implementation in the C-lightVer verification system. This approach contains method of generation of explanations in natural language about correspondence between subformulas of verification conditions and program fragments [7]. Also this approach contains strategies of error localization [14]. These strategies are to prove formulas about program properties that may indicate presence of errors. Andrey Belevantsev presented static analysis techniques for programs written in C/C++, Java, Kotlin, Go, and C#. These techniques includes intraprocedural analysis based on value ids, interprocedural summary based analysis, static symbolic execution, and some auxiliary algorithms [2]. This approach had been implemented in the Svace static analyzer family. Nikolay Shilov presented alias calculus for a model procedural programming language MoRe that has addressable memory. This calculus has been implemented in an aliasing analysis prototype tool for MoRe [27]. Presented calculus allows finding memory leak bugs in a number of short snippets of MoRe code.  
  3 Summary and Concluding Remarks A comprehensive study of research seminars history and experience during and after the pandemic caused by COVID-19 outbreak is a topic for more systematic and longterm sociological research, definitely beyond this ruSTEP experience report. We are aware just about a single publication on the topic [6]. At the same time, community concern about computer conferencing tools for collaborative online seminars may be tracked back to XX century (e.g. [9]). In contrast, issues related to education is already a hot topic (e.g. [19]). If to speak about ruSTEP experience, it is possible to say that the seminar has considered different approaches and views to different problems that are important in practice. The seminar, being interinstitutional and connecting people from different physical locations, was planned for a hybrid mode from the start. As such outside  
  Innopolis University: An Agile and Resilient Academic Institution Navigating the Rocky Waters of the COVID-19 Pandemic Yuliya Krasylnykova, Iouri Kotorov , Jaroslav Demel, Manuel Mazzara , and Evgeny Bobrov  
  Abstract Fierce winds produced by the Fourth Industrial Revolution, socioeconomic upheavals, geopolitical strife and other black swan events can lead to dangerous water conditions threatening the business and operational continuity of organisations. The unprecedented disruptions caused by COVID-19 have significantly impacted all industries worldwide, and the education sector is no exception. The pandemic forced university leaders to find ways to stay afloat on the fly and make swift decisions to ensure an uninterrupted flow of education. One such decision was the adoption of digital technologies. Whilst many higher education institutions had to revamp their strategies and redesign their business models and processes, Innopolis University continued to operate with minimal interruption. It switched to distance learning with minor problems, and its business model remained intact. Relying on its institutional values and emulating the moves of those in agile and resilient organisations, Innopolis University was able to stand firm in the storm and become better because of it. Unequivocally, universities are not all in the same boat, but we are all in the same sea. This paper shines a light on the University’s “sea-going” experience and strategies that helped it remain buoyant, which can be valuable to any institution of tertiary education worldwide.  
  Y. Krasylnykova et al.  
  Although almost nothing can surpass relational teaching, it is not to say that online learning has no value at all [28]. In our case, it worked perfectly well with more introverted students who were likelier to give presentations and participate in class or group discussions when they were online. Also, the fun fact is that virtually all students found video-recorded lectures (as part of flipped learning) even more valuable compared to live lectures. And it is no wonder. Recorded lecture material allowed students to (re)watch the recorded lectures at their convenience and at their own speeds (using video-accelerating technologies) without distorting the pitch of the instructor’s voice. Putting testing to the test: a creative overhaul of traditional exams Formative and summative assessments are integral to the learning journey and key tools instructors use to determine whether a student has achieved learning outcomes [29]. However, COVID-19 revealed that traditional assessment practices were losing their validity in ERT and had to be updated alongside teaching and learning provision [16]. At Innopolis University, the main issue was the delivery of formal summative assessments, with remote proctoring, security and privacy as primary concerns. Despite being well aware of the potentials offered by AI-enabled assessment systems, de facto, it was neither a wise nor rapid solution we required to keep up with the pace of the pandemic. We were joshing back then that knowing what not to do with technology is a key digital competence. We decided to temporarily throw summative assessments aside and instead continue looking for more flexible, practical and effective ways of testing and grading students’ knowledge. Surprisingly, we found that smart and creative application of alternative assessment arrangements, exempli gratia, group projects, presentations, performance tasks, reflective videos, peer and self-assessments, etc., could be just as valid as pen-and-paper summative tests. This is not to mention that such substitution helped our students be less stressed, more curious and overall more involved in the academic enquiry process—something that, for us, given the complex realities of COVID-19 lockdowns, was more precious than a multitude of rubies.  
  2.3 Beyond Access: Leveraging Open Educational Resources to Support Students and Faculty in Emergency Distance Learning With the ongoing spread of the disease, libraries were forced to close their on-site services, thus limiting students’ access to learning resources [30]. Students unable to access library collections had to be content with the material they could find online [31]. The library staff of Innopolis University tried to be as responsive to the needs of its community as possible. They revised their practices and innovated their  
  ciently and accurately than humans. In general, AI can be classified into two main categories: narrow (weak AI) and general (strong AI) [19]. Narrow AI means that AI system can execute one single task, while general AI system is capable of executing any intellectual task that a human can. Creating systems that are capable of independent thought and decision making is the goal of strong AI. This kind of systems are able to adapt and improve over time (by learning). The advancements in AI have brought about numerous innovations in different industries and fields. It has the potential to change the world in keen ways, helping us to solve complex problems and making our lives easier and more productive. One of its directions is in education. One of the most remarkable ways that AI may be implemented in education is through the AI-powered virtual tutors [10]. These tutors are able to provide individual feedback according to the needs of students. Increase engagement and motivation is another feature. Additionally, AI-powered tools and platforms are being used to analyze student data and provide insights into student learning and performance. This way allows instructors to make more informed decisions about teaching and supporting students. Assessment is another sector that AI can have impact on. AI-powered tools are being used to grade students’ works and provide instant individual feedback. Implementation of these tools significantly reduce the workload of instructors and for sure increase the speed of assessment process.  
  2.1 AI-Powered Virtual Tutor AI-powered virtual tutor is a AI tool in educational system that was created with the aim of providing individual feedback and support to students. Students’ data (tests scores and past performance) are collected and analysed by this virtual tutor. Then, this data is used to make customized individual lesson plans (syllabus) and exercises. These plans are created according to each student’s needs. One of the main features of AI-powered virtual tutors is providing instant feedback on the students’ works. This may increase the engagement and leads to motivation. These virtual tutors, analysing data, can identify difficult topics/areas for each student and accordingly provide a help (invite to office hours). In addition to providing individual support, AI-powered virtual tutors can be beneficial in increasing the learning process efficiency. This can be accomplished by automating certain tasks like grading and assessment. This can free up time for educators to focus on other important tasks: providing more office hours to work with students or developing new instructional materials. It is important to note, however, that AI-powered virtual tutors should be used as a supplement to human instruction, not as a replacement. Although these virtual tutors are effective at providing feedback and individual support, but they are not able to provide emotional support, empathy, and creativity as human.  
  By student Solving the assignment  
  Assignment submission  
  By AI Understanding the student’s answers and solution  
  Fig. 1 Grading scenario by AI tutor  
  For the rest of this section, we provide a scenario: how AI-powered virtual tutors can grade mathematical assignments [20–22]. First, we may ask: Can AI-powered virtual tutors grade mathematical assignments or tests? The answer is “yes”. AI algorithms can analyze student submissions, evaluate the accuracy of solutions, and provide grades and feedback in real-time. By this, the speed of assessment process will be increased as well as the efficiency of assessment. Here is an example of how an AI-powered virtual tutor can grade a mathematical assignment. The process is shown in Fig. 1. The student solves a mathematical assignment and submits the detailed solutions solving equations. Now, the process of evaluation and grading starts. The AI-powered virtual tutor with the help of image recognition and natural language processing (NLP) understands the student’s answers and solutions [23–25]. Utilizing algorithms virtual tutor then compares the student’s solutions with the correct answers. Then evaluation process starts to check how precise are the solutions. After that, virtual tutor grades the work and provides an individual feedback on the work, particularly, on weaknesses. Finally, the student receives a grade and feedback. This allows him/her to understand where are mistakes and it improves student’s understanding of the material.  
  Teaching the Future: The Vision of AI/ChatGPT in Education  
  M. R. Bahrami et al.  
  1. Personalized Tutoring: ChatGPT can serve as a virtual tutor, providing students with immediate feedback and assistance. This helps to increase student engagement and motivation and provides support for students who may need extra help. 2. Question Answering: ChatGPT can be utilized as an AI-powered questionanswering system, allowing students to ask questions and receive accurate, concise answers. This helps to increase the efficiency of the learning process and can be particularly helpful for students who are looking for quick answers to specific questions. 3. Content Generation: ChatGPT can generate educational content, such as practice problems, summaries, and study guides. This can help to save time and effort for educators, who can focus on other important tasks, and provide students with access to high-quality educational resources.  
  3.3 Potential Benefits of Using ChatGPT in Education The use of ChatGPT in education offers numerous benefits, including: 1. Enhanced Engagement: By providing personalized feedback and support, ChatGPT can improve student engagement and motivation. This can help to keep students interested in the learning process and increase their participation. 2. Increased Efficiency: ChatGPT can automate tasks such as grading and assessment, freeing up time for educators to focus on other important tasks. This can help to increase the efficiency of the learning process and improve the quality of education. 3. Personalized Learning: ChatGPT can be used to create customized lesson plans and exercises tailored to the individual needs of each student. This allows for more effective and personalized learning and can help to increase student engagement and motivation. 4. Access to Information: ChatGPT provides students with instant access to information and knowledge, allowing them to expand their understanding and deepen their learning. This helps to increase student engagement and motivation and can lead to improved learning outcomes. In conclusion, ChatGPT represents a major advancement in the field of education and has the potential to revolutionize the way we approach learning. By providing personalized feedback and support, improving the efficiency of the learning process, and increasing access to information, this technology has the power to benefit both students and educators alike.  
  6 Conclusion The integration of AI/ChatGPT in education has the potential to bring about a positive change in the learning experience. Its ability to personalize, streamline, and facilitate education has the potential to prepare the next generation for success in the twentyfirst century. The potential benefits of using ChatGPT in education are a lot. It may improve student engagement and performance, meanwhile reducing the instructors’ workload. AI in education may provide access to education for all learners. Acknowledgements We thank Innopolis University for generously funding this endeavor.  
  R. Marouf et al.  
  to gain in-depth knowledge about a research topic and dive deeper into a research question [11]. A thesis bridges the gap between an educational stage and work or further study at the end of a curriculum [11]. In addition to being resource-intensive, thesis supervision is often a critical factor in thesis quality [12]. As the main tasks of any HEIs rest on three pillars: (1) tuition, (2) research and (3) community/ regional/ national service [7, 13]. All three pillars are equally important, but in this paper, we want to focus primarily on the tuition’s final stage, the stage of preparation for writing a thesis and narrow our focus on the main issues associated with the thesis process.  
  2 State of the Art For many years, the thesis has been regarded as an essential component of the degree programs in HEIs [7, 14]. There are several works covering supervision philosophy and professional practice that span decades [15, 16]. Alternative solutions for completing doctorate theses have been examined, as have advancements in supervision approaches [17]. While many features of postgraduate research supervision are clearly transferrable to undergraduate thesis supervision, there are some notable distinctions [18]. According to [19] undergraduate students have minimal research experience, and transitioning from classroom to independent research has presented challenges and necessitated the need for detailed guidance and supervision. The undergraduate thesis’s significantly shorter duration exacerbates the absence of prior research experience and learning of the corresponding abilities [12, 20]. Among the most significant transformations in student thinking required to transition from guided learning in large group environments to independent learning with supervision [21]. The supervisor-student relationship has a strong influence on the supervision process, conferring to several authors. According to [11, 18, 21] student and supervisor responsibilities are analysed, and their expectations are compared based on who is responsible for what in the supervision process. They found a significant mismatch between supervisors and students regarding who is primarily responsible for which components of this process (for example, selecting the topic, launching meetings, examining progress, discussing methodology, or seeking feedback). To improve results, it is highly recommended that expectations are aligned [15]. Students must meet many criteria to gain the abilities required to reach the objective, for example, they must build in-depth knowledge of a certain topic based on what they have learnt during their studies [14]. They should do a thorough literature review and develop a research question [9, 22]. To work independently, students must establish and adhere to a project plan, which is a commitment to complete each assignment [15]. Finally, students must have excellent communication skills to satisfy supervisors’ expectations [7, 23]. Supervisors also faced several issues [24, 25]. Supervisors should not only be accomplished researchers but also be conversant with research methodology [16]. Furthermore, they should enable higher education students to acquire the necessary abilities for these methodologies [7]. For example, mentoring  
  adapted from [9]  
  the thesis is perhaps the most complex and challenging aspect of education in which faculty members are involved [19]. According to [26], the complicated interaction between students and supervisors stems from three different types of supervision: (1) traditional-academic, (2) technologicalscientific, and (3) psychological. Supervisors, according to [9], may be classified into four types, as illustrated in Table 1. As previously stated, a bachelor’s or master’s thesis is regarded as the students’ final preparation for professional careers. However, we have a few cases at Innopolis University who consider thesis completion a challenge while having completed all other required components of their study. Based on the conducted interviews with students and supervisors, we will identify the reasons behind this in the next sections. The following Sect. 3 elaborates on the study design, Sect. 4 states students’ impressions and feedback conveyed via survey, Sect. 5 presents the findings from the interviews with supervisors and students, Sect. 6 discusses the main outcomes and corresponding gamified solution ideas. Finally Sect. 7 draws main conclusions.  
  3 Study Design The study participants are two groups: supervisors and supervisees. We interviewed faculty members who supervise students in Computer Science (CS) laboratories, during thesis writing, on the one hand, and supervisees, in the same academic setting, on the other hand. The rationale for targeting both groups is to obtain a holistic picture of the thesis supervision process; in addition to investigating the correlation between both the supervisor’s and supervisee’s perspectives towards the thesis supervision process. From the supervisee’s group, we collected qualitative and quantitative data from two classes. First, we interviewed 3 CS graduate students. The semi-structured interviews were conducted online via Zoom videoconferencing. The objective is  
  R. Marouf et al.  
  6 Opportunities for Gamified Thesis Supervision This study focuses on the major challenges faced by supervisors and supervisees in CS, during the thesis supervision process. Therefore, familiarization of corresponding academic settings with these challenges can provide academics with the appropriate approach to addressing them. Furthermore, such awareness of these challenges can help students who will eventually complete the work successfully, optimize time and efforts, especially when thesis projects are system building ones. On the other hand, the underlined challenges are the compass to propose solutions that can reduce the challenges during the thesis supervision for students and supervisors. “The best ideas are coming from students,” a supervisor reported. The supervisor emphasized the importance of discussing students’ research ideas and areas of interest. “When I offer my ideas, I offer my bias because I already have a solution”. Thus, the supervisor chooses to begin with the students’ areas of interest. In relation to the survey results, 17 students stated that they chose their topics of interest. Therefore, such a tendency to encourage supervisees to select their own thesis topic seems to be promoted in the academic setting. According to one postgraduate student, “Thesis writing is boring, and it will be interesting to see how students will communicate with the system”. The findings show that students have high expectations of what the supervisor should do. Furthermore, expectations on what supervisors should do is a prominent element highlighted by supervisees. Hence, the feasibility of such expectations indicate that supervisors’ workload is worth investigating in the extended version of our work. We believe that introducing gamification in the thesis supervision process has the potential of making certain issues light. As an instance, we propose a gameplay scenario when students are looking for thesis supervisors. The student’s research interests and supervisor’s expertise are each represented as a card, the more matches lead to more points and students prefer that supervisor. This would require a system (preferably digital) where supervisors and students both have the opportunity to represent their interest as digital cards. Further, a reward system can be initiated where students get points based upon how regular they are in meetings. This might reduce the tension between the supervisor and the student as the supervisor would not need to push the student for meetings and student would see their progress in this area. Rewards can also be associated with the progress in the thesis writeup. To make it more interesting and fair, students can also be provided the opportunity to send reward points to supervisors whenever they feel satisfied with supervisor’s guidance. For this gamified system we prefer cooperative gameplay setting. As in game Hanabi [27], all players cooperate and receive collective points, the students and supervisors both have to play well as their collective rewards will decide which kind of trophy (from bronze, silver, and gold) they win at the thesis submission time.  
  Thesis Supervision in Computer Science—Challenges and a Gamified …  
  7 Conclusions This paper identifies challenges of CS thesis supervision process. We conducted interviews with both supervisors and students to understand their experiences and perspectives on supervision process. The data obtained highlights the importance of regular meetings and supervisor’s guidance in the areas of referring to relevant research articles and on thesis write-up. We also realized that both groups (supervisor and student) are trying their best to achieve good outcomes and most of the issues highlighted require a little more transparent communication between the both groups. For this reason, we believe that introducing gamification can make things smooth and both groups will be able to clearly see the quality of their deliverables during the supervision process. Gamification is a light-hearted approach to reduce challenges associated with timelines, and communicating the impressions on each others work. The gamified system is also much significant as post-covid we continue to work in hybrid mode [28]. The less in-person interactions must be somehow compensated with other ways that facilitate the flow of transparent communication among the supervisors and students. This work is in progress and we continue to collect data and work on designing the mechanics for the suitable gamified system.  
  and students to maximize the potential of Zoom by humanizing the classroom experience through creating opportunities for small group interactions, between-class collaborative projects, and individualized meetings between faculty and students.  
  1 Introduction Zoom, a widespread synchronous conferencing software (SCS) has several advantages and disadvantages for teaching [1, 2]. Zoom’s features such as synchronous text chat, screen sharing, emojis and reactions, and recordings offer learning opportunities not present in face-to-face classrooms [3]. In Qatar, as elsewhere in the world, Zoom allowed for education to continue during the COVID-19 pandemic when inperson teaching was either impossible or unfeasible. To study the experience of Zoom-mediated education, a multidisciplinary group of pedagogical researchers at Qatar’s Education City (EC) trained students and alumni to conduct 22 focus groups among their peers, identifying the benefits and drawbacks of Zoom during and after the pandemic. Using Sketch Engine corpus analysis software, student focus group data was coded and synthesized to reflect student experiences of Zoom-mediated classrooms. Sketch Engine analyses revealed that Zoom-mediated classrooms were often demotivating and that student behavior such as punctuality and participation were often negatively impacted. While some students reported advantages such as ease of access to classroom learning via Zoom, many others experienced alienation from their instructors and peers in these online settings due to a lack of accountability or incentive for student classroom participation. Repetitive lecture-based instruction further exacerbated students’ feelings of dissatisfaction and demotivation. As a result, the researchers advocate a proactive stance on the part of instructors and students to maximize the potential of Zoom by humanizing the classroom experience through creating opportunities for small group interactions, between-class collaborative projects, and individualized meetings between faculty and students. These concrete actions foster closer relationships, not only between faculty and students, but also among students themselves, leading to a richer, more engaging Zoom-mediated learning environment for all.  
  2 Literature Review 2.1 Background Technology has often been seen as a barrier to forming meaningful relationships with others especially among youth. Nevertheless, during the COVID-19 pandemic, several technologies became essential to the carrying out of everyday activities such  
  3 Research Methods 3.1 Research Design Ethnographic in nature, the data for this study consisted of student and teacher narratives captured using peer-moderated focus groups, a method noted for its efficiency in collecting significant amounts of information in a relatively short period of time [16]. The resulting textual data were then analyzed both quantitatively and qualitatively using the popular, commercially available corpus linguistic software, Sketch Engine [17]. The quantitative analysis consisted of the identification of statistically salient keywords from the data set. The qualitative analysis, in contrast, involved making research-informed decisions about which keywords to further analyze based on their connection to the themes identified in the literature review above relating to student motivation in online contexts.  
  3.2 Procedure A total of twenty-two (22) student focus groups were conducted; 21 in English and one in Arabic over two distinct time periods. First, thirteen (13) focus groups were convened in December 2022. Next, nine (9) additional focus groups were convened  
  Humanizing Zoom: Lessons from Higher Education in Qatar  
  it was easy to say, “Oh, I had connection issues” if you were not in class. So now you can’t really see my connection affecting my attendance. Both of these comments appear to speak to a common theme of a lack of accountability in online classes impacting the quality of student participation.  
  5 Discussion The keyword analyses above highlight the extent to which students experienced demotivation in online classrooms. This demotivation was due to several factors ranging from boring classrooms to heavy workloads to a lack of accountability for student participation. This last issue was seen in large part as a consequence of institutional policies that allowed students to turn off their cameras and microphones during Zoom-mediated lessons. Punctuality was also discussed frequently in classrooms, and while ease of attendance was clearly noted as students merely had to log on to attend a class on time, clearly, such punctuality was not synonymous with meaningful online participation and engagement when such participation was not incentivized in any meaningful way.  
  5.1 Limitations Several limitations to the research should be noted. First, since focus group participants were not drawn from a random sampling of all of Qatar, the findings cannot be considered generalizable beyond this study. Additionally, since the population reflected a strong female bias, future research with a more equally distributed gender ratio would likely reveal different results. Third, while the study utilizes sophisticated discourse-based software to analyze the data, the thick data associated with traditional ethnography are not included here, although this would be valuable for future research. Lastly, no attempt was made in the current study to investigate and analyze class, race, religion or cultural differences amongst focus group participants. Each of these social factors could be examined in future research.  
  Author Index
7. KES AMSTA_3 conference:
Home  
   KES-AMSTA 2023 conference   
 Springer publication homepage 
 We participate at the 17th International KES Conference on AGENTS AND MULTI-AGENT SYSTEMS: TECHNOLOGIES AND APPLICATIONS  , KES-AMSTA-23. KES-AMSTA is an international scientific conference for research in the field of agent and multi-agent systems. Agents and multi-agent systems are related to a modern software paradigm which has long been recognized as a promising technology for constructing autonomous, complex and intelligent systems. Consisting of keynote talks by experts in the field, oral and poster presentations, AMSTA-23 will provide an excellent opportunity for researchers to discuss modern approaches and techniques for agent and multi-agent systems and their applications, as well as intelligent systems in the field of social networks, self-organisation and trust.  
 Proceedings    
 Smart Digital Futures  
 Smart Digital Futures  2023 is a multi-conference event co-locating five conferences on leading edge smart systems topics in a beautiful location. One conference fee gives you entry to six high-quality conferences on Innovation in Medicine and Healthcare, Human Centred Intelligent Systems, Intelligent Decision Technologies, Intelligent Interactive Multimedia Systems and Services, Agent and Multi-agent Systems, Smart Transportation Systems and Smart Education and E-Learning, and a paper published in one set of proceedings.  
 Invited sessions on "AGENT-BASED MODELING AND SIMULATION (ABMS)" and "BUSINESS PROCESS MANAGEMENT (BPM)"  
 As usual we are also organizing an invited session on "AGENT-BASED MODELING AND SIMULATION (ABMS)". Computational social science involves the use of agent-based modeling and simulation (ABMS) to study complex social systems. ABMS consists of a set of agents and a framework for simulating their decisions and interactions. ABMS is related to a variety of other simulation techniques, including discrete event simulation and distributed artificial intelligence or multi-agent systems. Although many traits are shared, ABMS is differentiated from these approaches by its focus on finding the set of basic decision rules and behavioral interactions that can produce the complex results experienced in the real world. An agent is thus a software representation of a decision-making unit. Agents are self-directed objects with specific traits and typically exhibit bounded rationality, that is, they make decisions by using limited internal decision rules that depend only on imperfect local information. In practice, each agent has only partial knowledge of other agents and each agent makes its own decisions based on the partial knowledge about other agents in the system. We welcome all kinds of papers discussing aforementioned approaches from different domains in this section.
8. KES IDT_1 conference:
क्षमा करें, इस पेज को लोड करने में कोई समस्या थी. फिर से प्रयास करें.   
 This book gathers selected papers from the KES-IDT 2023 Conference, held in Rome, Italy, on June 14–16, 2023. The book presents and discusses the latest research results and generates new ideas in the field of intelligent decision-making. The range of topics discussed is classification, prediction, data analysis, big data, data science, decision support, knowledge engineering and modeling in diverse areas such as finance, cybersecurity, economics, health, management and transportation. The problems in industry 4.0 and IoT are also addressed. The book contains several sections devoted to specific topics, such as intelligent data processing and its applications, high-dimensional data analysis and its applications, multi-criteria decision analysis—theory and applications, large-scale systems for intelligent decision-making and knowledge engineering, decision technologies and related topics in big data analysis of social and financial issues and decision-making theory for economics.    
  ज़्यादा पढ़ें     
  प्रकाशक     Springer 
  प्रकाशन की तारीख     29 मई 2023 
  भाषा     अंग्रेज़ी 
 प्रोडक्ट विवरण  
 बैक कवर से   
 This book gathers selected papers from the KES-IDT 2023 Conference, held in Rome, Italy, on June 14-16, 2023. The book presents and discusses the latest research results and generates new ideas in the field of intelligent decision-making. The range of topics discussed is classification, prediction, data analysis, big data, data science, decision support, knowledge engineering and modeling in diverse areas such as finance, cybersecurity, economics, health, management and transportation. The problems in industry 4.0 and IoT are also addressed. The book contains several sections devoted to specific topics, such as intelligent data processing and its applications, high-dimensional data analysis and its applications, multi-criteria decision analysis--theory and applications, large-scale systems for intelligent decision-making and knowledge engineering, decision technologies and related topics in big data analysis of social and financial issues and decision-making theory for economics.    
 लेखक के बारे में   
  प्रोडक्ट का विवरण  
  प्रकाशक ‏ : ‎  Springer (29 मई 2023) 
  भाषा ‏ : ‎  अंग्रेज़ी
9. KES IDT_2 conference:
Intelligent Decision Technologies    
 Proceedings of the 15th KES-IDT 2023 Conference     
 by   Czarnowski, Ireneusz     Editor     Howlett, R.J.     Editor     Jain, Lakhmi C.     Editor       
 Product description  
 This book gathers selected papers from the KES-IDT 2023 Conference, held in Rome, Italy, on June 14-16, 2023. The book presents and discusses the latest research results and generates new ideas in the field of intelligent decision-making. The range of topics discussed is classification, prediction, data analysis, big data, data science, decision support, knowledge engineering and modeling in diverse areas such as finance, cybersecurity, economics, health, management and transportation. The problems in industry 4.0 and IoT are also addressed. The book contains several sections devoted to specific topics, such as intelligent data processing and its applications, high-dimensional data analysis and its applications, multi-criteria decision analysis-theory and applications, large-scale systems for intelligent decision-making and knowledge engineering, decision technologies and related topics in big data analysis of social and financial issues and decision-making theory for economics.      
 Read more     
 Publication town  Singapore     
 Publication country   Singapore      
 Publishing date  30/05/2023     
 Edition  2023     
 Series  Smart Innovation, Systems and Technologies      
 Proceedings of Congress on Control, Robotics, and Mechatronics     
 Proceedings of 22nd International Conference on Informatics in Economy (IE 2023)     
 Modeling, Simulation and Optimization     
 Proceedings of International Conference on Artificial Intelligence and Communication Technologies (ICAICT 2023)     
 Proceedings of 22nd International Conference on Informatics in Economy (IE 2023)     
 Multidimensional Signals, Augmented Reality and Information Technologies     
 Communication and Applied Technologies     
 Sustainable Design and Manufacturing 2023     
 Sustainability in Energy and Buildings 2023     
 Information Systems for Intelligent Systems     
 Intelligent Informatics     
 Proceedings of Third International Conference in Mechanical and Energy Technology     
 International Symposium on Intelligent Informatics     
 There are no reviews yet.   
 Author  
 Czarnowski, Ireneusz     Editor     Howlett, R.J.     Editor     Jain, Lakhmi C.     Editor       
 more     
 Agents and Multi-agent Systems: Technologies and Applications 2023     
 Innovation in Medicine and Healthcare     
 Human Centred Intelligent Systems     
 Agents and Multi-agent Systems: Technologies and Applications 2023     
 Innovation in Medicine and Healthcare     
 Human Centred Intelligent Systems     
 Agents and Multi-agent Systems: Technologies and Applications 2023     
 Advances in Artificial Intelligence-Empowered Decision Support Systems     
 Agents and Multi-agent Systems: Technologies and Applications 2023     
 Innovation in Medicine and Healthcare     
 human-machine interfaces      
 Intelligent Decision Technologies      
 KES-IDT 2023 Proceedings      
 knowledge management systems      
 Human-machine Interfaces      
 Intelligent Decision Technologies      
 KES-IDT 2023 Proceedings      
 knowledge management systems      
 Extend now   30  seconds   
 Session has expired  The session was terminated due to prolonged inactivity.   
 Reload page   Log in now
10. KES_3 conference:
Polski 
 Speech at the KES 2023 conference  
 September 14, 2023  September 25, 2024      
  During the international conference KES 2023 (27th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems), research results on the use of scientific information sources in multilingual Wikipedia on various topics were presented. This year’s edition of the KES conference was organized in Athens (Greece) on September 6-8 in a hybrid mode.   
 The KES 2023 website: kes2023.kesinternational.org   
 Source of illustration: commons.wikimedia.org   
 conferences  , DBpedia  , Greece  , information quality  , KES  , OpenFact  , reliability  , Wikidata  , Wikipedia  , Włodzimierz Lewoniewski    
 Post navigation  
 Did you know …  
 Researchers’ Night – is a popular science event that takes place all over Europe every year. The aim of the event is to bring scientists and society closer together, to create an opportunity to meet, get to know each other and work together in a fun atmosphere. [Source]        
 Calendar

output:1. KEOD_0 information:
2. KEOD_1 information:
3. KEOD_2 information:
4. KEOD_3 information:
5. KES AMSTA_1 information:
6. KES AMSTA_2 information:
7. KES AMSTA_3 information:
8. KES IDT_1 information:
9. KES IDT_2 information:
10. KES_3 information:
