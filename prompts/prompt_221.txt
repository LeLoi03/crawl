input:
1. OPODIS_1 conference:
About OPODIS  
 OPODIS  is an open forum for the exchange of state-of-the-art knowledge concerning distributed computing and distributed computer systems. All aspects of distributed systems are within the scope of OPODIS, including theory, specification, design, performance, and system building. With strong roots in the theory of distributed systems, OPODIS now covers the whole range between the theoretical aspects and practical implementations of distributed systems, as well as experimentation and quantitative assessments.   
 Next edition  
 OPODIS 2024  , Lucca, Italy , 11-13 December   
 Previous editions  
 OPODIS 2023 | , | Tokyo, Japan | , | 6-8 December | dblp 
  OPODIS 2022 | , | Brussels, Belgium | , | 13-15 December | dblp
2. OPODIS_2 conference:
Microservices 
  NG-RES 
  OPODIS 
   PARMA 
  RANDOM 
 27th International Conference on Principles of Distributed Systems (OPODIS 2023)  
  Part of: | Series: | Leibniz International Proceedings in Informatics (LIPIcs) | Part of: | Conference: | International Conference on Principles of Distributed Systems (OPODIS) 
 Event  
 OPODIS 2023, December 6-8, 2023, Tokyo, Japan   
 Editors  
 Alysson Bessani            
  Publisher: Schloss Dagstuhl ‚Äì Leibniz-Zentrum f√ºr Informatik 
  DBLP: | db/conf/opodis/opodis2023 
  Access Numbers  
 Authors:  Roger Wattenhofer  
  Abstract    
 With the book Thinking Fast and Slow, Daniel Kahneman popularized the idea that the human brain can think in two different modes. The fast mode is instinctive and automatic, while the slow mode is deliberative and logical. As of 2023, one can argue that machine learning understands how to think fast. Deep neural networks are remarkably successful in rapidly classifying and regressing data. Thinking slow on the other hand is still a mystery. Large language models may provide an illusion of being able to think slow. However, prompts that need multiple deductive steps are generally beyond the capabilities of large language models. Distributed algorithms have the potential to help understanding deductive reasoning. Distributed algorithms usually consist of several little steps, iteratively applied, each step being easily learnable. As such distributed computing may provide an interesting bridge towards understanding deduction, extrapolation, reasoning, and everything else needed to think slow. In the talk, we will discuss some exciting case studies from graph generation to origami folding.   
  Cite as    
 Authors:  Hagit Attiya, Antonella Del Pozzo, Alessia Milani, Ulysse Pavloff, and Alexandre Rapetti  
  Abstract    
 Auditability allows to track all the read operations performed on a register. It abstracts the need of data owners to control access to their data, tracking who read which information. This work considers possible formalizations of auditing and their ramification for the possibility of providing it. The natural definition is to require a linearization of all write, read and audit operations together (atomic auditing). The paper shows that atomic auditing is a powerful tool, as it can be used to solve consensus. The number of processes that can solve consensus using atomic audit depends on the number of processes that can read or audit the register. If there is a single reader or a single auditor (the writer), then consensus can be solved among two processes. If multiple readers and auditors are possible, then consensus can be solved among the same number of processes. This means that strong synchronization primitives are needed to support atomic auditing. We give implementations of atomic audit when there are either multiple readers or multiple auditors (but not both) using primitives with consensus number 2 (swap and fetch&add). When there are multiple readers and multiple auditors, the implementation uses compare&swap. These findings motivate a weaker definition, in which audit operations are not linearized together with read and write operations (regular auditing). We prove that regular auditing can be implemented from ordinary reads and writes on atomic registers.   
  Cite as    
 Authors:  Caterina Feletti, Carlo Mereghetti, and Beatrice Palano  
  Abstract    
 We study the Uniform Circle Formation (UCF) problem for a distributed system of n robots which are required to displace on the vertices of a regular n-gon. We consider a well-studied model of autonomous, anonymous, mobile robots that act on the plane through Look-Compute-Move cycles. Moreover, robots are unaware of the cardinality of the system, they are punctiform, completely disoriented, opaque, and luminous. Collisions among robots are not tolerated. In the literature, the UCF problem has been solved for such a model by a deterministic algorithm in the asynchronous mode, using a constant amount of light colors and ùí™(n) epochs in the worst case. In this paper, we provide an improved algorithm for solving the UCF problem for asynchronous robots, which uses ùí™(log n) epochs still maintaining a constant amount of colors.   
  Cite as    
 Authors:  Hagit Attiya and Jennifer L. Welch  
  Abstract    
 Algorithms to solve fault-tolerant consensus in asynchronous systems often rely on primitives such as crusader agreement, adopt-commit, and graded broadcast, which provide weaker agreement properties than consensus. Although these primitives have a similar flavor, they have been defined and implemented separately in ad hoc ways. We propose a new problem called connected consensus that has as special cases crusader agreement, adopt-commit, and graded broadcast, and generalizes them to handle multi-valued inputs. The generalization is accomplished by relating the problem to approximate agreement on graphs. We present three algorithms for multi-valued connected consensus in asynchronous message-passing systems, one tolerating crash failures and two tolerating malicious (unauthenticated Byzantine) failures. We extend the definition of binding, a desirable property recently identified as supporting binary consensus algorithms that are correct against adaptive adversaries, to the multi-valued input case and show that all our algorithms satisfy the property. Our crash-resilient algorithm has failure-resilience and time complexity that we show are optimal. When restricted to the case of binary inputs, the algorithm has improved time complexity over prior algorithms. Our two algorithms for malicious failures trade off failure resilience and time complexity. The first algorithm has time complexity that we prove is optimal but worse failure-resilience, while the second has failure-resilience that we prove is optimal but worse time complexity. When restricted to the case of binary inputs, the time complexity (as well as resilience) of the second algorithm matches that of prior algorithms. The contributions of the paper are first, a deeper insight into the connections between primitives commonly used to solve the fundamental problem of fault-tolerant consensus, and second, implementations of these primitives that can contribute to improved consensus algorithms.   
  Cite as    
 Authors:  Jamison W. Weber, Tishya Chhabra, Andr√©a W. Richa, and Joshua J. Daymude  
  Abstract    
 Individual modules of programmable matter participate in their system‚Äôs collective behavior by expending energy to perform actions. However, not all modules may have access to the external energy source powering the system, necessitating a local and distributed strategy for supplying energy to modules. In this work, we present a general energy distribution framework for the canonical amoebot model of programmable matter that transforms energy-agnostic algorithms into energy-constrained ones with equivalent behavior and an ùí™(n¬≤)-round runtime overhead - even under an unfair adversary - provided the original algorithms satisfy certain conventions. We then prove that existing amoebot algorithms for leader election (ICDCN 2023) and shape formation (Distributed Computing, 2023) are compatible with this framework and show simulations of their energy-constrained counterparts, demonstrating how other unfair algorithms can be generalized to the energy-constrained setting with relatively little effort. Finally, we show that our energy distribution framework can be composed with the concurrency control framework for amoebot algorithms (Distributed Computing, 2023), allowing algorithm designers to focus on the simpler energy-agnostic, sequential setting but gain the general applicability of energy-constrained, asynchronous correctness.   
  Cite as    
 Authors:  Lewis Tseng and Callie Sardina  
  Abstract    
 This paper studies the design of Byzantine consensus algorithms in an asynchronous single-hop network equipped with the "abstract MAC layer" [DISC09], which captures core properties of modern wireless MAC protocols. Newport [PODC14], Newport and Robinson [DISC18], and Tseng and Zhang [PODC22] study crash-tolerant consensus in the model. In our setting, a Byzantine faulty node may behave arbitrarily, but it cannot break the guarantees provided by the underlying abstract MAC layer. To our knowledge, we are the first to study Byzantine faults in this model. We harness the power of the abstract MAC layer to develop a Byzantine approximate consensus algorithm and a Byzantine randomized binary consensus algorithm. Both of our algorithms require only the knowledge of the upper bound on the number of faulty nodes f, and do not require the knowledge of the number of nodes n. This demonstrates the "power" of the abstract MAC layer, as consensus algorithms in traditional message-passing models require the knowledge of both n and f. Additionally, we show that it is necessary to know f in order to reach consensus. Hence, from this perspective, our algorithms require the minimal knowledge. The lack of knowledge of n brings the challenge of identifying a quorum explicitly, which is a common technique in traditional message-passing algorithms. A key technical novelty of our algorithms is to identify "implicit quorums" which have the necessary information for reaching consensus. The quorums are implicit because nodes do not know the identity of the quorums - such notion is only used in the analysis.   
  Cite as    
 Authors:  Orestis Alpos, Ignacio Amores-Sesar, Christian Cachin, and Michelle Yeo  
  Abstract    
 Traditional blockchains grant the miner of a block full control not only over which transactions but also their order. This constitutes a major flaw discovered with the introduction of decentralized finance and allows miners to perform MEV attacks. In this paper, we address the issue of sandwich attacks by providing a construction that takes as input a blockchain protocol and outputs a new blockchain protocol with the same security but in which sandwich attacks are not profitable. Furthermore, our protocol is fully decentralized with no trusted third parties or heavy cryptography primitives and carries a linear increase in latency and minimum computation overhead.   
  Cite as    
 Authors:  Ramy Fakhoury, Anastasia Braginsky, Idit Keidar, and Yoav Zuriel  
  Abstract    
 In recent years, we begin to see Java-based systems embrace off-heap allocation for their big data demands. As of today, these system rely on simple ad-hoc garbage-collection solutions, which restrict the usage of off-heap data. This paper introduces the abstraction of safe off-heap memory allocation and reclamation (SOMAR), a thread-safe memory allocation and reclamation scheme for off-heap data in otherwise managed environments. SOMAR allows multi-threaded Java programs to use off-heap memory seamlessly. To realize this abstraction, we present Nova, Novel Off-heap Versioned Allocator, a lock-free SOMAR implementation. Our experiments show that Nova can be used to store off-heap data in Java data structures with better performance than ones managed by Java‚Äôs automatic GC. We further integrate Nova into the open-source Oak concurrent map library, which allows Oak to reclaim keys while the data structure is being accessed.   
  Cite as    
 Authors:  Shalom M. Asbell and Eric Ruppert  
  Abstract    
 The amortized step complexity of operations on all previous lock-free implementations of double-ended queues is linear in the number of processes. This paper presents the first concurrent double-ended queue where the amortized step complexity of each operation is polylogarithmic. Since a stack is a special case of a double-ended queue, this is also the first concurrent stack with polylogarithmic step complexity. The implementation is wait-free and the amortized step complexity is O(log¬≤ p + log q) per operation, where p is the number of processes and q is the size of the double-ended queue.   
  Cite as    
 Authors:  Tomer Lev Lehman, Hagit Attiya, and Danny Hendler  
  Abstract    
 Recoverable algorithms tolerate failures and recoveries of processes by using non-volatile memory. Of particular interest are self-implementations of key operations, in which a recoverable operation is implemented from its non-recoverable counterpart (in addition to reads and writes). This paper presents two self-implementations of the swap operation. One works in the system-wide failures model, where all processes fail and recover together, and the other in the independent failures model, where each process crashes and recovers independently of the other processes. Both algorithms are wait-free in crash-free executions, but their recovery code is blocking. We prove that this is inherent for the independent failures model. The impossibility result is proved for implementations of distinguishable operations using interfering functions, and in particular, it applies to a recoverable self-implementation of swap.   
  Cite as    
 Authors:  Jo√£o Paulo Bezerra and Petr Kuznetsov  
  Abstract    
 The last decade has seen a variety of Asset-Transfer systems designed for decentralized environments. The major problem these systems address is double-spending, and solving it inherently imposes strong trust assumptions on the system participants. In this paper, we take a non-orthodox approach to the double-spending problem that might suit better realistic environments in which these systems are to be deployed. We consider the decentralized trust setting, where each user may independently choose who to trust by forming their local quorums. In this setting, we define k-Spending Asset Transfer, a relaxed version of asset transfer which bounds the number of times a system participant may spend an asset it received. We establish a precise relationship between the decentralized trust assumptions and k, the optimal spending number of the system.   
  Cite as    
 Authors:  Hagit Attiya and Jennifer L. Welch  
  Abstract    
 We study the worst-case time complexity of solving two agreement problems, consensus and broadcast, in systems with n processes subject to no more than t process failures. In both problems, correct processes must decide on a common value; in the consensus problem, each process has an input and if the inputs of correct processes are all the same, then that must be the common decision, whereas in the broadcast problem, only one process (the sender) has an input and if the sender is correct, then its input must be the common decision. We focus on systems where there is an upper bound Œî on the message delivery time but it is expected that typically, messages arrive much faster, say within some time d. While Œî may or may not be known in advance, d is inherently unknown and specific to each execution. The goal is to design deterministic algorithms whose running times have minimal to no dependence on Œî, a property called responsiveness. We present a generic algorithm transformation that, when applied to appropriate eventually-synchronous consensus (or broadcast) algorithms, results in consensus (or broadcast) algorithms for send omission failures, authenticated Byzantine failures, and unauthenticated Byzantine failures whose running times have no dependence on Œî; their worst-case time complexities are all O(td), which is asymptotically optimal. The algorithm for send omission failures requires n > 2t, while those for Byzantine failures, both authenticated and unauthenticated, require n > 3t. The failure-resilience of the unauthenticated Byzantine algorithm is optimal. For authenticated Byzantine failures, existing agreement algorithms provide worst-case time complexity O(t Œî) when n is at most 3t. (When n ‚â§ 2t, broadcast is solvable while consensus is not.) We prove a lower bound on the worst-case time complexity of ‚åä(3t-n)/2‚åã d + Œî when n is at most 3t. Although lower bounds of Œî and (t+1)d were already known, our new lower bound indicates that, at least when n ‚â§ 2t, it is impossible for an algorithm to pay these bounds in parallel.   
  Cite as    
 Authors:  Giuseppe A. Di Luna, Paola Flocchini, Giuseppe Prencipe, and Nicola Santoro  
  Abstract    
 In this paper we investigate the problem of searching for a black hole in a dynamic graph by a set of scattered agents (i.e., the agents start from arbitrary locations of the graph). The black hole is a node that silently destroys any agent visiting it. This kind of malicious node nicely models network failures such as a crashed host or a virus that erases the visiting agents. The black hole search problem is solved when at least one agent survives, and it has the entire map of the graph with the location of the black hole. We consider the case in which the underlining graph is a dynamic 1-interval connected ring: a ring graph in which at each round at most one edge can be missing. We first show that the problem cannot be solved if the agents can only communicate by using a face-to-face mechanism: this holds for any set of agents of constant size, with respect to the size n of the ring. To circumvent this impossibility we consider agents equipped with movable pebbles that can be left on nodes as a form of communication with other agents. When pebbles are available, three agents can localize the black hole in O(n¬≤) moves. We show that such a number of agents is optimal. We also show that the complexity is tight, that is Œ©(n¬≤) moves are required for any algorithm solving the problem with three agents, even with stronger communication mechanisms (e.g., a whiteboard on each node on which agents can write messages of unlimited size). To the best of our knowledge this is the first paper examining the problem of searching a black hole in a dynamic environment with scattered agents.   
  Cite as    
 Authors:  Vincent Kowalski, Achour Most√©faoui, and Matthieu Perrin  
  Abstract    
 The construction of the atomic register abstraction over crash-prone asynchronous message-passing systems has been extensively studied since the founding work of Attiya, Bar-Noy, and Dolev. It has been shown that t < n/2 (where t is the maximal number of processes that may be faulty) is a necessary and sufficient requirement to build an atomic register. However, little attention has been paid to systems where faulty processes may exhibit a Byzantine behavior. This paper studies three definitions of linearizable single-writer multi-reader registers encountered in the state of the art: Read/Write registers whose read perations return the last written value, Read/Write-Increment registers whose read perations return both the last written value and the number of previously written values, and Read/Append registers whose read perations return the sequence of all previously written values. More specifically, it compares their computing power and the necessary and sufficient conditions on the maximum ratio t/n which makes it possible to build reductions from one register to another. Namely, we prove that t < n/3 is necessary and sufficient to implement a Read/Write-Increment register from Read/Write registers whereas this bound is only t < n/2 for a reduction from a Read/Append register to Read/Write-Increment registers. Reduction algorithms meeting these bounds are also provided.   
  Cite as    
  TGDK ‚Äì Transactions on Graph Data and Knowledge 
 ¬© 2023-2024 Schloss Dagstuhl ‚Äì LZI GmbH  Imprint  Privacy  Contact
3. OPODIS_3 conference:
OPODIS 2023    
 Home 
  Call for Papers  
 OPODIS is an open forum for the exchange of state-of-the-art knowledge on distributed computing and distributed computer systems. OPODIS aims at having a balanced program that combines theory and practice of distributed systems. OPODIS 2023 solicits papers in all aspects of distributed systems, including theory, specification, design, system building, and performance.  
 Topics of interest include, but are not limited to:  
 Double-blind review  
 We will use double-blind peer review. All submissions must be anonymous. We will use a somewhat relaxed implementation of double-blind peer review: you are free to disseminate your work through arXiv and other online repositories and give presentations on your work as usual. However, please make sure you do not mention your own name or affiliation in the submission, and please do not include obvious references that reveal your identity. A reviewer who has not previously seen the paper should be able to read it without accidentally learning the identity of the authors. Please feel free to ask the PC chairs ( opodis_pc_chairs@coord.c.titech.ac.jp  ) if you have any questions about the double-blind policy of OPODIS 2023.  
 Submissions  
 Submissions must be in English, in pdf format. 
  Submissions must be prepared using the | LaTeX style templates for LIPIcs | , choosing the A4 paper option. 
  Length: a submission must not exceed 15 pages, including everything but the references. Unlimited pages are allowed for references. 
  Paper information: the following information must appear in the first page of the submission: the title of the paper a list of keywords an abstract summarizing the contributions of the submission and whether or not the paper is eligible for the best student paper award. 
  Appendix: the submission may contain a clearly-marked appendix, which will be read at the discretion of the reviewers. 
  Ready for double-blind reviewing (see above). 
 The submission must contain a clear presentation of the merits of the paper, including discussion of its importance, prior work, and an outline of key technical ideas and methods used to achieve the main claims. All of the ideas necessary for an expert to fully verify the central claims in the submission, including experimental results, should be included in the submission. A submission must report on original research that has not previously appeared in a journal or conference with published proceedings. It should not be concurrently submitted to such a journal or conference. Any overlap with a published or concurrently submitted paper must be clearly indicated. The Program Chairs reserve the right to reject submissions that are out of scope, of clearly inferior quality, or that violate the submission guidelines. Each of the remaining papers will undergo a thorough reviewing process.  
 Publication  
 OPODIS has post-proceedings published by Leibniz International Proceedings in Informatics (LIPIcs) in gold open access mode. The proceedings become available online, free of charge, after the conference. Preliminary versions of the proceedings will be available to participants at the conference electronically. The camera-ready version of a paper must have the same format and be of the same length as its main part of the submitted version.  
 Awards  
 Awards will be given to the best paper and the best student paper (i.e., primarily written by a student). Eligibility for the best student paper award should be clearly indicated on the first page.  
 Registration and presentation (NEW!)  
 At least one author of every accepted paper is expected to register (as a regular registration) for the conference and present the work on-site at the conference.  
  Important Dates  
 (all dates are " anywhere on earth  ")  
 Abstract registration: | Monday, August 28, 2023  Monday, September 4, 2023 
  Submission deadline: | Friday, September 1, 2023  Friday, September 8, 2023 
  Acceptance notification: | Sunday, October 29, 2023 
  Final version due: | Thursday, November 9, 2023 
  Conference: | Wednesday-Friday, December 6-8, 2023 
  OPODIS'23 is an open forum for the exchange of state-of-the-art knowledge concerning distributed computing and distributed computer systems. All aspects of distributed systems are within the scope of OPODIS, including theory, specification, design, performance, and system building.  
 Useful Links  
 Tokyo Tech 
  OPODIS conference series 
 Contact Us  
 OPODIS 2023 Technical Program Chairs  
  Email:  opodis_pc_chairs@coord.c.titech.ac.jp
4. OP_0 conference:
Job Board 
  Internships 
  Prizes and Awards | Deadline Calendar 
  SIAM Fellows Program 
  Policy & Procedures 
  SIAM Engage Online Community 
  Ways to Participate | Serve on Committees 
  Become an Author, Editor, or Referee 
  Nominate for Prizes 
  Network and Present at a Conference 
  Write for SIAM News 
  Ways to Support | Donate to SIAM 
 Event Details  
 May 31 ‚Äì  June 3 , 2023 
  Seattle, Washington, U.S. 
 This is the conference of the SIAM Activity Group on Optimization  .   
 This conference is co-located with SIAM Conference on Applied and Computational Discrete Algorithms (ACDA23)  .   
 The SIAM Conference on Optimization will feature the latest research in theory, algorithms, software and applications for optimization problems. The conference brings together mathematicians, operations researchers, computer scientists, engineers, software developers and practitioners, thus providing an ideal environment to share new ideas and important problems among specialists and users of optimization in academia, government, and industry.  
 Organizing Committee Co-chairs  
 Coralia Cartis  ,University of Oxford, United Kingdom
5. OP_1 conference:
Job Board 
  Internships 
  Prizes and Awards | Deadline Calendar 
  SIAM Fellows Program 
  Policy & Procedures 
  SIAM Engage Online Community 
  Ways to Participate | Serve on Committees 
  Become an Author, Editor, or Referee 
  Nominate for Prizes 
  Network and Present at a Conference 
  Write for SIAM News 
  Ways to Support | Donate to SIAM 
 Event Details  
 April 27 ‚Äì  29 , 2023 
  Minneapolis, Minnesota, U.S. 
 This conference is sponsored by the SIAM Activity Group on Data Science  .   
 Data mining is the computational process for discovering valuable knowledge from data ‚Äì the core of Data Science. It has enormous application in numerous fields, including science, engineering, healthcare, business, and medicine. Typical datasets in these fields are large, complex, and often noisy. Extracting knowledge from these datasets requires the use of sophisticated, high-performance, and principled analysis techniques and algorithms, which are based on sound theoretical and statistical foundations. These techniques in turn require implementations on high performance computational infrastructure that are carefully tuned for performance. Powerful visualization technologies along with effective user interfaces are also essential to make data mining tools appealing to researchers, analysts, data scientists and application developers from different disciplines, as well as usable by stakeholders.  
 The SDM conference provides a venue for researchers who are addressing these problems to present their work in a peer-reviewed forum. It also provides an ideal setting for graduate students to network and get feedback for their work (as part of the doctoral forum). Everyone new to the field can also learn about cutting-edge research by hearing outstanding invited speakers and attending presentations and tutorials (included with conference registration). A set of focused workshops is also held on the last day of the conference. The proceedings of the conference are published in archival form and are also made available online.  
 General Co-chairs  
 Shashi Shekhar  , University of Minnesota, U.S
6. OP_2 conference:
2023  
  SIAM Conference on Optimization  
  Tutorial on Riemannian Optimization  
 Here are the slides used during the tutorial  .  
 These are the Manopt example codes for Max-Cut  in Matlab.  
 The tutorial builds on my book published by Cambridge University Press  , 2023.  
 Go to the book webpage  for more info the free pre-publication pdf, lecture material (videos and slides of a full semester course) and exercises with solutions.  
 For software, check out the Manopt toolboxes, in Matlab  , Python  and Julia  . You can also find a list of much more Riemannian optimization software here  .  
 Last update: June 4, 2023.
7. OSDI_0 conference:
17th USENIX Symposium on Operating Systems Design and Implementation  
 July 10‚Äì12, 2023   
 Boston, MA, USA   
 Sponsored by USENIX in cooperation with ACM SIGOPS   
  Thanks to those who joined us in Boston for the 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI '23)  . We hope you enjoyed the event.  
 OSDI brings together professionals from academic and industrial backgrounds in what has become a premier forum for discussing the design, implementation, and implications of systems software. The symposium emphasizes innovative research as well as quantified or insightful experiences in systems design and implementation.  
 As part of our commitment to open access to research, the full proceedings and presentation slides are free and open to the public on the technical sessions  page. Videos are posted within a few weeks of the end of the event.  
 Registration Information  
 Early Bird Registration Deadline: Friday, June 23, 2023   
 See the Registration Information page  for details.  
 Authors and Speakers  
 Please contact the Conference Department  for registration information.  
 Discounts  
 In addition to our member discounts  , USENIX offers several discounts  to help you to attend OSDI '23.  
 Coronavirus/COVID-19 Health and Safety Plan  
 Please review USENIX's Coronavirus/COVID-19 Health and Safety Plan  prior to registering for the event. We appreciate your willingness to help safeguard your fellow event attendees and respect their personal safety choices.  
 We encourage you to learn more about USENIX's values  and how we put them into practice at our conferences.  
 Accessibility Requests  
 USENIX is committed to ensuring that our meetings are fully accessible to all attendees. Visit our accessibility information page  to find out about options and how to make a request. To ensure that we meet your needs, please make your request by Monday, June 19, 2023  . We cannot guarantee that we can meet requests received after this date, although we will make every reasonable effort to do so.  
 Press Registration and Information  
 If you are an accredited journalist, please contact Melanie Ensign, Discernible Inc., for a complimentary registration code: melanie@usenix.org  .  
 Refunds and Cancellations  
 The cancellation deadline is Monday, June 26, 2023  . Please review the USENIX Registration Substitution and Cancellation Policy  for more information.  
 Venue  
 Hotel Discount Deadline: Friday, June 23, 2023   
 Special Attendee Room Rate  
 The group rate is available until Friday, June 23, 2023,  or until the block sells out, whichever occurs first. After this date, contact the hotel directly to inquire about room availability.  
 Room Sharing  
 USENIX utilizes Google Groups  to facilitate room sharing. You can sign up for free to find attendees with whom you can share a hotel room, taxi, etc. Please include "OSDI '23"  in the subject line when posting a new room share request.  
 About OSDI '23  
 Conference Policies   
  Code of Conduct   
 Conference Sponsorship  
 The acceptance of any organization as a sponsor does not imply explicit or implicit approval by USENIX of the donor organization‚Äôs values or actions. In addition, sponsorship does not provide any control over conference program content. Questions? Contact the Sponsorship Department  .  
 Attend | Registration Information
8. OSDI_1 conference:
Questions 
 OSDI '23 Technical Sessions  
 All the times listed below are in Eastern Daylight Time (EDT). All sessions will be held in the Grand Ballroom Complex unless otherwise noted.   
 Papers and full proceedings are available for download below to registered attendees now and will be available to everyone beginning Monday, July 10. Paper abstracts are available to everyone now. Copyright to the individual works is retained by the author[s].  
 Proceedings Front Matter  
 Proceedings Cover  | Title Page and List of Organizers  | Message from the Program Co-Chairs  | Table of Contents   
 (Registered attendees: Sign in  to your USENIX account to download these files.)  
  OSDI '23 Attendee List (PDF)    
  OSDI '23 Wednesday Paper Archive (30 MB ZIP)    
 View mode:  condensed 
  Expanded 
 Monday, July 10   
 Continental Breakfast  
 OSDI '23 and USENIX ATC '23 Joint Keynote Address  
 Sky Computing   
 Ion Stoica, University of California, Berkeley   
 Available Media   
 As the number of processor cores increases, the efficiency of accessing shared variables through the lock-unlock method decreases. A NUMA-aware algorithm, which only considers the transmission delay between processors, may not fully utilize the connection network of a multi-core processor. This limits the scalability of a multi-core processor due to the large amount of low- and variable-cost data sharing between cores. The problem is that the reduction in communication cost cannot compensate for the increase in the time complexity of the spinlocks, and the farthest transmission distance becomes longer with more cores.  
 We propose a method called Routing on Network-on-chip (RON) to minimize the communication cost between cores by using a routing table and pre-calculating an optimized locking-unlocking order. RON delivers locks and data in a one-way circular manner among cores to (1) minimize global data movement cost and (2) achieve bounded waiting time. Microbenchmarks provide quantitative analysis, while multi-core benchmarks show performance under various workloads.  
 In terms of user space performance, RON improves the performance of Google LevelDB by 22.1% and 24.2% compared to ShflLock and C-BO-MCS, respectively. In the kernel space, RON is 1.8 times faster than using ShflLock for Google LevelDB. RON-plock solves the problem of oversubscription with constant space complexity and achieves 3.7 times and 18.9 times better performance than ShflLock-B and C-BO-MCS-B, respectively.  
 Triangulating Python Performance Issues with SCALENE   
 Emery D. Berger, Sam Stern, and Juan Altmayer Pizzorno, University of Massachusetts Amherst   
 Awarded Best Paper!  
 Available Media   
 Performance debugging is notoriously elusive‚Äîreal-world performance problems are rarely clear-cut failures, but manifest through the accumulation of fine-grained symptoms. Oftentimes, it is challenging to determine performance anomalies‚Äîabsolute measures are unreliable, as system performance is inherently relative to workloads. Existing techniques focus on identifying absolute predicates that deviate between executions, which limits their application to performance problems.  
 This paper introduces relational debugging, a new technique that automatically pinpoints the root causes of performance problems. The core idea is to capture and reason out relations between fine-grained runtime events. We show that relations provide immense utilities to explain performance anomalies and locate root causes. Relational debugging is highly effective with a minimal two executions (a good and a bad run), eliminating the pain point of producing and labeling many different executions required by traditional techniques.  
 Available Media   
 Authenticated storage access is the performance bottleneck of a blockchain, because each access can be amplified to potentially O  (log n  ) disk I/O operations in the standard Merkle Patricia Trie (MPT) storage structure. In this paper, we propose a multi-Layer Versioned Multipoint Trie (LVMT), a novel high-performance blockchain storage with significantly reduced I/O amplifications. LVMT uses the authenticated multipoint evaluation tree (AMT) vector commitment protocol to update commitment proofs in constant time. LVMT adopts a multi-layer design to support unlimited key-value pairs and stores version numbers instead of value hashes to avoid costly elliptic curve multiplication operations. In our experiment, LVMT outperforms the MPT in real Ethereum traces, delivering read and write operations six times faster. It also boosts blockchain system execution throughput by up to 2.7 times.  
 Honeycomb: Secure and Efficient GPU Executions via Static Validation   
 Graphics Processing Units (GPUs) unlock emerging use cases like large language models and autonomous driving. They process a large amount of sensitive data, where security is of critical importance. GPU Trusted Execution Environments (TEEs) generally provide security to GPU applications with modest overheads. Recent proposals for GPU TEEs are promising, but many of them require hardware changes that have a long lead time to deploy in production environments.  
 This paper presents Honeycomb, a software-based, secure and efficient TEE for GPU applications. The key idea of Honeycomb is to leverage static analysis to validate the security of GPU applications at load time. Co-designing with the CPU TEE, as well as adding OS and driver support, Honeycomb is able to remove both the OS and the driver from the trusted computing base (TCB). Validation also ensures that all applications inside the system are secure, enabling a concise and secure approach to exchange data in plaintext via shared device memory on the GPU.  
 Available Media   
 This paper introduces Nimble, a cloud service that helps applications running in trusted execution environments (TEEs) to detect rollback attacks (i.e., detect whether a data item retrieved from persistent storage is the latest version). To achieve this, Nimble realizes an append-only ledger service by employing a simple state machine running in a TEE in conjunction with a crash fault-tolerant storage service. Nimble then replicates this trusted state machine to ensure the system is available even if a minority of state machines crash. A salient aspect of Nimble is a new reconfiguration protocol that allows a cloud provider to replace the set of nodes running the trusted state machine whenever it wishes‚Äîwithout affecting safety. We have formally verified Nimble‚Äôs core protocol in Dafny, and have implemented Nimble such that its trusted state machine runs in multiple TEE platforms (Intel SGX and AMD SNP-SEV). Our results show that a deployment of Nimble on machines running in different availability zones can achieve from tens of thousands of requests/sec with an end-to-end latency of under 3.2 ms (based on an in-memory key-value store) to several thousands of requests/sec with a latency of 30ms (based on Azure Table).  
 Kerveros: Efficient and Scalable Cloud Admission Control   
 Available Media   
 The infinite capacity of cloud computing is an illusion: in reality, cloud providers cannot always have enough capacity of the right type, in the right place, at the right time to meet all demand. Consequently, cloud providers need to implement admission-control policies to ensure accepted capacity requests experience high availability. However, admission control in the public cloud is hard due to dynamic changes in both supply and demand: hardware might become unavailable, and actual VM consumption could vary for a variety of reasons including tenant scale-outs and fulfillment of VM reservations made by customers ahead of time. In this paper, we design and implement Kerveros, a flexible admission-control system that has three desired properties: i) high computational scalability to handle a large inventory, ii) accurate capacity provisioning for high VM availability, and iii) good packing efficiency to optimize resource usage. To achieve this, Kerveros uses novel bookkeeping techniques to quickly estimate the capacity available for incoming VM requests. Our system has been deployed in Microsoft Azure. Results from both simulations and production confirm that Kerveros achieves more than four nines of availability while sustaining request processing latencies of a few milliseconds.  
 Security and Performance in the Delegated User-level Virtualization   
 Available Media   
 Virtual machines are the basis of resource isolation in today's public clouds, yet the security risks of entrusting that isolation to a cloud provider's hypervisor are substantial. Such concerns have motivated the design of hardware extensions for "confidential VMs" that seek to remove the hypervisor from the trusted computing base by adding a highly-privileged firmware layer that checks hypervisor actions, and supports memory encryption and remote attestation. However, the hypervisor retains control of resource management and observes associated actions of the guest including nested page table faults and CPU scheduling, and thus confidential VMs remain vulnerable to an ever-changing variety of hypervisor-level side channel attacks. Bare-metal cloud servers avoid such leaks, but remain a niche due to the high cost of dedicated hardware.  
 OSDI '23 Poster Session and Reception  
 Sponsored by Amazon   
 Back Bay Ballroom  
 Would you like to share a provocative opinion, interesting preliminary work, or a cool idea that will spark discussion at this year's OSDI? The poster session is the perfect venue to introduce such new or ongoing work. Poster presenters will have the opportunity to discuss their work, get exposure, and receive feedback from other attendees during the in-person evening reception. View the list of accepted posters  .  
 Tuesday, July 11   
 Continental Breakfast  
 We present Chardonnay, a scalable, on-disk, multi-versioned transactional key-value store optimized for single datacenter deployments with fast 2PC. Chardonnay has a general  interface supporting point reads, scans, and writes within multi-step strictly serializable ACID transactions. The key mechanism underlying Chardonnay's design is strongly consistent snapshot reads on commodity hardware, using a novel lock-free read protocol. Chardonnay uses this protocol to cheaply determine the read-write sets of queries, enabling Chardonnay to transparently prefetch data needed for a transaction prior to the execution of the transaction and the acquisition of locks. This enables Chardonnay to achieve fast  transactions by minimizing contention, and avoids aborts due to deadlocks by ordering lock requests.  
 ScaleDB: A Scalable, Asynchronous In-Memory Database   
 Available Media   
 Approximate similarity queries on high-dimensional vector indices have become the cornerstone for many critical online services. An increasing need for more sophisticated vector queries requires integrating vector search systems with relational databases. However, high-dimensional vector indices do not exhibit monotonicity, a critical property of conventional indices. The lack of monotonicity forces existing vector systems to rely on monotonicity-preserving tentative indices, set up temporarily for a target vector's TopK nearest neighbors, to facilitate queries. This leads to suboptimal performance due to the difficulty to predict the optimal K.  
 We demonstrate that hardware management of a tiered memory system offers better performance for many applications than current methods of software management. Hardware management treats the fast memory as a cache on slower memory. The advantages are that caching can be done at cache line granularity and that data appears in fast memory as soon as it is accessed. The potential for cache conflicts has, however, led previous works to conclude these hardware methods generally perform poorly.  
 In this paper, we show that the use of low-overhead conflict avoidance techniques eliminates conflicts almost entirely and thereby address the above limitation. We explore two techniques. One technique simply tries to avoid conflicts between pages at page allocation time. The other technique relies on sampling memory accesses to avoid specifically conflicts between hot pages, at page allocation time and by dynamic remapping in the case of conflicts between hot pages missed at allocation time.  
 We have implemented these techniques in the Linux kernel on an Intel Optane machine in a system called JC. We use HPC applications, key-value stores and databases to compare JC to the default Linux tiered memory management implementation and to a state-of-the-art software management approach, as exemplified by the HeMem system.  
 Our measurements show that JC outperforms Linux and HeMem by up to 5x. A surprising conclusion of this paper is that a cache can be made to perform close-to-optimally by minimizing conflicts at page allocation time, without any monitoring of hot pages or dynamic page remapping.  
 TAILCHECK: A Lightweight Heap Overflow Detection Mechanism with Page Protection and Tagged Pointers   
 Disaggregated memory (DM) is an increasingly prevalent architecture in academia and industry with high resource utilization. It separates computing and memory resources into two pools and interconnects them with fast networks. Existing range indexes on DM are based on B+ trees, which suffer from large inherent read and write amplifications. The read and write amplifications rapidly saturate the network bandwidth, resulting in low request throughput and high access latency of B+ trees on DM.  
 In this paper, we propose to use the radix tree, which is more suitable for DM than the B+ tree due to smaller read and write amplifications. However, constructing a radix tree on DM is challenging due to the costly lock-based concurrency control, the bounded memory-side IOPS, and the complicated computing-side cache validation. To address these challenges, we design SMART, the first radix tree for disaggregated memory with high performance. Specifically, we leverage 1) a hybrid concurrency control scheme including lock-free internal nodes and fine-grained lock-based leaf nodes to reduce lock overhead, 2) a computing-side read-delegation and write-combining technique to break through the IOPS upper bound by reducing redundant I/Os, and 3) a simple yet effective reverse check mechanism for computing-side cache validation. Experimental results show that SMART achieves 6.1x higher throughput under typical write-intensive workloads and 2.8x higher throughput under read-only workloads, compared with state-of-the-art B+ trees on DM.  
 ORC: Increasing Cloud Memory Density via Object Reuse with Capabilities   
 Every day, billions of people depend on Internet services for communication, commerce, and entertainment. Yet planetary-scale data center infrastructures consisting of millions of servers experience unplanned capacity outages and unexpected demand for resources; how can such infrastructures remain reliable in the face of capacity and workload flux?  
 In this paper, we introduce Defcon, a system for improving the availability of large-scale, globally-distributed Internet services using graceful feature  degradation. In response to overload conditions, Defcon enables site operators to gradually disable less-critical features in order to reduce resource demand. Defcon presents a common interface to product developers to define feature knobs  that represent degradation capabilities. Defcon automatically tests knobs to understand each knob‚Äôs product- and infrastructure-level trade-offs. At Meta, we have used Defcon to improve global product availability in the face of worldwide demand-surges in addition to large-scale infrastructure failures.  
 Cilantro: Performance-Aware Resource Allocation for General Objectives via Online Feedback   
 Available Media   
 The classical max-min fairness algorithm for resource allocation provides many desirable properties, e.g., Pareto efficiency, strategy-proofness and fairness. This paper builds upon the observation that max-min fairness guarantees these properties under a strong assumption---user demands being static over time---and that, for the realistic case of dynamic user demands, max-min fairness loses one or more of these properties.  
 We present Karma, a generalization of max-min fairness for dynamic user demands. The key insight in Karma is to introduce "memory" into max-min fairness --- when allocating resources, Karma takes users' past allocations into account: in each quantum, users donate their unused resources and are assigned credits when other users borrow these resources; Karma carefully orchestrates exchange of credits across users (based on their instantaneous demands, donated resources and borrowed resources), and performs prioritized resource allocation based on users' credits. We prove theoretically that Karma guarantees Pareto efficiency, online strategy-proofness, and optimal fairness for dynamic user demands (without future knowledge of user demands). Empirical evaluations over production workloads show that these properties translate well into practice: Karma is able to reduce disparity in performance across users to a bare minimum while maintaining Pareto-optimal system-wide performance.  
 USENIX ATC '23 Poster Session and Reception  
 Back Bay Ballroom  
 The USENIX ATC '23 poster session and reception will feature posters by authors presenting their work in person at the conference. View the list of accepted posters  .  
 Wednesday, July 12   
 Continental Breakfast  
 Fully exploiting the computing power of an accelerator specialized for deep neural networks (DNNs) calls for the synergy between network and hardware architectures, but existing approaches partition a computational graph of DNN into multiple sub-graphs by abstracting away hardware architecture and assign resources to each sub-graph, not only producing redundant off-core data movements but also under-utilizing the hardware resources of a domain-specific architecture (DSA).  
 This paper introduces a systematic approach for effectively scheduling DNN computational graphs on DSA platforms. By fully taking into account hardware architecture when partitioning a computational graph into coarse-grained sub-graphs, our work enables the synergy between network and hardware architectures, addressing several challenges of prior work: (1) it produces larger but fewer kernels, converting a large number of off-core data movements into on-core data exchanges; (2) it exploits the imbalanced memory usage distribution across DNN network architecture, better saturating the DSA memory hierarchy; (3) it enables across-layer instruction scheduling not studied before, further exploiting the parallelism across different specialized compute units.  
 Results of seven DNN inference models on a DSA platform show that our work outperforms TVM and AStitch by 11.15√ó and 6.16√ó, respectively, and obtains throughput competitive to the vendor-crafted implementation. A case study on GPU also demonstrates that generating kernels for our sub-graphs can surpass CUTLASS with and without convolution fusion by 1.06√ó and 1.23√ó, respectively.  
 Available Media   
 Dynamic neural networks (NNs), which can adapt sparsely activated sub-networks to inputs during inference, have shown significant advantages over static ones in terms of accuracy, computational efficiency, and adaptiveness. However, existing deep learning frameworks and compilers mainly focus on optimizing static NNs with deterministic execution, missing optimization opportunities brought by non-uniform distribution of activation in dynamic NNs. The key to optimizing dynamic NNs is the traceability of how data are dynamically dispatched to different paths at inference. Such dynamism often happens at sub-tensor level (e.g., conditional dispatching tokens of a tensor), thus hard for existing tensor-centric frameworks to trace due to misaligned expression granularity.  
 Serverless computing has become a popular cloud computing paradigm. By default, when a serverless function fails, the serverless platform re-executes the function to tolerate the failure. However, such a retry-based approach requires functions to be idempotent, which means that functions should expose the same behavior regardless of retries. This requirement is challenging for developers, especially when functions are stateful. Failures may cause functions to repeatedly read and update shared states, potentially corrupting data consistency.  
 This paper presents Flux, the first toolkit that automatically verifies the idempotence of serverless applications. It proposes a new correctness definition, idempotence consistency, which stipulates that a serverless function‚Äôs retry is transparent to users. To verify idempotence consistency, Flux defines a novel property, idempotence simulation, which decomposes the proof for a concurrent serverless application into the reasoning of individual functions. Furthermore, Flux extends existing verification techniques to realize automated reasoning, enabling Flux to identify idempotence-violating operations and fix them with existing log-based methods.  
 We demonstrate the efficacy of Flux with 27 representative serverless applications. Flux has successfully identified previously unknown issues in 12 applications. Developers have confirmed 8 issues. Compared to state-of-the-art systems (namely Beldi and Boki) that log every operation, Flux achieves up to 6√ó lower latency and 10√ó higher peak throughput, as it logs only the identified idempotence-violating ones.  
 Available Media   
 SmartNICs have recently emerged as an appealing device for accelerating distributed systems. However, there has not been a comprehensive characterization of SmartNICs, and existing designs typically only leverage a single communication path for workload offloading. This paper presents the first holistic study of a representative off-path SmartNIC, specifically the Bluefield-2, from a communication-path perspective. Our experimental study systematically explores the key performance characteristics of communication among the client, on-board SoC, and host, and offers insightful findings and advice for designers. Moreover, we propose the concurrent use of multiple communication paths of a SmartNIC and present a pioneering guideline to expose new optimization opportunities for various distributed systems. To demonstrate the effectiveness of our approach, we conducted case studies on a SmartNIC-based distributed file system (LineFS) and an RDMA-based disaggregated key-value store (DrTM-KV). Our experimental results show improvements of up to 30% and 25% for LineFS and DrTM-KV, respectively.  
 Ens≈ç: A Streaming Interface for NIC-Application Communication   
 Hugo Sadok and Nirav Atre, Carnegie Mellon University;  Zhipeng Zhao, Microsoft;  Daniel S. Berger, Microsoft Research & University of Washington;  James C. Hoe, Carnegie Mellon University;  Aurojit Panda, New York University;  Justine Sherry, Carnegie Mellon University;  Ren Wang, Intel   
 Awarded Best Paper!  
 Available Media
9. OSDI_2 conference:
Questions 
 OSDI '23 Call for Papers  
 Sponsored by USENIX  in cooperation with ACM SIGOPS.   
 The 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI '23)  will be co-located with the 2023 USENIX Annual Technical Conference (USENIX ATC '23)  and take place on July 10‚Äì12, 2023, in Boston, MA, USA.  
 Important Dates  
  Author Response Period | Reviews available: | Monday, March 6, 2023 
  Author responses due: | Wednesday, March 8, 2023 
  Notification to authors: | Thursday, March 23, 2023 
  Final paper files due: | Thursday, June 1, 2023 
  Download Call for Papers PDF    
 Conference Organizers  
 Overview  
 The 17th USENIX Symposium on Operating Systems Design and Implementation seeks to present innovative, exciting research in computer systems. OSDI brings together professionals from academic and industrial backgrounds in a premier forum for discussing the design, implementation, and implications of systems software. OSDI emphasizes innovative research and quantified or insightful experiences in systems design and implementation.  
 OSDI takes a broad view of the systems area and solicits contributions from many fields of systems practice, including operating systems, file and storage systems, distributed systems, cloud computing, mobile systems, secure and reliable systems, systems aspects of big data, embedded systems, virtualization, networking as it relates to operating systems, and management and troubleshooting of complex systems. We also welcome work that explores the interface to related areas such as computer architecture, networking, programming languages, analytics, and databases. We encourage contributions with highly original ideas, new approaches, and groundbreaking results.  
 Submitting a Paper  
 Submissions will be judged on novelty, significance, interest, clarity, relevance, and correctness. Accepted papers will be shepherded through an editorial review process by a member of the program committee.  
 A good paper will:  
 Motivate a significant problem 
  Propose an interesting and compelling solution 
  Demonstrate the practicality and benefits of the solution 
  Draw appropriate conclusions 
  Clearly describe the paper's contributions 
  Clearly articulate the advances beyond previous work 
  Accepted papers will generally be available online to registered attendees before the conference. If your accepted paper should not be published prior to the event, please notify production@usenix.org  . The papers will be available online to everyone beginning on the first day of the conference.  
 Papers accompanied by nondisclosure agreement forms will not be considered. All submissions will be treated as confidential prior to publication on the USENIX OSDI '23 website; rejected submissions will be permanently treated as confidential.  
 Simultaneous submission of the same work to multiple venues, submission of previously published work, or plagiarism constitutes dishonesty or fraud. USENIX, like other scientific and technical conferences and journals, prohibits these practices and may, on the recommendation of a program chair, take action against authors who have committed them. See the USENIX Conference Submissions Policy  for details.  
 Prior or concurrent workshop publication does not preclude publishing a related paper in OSDI. Authors should email the program co-chairs, osdi23chairs@usenix.org  , a copy of the related workshop paper and a short explanation of the new material in the conference paper beyond that published in the workshop version. The co-chairs may then share that paper with the workshop's organizers and discuss it with them.  
 Prior or concurrent publication in non-peer-reviewed contexts, like arXiv.org  , technical reports, talks, and social media posts, is permitted. However, your OSDI submission must use an anonymized name for your project or system that differs from any used in such contexts.  
 USENIX discourages program co-chairs from submitting papers to the conferences they organize, although they are allowed to do so. Should either program co-chair submit work to OSDI '23, their papers will be handled exclusively by the other program co-chair and reviewed according to the same rigorous and double blinded procedures that the program committee applies to all papers. In the event that a paper is co-authored by, or otherwise conflicted with, both co-chairs, the co-chairs will designate a PC member to manage the reviewing process for that paper.  
 Questions? Contact your program co-chairs, osdi23chairs@usenix.org  , or the USENIX office, submissionspolicy@usenix.org  .  
 By submitting a paper, you agree that at least one of the authors will attend the conference to present it. If the conference registration fee will pose a hardship for the presenter of the accepted paper, please contact conference@usenix.org  . For very special circumstances (such as travel restrictions or health issues), USENIX may provide the possibility of remote presentation and Q&A.  
 If your paper is accepted and you need an invitation letter to apply for a visa to attend the conference, please contact conference@usenix.org  as soon as possible. (Visa applications can take at least 30 working days to process.) Please identify yourself as a presenter and include your mailing address in your email.  
 Deadline and Submission Instructions  
 Authors must register abstracts and submit full papers by the dates indicated above. These are hard deadlines, and no extensions will be given. Submitted papers must be no longer than 12 single-spaced 8.5" x 11" pages, including figures and tables, plus as many pages as needed for references, using 10-point type on 12-point (single-spaced) leading, two-column format, Times Roman or a similar font, within a text block 7" wide x 9" deep. Accepted papers will be allowed 14 pages in the proceedings, plus references. Papers not meeting these criteria will be rejected without review, and no deadline extensions will be granted for reformatting. Pages should be numbered, and figures and tables should be legible in black and white, without requiring magnification. Papers so short as to be considered "extended abstracts" will not receive full consideration.  
 Supplementary Material  
 Authors may upload supplementary material in files separate from their submissions. PC members are not required to read supplementary material when reviewing the paper, so each paper should stand alone without it. Authors may use this for content that may be of interest to some readers but is peripheral to the main technical contributions of the paper. Examples of materials that may be included are: formal proofs that are only sketched in the paper; snippets of code that detail an algorithm presented in the paper; and methodological details that are not essential for the PC‚Äôs assessment but are important for reproducibility. Importantly, because the PC members are NOT required to read the supplementary material, the submission must stand alone without it. Attaching supplementary material is optional; if your paper says that you have formal proofs or source code, you need not attach them to convince the PC of their existence.  
 Identity Blinding  
 The paper review process is double-blind. Authors must make a good faith effort to anonymize their submissions, and they should not identify themselves or their institutions either explicitly or by implication (e.g., through the references or acknowledgments). Submissions violating the detailed formatting and anonymization rules will be rejected without review. If you are uncertain about how to anonymize your submission, contact the program co-chairs, osdi23chairs@usenix.org  , well in advance of the submission deadline.  
 Abstract Registration  
 Registering abstracts a week before paper submission is an essential part of the paper-reviewing process, as PC members use this time to identify which papers they are qualified to review. Abstract registrations that do not provide sufficient information to understand the topic and contribution (e.g., empty abstracts, placeholder abstracts, or trivial abstracts) will be rejected, thereby precluding paper submission.  
 Conflicts  
 When registering your abstract, you must provide information about conflicts with PC members. A PC member is a conflict if and only if one or more of the following circumstances applies:  
 Personal:  You are close family relatives (spouses, domestic partners, parents, or children).  
 You must not identify a PC member as a conflict if none of these circumstances applies. For instance, the following are not sufficient grounds to specify a conflict with a PC member: they have reviewed the work before, they are employed by your competitor, they are your friend, they were your post-doc advisor or advisee, or they had the same advisor as you.  
 The chairs will review paper conflicts to ensure the integrity of the reviewing process, adding or removing conflicts if necessary. The chairs may reject abstracts or papers on the basis of missing or extraneous conflicts. If you have any questions about conflicts, please contact the program co-chairs.  
 Authors are also encouraged to contact the program co-chairs, osdi23chairs@usenix.org  , if needed to relate their OSDI submissions to relevant submissions of their own that are simultaneously under review or awaiting publication at other venues. The program co-chairs will use this information at their discretion to preserve the anonymity of the review process without jeopardizing the outcome of the current OSDI submission.  
 Papers must be in PDF format and must be submitted via the submission system  . For more details on the submission process, and for templates to use with LaTeX, Word, etc., authors should consult the detailed submission requirements  .  
 SUBMIT YOUR WORK    
 Author Response Period  
 OSDI will provide an opportunity for authors to respond to reviews prior to final consideration of the papers at the program committee meeting, during the dates shown above. Authors must limit their responses to (a) correcting factual errors in the reviews or (b) directly addressing questions posed by reviewers. Responses should be limited to clarifying the submitted work. In particular, responses must not include new experiments or data, describe additional work completed since submission, or promise additional work to follow.  
 Revise and resubmit  
 OSDI '23 introduces the option to revise and resubmit for some of the rejected papers. If a paper is given this option as indicated in the acceptance notification, authors will be provided with a list of issues to address. Authors may then resubmit a revised version with a response to the issues. If accepted, the paper will be presented at OSDI '24.  
 Attend | Registration Information
10. OSDI_3 conference:
Questions 
 OSDI '23 Exhibitor Services  
 Upcoming Dates and Deadlines  
 Deadline to confirm exhibit table | : Monday, June 12, 2023 
  Hotel discount deadline | : | Monday, June 19, 2023 | extended to Friday, June 23 
  Early bird registration deadline | : | Monday, June 19, 2023 | extended to Friday, June 23 
  Monitor orders  due | : Thursday, June 22, 2023 
  Shipping window for tabletop items | : Wednesday, July 5 to Friday, July 7 (tent) 
  Deadline to send your shipment‚Äôs tracking numbers | : Wednesday, July 5 (if applicable) 
  LeadCapture orders  due | : Tuesday, June 27, 2023 
 Exhibitor Badges  
 You may have up to three exhibitors staffing your tabletop space. Each exhibitor must register for their own badge: We will provide the exhibitor registration link upon confirmation of your tabletop. Exhibitor badges grant access to the foyer exhibit area and hallway track.  
 If there's anyone from your company who wishes to attend the conference and also staff the table, their main conference registration will suffice. We can also give them an exhibitor ribbon at the registration desk on site on request.  
 If you will NOT be using your table, please let us know  by Monday, June 19, so that we can release the space to another vendor.  
 Exhibiting Hours  
 Tables will be available Monday morning through Wednesday afternoon and will receive peak traffic during the breaks, lunches, and the half hour after the last session ends on the evenings on Monday and Tuesday. Please note that lunch is ‚Äúon your own‚Äù on Wednesday, so fewer attendees will be on site during this break. Overnight security for tables will not be available but USENIX staff can secure items in the show office on request.  
 You can see the overview of the OSDI program  to help plan your schedule.  
 Tabletop Exhibit Information  
 Tables will be located in the Grand Ballroom Pre-function area. Badge pickup is also in the pre-function area.  
  Access to electrical 
  Use of the conference wireless network 
  You are welcome to bring pop-up banners, table swag, desktop literature racks, and giveaways, but please be aware that you will need to transport these in and out on your own. See below if you need assistance with advance shipping and be sure to advise us of your tracking information  by July 5.  
 Food and Beverage  
 Lunches and the Reception are available for registered attendees only. Plentiful food options are available in the Prudential Center  adjacent to the hotel.  
 Order Forms  
 Monitor Order  
 If you want to order a monitor or other AV, you can do so using this form: OSDI '23 Monitor Order form   
 Lead Retrieval  
 You may order lead retrieval via Cvent LeadCapture: OSDI '23 LeadCapture Flyer   
 Tabletop Shipping  
 Please plan to send all boxes to arrive no earlier than Wednesday, July 5 and no later than Friday July 7, 2023. Shipping information to come soon.  
 Casey Henderson c/o FedEx Office at Sheraton Boston Hotel  
  USENIX ATC '23/OSDI '23  
  [COMPANY NAME] Box __ of __   
 Clearly label all boxes per above, and email us  all shipping and tracking information by July 5 to ensure arrival and correct delivery. Please take note of the 4th of July holiday when planning shipping.   
 Ship Out

output:1. OPODIS_1 information:
2. OPODIS_2 information:
3. OPODIS_3 information:
4. OP_0 information:
5. OP_1 information:
6. OP_2 information:
7. OSDI_0 information:
8. OSDI_1 information:
9. OSDI_2 information:
10. OSDI_3 information:
