input:
1. CONCUR_3 conference:
Conference series 
      concur 2023: CONCUR 2023  
 in ,  Conference 
   Call for Papers 
   Program
2. CoNLL_0 conference:
Github 
 Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)   
 Jing Jiang  , David Reitter  , Shumin Deng  (Editors)   
  Show all abstracts   Hide all abstracts    pdf  bib   
   Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)    
  Jing Jiang  | David Reitter  | Shumin Deng    
 pdf  bib  abs   
     T  o MC  hallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind    
  Xiaomeng Ma  | Lingyu Gao  | Qihui Xu    
 Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted these models are capable of exhibiting ToM, while others suggest the opposite. In this study, We present ToMChallenges, a dataset for comprehensively evaluating the Theory of Mind based on Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper wants to raise awareness in evaluating the ToM in LLMs and we want to invite more discussion on how to design the prompts and tasks for ToM tasks that can better access the LLMs‚Äô ability.   
 pdf  bib  abs   
     A  rch BERT  : Bi-Modal Understanding of Neural Architectures and Natural Languages    
  Mohammad Akbari  | Saeed Ranjbar Alvar  | Behnam Kamranian  | Amin Banitalebi-Dehkordi  | Yong Zhang    
 Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Masked Architecture Modeling (MAM) for a more generalized joint learning. Moreover, we introduce and publicly release two new bi-modal datasets for training and validating our methods. The ArchBERT‚Äôs performance is verified through a set of numerical experiments on different downstream tasks such as architecture-oriented reasoning, question answering, and captioning (summarization). Datasets, codes, and demos are available as supplementary materials.   
 pdf  bib  abs   
   A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models    
  Karin de Langis  | Dongyeop Kang    
 There is growing interest in incorporating eye-tracking data and other implicit measures of human language processing into natural language processing (NLP) pipelines. The data from human language processing contain unique insight into human linguistic understanding that could be exploited by language models. However, many unanswered questions remain about the nature of this data and how it can best be utilized in downstream NLP tasks. In this paper, we present EyeStyliency, an eye-tracking dataset for human processing of stylistic text (e.g., politeness). We develop an experimental protocol to collect these style-specific eye movements. We further investigate how this saliency data compares to both human annotation methods and model-based interpretability metrics. We find that while eye-tracking data is unique, it also intersects with both human annotations and model-based importance scores, providing a possible bridge between human- and machine-based perspectives. We propose utilizing this type of data to evaluate the cognitive plausibility of models that interpret style. Our eye-tracking data and processing code are publicly available.   
 pdf  bib  abs   
   PROPRES  : Investigating the Projectivity of Presupposition with Various Triggers and Environments    
  Daiki Asami  | Saku Sugawara    
 What makes a presupposition of an utterance ‚Äîinformation taken for granted by its speaker‚Äî different from other pragmatic inferences such as an entailment is projectivity (e.g., the negative sentence the boy did not stop shedding tears presupposes the boy had shed tears before). The projectivity may vary depending on the combination of presupposition triggers and environments. However, prior natural language understanding studies fail to take it into account as they either use no human baseline or include only negation as an entailment-canceling environment to evaluate models‚Äô performance. The current study attempts to reconcile these issues. We introduce a new dataset, projectivity of presupposition (PROPRES), which includes 12k premise‚Äìhypothesis pairs crossing six triggers involving some lexical variety with five environments. Our human evaluation reveals that humans exhibit variable projectivity in some cases. However, the model evaluation shows that the best-performed model, DeBERTa, does not fully capture it. Our findings suggest that probing studies on pragmatic inferences should take extra care of the human judgment variability and the combination of linguistic items.   
 pdf  bib  abs   
   A Minimal Approach for Natural Language Action Space in Text-based Games    
  Dongwon Ryu  | Meng Fang  | Gholamreza Haffari  | Shirui Pan  | Ehsan Shareghi    
 Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose ùúñ  -admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.   
 pdf  bib  abs   
   Structural Ambiguity and its Disambiguation in Language Model Based Parsers: the Case of D  utch Clause Relativization    
  Gijs Wijnholds  | Michael Moortgat    
 This paper addresses structural ambiguity in Dutch relative clauses. By investigating the task of disambiguation by grounding, we study how the presence of a prior sentence can resolve relative clause ambiguities. We apply this method to two parsing architectures in an attempt to demystify the parsing and language model components of two present-day neural parsers. Results show that a neurosymbolic parser, based on proof nets, is more open to data bias correction than an approach based on universal dependencies, although both set-ups suffer from a comparable initial data bias.   
 pdf  bib  abs   
       Quantifying Information of Tokens for Simple and Flexible Simultaneous Machine Translation    
  DongHyun Lee  | Minkyung Park  | Byung-Jun Lee    
 Simultaneous Translation (ST) involves translating with only partial source inputs instead of the entire source inputs, a process that can potentially result in translation quality degradation. Previous approaches to balancing translation quality and latency have demonstrated that it is more efficient and effective to leverage an offline model with a reasonable policy. However, using an offline model also leads to a distribution shift since it is not trained with partial source inputs, and it can be improved by training an additional module that informs us when to translate. In this paper, we propose an Information Quantifier (IQ) that models source and target information to determine whether the offline model has sufficient information for translation, trained with oracle action sequences generated from the offline model. IQ, by quantifying information, helps in formulating a suitable policy for Simultaneous Translation that better generalizes and also allows us to control the trade-off between quality and latency naturally. Experiments on various language pairs show that our proposed model outperforms baselines.   
 pdf  bib  abs   
   Enhancing Code-mixed Text Generation Using Synthetic Data Filtering in Neural Machine Translation    
  Dama Sravani  | Radhika Mamidi    
 Code-Mixing, the act of mixing two or more languages, is a common communicative phenomenon in multi-lingual societies. The lack of quality in code-mixed data is a bottleneck for NLP systems. On the other hand, Monolingual systems perform well due to ample high-quality data. To bridge the gap, creating coherent translations of monolingual sentences to their code-mixed counterparts can improve accuracy in code-mixed settings for NLP downstream tasks. In this paper, we propose a neural machine translation approach to generate high-quality code-mixed sentences by leveraging human judgements. We train filters based on human judgements to identify natural code-mixed sentences from a larger synthetically generated code-mixed corpus, resulting in a three-way silver parallel corpus between monolingual English, monolingual Indian language and code-mixed English with an Indian language. Using these corpora, we fine-tune multi-lingual encoder-decoder models viz, mT5 and mBART, for the translation task. Our results indicate that our approach of using filtered data for training outperforms the current systems for code-mixed generation in Hindi-English. Apart from Hindi-English, the approach performs well when applied to Telugu, a low-resource language, to generate Telugu-English code-mixed sentences.   
 pdf  bib  abs   
     The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks    
  Kaiser Sun  | Adina Williams  | Dieuwke Hupkes    
 NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than with synthetic datasets, or than the latter among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compositionality; and iv) specific lexical items in dataset impacts the measurement consistency. Overall, our results demonstrate that much work remains to be done when it comes to assessing whether popular evaluation datasets measure what they intend to measure, and suggests that elucidating more rigorous standards for establishing the validity of evaluation sets could benefit the field.   
 pdf  bib  abs   
   M  ed- HALT  : Medical Domain Hallucination Test for Large Language Models    
  Ankit Pal  | Logesh Kumar Umapathi  | Malaikannan Sankarasubbu    
 This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs‚Äô problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io   
 pdf  bib  abs   
     HNC  : Leveraging Hard Negative Captions towards Models with Fine-Grained Visual-Linguistic Comprehension Capabilities    
  Esra D√∂nmez  | Pascal Tilli  | Hsiu-Yu Yang  | Ngoc Thang Vu  | Carina Silberer    
 Image-Text-Matching (ITM) is one of the defacto methods of learning generalized representations from a large corpus in Vision and Language (VL). However, due to the weak association between the web-collected image‚Äìtext pairs, models fail to show fine-grained understanding of the combined semantics of these modalities. To this end, we propose Hard Negative Captions (HNC): an automatically created dataset containing foiled hard negative captions for ITM training towards achieving fine-grained cross-modal comprehension in VL. Additionally, we provide a challenging manually-created test set for benchmarking models on a fine-grained cross-modal mismatch with varying levels of compositional complexity. Our results show the effectiveness of training on HNC by improving the models‚Äô zero-shot capabilities in detecting mismatches on diagnostic tasks and performing robustly under noisy visual input scenarios. Also, we demonstrate that HNC models yield a comparable or better initialization for fine-tuning. Our code and data are publicly available.   
 pdf  bib  abs   
   Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests    
  Max van Duijn  | Bram van Dijk  | Tom Kouwenhoven  | Werner de Valk  | Marco Spruit  | Peter van der Putten    
 To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs‚Äô robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.   
 pdf  bib  abs   
   A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation    
  Jarad Forristal  | Fatemehsadat Mireshghallah  | Greg Durrett  | Taylor Berg-Kirkpatrick    
 Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.   
 pdf  bib  abs   
   Challenging the ‚ÄúOne Single Vector per Token‚Äù Assumption    
  Mathieu Dehouck    
 In this paper we question the almost universal assumption that in neural networks each token should be represented by a single vector. In fact, it is so natural to use one vector per word that most people do not even consider it as an assumption of their various models. Via a series of experiments on dependency parsing, in which we let each token in a sentence be represented by a sequence of vectors, we show that the ‚Äúone single vector per token‚Äù assumption might be too strong for recurrent neural networks. Indeed, biaffine parsers seem to work better when their encoder accesses its input‚Äôs tokens‚Äô representations in several time steps rather than all at once. This seems to indicate that having only one occasion to look at a token through its vector is too strong a constraint for recurrent neural networks and calls for further studies on the way tokens are fed to neural networks.   
 pdf  bib  abs   
   Strategies to Improve Low-Resource Agglutinative Languages Morphological Inflection    
  Gulinigeer Abudouwaili  | Wayit Ablez  | Kahaerjiang Abiderexiti  | Aishan Wumaier  | Nian Yi    
 Morphological inflection is a crucial task in the field of morphology and is typically considered a sequence transduction task. In recent years, it has received substantial attention from researchers and made significant progress. Models have achieved impressive performance levels for both high- and low-resource languages. However, when the distribution of instances in the training dataset changes, or novel lemma or feature labels are predicted, the model‚Äôs accuracy declines. In agglutinative languages, morphological inflection involves phonological phenomena while generating new words, which can alter the syllable patterns at the boundary between the lemma and the suffixes. This paper proposes four strategies for low-resource agglutinative languages to enhance the model‚Äôs generalization ability. Firstly, a convolution module extracts syllable-like units from lemmas, allowing the model to learn syllable features. Secondly, the lemma and feature labels are represented separately in the input, and the position encoding of the feature labels is marked so that the model learns the order between suffixes and labels. Thirdly, the model recognizes the common substrings in lemmas through two special characters and copies them into words. Finally, combined with syllable features, we improve the data augmentation method. A series of experiments show that the proposed model in this paper is superior to other baseline models.   
 pdf  bib  abs   
   Tree-shape Uncertainty for Analyzing the Inherent Branching Bias of Unsupervised Parsing Models    
  Taiga Ishii  | Yusuke Miyao    
 This paper presents the formalization of tree-shape uncertainty that enables us to analyze the inherent branching bias of unsupervised parsing models using raw texts alone. Previous work analyzed the branching bias of unsupervised parsing models by comparing the outputs of trained parsers with gold syntactic trees. However, such approaches do not consider the fact that texts can be generated by different grammars with different syntactic trees, possibly failing to clearly separate the inherent bias of the model and the bias in train data learned by the model. To this end, we formulate tree-shape uncertainty and derive sufficient conditions that can be used for creating texts that are expected to contain no biased information on branching. In the experiment, we show that training parsers on such unbiased texts can effectively detect the branching bias of existing unsupervised parsing models. Such bias may depend only on the algorithm, or it may depend on seemingly unrelated dataset statistics such as sequence length and vocabulary size.   
 pdf  bib  abs   
   Cross-Document Event Coreference Resolution: Instruct Humans or Instruct GPT  ?    
  Jin Zhao  | Nianwen Xue  | Bonan Min    
 This paper explores utilizing Large Language Models (LLMs) to perform Cross-Document Event Coreference Resolution (CDEC) annotations and evaluates how they fare against human annotators with different levels of training. Specifically, we formulate CDEC as a multi-category classification problem on pairs of events that are represented as decontextualized sentences, and compare the predictions of GPT-4 with the judgment of fully trained annotators and crowdworkers on the same data set. Our study indicates that GPT-4 with zero-shot learning outperformed crowd-workers by a large margin and exhibits a level of performance comparable to trained annotators. Upon closer analysis, GPT-4 also exhibits tendencies of being overly confident, and force annotation decisions even when such decisions are not warranted due to insufficient information. Our results have implications on how to perform complicated annotations such as CDEC in the age of LLMs, and show that the best way to acquire such annotations might be to combine the strengths of LLMs and trained human annotators in the annotation process, and using untrained or undertrained crowdworkers is no longer a viable option to acquire high-quality data to advance the state of the art for such problems.   
 pdf  bib  abs
3. CoNLL_2 conference:
Skip to main content         
 CoNLL    
 Menu      
 User account menu  Log in 
 CoNLL 2024   
 May 24, 2024   
 CoNLL is a yearly conference organized by SIGNLL  (ACL's Special Interest Group on Natural Language Learning), focusing on theoretically, cognitively and scientifically motivated approaches to computational linguistics. This year, CoNLL will be colocated with EMNLP 2024  . Registrations for CoNLL can be made through EMNLP (workshop 1).  
 Submission page available here  .  
 Papers that have received reviews in current or previous ARR cycles can be committed to CoNLL 2024  here   by August 30, 2024.   
 Accepted papers   
 A list of papers that have been accepted for CoNLL 2024 is available here  .  
 Program   
 The program for CoNLL 2024 is available here  .  
 Call for Papers  
 SIGNLL invites submissions to the 28th Conference on Computational Natural Language Learning (CoNLL 2024). The focus of CoNLL is on theoretically, cognitively and scientifically motivated approaches to computational linguistics, rather than on work driven by particular engineering applications  . Such approaches include:  
 Computational learning theory and other techniques for theoretical analysis of machine learning models for NLP 
  Models of first, second and bilingual language acquisition by humans 
  Sociolinguistics 
  Multimodal and grounded language learning 
  We do not restrict the topic of submissions to fall into this list. However, the submissions‚Äô relevance to the conference‚Äôs focus on theoretically, cognitively and scientifically motivated approaches  will play an important role in the review process.  
 Submitted papers must be anonymous  and use the EMNLP 2024 template  . Submitted papers may consist of up to 8 pages  of content plus unlimited space for references. Authors of accepted papers will have an additional page to address reviewers‚Äô comments in the camera-ready version (9 pages of content in total, excluding references). Optional anonymized supplementary materials and a PDF appendix  are allowed. The appendix should be submitted as a separate PDF file (reviewers are not required to consider the materials in the appendix so it should not include any essential content to the understanding of the paper). Please refer to the EMNLP 2024 Call for Papers  for more details on the submission format. Note that, unlike EMNLP, we do not mandate that papers have a discussion section of the limitations of the work. However, we strongly encourage authors to have such a section in the appendix.  
 Please submit via Open Review  . CoNLL 2024 will accept ARR submission depending on the full review to be completed by Jul 1, 2024  . Please note that CoNLL 2024 is an in-person conference. We expect all accepted papers to be presented physically and presenting authors must register through EMNLP (workshop).</p>  
 Timeline   
  Submission deadline: Monday July 1, 2024  (EXTENDED) Sunday, July 7, 2024  
  ARR Commitment deadline: Friday, August 30, 2024  
  Notification of acceptance: Friday, September 20  (DELAYED), Tuesday, September 24, 2024  
  Camera ready papers due: Friday, October 11, 2024  
  Conference: November 15 - 16, 2024  
 Venue   
  CoNLL 2024 will be held in-person, along with EMNLP  in Miami, Florida  .  
 Multiple submission policy   
  CoNLL 2024 will refuse papers that are currently under submission, or that will be submitted to other meetings or publications, including EMNLP. Papers submitted elsewhere and papers that overlap significantly in content or results with papers that will be (or have been) published elsewhere will be rejected. Authors submitting more than one paper to CoNLL 2024 must ensure that the submissions do not overlap significantly (>25%) with each other in content or results.  
 Information About Travel Visas   
  If you will be requiring travel visas to Miami, Florida, please fill out this form: Travel Visa Form   
 This has been prepared by the EMNLP organizers to facilitate the process of acquiring visas. If visas are needed, your information should be provided as early as possible. If you have more questions, please contact Mark Finlayson and Zoey Liu, who are the local chairs here: EMNLP Organizers   
 CoNLL 2024 Chairs and Organizers  
 The conference's co-chairs are:   
 Malihe Alikhani  (Northeastern University, MA, USA)
4. CoNLL_3 conference:
Skip to main content         
 CoNLL    
 Menu      
 Previous shared tasks   
 2023 | BabyLM Challenge | English | Proceedings
5. CoopIS_0 conference:
About CoopIS 
  Upcoming Edition 
  CoopIS Conferences 
  Steering committee 
  International Conferences on Cooperative Information Systems  
 About CoopIS  
 Cooperative Information Systems (CIS) facilitate the cooperation between individuals, organisations, smart devices, and systems of systems, by providing flexible, scalable and intelligent services to enterprises, public institutions and user communities. As a result, people and smart devices can interact, share information and work together across physical barriers. The domain of CIS integrates research results from different related computing areas, such as: distributed systems, coordination technologies, collaborative decision making, enterprise architecture, business process management and conceptual modelling.  
  In developing the next generation CISs, research is needed towards (1) the applicability and use of the above mentioned innovative technologies, (2) approaches to develop CISs in particular catering towards the multitude of stakeholders involved in the development of socio-cyber-physical systems, and (3) associated modelling techniques to express and analyse the different aspects of these systems in cohesion.  
  The CoopIS conference series is an established international event for presenting and discussing scientific contributions about technical, economical, and societal aspects of distributed information systems at scale.  
 Upcoming Edition  
 CoopIS Conferences  
 Below is a complete list of CoopIS editions:  
 2024 
  2023 
  2022 
 Location | Porto, Portugal 
 Website | coopis.scitevents.org 
 General Chair | Mohamed Sellami and Zhangbing Zhou 
 PC Co-Chairs | Marco Comuzzi and Daniela Grigori 
 Location | Groningen, The Netherlands 
 Website | www.coopisconference.org/2023/ 
 General Chair | Herv√© Panetto and Walid Gaaloul 
 PC Co-Chairs | Maria-Esther Vidal and Boudewijn van Dongen
6. CoopIS_1 conference:
Conference | About  Submission  Dates  Demo track  Camera Ready Instructions  Registration  Organization 
  Accepted papers | Main Conference  Demo Track  Awards 
  Keynote Speakers 
  Partners 
 CoopIS 2023   
   October 30 - November 3, 2023  
   Groningen, The Netherlands   
 International Conference on Cooperative Information Systems    
 Collocated with   EDOC 2023     
 [August 21, 2023] Registration information added    
 [October 31, 2023] CoopIS Opening    
 [October 31, 2022] Best paper candidates announcement    
 [November 1, 2023] Best Paper Awards    
 About CoopIS 2023  
 Cooperative Information Systems (CIS) facilitate the cooperation between individuals, organisations, smart devices, and systems of systems, by providing flexible, scalable and intelligent services to enterprises, public institutions and user communities. As a result, people and smart devices can interact, share information and work together across physical barriers. The domain of CIS integrates research results from different related computing areas, such as: distributed systems, coordination technologies, collaborative decision making, enterprise architecture, business process management and conceptual modelling.  
  In developing the next generation CISs, research is needed towards (1) the applicability and use of the above mentioned innovative technologies, (2) approaches to develop CISs in particular catering towards the multitude of stakeholders involved in the development of socio-cyber-physical systems, and (3) associated modelling techniques to express and analyse the different aspects of these systems in cohesion.  
  The CoopIS conference series is an established international event for presenting and discussing scientific contributions about technical, economical, and societal aspects of distributed information systems at scale.  
  This 29th edition will be hosted by the Bernoulli Institute  for Mathematics, Computer Science and Artificial Intelligence of the University of Groningen, in Groningen, the Netherlands. More information are available in the Venue section  .It will be collocated with EDOC 2023  and its guiding theme is "Human-centric Information Systems", with a particular focus on the following areas: (click on the topic for details)   
  Topic 1: Knowledge Graphs, Data, Information, and Knowledge Engineering | Semantic Data Integration 
  Ontologies in the Digital World 
 Submission Guidelines  
 Authors are invited to submit original, unpublished research papers that are not under review for any other conference, workshop, or journal. Papers must be written in English. The contributions should address research questions that relate to one of the topics listed above.  
 We particularly encourage:  
 Contributions that introduce and evaluate technological innovations (e.g. new techniques, tools, methods or software). 
  The questions addressed should both be practically relevant and appealing to the general IS field. Full papers should include a systematic evaluation of the contribution and relate this contribution to related scientific work. Short papers may present work supported by preliminary evidence only.  
 Submitted papers will be peer-reviewed by at least 3 reviewers. Papers are evaluated in terms of originality, significance, technical soundness, and clarity.  
 Submissions for full papers must not exceed 18 pages (including main contents and references) in the final camera-ready paper style. Short papers can cover up to 8 pages (including main contents and references). Submissions must be laid out according to the final camera-ready formatting instructions and must be submitted in PDF format.  
 Each accepted paper must have one of its authors registered to the conference before the camera-ready deadline. The conference organizers reserve the right of removing a paper from the proceedings if no author is officially registered by the camera-ready deadline. Moreover, only papers that have been presented by their authors during the conference will be published in the conference proceedings.  
 The final proceedings will be published by Springer Verlag in their Lecture Notes in Computer Science (LNCS). Author instructions can be found at:  
 It is mandatory to submit manuscripts in electronic form (in PDF format). Papers should be submitted at  
 EasyChair Submission System    
 Important Dates (Anywhere on Earth)  
 Abstract submission | (Extended) | : | June 21, 2023 | July 8, 2023 
  Full paper submission | (Extended) | : | July 1, 2023 | July 15, 2023 
  Full paper notification: | August 7, 2023 | August 15, 2023 
  Camera ready due: September 3, 2023 
  Author registration due: September 3, 2023 
 Demo Track  
 We welcome demo paper submissions for the CoopIS 2023 Demo Track, which will be a part of the larger conference and will focus on showcasing innovative tools, services, and applications related to the conference's guiding theme of "Human-centric Information Systems". Topics related to this area, but not limited to, include:  
 Information Engineering and Knowledge Discovery | , which focuses on the technical aspects of information systems and the tools and techniques used to process data and information: | Knowledge Graphs, Data, Information, and Knowledge Engineering 
  Inductive Learning, Machine-Learning and Knowledge Discovery 
 We encourage authors to submit demo papers that showcase the practical application of research papers submitted to the conference, as well as innovative tools and applications originating from research initiatives or industry. Previously demonstrated tools are also welcome if they have added significant value, such as new features, adaptations, or new practical applications.  
 To assess the maturity and robustness of the tool, reviewers need access to the tool for testing. If the tool requires a license, it should be provided to reviewers, at least for the review period. The procedure to obtain the license must not disclose the identity of the reviewers.  
 Submission  
 Please submit a 5-page (1 column format)  extended abstract in PDF format, following the CEUR-ART formatting guidelines  . The extended abstract must contain the following information:  
 Title, authors, and affiliations; 
  A video demonstrating the tool through screen recording, preferably with voiceover, not exceeding 4 minutes in length. 
 Demo abstracts should be submitted through the EasyChair Submission System  , selecting "Demo papers".   
 All demo submissions will be reviewed by the demo reviewing committee, who will also vote on the best demo to be granted the ‚ÄúcoopIS 2023 Best Demo Award‚Äù.   
 Accepted papers will be submitted for publication to CEUR  (indexed by DBLP and SCOPUS).  
 Important Dates (Anywhere on Earth)  
 Demo submission: | September 8, 2023 
  Notification of acceptance: | October 2, 2023 
  Camera ready due: | October 9, 2023 
  Demo Chairs  
 Nour Assy | , Bonitasoft, France. 
  Moe Thandar Wynn, Queensland University of Technology, Australia 
 Camera Ready Instructions  
 Authors of accepted papers must submit a zip file that includes:  
 The final source files, incl. bib files, images, etc. (no older source files) or a Microsoft Word file. 
  A final PDF file corresponding exactly to the final source files. 
  A license-to-publish agreement, signed by hand by the corresponding author on behalf of all of the authors of the paper. 
  A suggestion for an abbreviated running head, if appropriate. 
  Information about correct representation of authors‚Äô names, where necessary. 
 When filling the licence form, please use the following information:  
 Title of the Proceedings: Proceedings of the International Conference on Cooperative Information Systems (CoopIS) 2023 
 Registration  
  CoopIS/EDOC Registration Fees  
 Full Conference  registration gives access to all week's activities, including CoopIS, EDOC, workshops, demos, doctoral consortium, keynotes, panels, tutorial, the conference reception and the conference dinner.  
 CoopIS 2023 employs a mutually utilized financial service in conjunction with the EDOC conference.  
 Please use the below registration link.  
 Register here    
 CoopIS Authors  
 Each accepted paper must be associated with one registration of an author to the conference. Each author registration can only be associated with a single accepted paper. Every author registration must be performed before the deadline, according to the instructions provided by the chairs.  
 Cancellation Policy  
 Participation is not granted until full payment of the registration fee is received.  
 The conference program is subject to changes.  
 Payments will be refunded if the conference will be canceled by the organizer. In that case, the organizer will have no further liability to the client. Registrations remain valid if the conference has to be postponed.  
 You will receive an order confirmation after the payment.  
 An invoice can be required to declare, via this email address webshop-fssc(at)rug.nl, if desired with the addition of the VAT number of the participant.  
  Graph Collaborative Filtering and Data Augmentation Strategies in Dual-Target CDR. | Xiaowen Shao, Baisong Liu, Xueyuan Zhang, Junru Li, Ercong Xu and Shiqi Wu. 
  Clustering Raw Sensor Data in Process Logs to Detect Data Streams. | Matthias Ehrendorfer, Juergen Mangler and Stefanie Rinderle-Ma. 
  Comparing the Performance of GPT-3 with BERT for Decision Requirements Modeling. | Alexandre Goossens, Johannes De Smedt and Jan Vanthienen. 
  A Requirements Study on Model Repositories for Digital Twins in Construction Engineering. | Philipp Zech, Georg Fr√∂ch and Ruth Breu. 
  Joint Dynamic Resource Allocation and Trajectory Optimization for UAV-Assisted Mobile Edge Computing in Internet of Vehicles. | Runji Li and Haifeng Sun. 
  Deep Dive Survey Miner. | Marwa Elleuch, Oumaima Alaoui Ismaili and Philippe Legay. 
  Awards  
 Best Paper Award  
 Sijin Cheng, Sebasti√°n Ferrada and Olaf Hartig | from the Link√∂ping University received the Best Paper Award at the 29th International Conference on Cooperative Information Systems (CoopIS 2023) for their paper | Considering Vocabulary Mappings in Query Plans for Federations of RDF Data Sources. 
  Best Demo Paper Award  
 Lucien Kiven Tamo, Amine Abbad-Andaloussi, Dung My Thi Trinh and Hugo A. L√≥pez | received the Best Demo Paper Award at the 29th International Conference on Cooperative Information Systems (CoopIS 2023) for their paper | An Open-Source Modeling Editor for Declarative Process Models. | Best Paper Runner-up Award | Two best paper runner-ups: 
  From Process Mining Insights to Process Improvement: All Talk and No Action? | Vinicius Stein Dani, Henrik Leopold, Jan Martijn van der Werf, Iris Beerepoot and Hajo A. Reijers. 
  LABPMN: Location-Aware Business Process Modeling and Notation | Leo Poss, Lukas Dietz and Stefan Sch√∂nig. 
 Keynote Speakers  
  CoopIS keynote  
   Boualem Benatallah  
 Full professor, Dublin City University  , Ireland   
 Read more ‚Üí    
 CoopIS keynote  
   Fabio Casati  
 Principal Machine Learning Architect, ServiceNow    
  Time  : Groningen, Netherlands Local Time  
 Please note that CoopIS 2023 is collocated with EDOC 2023 and full registration give access to all week's activities of both conferences. EDOC 2023 program is available at: EDOC     
 Monday  October 30 2023 
  Tuesday  October 31 2023 
  Wednesday  November 1 2023 
  Thursday  November 2 2023 
  Friday  November 3 2023 
 Graph Collaborative Filtering and Data Augmentation Strategies in Dual-Target CDR. Xiaowen Shao, Baisong Liu, Xueyuan Zhang, Junru Li, Ercong Xu and Shiqi Wu. 
 Clustering Raw Sensor Data in Process Logs to Detect Data Streams. Matthias Ehrendorfer, Juergen Mangler and Stefanie Rinderle-Ma 
 Comparing the Performance of GPT-3 with BERT for Decision Requirements Modeling. Alexandre Goossens, Johannes De Smedt and Jan Vanthienen 
 Proceedings  
 CoopIS 2023 Proceedings  
  CoopIS Demonstration Track 2023 Proceedings  
 Sponsoring  
 The joint CoopIS and EDOC conferences provide sponsors with a carefully selected, high-level audience with which to interact. Leading technologists mingle with brand names, world-renowned scientists and key business users from various application markets to provide a unique networking experience. In addition, carefully placed speaker slots and named sessions ensure your brand is showcased in a targeted environment. We offer 3 main sponsorship packages to help you meet a range of visibility and lead generation needs. Please feel free to contact us to discuss these and tailor our standard offering to your specific needs.  
 Prominent logo placement on the Website |  
 Logo placement in the printed conference program |  
 Display of company logo during plenary speeches |  
 Your leaflet in the attendees box |  
 Mentioning of the sponsors in all social media activities |  
 Two registrations to non-author persons of your choice or one registration for one author at the EDOC+CoopIS forum |  
 Your posters on display at the entrance of the conference location |  
 We cover 2 nights accommodation, you cover your travel costs |  
 Your logo on badges |  
 Promo booth at conference facility |  
 CoopIS 2023 will take place in Groningen, The Netherlands Nethelands (30 October‚Äì3 November 2023). It will be hosted at the Bernoulli Institute  at the University of Groningen.  
 About Groningen  
 Details in the registration section.   
 Best paper candidates  
 From Process Mining Insights to Process Improvement: All Talk and No Action? | Vinicius Stein Dani, Henrik Leopold, Jan Martijn van der Werf, Iris Beerepoot and Hajo A. Reijers. 
  LABPMN: Location-Aware Business Process Modeling and Notation | Leo Poss, Lukas Dietz and Stefan Sch√∂nig. 
 Best Paper Award  
 Sijin Cheng, Sebasti√°n Ferrada and Olaf Hartig | from the Link√∂ping University received the Best Paper Award at the 29th International Conference on Cooperative Information Systems (CoopIS 2023) for their paper | Considering Vocabulary Mappings in Query Plans for Federations of RDF Data Sources. 
  Best Demo Paper Award  
 Lucien Kiven Tamo, Amine Abbad-Andaloussi, Dung My Thi Trinh and Hugo A. L√≥pez | received the Best Demo Paper Award at the 29th International Conference on Cooperative Information Systems (CoopIS 2023) for their paper | An Open-Source Modeling Editor for Declarative Process Models. | Best Paper Runner-up Award | Two best paper runner-ups: 
  From Process Mining Insights to Process Improvement: All Talk and No Action? | Vinicius Stein Dani, Henrik Leopold, Jan Martijn van der Werf, Iris Beerepoot and Hajo A. Reijers. 
  LABPMN: Location-Aware Business Process Modeling and Notation | Leo Poss, Lukas Dietz and Stefan Sch√∂nig.
7. CoopIS_2 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
 Documents  Actions  On-line Registration  Registration Fees  Deadlines and Policies  Submit Paper  Guidelines  Preparing your Presentation  Templates  Glossary  Author's Login  Reviewer's Login  Ethics of Review  Information  Conference Details  Important Dates  Technical Program  Social Event  Call for Papers  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Panels  Demo Track  Travel and Accommodation  Conference Venue  About the Region  Reaching the City  Visa Information  Hotel Reservation  Partners  Academic Partners  Industrial Partners  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Websites  Invited Speakers    
 Sponsored by:    
 Logistics:    
 CoopIS 2024 will be held in conjunction with ICINCO 2024    , WEBIST 2024    , icSPORTS 2024    , CHIRA 2024    , IN4PL 2024    , IJCCI 2024    , IC3K 2024    and EXPLAINS 2024    .   
  Registration to CoopIS allows free access to the ICINCO, WEBIST, icSPORTS, CHIRA, IN4PL, IJCCI, IC3K and EXPLAINS conferences (as a non-speaker).  
  New registrations are now only available at the conference welcome desk    
  Barbara Weber  ,  University of St.Gallen, Switzerland   
 Demo Track of CoopIS 2024 - CoopIS-DT   
  Chairs:  Marwa Elleuch and Marwan Hassani
8. CoopIS_3 conference:
Read instantly on your browser with Kindle for Web.  
  Using your mobile phone camera - scan the code below and download the Kindle app.  
 Image Unavailable  
  Edition     1st ed. 2024 
  Publisher     Springer 
  Publication date     25 Oct. 2023 
  Language     English 
  Dimensions     15.5 x 3.02 x 23.5 cm 
  Product details  
 Publisher ‚Äè : ‚Äé  Springer; 1st ed. 2024 edition (25 Oct. 2023) 
  Language ‚Äè : ‚Äé  English
9. COPLAS_0 conference:
SPARK 
  MCTS 
  COPLAS 
  VVPS 
  Call for Workshops 
 COPLAS. Constraint Satisfaction Techniques for Planning and Scheduling Problems  
 The areas of AI planning and scheduling have seen important advances thanks to the application of constraint satisfaction models and techniques. Solutions to many real-world problems need to integrate plan synthesis capabilities with resource allocation, which can be efficiently managed by using constraint satisfaction techniques.  
 The workshop will aim at providing a forum for researchers in the field of Artificial Intelligence to discuss novel issues on planning, scheduling constraint programming/constraint satisfaction problems (CSPs) and many other common areas that exist among them. On the whole, the workshop will mainly focus on managing complex problems where planning, scheduling and constraint satisfaction must be combined and/or interrelated, which entails an enormous potential for practical applications and future research.  
 COPLAS is ranked as CORE B  in ERA Conference Ranking.  
 Proceedings  
 The COPLAS proceedings  are available as pdf  .  
 Schedule  
 This schedule as a PDF file.   
  real-life applications 
 A journal special issue is under consideration for best papers of COPLAS'11.  
 Important Dates  
  Workshop Format  
 The workshop will be organized in technical sessions; each session will be concluded by a commentary of the session chair. The goal of the commentary is not only to summarize the content of talks but to highlight important novel points and raise questions to foster further discussion. We also plan to have individual commentary of each paper done by other authors in the session. Rather than being a mini-conference, the workshop will focus on opening new research areas and topics, discussing possible solutions, exchanging ideas and approaches, and in general inspiring forthcoming research.  
 Organizers  
 Miguel A. Salido | (contact person) | Universidad Polit√©cnica de Valencia (Spain) 
  Toby Walsh | , UNSW, Sydney and NICTA, Australia 
  Neil Yorke-Smith | , American University of Beirut/SRI International, USA 
  List of previous editions of COPLAS
10. COPLAS_1 conference:
HSDIP 
  PlanRob 
  COPLAS 
  SPARK 
  DMAP 
 Research Workshop of the Israel Science Foundation  
  COPLAS. Constraint Satisfaction Techniques for Planning and Scheduling Problems  
 The workshop aims at providing a forum to discuss novel issues on planning, scheduling, and constraint satisfaction problems. Solutions to many real-world problems need to integrate plan synthesis capabilities with time and resource allocation, which can be efficiently managed by constraint satisfaction and OR techniques. Formulations of P&S problems as CSPs, resource and temporal global constraints, and inference techniques are of particular interest of COPLAS.  
 Objectives and Topics  
 The areas of AI planning and scheduling have seen important advances thanks to the application of constraint satisfaction models and techniques. Especially solutions to many real-world problems need to integrate plan synthesis capabilities with resource allocation, which can be efficiently managed by using constraint satisfaction techniques. The workshop will aim at providing a forum for researchers in the field of Artificial Intelligence to discuss novel issues on planning, scheduling, constraint programming/constraint satisfaction problems (CSPs) and many other common areas that exist among them. On the whole, the workshop will mainly focus on managing complex problems where planning, scheduling and constraint satisfaction must be combined and/or interrelated, which entails an enormous potential for practical applications and future research. COPLAS is ranked as CORE B in ERA Conference Ranking and it is covered in selected Elsevier database products.  
 Proceedings  
 The full COPLAS proceedings are available as a pdf file  .  
 Schedule  
 COPLAS workshop (each paper is 20 minutes + 5 minutes for questions/discussion)

output:1. CONCUR_3 information:
2. CoNLL_0 information:
3. CoNLL_2 information:
4. CoNLL_3 information:
5. CoopIS_0 information:
6. CoopIS_1 information:
7. CoopIS_2 information:
8. CoopIS_3 information:
9. COPLAS_0 information:
10. COPLAS_1 information:
