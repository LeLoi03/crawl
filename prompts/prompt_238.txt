input:
1. RAMiCS_0 conference:
Relational and Algebraic Methods in Computer Science (RAMiCS 2023)     
 Contents   
 Home  Program  Proceedings  Invited speakers  Registration  Venue  Important dates  Sponsors  Participants  Contact  History    
 20th International Conference on  
  Relational and Algebraic Methods in Computer Science  
  RAMiCS 2023  
 RAMiCS 2023 will take place in Augsburg from 3 to 6 April, 2023  .  
 Theoretical aspects include semigroups, residuated lattices semirings, Kleene algebras, relation algebras, quantales and other algebras; their connections with program logics and other logics; their use in the theories of automata concurrency, formal languages, games, networks and programming languages; the development of algebraic, algorithmic category-theoretic, coalgebraic and proof-theoretic methods for these theories; their formalisation with theorem provers.  
 Applications include tools and techniques for program correctness, specification and verification; quantitative and qualitative models and semantics of computing systems and processes; algorithm design, automated reasoning, network protocol analysis, social choice, optimisation and control.  
 Proceedings  
 The proceedings are available from 30 March 2023 through 30 April 2023 via this link  .  
 Invited Speakers  
 Registration  
 RAMiCS 2023 will take place at the Technologiezentrum Augsburg, as a physical conference. There are no conference fees but, due to limited resources, participation is subject to approval by the conference organisers. Please apply for participation by February 28, 2023, by sending an email to roland.glueck@dlr.de  . Please use this email template   .  
 Venue  
 RAMiCS 2023 will be organized by the Institute of Stuctures and Design  of the German Aerospace Center  and will take place in the room "Innovation" at the Technologiezentrum Augsburg  in Augsburg  .  
   Augsburg can be reached by public transport from both Munich airport  and Memmingen airport  . To get to the venue  from Augsburg central station use the tramway line 3 in the direction of "Inninger Straße/Königsbrunn" or "Königsbrunn Zentrum" (the important part is "Königsbrunn") and leave at "Innovationspark/LfU". The venue is on the right in the direction of travel of the tramway. The following hotels are close to the venue: NinetyNine Hotel 
  ANA Style Hotel 
 All times in CEST (UTC+2). Speakers are underlined; click on titles to see abstracts.  
 Monday April 03, 2023 
 Tuesday April 04, 2023 
 Wednesday April 05, 2023 
 Thursday April 06, 2023 
 Submissions must provide sufficient information to judge their merits. Additional material may be provided in a clearly marked appendix or by a reference to a manuscript on a web site. Experimental data, software or mathematical components for theorem provers must be available in sufficient detail for referees. Deviation from these requirements may lead to rejection.  
 As for earlier RAMiCS conferences, we intend to publish a journal special issue with revised and extended versions of a selection of the best papers.  
 Important dates  
 All dates are AoE (anywhere on Earth):  
 Abstract Submission: | September 23  October 7, 2022 
  Paper Submission: | September 30  October 14, 2022 
  Author Notification: | December 09  December 16, 2022 
  Final version: | January 06, 2023 
  RAMiCS 2023: April 03, 2023 -> April 06, 2023 
 RAMiCS 2023 Sponsors  
      Contact  
 ramics2023@easychair.org   
 History of RAMiCS
2. RAMiCS_1 conference:
>> Google Books      Bookmarked   
 Relational and Algebraic Methods in Computer Science: 20th International Conference, RAMiCS 2023, Augsburg, Germany, April 3–6, 2023, Proceedings / Roland Glück, Luigi Santocanale, Michael Winter  
 Format:  eBook   Published:  SpringerLink Books - AutoHoldings  
  Springer Nature, 2023  Authors:  Roland Glück 
  Santocanale, Luigi 
  Winter, Michael
3. RAMiCS_2 conference:
default search action   
 combined dblp search 
  author search 
  venue search 
  publication search 
  contact dblp 
 Roland Glück, Luigi Santocanale, Michael Winter (2023)   
  Trier   
 access:  closed  
 type:  Editorship  
 metadata version:  2023-03-10  
 Open Alex   
  XML 
  dblp key:   
 conf/RelMiCS/2023 
  ask others   
 Google 
 Please also note that this feature is work in progress  and that it is still far from being perfect. That is, in particular,  
 the lists below may be incomplete due to unavailable citation data, 
  reference strings may not have been successfully mapped to the items listed in dblp, and 
  we do not have complete and curated metadata for all items given in these lists.
4. RANLP_0 conference:
Github 
 Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing   
 Ruslan Mitkov  , Galia Angelova   
 Markdown (Informal)  
 Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing | (Mitkov & Angelova, RANLP 2023) 
  ACL  
 Ruslan Mitkov and Galia Angelova. 2023. | Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing | . INCOMA Ltd., Shoumen, Bulgaria, Varna, Bulgaria, edition. 
   Copy Markdown to Clipboard   Copy ACL to Clipboard
5. RANLP_1 conference:
Github 
 International Conference Recent Advances in Natural Language Processing (2023)  
  Proceedings of the 8th Student Research Workshop associated with the International Conference Recent Advances in Natural Language Processing | 12 papers 
  Proceedings of the Ancient Language Processing Workshop | 26 papers 
  Proceedings of the 6th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text | 23 papers 
 Show all abstracts   Hide all abstracts     up   pdf (full)   
  bib (full)   Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing   
 pdf  bib   
   Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing    
  Ruslan Mitkov  | Galia Angelova    
 pdf  bib  abs   
   Cross-lingual Classification of Crisis-related Tweets Using Machine Translation    
  Shareefa Al Amer  | Mark Lee  | Phillip Smith    
 Utilisation of multilingual language models such as mBERT and XLM-RoBERTa has increasingly gained attention in recent work by exploiting the multilingualism of such models in different downstream tasks across different languages. However, performance degradation is expected in transfer learning across languages compared to monolingual performance although it is an acceptable trade-off considering the sparsity of resources and lack of available training data in low-resource languages. In this work, we study the effect of machine translation on the cross-lingual transfer learning in a crisis event classification task. Our experiments include measuring the effect of machine-translating the target data into the source language and vice versa. We evaluated and compared the performance in terms of accuracy and F1-Score. The results show that translating the source data into the target language improves the prediction accuracy by 14.8% and the Weighted Average F1-Score by 19.2% when compared to zero-shot transfer to an unseen language.   
 pdf  bib  abs   
   Lexicon-Driven Automatic Sentence Generation for the Skills Section in a Job Posting    
  Vera Aleksic  | Mona Brems  | Anna Mathes  | Theresa Bertele    
 This paper presents a sentence generation pipeline as implemented on the online job board Stepstone. The goal is to automatically create a set of sentences for the candidate profile and the task description sections in a job ad, related to a given input skill. They must cover two different “tone of voice” variants in German (Du, Sie), three experience levels (junior, mid, senior), and two optionality values (skill is mandatory or optional/nice to have). The generation process considers the difference between soft skills, natural language competencies and hard skills, as well as more specific sub-categories such as IT skills, programming languages and similar. To create grammatically consistent text, morphosyntactic features from the proprietary skill ontology and lexicon are consulted. The approach is a lexicon-driven generation process that compares all lexical features of the new input skills with the ones already added to the sentence database and creates new sentences according to the corresponding templates.   
 pdf  bib  abs   
   A Review in Knowledge Extraction from Knowledge Bases    
  Fabio Yanez  | Andrés Montoyo  | Yoan Gutierrez  | Rafael Muñoz  | Armando Suarez    
 Generative language models achieve the state of the art in many tasks within natural language processing (NLP). Although these models correctly capture syntactic information, they fail to interpret knowledge (semantics). Moreover, the lack of interpretability of these models promotes the use of other technologies as a replacement or complement to generative language models. This is the case with research focused on incorporating knowledge by resorting to knowledge bases mainly in the form of graphs. The generation of large knowledge graphs is carried out with unsupervised or semi-supervised techniques, which promotes the validation of this knowledge with the same type of techniques due to the size of the generated databases. In this review, we will explain the different techniques used to test and infer knowledge from graph structures with machine learning algorithms. The motivation of validating and inferring knowledge is to use correct knowledge in subsequent tasks with improved embeddings.   
 pdf  bib  abs   
   Was That a Question? Automatic Classification of Discourse Meaning in S  panish    
  Santiago Arróniz  | Sandra Kübler    
 This paper examines the effectiveness of different feature representations of audio data in accurately classifying discourse meaning in Spanish. The task involves determining whether an utterance is a declarative sentence, an interrogative, an imperative, etc. We explore how pitch contour can be represented for a discourse-meaning classification task, employing three different audio features: MFCCs, Mel-scale spectrograms, and chromagrams. We also determine if utilizing means is more effective in representing the speech signal, given the large number of coefficients produced during the feature extraction process. Finally, we evaluate whether these feature representation techniques are sensitive to speaker information. Our results show that a recurrent neural network architecture in conjunction with all three feature sets yields the best results for the task.   
 pdf  bib  abs   
   Beyond Information: Is C  hat GPT  Empathetic Enough?    
  Ahmed Belkhir  | Fatiha Sadat    
 This paper aims to explore and enhance ChatGPT’s abilities to generate more human-like conversations by taking into account the emotional state of the user. To achieve this goal, a prompt-driven Emotional Intelligence is used through the empathetic dialogue dataset in order to propose a more empathetic conversational language model. We propose two altered versions of ChatGPT as follows: (1) an emotion-infused version which takes the user’s emotion as input before generating responses using an emotion classifier based on ELECTRA ; and (2) the emotion adapting version that tries to accommodate for how the user feels without any external component. By analyzing responses of the two proposed altered versions and comparing them to the standard version of ChatGPT, we find that using the external emotion classifier leads to more frequent and pronounced use of positive emotions compared to the standard version. On the other hand, using simple prompt engineering to take the user emotion into consideration, does the opposite. Finally, comparisons with state-of-the-art models highlight the potential of prompt engineering to enhance the emotional abilities of chatbots based on large language models.   
 pdf  bib  abs   
   Multimodal Learning for Accurate Visual Question Answering: An Attention-Based Approach    
  Jishnu Bhardwaj  | Anurag Balakrishnan  | Satyam Pathak  | Ishan Unnarkar  | Aniruddha Gawande  | Benyamin Ahmadnia    
 This paper proposes an open-ended task for Visual Question Answering (VQA) that leverages the InceptionV3 Object Detection model and an attention-based Long Short-Term Memory (LSTM) network for question answering. Our proposed model provides accurate natural language answers to questions about an image, including those that require understanding contextual information and background details. Our findings demonstrate that the proposed approach can achieve high accuracy, even with complex and varied visual information. The proposed method can contribute to developing more advanced vision systems that can process and interpret visual information like humans.   
 pdf  bib  abs   
   Measuring Spurious Correlation in Classification: “Clever Hans” in Translationese    
  Angana Borah  | Daria Pylypenko  | Cristina España-Bonet  | Josef van Genabith    
 Recent work has shown evidence of “Clever Hans” behavior in high-performance neural translationese classifiers, where BERT-based classifiers capitalize on spurious correlations, in particular topic information, between data and target classification labels, rather than genuine translationese signals. Translationese signals are subtle (especially for professional translation) and compete with many other signals in the data such as genre, style, author, and, in particular, topic. This raises the general question of how much of the performance of a classifier is really due to spurious correlations in the data versus the signals actually targeted for by the classifier, especially for subtle target signals and in challenging (low resource) data settings. We focus on topic-based spurious correlation and approach the question from two directions: (i) where we have no knowledge about spurious topic information and its distribution in the data, (ii) where we have some indication about the nature of spurious topic correlations. For (i) we develop a measure from first principles capturing alignment of unsupervised topics with target classification labels as an indication of spurious topic information in the data. We show that our measure is the same as purity in clustering and propose a “topic floor” (as in a “noise floor”) for classification. For (ii) we investigate masking of known spurious topic carriers in classification. Both (i) and (ii) contribute to quantifying and (ii) to mitigating spurious correlations.   
 pdf  bib  abs   
   WIKITIDE  : A W  ikipedia-Based Timestamped Definition Pairs Dataset    
  Hsuvas Borkakoty  | Luis Espinosa Anke    
 A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to “learn” new information. While model-centric solutions like continual learning or parameter-efficient fine-tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resources can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better-fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, showing promising results with respect to competitive baselines.   
 pdf  bib  abs   
   BERT  abaporu: Assessing a Genre-Specific Language Model for P  ortuguese NLP     
  Pablo Botton Costa  | Matheus Camasmie Pavan  | Wesley Ramos Santos  | Samuel Caetano Silva  | Ivandré Paraboni    
 Transformer-based language models such as Bidirectional Encoder Representations from Transformers (BERT) are now mainstream in the NLP field, but extensions to languages other than English, to new domains and/or to more specific text genres are still in demand. In this paper we introduced BERTabaporu, a BERT language model that has been pre-trained on Twitter data in the Brazilian Portuguese language. The model is shown to outperform the best-known general-purpose model for this language in three Twitter-related NLP tasks, making a potentially useful resource for Portuguese NLP in general.   
 pdf  bib  abs   
   Comparison of Multilingual Entity Linking Approaches    
  Ivelina Bozhinova  | Andrey Tagarev    
 Despite rapid developments in the field of Natural Language Processing (NLP) in the past few years, the task of Multilingual Entity Linking (MEL) and especially its end-to-end formulation remains challenging. In this paper we aim to evaluate solutions for general end-to-end multilingual entity linking by conducting experiments using both existing complete approaches and novel combinations of pipelines for solving the task. The results identify the best performing current solutions and suggest some directions for further research.   
 pdf  bib  abs   
   Automatic Extraction of the R  omanian Academic Word List: Data and Methods    
  Ana-Maria Bucur  | Andreea Dincă  | Madalina Chitez  | Roxana Rogobete    
 This paper presents the methodology and data used for the automatic extraction of the Romanian Academic Word List (Ro-AWL). Academic Word Lists are useful in both L2 and L1 teaching contexts. For the Romanian language, no such resource exists so far. Ro-AWL has been generated by combining methods from corpus and computational linguistics with L2 academic writing approaches. We use two types of data: (a) existing data, such as the Romanian Frequency List based on the ROMBAC corpus, and (b) self-compiled data, such as the expert academic writing corpus EXPRES. For constructing the academic word list, we follow the methodology for building the Academic Vocabulary List for the English language. The distribution of Ro-AWL features (general distribution, POS distribution) into four disciplinary datasets is in line with previous research. Ro-AWL is freely available and can be used for teaching, research and NLP applications.   
 pdf  bib  abs   
   BB  25 HL  egal S  um: Leveraging BM  25 and BERT  -Based Clustering for the Summarization of Legal Documents    
  Leonardo de Andrade  | Karin Becker    
 Legal document summarization aims to provide a clear understanding of the main points and arguments in a legal document, contributing to the efficiency of the judicial system. In this paper, we propose BB25HLegalSum, a method that combines BERT clusters with the BM25 algorithm to summarize legal documents and present them to users with highlighted important information. The process involves selecting unique, relevant sentences from the original document, clustering them to find sentences about a similar subject, combining them to generate a summary according to three strategies, and highlighting them to the user in the original document. We outperformed baseline techniques using the BillSum dataset, a widely used benchmark in legal document summarization. Legal workers positively assessed the highlighted presentation.   
 pdf  bib  abs   
   SSSD  : Leveraging Pre-trained Models and Semantic Search for Semi-supervised Stance Detection    
  André de Sousa  | Karin Becker    
 Pre-trained models (PTMs) based on the Transformers architecture are trained on massive amounts of data and can capture nuances and complexities in linguistic expressions, making them a powerful tool for many natural language processing tasks. In this paper, we present SSSD (Semantic Similarity Stance Detection), a semi-supervised method for stance detection on Twitter that automatically labels a large, domain-related corpus for training a stance classification model. The method assumes as input a domain set of tweets about a given target and a labeled query set of tweets of representative arguments related to the stances. It scales the automatic labeling of a large number of tweets, and improves classification accuracy by leveraging the power of PTMs and semantic search to capture context and meaning. We largely outperformed all baselines in experiments using the Semeval benchmark.   
 pdf  bib  abs   
   Identifying Semantic Argument Types in Predication and Copredication Contexts: A Zero-Shot Cross-Lingual Approach    
  Deniz Ekin Yavas  | Laura Kallmeyer  | Rainer Osswald  | Elisabetta Jezek  | Marta Ricchiardi  | Long Chen    
 Identifying semantic argument types in predication contexts is not a straightforward task for several reasons, such as inherent polysemy, coercion, and copredication phenomena. In this paper, we train monolingual and multilingual classifiers with a zero-shot cross-lingual approach to identify semantic argument types in predications using pre-trained language models as feature extractors. We train classifiers for different semantic argument types and for both verbal and adjectival predications. Furthermore, we propose a method to detect copredication using these classifiers through identifying the argument semantic type targeted in different predications over the same noun in a sentence. We evaluate the performance of the method on copredication test data with Food•Event nouns for 5 languages.   
 pdf  bib  abs   
   A Review of Research-Based Automatic Text Simplification Tools    
  Isabel Espinosa-Zaragoza  | José Abreu-Salas  | Elena Lloret  | Paloma Moreda  | Manuel Palomar    
 In the age of knowledge, the democratisation of information facilitated through the Internet may not be as pervasive if written language poses challenges to particular sectors of the population. The objective of this paper is to present an overview of research-based automatic text simplification tools. Consequently, we describe aspects such as the language, language phenomena, language levels simplified, approaches, specific target populations these tools are created for (e.g. individuals with cognitive impairment, attention deficit, elderly people, children, language learners), and accessibility and availability considerations. The review of existing studies covering automatic text simplification tools is undergone by searching two databases: Web of Science and Scopus. The eligibility criteria involve text simplification tools with a scientific background in order to ascertain how they operate. This methodology yielded 27 text simplification tools that are further analysed. Some of the main conclusions reached with this review are the lack of resources accessible to the public, the need for customisation to foster the individual’s independence by allowing the user to select what s/he finds challenging to understand while not limiting the user’s capabilities and the need for more simplification tools in languages other than English, to mention a few.   
 pdf  bib  abs   
   T  2 KG  : Transforming Multimodal Document to Knowledge Graph    
  Santiago Galiano  | Rafael Muñoz  | Yoan Gutiérrez  | Andrés Montoyo  | Jose Ignacio Abreu  | Luis Alfonso Ureña    
 The large amount of information in digital format that exists today makes it unfeasible to use manual means to acquire the knowledge contained in these documents. Therefore, it is necessary to develop tools that allow us to incorporate this knowledge into a structure that is easy to use by both machines and humans. This paper presents a system that can incorporate the relevant information from a document in any format, structured or unstructured, into a semantic network that represents the existing knowledge in the document. The system independently processes from structured documents based on its annotation scheme to unstructured documents, written in natural language, for which it uses a set of sensors that identifies the relevant information and subsequently incorporates it to enrich the semantic network that is created by linking all the information based on the knowledge discovered.   
 pdf  bib  abs   
   !Translate: When You Cannot Cook Up a Translation, Explain    
  Federico Garcea  | Margherita Martinelli  | Maja Milicević Petrović  | Alberto Barrón-Cedeño    
 In the domain of cuisine, both dishes and ingredients tend to be heavily rooted in the local context they belong to. As a result, the associated terms are often realia tied to specific cultures and languages. This causes difficulties for non-speakers of the local language and ma- chine translation (MT) systems alike, as it implies a lack of the concept and/or of a plausible translation. MT typically opts for one of two alternatives: keeping the source language terms untranslated or relying on a hyperonym/near-synonym in the target language, provided one exists. !Translate proposes a better alternative: explaining. Given a cuisine entry such as a restaurant menu item, we identify culture-specific terms and enrich the output of the MT system with automatically retrieved definitions of the non-translatable terms in the target language, making the translation more actionable for the final user.   
 pdf  bib  abs   
   Exploring Unsupervised Semantic Similarity Methods for Claim Verification in Health Care News Articles    
  Vishwani Gupta  | Astrid Viciano  | Holger Wormer  | Najmehsadat Mousavinezhad    
 In the 21st century, the proliferation of fake information has emerged as a significant threat to society. Particularly, healthcare medical reporters face challenges when verifying claims related to treatment effects, side effects, and risks mentioned in news articles, relying on scientific publications for accuracy. The accurate communication of scientific information in news articles has long been a crucial concern in the scientific community, as the dissemination of misinformation can have dire consequences in the healthcare domain. Healthcare medical reporters would greatly benefit from efficient methods to retrieve evidence from scientific publications supporting specific claims. This paper delves into the application of unsupervised semantic similarity models to facilitate claim verification for medical reporters, thereby expediting the process. We explore unsupervised multilingual evidence retrieval techniques aimed at reducing the time required to obtain evidence from scientific studies. Instead of employing content classification, we propose an approach that retrieves relevant evidence from scientific publications for claim verification within the healthcare domain. Given a claim and a set of scientific publications, our system generates a list of the most similar paragraphs containing supporting evidence. Furthermore, we evaluate the performance of state-of-the-art unsupervised semantic similarity methods in this task. As the claim and evidence are present in a cross-lingual space, we find that the XML-RoBERTa model exhibits high accuracy in achieving our objective. Through this research, we contribute to enhancing the efficiency and reliability of claim verification for healthcare medical reporters, enabling them to accurately source evidence from scientific publications in a timely manner.   
 pdf  bib  abs   
   Performance Analysis of A  rabic Pre-trained Models on Named Entity Recognition Task    
  Abdelhalim Hafedh Dahou  | Mohamed Amine Cheragui  | Ahmed Abdelali    
 Named Entity Recognition (NER) is a crucial task within natural language processing (NLP) that entails the identification and classification of entities, such as person, organization and location. This study delves into NER specifically in the Arabic language, focusing on the Algerian dialect. While previous research in NER has primarily concentrated on Modern Standard Arabic (MSA), the advent of social media has prompted a need to address the variations found in different Arabic dialects. Moreover, given the notable achievements of Large-scale pre-trained models (PTMs) based on the BERT architecture, this paper aims to evaluate Arabic pre-trained models using an Algerian dataset that covers different domains and writing styles. Additionally, an error analysis is conducted to identify PTMs’ limitations, and an investigation is carried out to assess the performance of trained MSA models on the Algerian dialect. The experimental results and subsequent analysis shed light on the complexities of NER in Arabic, offering valuable insights for future research endeavors.   
 pdf  bib  abs   
   Discourse Analysis of Argumentative Essays of E  nglish Learners Based on CEFR  Level    
  Blaise Hanel  | Leila Kosseim    
 In this paper, we investigate the relationship between the use of discourse relations and the CEFR-level of argumentative English learner essays. Using both the Rhetorical Structure Theory (RST) and the Penn Discourse TreeBank (PDTB) frameworks, we analyze essays from The International Corpus Network of Asian Learners (ICNALE), and the Corpus and Repository of Writing (CROW). Results show that the use of the RST relations of Explanation and Background, as well as the first-level PDTB sense of Contingency, are influenced by the English proficiency level of the writer.   
 pdf  bib  abs   
   Improving Translation Quality for Low-Resource I  nuktitut with Various Preprocessing Techniques    
  Mathias Hans Erik Stenlund  | Mathilde Nanni  | Micaella Bruton  | Meriem Beloucif    
 Neural machine translation has been shown to outperform all other machine translation paradigms when trained in a high-resource setting. However, it still performs poorly when dealing with low-resource languages, for which parallel data for training is scarce. This is especially the case for morphologically complex languages such as Turkish, Tamil, Uyghur, etc. In this paper, we investigate various preprocessing methods for Inuktitut, a low-resource indigenous language from North America, without a morphological analyzer. On both the original and romanized scripts, we test various preprocessing techniques such as Byte-Pair Encoding, random stemming, and data augmentation using Hungarian for the Inuktitut-to-English translation task. We found that there are benefits to retaining the original script as it helps to achieve higher BLEU scores than the romanized models.   
 pdf  bib  abs   
   Explainable Event Detection with Event Trigger Identification as Rationale Extraction    
  Hansi Hettiarachchi  | Tharindu Ranasinghe    
 Most event detection methods act at the sentence-level and focus on identifying sentences related to a particular event. However, identifying certain parts of a sentence that act as event triggers is also important and more challenging, especially when dealing with limited training data. Previous event detection attempts have considered these two tasks separately and have developed different methods. We hypothesise that similar to humans, successful sentence-level event detection models rely on event triggers to predict sentence-level labels. By exploring feature attribution methods that assign relevance scores to the inputs to explain model predictions, we study the behaviour of state-of-the-art sentence-level event detection models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect event triggers. We, therefore, (i) introduce a novel weakly-supervised method for event trigger detection; and (ii) propose to use event triggers as an explainable measure in sentence-level event detection. To the best of our knowledge, this is the first explainable machine learning approach to event trigger identification.   
 pdf  bib  abs   
   Towards a Consensus Taxonomy for Annotating Errors in Automatically Generated Text    
  Rudali Huidrom  | Anya Belz    
 Error analysis aims to provide insights into system errors at different levels of granularity. NLP as a field has a long-standing tradition of analysing and reporting errors which is generally considered good practice. There are existing error taxonomies tailored for different types of NLP task. In this paper, we report our work reviewing existing research on meaning/content error types in generated text, attempt to identify emerging consensus among existing meaning/content error taxonomies, and propose a standardised error taxonomy on this basis. We find that there is virtually complete agreement at the highest taxonomic level where errors of meaning/content divide into (1) Content Omission, (2) Content Addition, and (3) Content Substitution. Consensus in the lower levels is less pronounced, but a compact standardised consensus taxonomy can nevertheless be derived that works across generation tasks and application domains.   
 pdf  bib  abs   
   Uncertainty Quantification of Text Classification in a Multi-Label Setting for Risk-Sensitive Systems    
  Jinha Hwang  | Carol Gudumotu  | Benyamin Ahmadnia    
 This paper addresses the challenge of uncertainty quantification in text classification for medical purposes and provides a three-fold approach to support robust and trustworthy decision-making by medical practitioners. Also, we address the challenge of imbalanced datasets in the medical domain by utilizing the Mondrian Conformal Predictor with a Naïve Bayes classifier.   
 pdf  bib  abs   
   Pretraining Language- and Domain-Specific BERT  on Automatically Translated Text    
  Tatsuya Ishigaki  | Yui Uehara  | Goran Topić  | Hiroya Takamura    
 Domain-specific pretrained language models such as SciBERT are effective for various tasks involving text in specific domains. However, pretraining BERT requires a large-scale language resource, which is not necessarily available in fine-grained domains, especially in non-English languages. In this study, we focus on a setting with no available domain-specific text for pretraining. To this end, we propose a simple framework that trains a BERT on text in the target language automatically translated from a resource-rich language, e.g., English. In this paper, we particularly focus on the materials science domain in Japanese. Our experiments pertain to the task of entity and relation extraction for this domain and language. The experiments demonstrate that the various models pretrained on translated texts consistently perform better than the general BERT in terms of F1 scores although the domain-specific BERTs do not use any human-authored domain-specific text. These results imply that BERTs for various low-resource domains can be successfully trained on texts automatically translated from resource-rich languages.   
 pdf  bib  abs   
   Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of the COVID  -19 Infodemic    
  Ye Jiang  | Xingyi Song  | Carolina Scarton  | Iknoor Singh  | Ahmet Aker  | Kalina Bontcheva    
 The spread of COVID-19 misinformation on social media became a major challenge for citizens, with negative real-life consequences. Prior research focused on detection and/or analysis of COVID-19 misinformation. However, fine-grained classification of misinformation claims has been largely overlooked. The novel contribution of this paper is in introducing a new dataset which makes fine-grained distinctions between statements that assert, comment or question on false COVID-19 claims. This new dataset not only enables social behaviour analysis but also enables us to address both evidence-based and non-evidence-based misinformation classification tasks. Lastly, through leave claim out cross-validation, we demonstrate that classifier performance on unseen COVID-19 misinformation claims is significantly different, as compared to performance on topics present in the training data.   
 pdf  bib  abs   
   Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models    
  Shun Kiyono  | Sho Takase  | Shengzhe Li  | Toshinori Sato    
 Pretrained language models require the use of consistent segmentation (e.g., subword- or character-level segmentation) in pretraining and finetuning. In NLP, many tasks are modeled by subword-level segmentation better than by character-level segmentation. However, because of their format, several tasks require the use of character-level segmentation. Thus, in order to tackle both types of NLP tasks, language models must be independently pretrained for both subword and character-level segmentation. However, this is an inefficient and costly procedure. Instead, this paper proposes a method for training a language model with unified segmentation. This means that the trained model can be finetuned on both subword- and character-level segmentation. The principle of the method is to apply the subword regularization technique to generate a mixture of subword- and character-level segmentation. Through experiment on BERT models, we demonstrate that our method can halve the computational cost of pretraining.   
 pdf  bib  abs   
   Evaluating Data Augmentation for Medication Identification in Clinical Notes    
  Jordan Koontz  | Maite Oronoz  | Alicia Pérez    
 We evaluate the effectiveness of using data augmentation to improve the generalizability of a Named Entity Recognition model for the task of medication identification in clinical notes. We compare disparate data augmentation methods, namely mention-replacement and a generative model, for creating synthetic training examples. Through experiments on the n2c2 2022 Track 1 Contextualized Medication Event Extraction data set, we show that data augmentation with supplemental examples created with GPT-3 can boost the performance of a transformer-based model for small training sets.   
 pdf  bib  abs   
   Challenges of GPT  -3-Based Conversational Agents for Healthcare    
  Fabian Lechner  | Allison Lahnala  | Charles Welch  | Lucie Flek    
 The potential of medical domain dialogue agents lies in their ability to provide patients with faster information access while enabling medical specialists to concentrate on critical tasks. However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences. This paper investigates the challenges and risks of using GPT-3-based models for medical question-answering (MedQA). We perform several evaluations contextualized in terms of standard medical principles. We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.   
 pdf  bib  abs   
   Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks    
  João Leite  | Carolina Scarton  | Diego Silva    
 Online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. Creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. However, unlabelled data is abundant, easier, and cheaper to obtain. In this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. Recent “noisy” self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. In this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained BERT architectures varying in size. We evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% F1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.   
 pdf  bib  abs   
   A Practical Survey on Zero-Shot Prompt Design for In-Context Learning    
  Yinheng Li    
 The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single “best” prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.   
 pdf  bib  abs   
   Sign Language Recognition and Translation: A Multi-Modal Approach Using Computer Vision and Natural Language Processing    
  Jacky Li  | Jaren Gerdes  | James Gojit  | Austin Tao  | Samyak Katke  | Kate Nguyen  | Benyamin Ahmadnia    
 Sign-to-Text (S2T) is a hand gesture recognition program in the American Sign Language (ASL) domain. The primary objective of S2T is to classify standard ASL alphabets and custom signs and convert the classifications into a stream of text using neural networks. This paper addresses the shortcomings of pure Computer Vision techniques and applies Natural Language Processing (NLP) as an additional layer of complexity to increase S2T’s robustness.   
 pdf  bib  abs   
   Classification-Aware Neural Topic Model Combined with Interpretable Analysis - for Conflict Classification    
  Tianyu Liang  | Yida Mu  | Soonho Kim  | Darline Kuate  | Julie Lang  | Rob Vos  | Xingyi Song    
 A large number of conflict events are affecting the world all the time. In order to analyse such conflict events effectively, this paper presents a Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information Classification and Topic Discovery. The model provides a reliable interpretation of classification results and discovered topics by introducing interpretability analysis. At the same time, interpretation is introduced into the model architecture to improve the classification performance of the model and to allow interpretation to focus further on the details of the data. Finally, the model architecture is optimised to reduce the complexity of the model.   
 pdf  bib  abs   
   Transformer-Based Language Models for B  ulgarian    
  Iva Marinova  | Kiril Simov  | Petya Osenova    
 This paper presents an approach for training lightweight and robust language models for Bulgarian that mitigate gender, political, racial, and other biases in the data. Our method involves scraping content from major Bulgarian online media providers using a specialized procedure for source filtering, topic selection, and lexicon-based removal of inappropriate language during the pre-training phase. We continuously improve the models by incorporating new data from various domains, including social media, books, scientific literature, and linguistically modified corpora. Our motivation is to provide a solution that is sufficient for all natural language processing tasks in Bulgarian, and to address the lack of existing procedures for guaranteeing the robustness of such models.   
 pdf  bib  abs   
   Data Fusion for Better Fake Reviews Detection    
  Alimuddin Melleng  | Anna Jurek-Loughrey  | Deepak P    
 Online reviews have become critical in informing purchasing decisions, making the detection of fake reviews a crucial challenge to tackle. Many different Machine Learning based solutions have been proposed, using various data representations such as n-grams or document embeddings. In this paper, we first explore the effectiveness of different data representations, including emotion, document embedding, n-grams, and noun phrases in embedding for mat, for fake reviews detection. We evaluate these representations with various state-of-the-art deep learning models, such as BILSTM, LSTM, GRU, CNN, and MLP. Following this, we propose to incorporate different data repre- sentations and classification models using early and late data fusion techniques in order to im- prove the prediction performance. The experiments are conducted on four datasets: Hotel, Restaurant, Amazon, and Yelp. The results demonstrate that combination of different data representations significantly outperform any of the single data representations   
 pdf  bib  abs   
   Automatic Assessment Of Spoken E  nglish Proficiency Based on Multimodal and Multitask Transformers    
  Kamel Nebhi  | György Szaszák    
 This paper describes technology developed to automatically grade students on their English spontaneous spoken language proficiency with common european framework of reference for languages (CEFR) level. Our automated assessment system contains two tasks: elicited imitation and spontaneous speech assessment. Spontaneous speech assessment is a challenging task that requires evaluating various aspects of speech quality, content, and coherence. In this paper, we propose a multimodal and multitask transformer model that leverages both audio and text features to perform three tasks: scoring, coherence modeling, and prompt relevancy scoring. Our model uses a fusion of multiple features and multiple modality attention to capture the interactions between audio and text modalities and learn from different sources of information.   
 pdf  bib  abs   
   MQDD  : Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain    
  Jan Pasek  | Jakub Sido  | Miloslav Konopik  | Ondrej Prazak    
 This work proposes a new pipeline for leveraging data collected on the Stack Overflow website for pre-training a multimodal model for searching duplicates on question answering websites. Our multimodal model is trained on question descriptions and source codes in multiple programming languages. We design two new learning objectives to improve duplicate detection capabilities. The result of this work is a mature, fine-tuned Multimodal Question Duplicity Detection (MQDD) model, ready to be integrated into a Stack Overflow search system, where it can help users find answers for already answered questions. Alongside the MQDD model, we release two datasets related to the software engineering domain. The first Stack Overflow Dataset (SOD) represents a massive corpus of paired questions and answers. The second Stack Overflow Duplicity Dataset (SODD) contains data for training duplicate detection models.   
 pdf  bib  abs   
   Forming Trees with Treeformers    
  Nilay Patel  | Jeffrey Flanigan    
 Human language is known to exhibit a nested, hierarchical structure, allowing us to form complex sentences out of smaller pieces. However, many state-of-the-art neural networks models such as Transformers have no explicit hierarchical structure in their architecture—that is, they don’t have an inductive bias toward hierarchical structure. Additionally, Transformers are known to perform poorly on compositional generalization tasks which require such structures. In this paper, we introduce Treeformer, a general-purpose encoder module inspired by the CKY algorithm which learns a composition operator and pooling function to construct hierarchical encodings for phrases and sentences. Our extensive experiments demonstrate the benefits of incorporating hierarchical structure into the Transformer and show significant improvements in compositional generalization as well as in downstream tasks such as machine translation, abstractive summarization, and various natural language understanding tasks.   
 pdf  bib  abs   
   Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model    
  Pavel Přibáň  | Ondrej Prazak    
 This paper presents a series of approaches aimed at enhancing the performance of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic information from a Semantic Role Labeling (SRL) model. We propose a novel end-to-end Semantic Role Labeling model that effectively captures most of the structured semantic information within the Transformer hidden state. We believe that this end-to-end model is well-suited for our newly proposed models that incorporate semantic information. We evaluate the proposed models in two languages, English and Czech, employing ELECTRA-small models. Our combined models improve ABSA performance in both languages. Moreover, we achieved new state-of-the-art results on the Czech ABSA.   
 pdf  bib  abs   
   Topic Modeling Using Community Detection on a Word Association Graph    
  Mahfuzur Rahman Chowdhury  | Intesur Ahmed  | Farig Sadeque  | Muhammad Yanhaona    
 Topic modeling of a text corpus is one of the most well-studied areas of information retrieval and knowledge discovery. Despite several decades of research in the area that begets an array of modeling tools, some common problems still obstruct automated topic modeling from matching users’ expectations. In particular, existing topic modeling solutions suffer when the distribution of words among the underlying topics is uneven or the topics are overlapped. Furthermore, many solutions ask the user to provide a topic count estimate as input, which limits their usefulness in modeling a corpus where such information is unavailable. We propose a new topic modeling approach that overcomes these shortcomings by formulating the topic modeling problem as a community detection problem in a word association graph/network that we generate from the text corpus. Experimental evaluation using multiple data sets of three different types of text corpora shows that our approach is superior to prominent topic modeling alternatives in most cases. This paper describes our approach and discusses the experimental findings.   
 pdf  bib  abs   
   Exploring Techniques to Detect and Mitigate Non-Inclusive Language Bias in Marketing Communications Using a Dictionary-Based Approach    
  Bharathi Raja Chakravarthi  | Prasanna Kumar Kumaresan  | Rahul Ponnusamy  | John P. McCrae  | Michaela Comerford  | Jay Megaro  | Deniz Keles  | Last Feremenga    
 We propose a new dataset for detecting non-inclusive language in sentences in English. These sentences were gathered from public sites, explaining what is inclusive and what is non-inclusive. We also extracted potentially non-inclusive keywords/phrases from the guidelines from business websites. A phrase dictionary was created by using an automatic extension with a word embedding trained on a massive corpus of general English text. In the end, a phrase dictionary was constructed by hand-editing the previous one to exclude inappropriate expansions and add the keywords from the guidelines. In a business context, the words individuals use can significantly impact the culture of inclusion and the quality of interactions with clients and prospects. Knowing the right words to avoid helps customers of different backgrounds and historically excluded groups feel included. They can make it easier to have productive, engaging, and positive communications. You can find the dictionaries, the code, and the method for making requests for the corpus at (we will release the link for data and code once the paper is accepted).   
 pdf  bib  abs   
   Modeling Easiness for Training Transformers with Curriculum Learning    
  Leonardo Ranaldi  | Giulia Pucci  | Fabio Massimo Zanzotto    
 Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art.   
 pdf  bib  abs   
   The Dark Side of the Language: Pre-trained Transformers in the D  ark N  et    
  Leonardo Ranaldi  | Aria Nourbakhsh  | Elena Sofia Ruzzetti  | Arianna Patrizi  | Dario Onorati  | Michele Mastromattei  | Francesca Fallucchi  | Fabio Massimo Zanzotto    
 Pre-trained Transformers are challenging human performances in many Natural Language Processing tasks. The massive datasets used for pre-training seem to be the key to their success on existing tasks. In this paper, we explore how a range of pre-trained natural language understanding models performs on definitely unseen sentences provided by classification tasks over a DarkNet corpus. Surprisingly, results show that syntactic and lexical neural networks perform on par with pre-trained Transformers even after fine-tuning. Only after what we call extreme domain adaptation, that is, retraining with the masked language model task on all the novel corpus, pre-trained Transformers reach their standard high results. This suggests that huge pre-training corpora may give Transformers unexpected help since they are exposed to many of the possible sentences.   
 pdf  bib  abs   
   ‘ C  hem X  tract’ A System for Extraction of Chemical Events from Patent Documents    
  Pattabhi RK Rao  | Sobha Lalitha Devi    
 ChemXtraxt main goal is to extract the chemical events from patent documents. Event extraction requires that we first identify the names of chemical compounds involved in the events. Thus, in this work two extractions are done and they are (a) names of chemical compounds and (b) event that identify the specific involvement of the chemical compounds in a chemical reaction. Extraction of essential elements of a chemical reaction, generally known as Named Entity Recognition (NER), extracts the compounds, condition and yields, their specific role in reaction and assigns a label according to the role it plays within a chemical reaction. Whereas event extraction identifies the chemical event relations between the chemical compounds identified. Here in this work we have used Neural Conditional Random Fields (NCRF), which combines the power of artificial neural network (ANN) and CRFs. Different levels of features that include linguistic, orthographical and lexical clues are used. The results obtained are encouraging.   
 pdf  bib  abs   
 pdf  bib  abs   
   Event Annotation and Detection in K  annada- E  nglish Code-Mixed Social Media Data    
  Sumukh S  | Abhinav Appidi  | Manish Shrivastava    
 Code-mixing (CM) is a frequently observed phenomenon on social media platforms in multilingual societies such as India. While the increase in code-mixed content on these platforms provides good amount of data for studying various aspects of code-mixing, the lack of automated text analysis tools makes such studies difficult. To overcome the same, tools such as language identifiers, Parts-of-Speech (POS) taggers and Named Entity Recognition (NER) for analysing code-mixed data have been developed. One such important tool is Event Detection, an important information retrieval task which can be used to identify critical facts occurring in the vast streams of unstructured text data available. While event detection from text is a hard problem on its own, social media data adds to it with its informal nature, and code-mixed (Kannada-English) data further complicates the problem due to its word-level mixing, lack of structure and incomplete information. In this work, we have tried to address this problem. We have proposed guidelines for the annotation of events in Kannada-English CM data and provided some baselines for the same with careful feature selection.   
 pdf  bib  abs   
   Three Approaches to Client Email Topic Classification    
  Branislava Šandrih Todorović  | Katarina Josipović  | Jurij Kodre    
 This paper describes a use case that was implemented and is currently running in production at the Nova Ljubljanska Banka, that involves classifying incoming client emails in the Slovenian language according to their topics and priorities. Since the proposed approach relies only on the Named Entity Recogniser (NER) of personal names as a language-dependent resource (for the purpose of anonymisation), that is the only prerequisite for applying the approach to any other language.   
 pdf  bib  abs   
   Prompt-Based Approach for C  zech Sentiment Analysis    
  Jakub Šmíd  | Pavel Přibáň    
 This paper introduces the first prompt-based methods for aspect-based sentiment analysis and sentiment classification in Czech. We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our prompt-based approach over traditional fine-tuning. In addition, we conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples compared to traditional fine-tuning. We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario.   
 pdf  bib  abs   
   Measuring Gender Bias in Natural Language Processing: Incorporating Gender-Neutral Linguistic Forms for Non-Binary Gender Identities in Abusive Speech Detection    
  Nasim Sobhani  | Kinshuk Sengupta  | Sarah Jane Delany    
 Predictions from machine learning models can reflect bias in the data on which they are trained. Gender bias has been shown to be prevalent in natural language processing models. The research into identifying and mitigating gender bias in these models predominantly considers gender as binary, male and female, neglecting the fluidity and continuity of gender as a variable. In this paper, we present an approach to evaluate gender bias in a prediction task, which recognises the non-binary nature of gender. We gender-neutralise a random subset of existing real-world hate speech data. We extend the existing template approach for measuring gender bias to include test examples that are gender-neutral. Measuring the bias across a selection of hate speech datasets we show that the bias for the gender-neutral data is closer to that seen for test instances that identify as male than those that identify as female.   
 pdf  bib  abs   
   L  e SS  : A Computationally-Light Lexical Simplifier for S  panish    
  Sanja Stajner  | Daniel Ibanez  | Horacio Saggion    
 Due to having knowledge of only basic vocabulary, many people cannot understand up-to-date written information and thus make informed decisions and fully participate in the society. We propose LeSS, a modular lexical simplification architecture that outperforms state-of-the-art lexical simplification systems for Spanish. In addition to its state-of-the-art performance, LeSS is computationally light, using much less disk space, CPU and GPU, and having faster loading and execution time than the transformer-based lexical simplification models which are predominant in the field.   
 pdf  bib  abs   
   N  o H  ate B  razil: A B  razilian P  ortuguese Text Offensiveness Analysis System    
  Francielle Vargas  | Isabelle Carvalho  | Wolfgang Schmeisser-Nieto  | Fabrício Benevenuto  | Thiago Pardo    
 Hate speech is a surely relevant problem in Brazil. Nevertheless, its regulation is not effective due to the difficulty to identify, quantify and classify offensive comments. Here, we introduce a novel system for offensive comment analysis in Brazilian Portuguese. The system titled “NoHateBrazil” recognizes explicit and implicit offensiveness in context at a fine-grained level. Specifically, we propose a framework for data collection, human annotation and machine learning models that were used to build the system. In addition, we assess the potential of our system to reflect stereotypical beliefs against marginalized groups by contrasting them with counter-stereotypes. As a result, a friendly web application was implemented, which besides presenting relevant performance, showed promising results towards mitigation of the risk of reinforcing social stereotypes. Lastly, new measures were proposed to improve the explainability of offensiveness classification and reliability of the model’s predictions.   
 pdf  bib  abs   
   Socially Responsible Hate Speech Detection: Can Classifiers Reflect Social Stereotypes?    
  Francielle Vargas  | Isabelle Carvalho  | Ali Hürriyetoğlu  | Thiago Pardo  | Fabrício Benevenuto    
 Recent studies have shown that hate speech technologies may propagate social stereotypes against marginalized groups. Nevertheless, there has been a lack of realistic approaches to assess and mitigate biased technologies. In this paper, we introduce a new approach to analyze the potential of hate-speech classifiers to reflect social stereotypes through the investigation of stereotypical beliefs by contrasting them with counter-stereotypes. We empirically measure the distribution of stereotypical beliefs by analyzing the distinctive classification of tuples containing stereotypes versus counter-stereotypes in machine learning models and datasets. Experiment results show that hate speech classifiers attribute unreal or negligent offensiveness to social identity groups by reflecting and reinforcing stereotypical beliefs regarding minorities. Furthermore, we also found that models that embed expert and context information from offensiveness markers present promising results to mitigate social stereotype bias towards socially responsible hate speech detection.   
 pdf  bib  abs   
   Kāraka-Based Answer Retrieval for Question Answering in I  ndic Languages    
  Devika Verma  | Ramprasad S. Joshi  | Aiman A. Shivani  | Rohan D. Gupta    
 Kārakas from ancient Paninian grammar form a concise set of semantic roles that capture crucial aspect of sentence meaning pivoted on the action verb. In this paper, we propose employing a kāraka-based approach for retrieving answers in Indic question-answering systems. To study and evaluate this novel approach, empirical experiments are conducted over large benchmark corpora in Hindi and Marathi. The results obtained demonstrate the effectiveness of the proposed method. Additionally, we explore the varying impact of two approaches for extracting kārakas. The literature surveyed and experiments conducted encourage hope that kāraka annotation can improve communication with machines using natural languages, particularly in low-resource languages.   
 pdf  bib  abs   
   Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain    
  Gayashan Weerasundara  | Nisansa de Silva    
 Some Natural Language Processing (NLP) tasks that are in the sufficiently solved state for general domain English still struggle to attain the same level of performance in specific domains. Named Entity Recognition (NER), which aims to find and categorize entities in text is such a task met with difficulties in adapting to domain specificity. This paper compares the performance of 10 NER models on 7 adventure books from the Dungeons and Dragons (D&D) domain which is a subdomain of fantasy literature. Fantasy literature, being rich and diverse in vocabulary, poses considerable challenges for conventional NER. In this study, we use open-source Large Language Models (LLM) to annotate the named entities and character names in each number of official D&D books and evaluate the precision and distribution of each model. The paper aims to identify the challenges and opportunities for improving NER in fantasy literature. Our results show that even in the off-the-shelf configuration, Flair, Trankit, and Spacy achieve better results for identifying named entities in the D&D domain compared to their peers.   
 pdf  bib  abs   
   Poetry Generation Combining Poetry Theme Labels Representations    
  Yingyu Yan  | Dongzhen Wen  | Liang Yang  | Dongyu Zhang  | Hongfei Lin    
 Ancient Chinese poetry is the earliest literary genre that took shape in Chinese literature and has a dissemination effect, showing China’s profound cultural heritage. At the same time, the generation of ancient poetry is an important task in the field of digital humanities, which is of great significance to the inheritance of national culture and the education of ancient poetry. The current work in the field of poetry generation is mainly aimed at improving the fluency and structural accuracy of words and sentences, ignoring the theme unity of poetry generation results. In order to solve this problem, this paper proposes a graph neural network poetry theme representation model based on label embedding. On the basis of the network representation of poetry, the topic feature representation of poetry is constructed and learned from the granularity of words. Then, the features of the poetry theme representation model are combined with the autoregressive language model to construct a theme-oriented ancient Chinese poetry generation model TLPG (Poetry Generation with Theme Label). Through machine evaluation and evaluation by experts in related fields, the model proposed in this paper has significantly improved the topic consistency of poetry generation compared with existing work on the premise of ensuring the fluency and format accuracy of poetry.   
 pdf  bib  abs   
   Systematic T  ext R  ank Optimization in Extractive Summarization    
  Morris Zieve  | Anthony Gregor  | Frederik Juul Stokbaek  | Hunter Lewis  | Ellis Marie Mendoza  | Benyamin Ahmadnia    
 With the ever-growing amount of textual data, extractive summarization has become increasingly crucial for efficiently processing information. The TextRank algorithm, a popular unsupervised method, offers excellent potential for this task. In this paper, we aim to optimize the performance of TextRank by systematically exploring and verifying the best preprocessing and fine-tuning techniques. We extensively evaluate text preprocessing methods, such as tokenization, stemming, and stopword removal, to identify the most effective combination with TextRank. Additionally, we examine fine-tuning strategies, including parameter optimization and incorporation of domain-specific knowledge, to achieve superior summarization quality.   
   up   pdf (full)   
  bib (full)   Proceedings of the 8th Student Research Workshop associated with the International Conference Recent Advances in Natural Language Processing   
 pdf  bib   
   Proceedings of the 8th Student Research Workshop associated with the International Conference Recent Advances in Natural Language Processing    
  Momchil Hardalov  | Zara Kancheva  | Boris Velichkov  | Ivelina Nikolova-Koleva  | Milena Slavcheva    
 pdf  bib  abs   
   Evaluating Hallucinations in Large Language Models for B  ulgarian Language    
  Melania Berbatova  | Yoan Salambashev    
 In this short paper, we introduce the task of evaluating the hallucination of large language models for the Bulgarian language. We first give definitions of what is a hallucination in large language models and what evaluation methods for measuring hallucinations exist. Next, we give an overview of the multilingual evaluation of the latest large language models, focusing on the evaluation of the performance in Bulgarian on tasks, related to hallucination. We then present a method to evaluate the level of hallucination in a given language with no reference data, and provide some initial experiments with this method in Bulgarian. Finally, we provide directions for future research on the topic.   
 pdf  bib  abs   
   Leveraging Probabilistic Graph Models in Nested Named Entity Recognition for P  olish    
  Jędrzej Jamnicki    
 This paper presents ongoing work on leveraging probabilistic graph models, specifically conditional random fields and hidden Markov models, in nested named entity recognition for the Polish language. NER is a crucial task in natural language processing that involves identifying and classifying named entities in text documents. Nested NER deals with recognizing hierarchical structures of entities that overlap with one another, presenting additional challenges. The paper discusses the methodologies and approaches used in nested NER, focusing on CRF and HMM. Related works and their contributions are reviewed, and experiments using the KPWr dataset are conducted, particularly with the BiLSTM-CRF model and Word2Vec and HerBERT embeddings. The results show promise in addressing nested NER for Polish, but further research is needed to develop robust and accurate models for this complex task.   
 pdf  bib  abs   
   Exploring Low-resource Neural Machine Translation for S  inhala- T  amil Language Pair    
  Ashmari Pramodya    
 At present, Neural Machine Translation is a promising approach for machine translation. Transformer-based deep learning architectures in particular show a substantial performance increase in translating between various language pairs. However, many low-resource language pairs still struggle to lend themselves to Neural Machine Translation due to their data-hungry nature. In this article, we investigate methods of expanding the parallel corpus to enhance translation quality within a model training pipeline, starting from the initial collection of parallel data to the training process of baseline models. Grounded on state-of-the-art Neural Machine Translation approaches such as hyper-parameter tuning, and data augmentation with forward and backward translation, we define a set of best practices for improving Tamil-to-Sinhala machine translation and empirically validate our methods using standard evaluation metrics. Our results demonstrate that the Neural Machine Translation models trained on larger amounts of back-translated data outperform other synthetic data generation approaches in Transformer base training settings. We further demonstrate that, even for language pairs with limited resources, Transformer models are able to tune to outperform existing state-of-the-art Statistical Machine Translation models by as much as 3.28 BLEU points in the Tamil to Sinhala translation scenarios.   
 pdf  bib  abs   
     Prompting C  hat GPT  to Draw Morphological Connections for New Word Comprehension    
  Bianca-Madalina Zgreaban  | Rishabh Suresh    
 Though more powerful, Large Language Models need to be periodically retrained for updated information, consuming resources and energy. In this respect, prompt engineering can prove a possible solution to re-training. To explore this line of research, this paper uses a case study, namely, finding the best prompting strategy for asking ChatGPT to define new words based on morphological connections. To determine the best prompting strategy, each definition provided by the prompt was ranked in terms of plausibility and humanlikeness criteria. The findings of this paper show that adding contextual information, operationalised as the keywords ‘new’ and ‘morpheme’, significantly improve the performance of the model for any prompt. While no single prompt significantly outperformed all others, there were differences between performances on the two criteria for most prompts. ChatGPT also provided the most correct definitions with a persona-type prompt.   
   up   pdf (full)   
   Sentence Embedding Models for A  ncient G  reek Using Multilingual Knowledge Distillation    
  Kevin Krahn  | Derrick Tate  | Andrew C. Lamicela    
 Contextual language models have been trained on Classical languages, including Ancient Greek and Latin, for tasks such as lemmatization, morphological tagging, part of speech tagging, authorship attribution, and detection of scribal errors. However, high-quality sentence embedding models for these historical languages are significantly more difficult to achieve due to the lack of training data. In this work, we use a multilingual knowledge distillation approach to train BERT models to produce sentence embeddings for Ancient Greek text. The state-of-the-art sentence embedding approaches for high-resource languages use massive datasets, but our distillation approach allows our Ancient Greek models to inherit the properties of these models while using a relatively small amount of translated sentence data. We build a parallel sentence dataset using a sentence-embedding alignment method to align Ancient Greek documents with English translations, and use this dataset to train our models. We evaluate our models on translation search, semantic similarity, and semantic retrieval tasks and investigate translation bias. We make our training and evaluation datasets freely available.   
 pdf  bib  abs   
   L  atin Morphology through the Centuries: Ensuring Consistency for Better Language Processing    
  Federica Gamba  | Daniel Zeman    
 This paper focuses on the process of harmonising the five Latin treebanks available in Universal Dependencies with respect to morphological annotation. We propose a workflow that allows to first spot inconsistencies and missing information, in order to detect to what extent the annotations differ, and then correct the retrieved bugs, with the goal of equalising the annotation of morphological features in the treebanks and producing more consistent linguistic data. Subsequently, we present some experiments carried out with UDPipe and Stanza in order to assess the impact of such harmonisation on parsing accuracy.   
 pdf  bib  abs   
   Morphological and Semantic Evaluation of A  ncient C  hinese Machine Translation    
  Kai Jin  | Dan Zhao  | Wuying Liu    
 Machine translation (MT) of ancient Chinese texts presents unique challenges due to the complex grammatical structures, cultural nuances, and polysemy of the language. This paper focuses on evaluating the translation quality of different platforms for ancient Chinese texts using The Analects as a case study. The evaluation is conducted using the BLEU, LMS, and ESS metrics, and the platforms compared include three machine translation platforms (Baidu Translate, Bing Microsoft Translator, and DeepL), and one language generation model ChatGPT that can engage in translation endeavors. Results show that Baidu performs the best, surpassing the other platforms in all three metrics, while ChatGPT ranks second and demonstrates unique advantages. The translations generated by ChatGPT are deemed highly valuable as references. The study contributes to understanding the challenges of MT for ancient Chinese texts and provides insights for users and researchers in this field. It also highlights the importance of considering specific domain requirements when evaluating MT systems.   
 pdf  bib  abs   
   Evaluating Existing Lemmatisers on Unedited Byzantine G  reek Poetry    
  Colin Swaelens  | Ilse De Vos  | Els Lefever    
 This paper reports on the results of a comparative evaluation in view of the development of a new lemmatizer for unedited, Byzantine Greek texts. For the experiment, the performance of four existing lemmatizers, all pre-trained on Ancient Greek texts, was evaluated on how well they could handle texts stemming from the Middle Ages and displaying quite some peculiarities. The aim of this study is to get insights into the pitfalls of existing lemmatistion approaches as well as the specific challenges of our Byzantine Greek corpus, in order to develop a lemmatizer that can cope with its peculiarities. The results of the experiment show an accuracy drop of 20pp. on our corpus, which is further investigated in a qualitative error analysis.   
 pdf  bib  abs   
   Vector Based Stylistic Analysis on A  ncient C  hinese Books: Take the Three Commentaries on the Spring and Autumn Annals as an Example    
  Yue Qi  | Liu Liu  | Bin Li  | Dongbo Wang    
 Commentary of Gongyang, Commentary of Guliang, and Commentary of Zuo are collectively called the Three Commentaries on the Spring and Autumn Annals, which are the supplement and interpretation of the content of Spring and Autumn Annals with value in historical and literary research. In traditional research paradigms, scholars often explored the differences between the Three Commentaries within the details in contexts. Starting from the view of computational humanities, this paper examines the differences in the language style of the Three Commentaries through the representation of language, which takes the methods of deep learning. Specifically, this study vectorizes the context at word and sentence levels. It maps them into the same plane to find the differences between the use of words and sentences in the Three Commentaries. The results show that the Commentary of Gongyang and the Commentary of Guliang are relatively similar, while the Commentary of Zuo is significantly different. This paper verifies the feasibility of deep learning methods in stylistics study under computational humanities. It provides a valuable perspective for studying the Three Commentaries on the Spring and Autumn Annals.   
 pdf  bib  abs   
   Coding Design of Oracle Bone Inscriptions Input Method Based on “ Z  hong H  ua Z  i K  u” Database    
  Dongxin Hu    
 Abstract : Based on the oracle bone glyph data in the “ZhongHuaZiKu”database, this paper designs a new input method coding scheme which is easy to search in the database, and provides a feasible scheme for the design of oracle bone glyph input method software in the future. The coding scheme in this paper is based on the experience of the past oracle bone inscriptions input method design. In view of the particularity of oracle bone inscriptions, the difference factors such as component combination, sound code and shape code ( letter ) are added, and the coding format is designed as follows : The single component characters in the identified characters are arranged according to the format of " structural code + pronunciation full spelling code + tone code " ; the multi-component characters in the identified characters are arranged according to the format of " structure code + split component pronunciation full spelling code + overall glyph pronunciation full spelling code”; unidentified characters are arranged according to the format of " y + identified component pronunciation full spelling + unidentified component shape code ( letter ) ".Among them, the identified component code and the unidentified component shape code are input in turn according to the specific glyph from left to right, from top to bottom, and from outside to inside. Encoding through these coding formats, the heavy code rate is low, and the input habits of most people are also taken into account. Keywords : oracle bone inscriptions ; input method ; coding   
 pdf  bib  abs   
   Enhancing State-of-the-Art NLP  Models for Classical A  rabic    
  Tariq Yousef  | Lisa Mischer  | Hamid Reza Hakimi  | Maxim Romanov    
 Classical Arabic, like all other historical languages, lacks adequate training datasets and accurate “off-the-shelf” models that can be directly employed in the processing pipelines. In this paper, we present our in-progress work in developing and training deep learning models tailored for handling diverse tasks relevant to classical Arabic texts. Specifically, we focus on Named Entities Recognition, person relationships classification, toponym sub-classification, onomastic section boundaries detection, onomastic entities classification, as well as date recognition and classification. Our work aims to address the challenges associated with these tasks and provide effective solutions for analyzing classical Arabic texts. Although this work is still in progress, the preliminary results reported in the paper indicate excellent to satisfactory performance of the fine-tuned models, effectively meeting the intended goal for which they were trained.   
 pdf  bib  abs   
   Classical Philology in the Time of AI  : Exploring the Potential of Parallel Corpora in Ancient Language    
  Tariq Yousef  | Chiara Palladino  | Farnoosh Shamsian    
 This paper provides an overview of diverse applications of parallel corpora in ancient languages, particularly Ancient Greek. In the first part, we provide the fundamental principles of parallel corpora and a short overview of their applications in the study of ancient texts. In the second part, we illustrate how to leverage on parallel corpora to perform various NLP tasks, including automatic translation alignment, dynamic lexica induction, and Named Entity Recognition. In the conclusions, we emphasize current limitations and future work.   
 pdf  bib  abs   
   Using Word Embeddings for Identifying Emotions Relating to the Body in a N  eo- A  ssyrian Corpus    
  Ellie Bennett  | Aleksi Sahala    
 Research into emotions is a developing field within Assyriology, and NLP tools for Akkadian texts offers a new perspective on the data. In this submission, we use PMI-based word embeddings to explore the relationship between parts of the body and emotions. Using data downloaded from Oracc, we ask which parts of the body were semantically linked to emotions. We do this through examining which of the top 10 results for a body part could be used to express emotions. After identifying two words for the body that have the most emotion words in their results list ( libbu  and kabattu  ), we then examine whether the emotion words in their results lists were indeed used in this manner in the Neo-Assyrian textual corpus. The results indicate that of the two body parts, kabattu  was semantically linked to happiness and joy, and had a secondary emotional field of anger.   
 pdf  bib  abs   
   T  ibetan Dependency Parsing with Graph Convolutional Neural Networks    
  Bo An    
 Dependency parsing is a syntactic analysis method to analyze the dependency relationships between words in a sentence. The interconnection between words through dependency relationships is typical graph data. Traditional Tibetan dependency parsing methods typically model dependency analysis as a transition-based or sequence-labeling task, ignoring the graph information between words. To address this issue, this paper proposes a graph neural network (GNN)-based Tibetan dependency parsing method. This method treats Tibetan words as nodes and the dependency relationships between words as edges, thereby constructing the graph data of Tibetan sentences. Specifically, we use BiLSTM to learn the word representations of Tibetan, utilize GNN to model the relationships between words and employ MLP to predict the types of relationships between words. We conduct experiments on a Tibetan dependency database, and the results show that the proposed method can achieve high-quality Tibetan dependency parsing results.   
 pdf  bib  abs   
   Classifying Organized Criminal Violence in M  exico using ML  and LLM  s    
  Javier Osorio  | Juan Vasquez    
 Natural Language Processing (NLP) tools have been rapidly adopted in political science for the study of conflict and violence. In this paper, we present an application to analyze various lethal and non-lethal events conducted by organized criminal groups and state forces in Mexico. Based on a large corpus of news articles in Spanish and a set of high-quality annotations, the application evaluates different Machine Learning (ML) algorithms and Large Language Models (LLMs) to classify documents and individual sentences, and to identify specific behaviors related to organized criminal violence and law enforcement efforts. Our experiments support the growing evidence that BERT-like models achieve outstanding classification performance for the study of organized crime. This application amplifies the capacity of conflict scholars to provide valuable information related to important security challenges in the developing world.   
 pdf  bib  abs   
   Where “where” Matters : Event Location Disambiguation with a BERT  Language Model    
  Hristo Tanev  | Bertrand De Longueville    
 The method method presented in this paper uses a BERT model for classifying location mentions in event reporting news texts into two classes: a place of an event, called main location, or another location mention, called here secondary location. Our evaluation on articles, reporting protests, shows promising results and demonstrates the feasibility of our approach and the event geolocation task in general. We evaluate our method against a simple baseline and state of the art ML models and we achieve a significant improvement in all cases by using the BERT model. In contrast to other location classification approaches, we completelly avoid lingusitic pre processing and feature engineering, which is a pre-requisite for all multi-domain and multilingual applications.   
 pdf  bib  abs   
 pdf  bib  abs   
   MLM  odeler5 @ Causal News Corpus 2023: Using R  o BERT  a for Casual Event Classification    
  Amrita Bhatia  | Ananya Thomas  | Nitansh Jain  | Jatin Bedi    
 Identifying cause-effect relations plays an integral role in the understanding and interpretation of natural languages. Furthermore, automated mining of causal relations from news and text about socio-political events is a stepping stone in gaining critical insights, including analyzing the scale, frequency and trends across timelines of events, as well as anticipating future ones. The Shared Task 3, part of the 6th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE @ RANLP 2023), involved the task of Event Causality Identification with Causal News Corpus. We describe our approach to Subtask 1, dealing with causal event classification, a supervised binary classification problem to annotate given event sentences with whether they contained any cause-effect relations. To help achieve this task, a BERT based architecture - RoBERTa was implemented. The results of this model are validated on the dataset provided by the organizers of this task.   
 pdf  bib  abs   
   B  osch AI  @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction using Multi-Layer Sequence Tagging and Data Augmentation    
  Timo Pierre Schrader  | Simon Razniewski  | Lukas Lange  | Annemarie Friedrich    
 pdf  bib  abs   
   An Evaluation Framework for Mapping News Headlines to Event Classes in a Knowledge Graph    
  Steve Fonin Mbouadeu  | Martin Lorenzo  | Ken Barker  | Oktie Hassanzadeh    
 Mapping ongoing news headlines to event-related classes in a rich knowledge base can be an important component in a knowledge-based event analysis and forecasting solution. In this paper, we present a methodology for creating a benchmark dataset of news headlines mapped to event classes in Wikidata, and resources for the evaluation of methods that perform the mapping. We use the dataset to study two classes of unsupervised methods for this task: 1) adaptations of classic entity linking methods, and 2) methods that treat the problem as a zero-shot text classification problem. For the first approach, we evaluate off-the-shelf entity linking systems. For the second approach, we explore a) pre-trained natural language inference (NLI) models, and b) pre-trained large generative language models. We present the results of our evaluation, lessons learned, and directions for future work. The dataset and scripts for evaluation are made publicly available.   
 pdf  bib  abs   
   Ometeotl@Multimodal Hate Speech Event Detection 2023: Hate Speech and Text-Image Correlation Detection in Real Life Memes Using Pre-Trained BERT  Models over Text    
  Jesus Armenta-Segura  | César Jesús Núñez-Prado  | Grigori Olegovich Sidorov  | Alexander Gelbukh  | Rodrigo Francisco Román-Godínez    
 pdf  bib  abs   
   I  nteros ML  @Causal News Corpus 2023: Understanding Causal Relationships: Supervised Contrastive Learning for Event Classification    
  Rajat Patel    
 pdf  bib  abs   
   SSN  - NLP  - ACE  @Multimodal Hate Speech Event Detection 2023: Detection of Hate Speech and Targets using Logistic Regression and SVM     
  Avanthika K  | Mrithula Kl  | Thenmozhi D    
 In this research paper, we propose a multimodal approach to hate speech detection, directed towards the identification of hate speech and its related targets. Our method uses logistic regression and support vector machines (SVMs) to analyse textual content extracted from social media platforms. We exploit natural language processing techniques to preprocess and extract relevant features from textual content, capturing linguistic patterns, sentiment, and contextual information.   
 pdf  bib  abs   
   ARC  - NLP  at Multimodal Hate Speech Event Detection 2023: Multimodal Methods Boosted by Ensemble Learning, Syntactical and Entity Features    
  Umitcan Sahin  | Izzet Emre Kucukkaya  | Oguzhan Ozcelik  | Cagri Toraman    
 Text-embedded images can serve as a means of spreading hate speech, propaganda, and extremist beliefs. Throughout the Russia-Ukraine war, both opposing factions heavily relied on text-embedded images as a vehicle for spreading propaganda and hate speech. Ensuring the effective detection of hate speech and propaganda is of utmost importance to mitigate the negative effect of hate speech dissemination. In this paper, we outline our methodologies for two subtasks of Multimodal Hate Speech Event Detection 2023. For the first subtask, hate speech detection, we utilize multimodal deep learning models boosted by ensemble learning and syntactical text attributes. For the second subtask, target detection, we employ multimodal deep learning models boosted by named entity features. Through experimentation, we demonstrate the superior performance of our models compared to all textual, visual, and text-visual baselines employed in multimodal hate speech detection. Furthermore, our models achieve the first place in both subtasks on the final leaderboard of the shared task.   
 pdf  bib  abs   
   V  erba V  isor@Multimodal Hate Speech Event Detection 2023: Hate Speech Detection using Transformer Model    
  Sarika Esackimuthu  | Prabavathy Balasundaram    
 Hate speech detection has emerged as a critical research area in recent years due to the rise of online social platforms and the proliferation of harmful content targeting individuals or specific groups.This task highlights the importance of detecting hate speech in text-embedded images.By leveraging deep learning models,this research aims to uncover the connection between hate speech and the entities it targets.   
 pdf  bib  abs   
   Lexical Squad@Multimodal Hate Speech Event Detection 2023: Multimodal Hate Speech Detection using Fused Ensemble Approach    
  Mohammad Kashif  | Mohammad Zohair  | Saquib Ali    
   On the Road to a Protest Event Ontology for B  ulgarian: Conceptual Structures and Representation Design    
  Milena Slavcheva  | Hristo Tanev  | Onur Uca    
 The paper presents a semantic model of protest events, called Semantic Interpretations of Protest Events (SemInPE). The analytical framework used for building the semantic representations is inspired by the object-oriented paradigm in computer science and a cognitive approach to the linguistic analysis. The model is a practical application of the Unified Eventity Representation (UER) formalism, which is based on the Unified Modeling Language (UML). The multi-layered architecture of the model provides flexible means for building the semantic representations of the language objects along a scale of generality and specificity. Thus, it is a suitable environment for creating the elements of ontologies on various topics and for different languages.   
 pdf  bib  abs   
   CSECU  - DSG  @Multimodal Hate Speech Event Detection 2023: Transformer-based Multimodal Hierarchical Fusion Model For Multimodal Hate Speech Detection    
  Abdul Aziz  | MD. Akram Hossain  | Abu Nowshed Chy    
 The emergence of social media and e-commerce platforms enabled the perpetrator to spread negativity and abuse individuals or organisations worldwide rapidly. It is critical to detect hate speech in both visual and textual content so that it may be moderated or excluded from online platforms to keep it sound and safe for users. However, multimodal hate speech detection is a complex and challenging task as people sarcastically present hate speech and different modalities i.e., image and text are involved in their content. This paper describes our participation in the CASE 2023 multimodal hate speech event detection task. In this task, the objective is to automatically detect hate speech and its target from the given text-embedded image. We proposed a transformer-based multimodal hierarchical fusion model to detect hate speech present in the visual content. We jointly fine-tune a language and a vision pre-trained transformer models to extract the visual-contextualized features representation of the text-embedded image. We concatenate these features and fed them to the multi-sample dropout strategy. Moreover, the contextual feature vector is fed into the BiLSTM module and the output of the BiLSTM module also passes into the multi-sample dropout. We employed arithmetic mean fusion to fuse all sample dropout outputs that predict the final label of our proposed method. Experimental results demonstrate that our model obtains competitive performance and ranked 5th among the participants   
 pdf  bib  abs   
   CSECU  - DSG  @ Causal News Corpus 2023: Leveraging R  o BERT  a and D  e BERT  a Transformer Model with Contrastive Learning for Causal Event Classification    
  MD. Akram Hossain  | Abdul Aziz  | Abu Nowshed Chy    
 Cause-effect relationships play a crucial role in human cognition, and distilling cause-effect relations from text helps in ameliorating causal networks for predictive tasks. There are many NLP applications that can benefit from this task, including natural language-based financial forecasting, text summarization, and question-answering. However, due to the lack of syntactic clues, the ambivalent semantic meaning of words, complex sentence structure, and implicit meaning of numerical entities in the text make it one of the challenging tasks in NLP. To address these challenges, CASE-2023 introduced a shared task 3 task focusing on event causality identification with causal news corpus. In this paper, we demonstrate our participant systems for this task. We leverage two transformers models including DeBERTa and Twitter-RoBERTa along with the weighted average fusion technique to tackle the challenges of subtask 1 where we need to identify whether a text belongs to either causal or not. For subtask 2 where we need to identify the cause, effect, and signal tokens from the text, we proposed a unified neural network of DeBERTa and DistilRoBERTa transformer variants with contrastive learning techniques. The experimental results showed that our proposed method achieved competitive performance among the participants’ systems.   
 pdf  bib  abs   
   Negative documents are positive: Improving event extraction performance using overlooked negative data    
  Osman Mutlu  | Ali Hürriyetoğlu    
 The scarcity of data poses a significant challenge in closed-domain event extraction, as is common in complex NLP tasks. This limitation primarily arises from the intricate nature of the annotation process. To address this issue, we present a multi-task model structure and training approach that leverages the additional data, which is found as not having any event information at document and sentence levels, generated during the event annotation process. By incorporating this supplementary data, our proposed framework demonstrates enhanced robustness and, in some scenarios, improved performance. A particularly noteworthy observation is that including only negative documents in addition to the original data contributes to performance enhancement. Our findings offer promising insights into leveraging extra data to mitigate data scarcity challenges in closed-domain event extraction.   
 pdf  bib  abs   
   IIC  _ T  eam@Multimodal Hate Speech Event Detection 2023: Detection of Hate Speech and Targets using Xlm-Roberta-base    
  Karanpreet Singh  | Vajratiya Vajrobol  | Nitisha Aggarwal    
 pdf  bib  abs   
   Event Causality Identification - Shared Task 3, CASE  2023    
  Fiona Anting Tan  | Hansi Hettiarachchi  | Ali Hürriyetoğlu  | Nelleke Oostdijk  | Onur Uca  | Surendrabikram Thapa  | Farhana Ferdousi Liza    
 pdf  bib  abs   
   Multimodal Hate Speech Event Detection - Shared Task 4, CASE  2023    
  Surendrabikram Thapa  | Farhan Jafri  | Ali Hürriyetoğlu  | Francielle Vargas  | Roy Ka-Wei Lee  | Usman Naseem    
 pdf  bib  abs   
   Detecting and Geocoding Battle Events from Social Media Messages on the Russo- U  krainian War: Shared Task 2, CASE  2023    
  Hristo Tanev  | Nicolas Stefanovitch  | Andrew Halterman  | Onur Uca  | Vanni Zavarella  | Ali Hurriyetoglu  | Bertrand De Longueville  | Leonida Della Rocca    
 pdf  bib  abs   
   Challenges and Applications of Automated Extraction of Socio-political Events from Text ( CASE  2023): Workshop and Shared Task Report    
  Ali Hürriyetoğlu  | Hristo Tanev  | Osman Mutlu  | Surendrabikram Thapa  | Fiona Anting Tan  | Erdem Yörük    
 We provide a summary of the sixth edition of the CASE workshop that is held in the scope of RANLP 2023. The workshop consists of regular papers, three keynotes, working papers of shared task participants, and shared task overview papers. This workshop series has been bringing together all aspects of event information collection across technical and social science fields. In addition to contributing to the progress in text based event extraction, the workshop provides a space for the organization of a multimodal event information collection task.   
   up   pdf (full)   
   Termout: a tool for the semi-automatic creation of term databases    
  Rogelio Nazar  | Nicolas Acosta    
 We propose a tool for the semi-automatic production of terminological databases, divided in the steps of corpus processing, terminology extraction, database population and management. With this tool it is possible to obtain a draft macrostructure (a lemma-list) and data for the microstructural level, such as grammatical (morphosyntactic patterns, gender, formation process) and semantic information (hypernyms, equivalence in another language, definitions and synonyms). In this paper we offer an overall description of the software and an evaluation of its performance, for which we used a linguistics corpus in English and Spanish.   
 pdf  bib  abs   
   On the Evaluation of Terminology Translation Errors in NMT  and PB  - SMT  in the Legal Domain: a Study on the Translation of A  rabic Legal Documents into E  nglish and F  rench    
  Khadija Ait ElFqih  | Johanna Monti    
 In the translation process, terminological resources are used to solve translation problems, so information on terminological equivalence is crucial to make the most appropriate choices in terms of translation equivalence. In the context of Machine translation, indeed, neural models have improved the state-of-the-art in Machine Translation considerably in recent years. However, they still underperform in domain-specific fields and in under-resourced languages. This is particularly evident in translating legal terminology for Arabic, where current Machine Translation outputs do not adhere to the contextual, linguistic, cultural, and terminological constraints posed by translating legal terms in Arabic. In this paper, we conduct a comparative qualitative evaluation and comprehensive error analysis on legal terminology translation in Phrase-Based Statistical Machine Translation and Neural Machine Translation in two translation language pairs: Arabic-English and Arabic-French. We propose an error typology taking the legal terminology translation from Arabic into account. We demonstrate our findings, highlighting the strengths and weaknesses of both approaches in the area of legal terminology translation for Arabic. We also introduce a multilingual gold standard dataset that we developed using our Arabic legal corpus. This dataset serves as a reliable benchmark and/or reference during the evaluation process to decide the degree of adequacy and fluency of the Phrase-Based Statistical Machine Translation and Neural Machine Translation systems.   
 pdf  bib  abs   
   B  an MANI  : A Dataset to Identify Manipulated Social Media News in B  angla    
  Mahammed Kamruzzaman  | Md. Minul Islam Shovon  | Gene Kim    
 Initial work has been done to address fake news detection and misrepresentation of news in the Bengali language. However, no work in Bengali yet addresses the identification of specific claims in social media news that falsely manipulate a related news article. At this point, this problem has been tackled in English and a few other languages, but not in the Bengali language. In this paper, we curate a dataset of social media content labeled with information manipulation relative to reference articles, called BanMANI. The dataset collection method we describe works around the limitations of the available NLP tools in Bangla. We expect these techniques will carry over to building similar datasets in other low-resource languages. BanMANI forms the basis both for evaluating the capabilities of existing NLP systems and for training or fine-tuning new models specifically on this task. In our analysis, we find that this task challenges current LLMs both under zero-shot and fine-tuned set- things   
 pdf  bib  abs   
   On the Errors in Code-Mixed T  amil- E  nglish Offensive Span Identification    
  Manikandan Ravikiran  | Bharathi Raja Chakravarthi    
 In recent times, offensive span identification in code-mixed Tamil-English language has seen traction with the release of datasets, shared tasks, and the development of multiple methods. However, the details of various errors shown by these methods are currently unclear. This paper presents a detailed analysis of various errors in state-of-the-art Tamil-English offensive span identification methods. Our study reveals the strengths and weaknesses of the widely used sequence labeling and zero-shot models for offensive span identification. In the due process, we identify data-related errors, improve data annotation and release additional diagnostic data to evaluate models’ quality and stability. Disclaimer: This paper contains examples that may be considered profane, vulgar, or offensive. The examples do not represent the views of the authors or their employers/graduate schools towards any person(s), group(s), practice(s), or entity/entities. Instead, they emphasize the complexity of various errors and linguistic research challenges.    
 pdf  bib  abs   
   Hate and Offensive Keyword Extraction from C  ode M  ix M  alayalam Social Media Text Using Contextual Embedding    
  Mariya Raphel  | Premjith B  | Sreelakshmi K  | Bharathi Raja Chakravarthi    
 This paper focuses on identifying hate and offensive keywords from codemix Malayalam social media text. As part of this work, a dataset for hate and offensive keyword extraction for codemix Malayalam language was created. Two different methods were experimented to extract Hate and Offensive language (HOL) keywords from social media text. In the first method, intrinsic evaluation was performed on the dataset to identify the hate and offensive keywords. Three different approaches namely – unigram approach, bigram approach and trigram approach were performed to extract the HOL keywords, sequence of HOL words and the sequence that contribute HOL meaning even in the absence of a HOL word. Five different transformer models were used in each of the pproaches for extracting the embeddings for the ngrams. Later, HOL keywords were extracted based on the similarity score obtained using the cosine similarity. Out of the five transformer models, the best results were obtained with multilingual BERT. In the second method, multilingual BERT transformer model was fine tuned with the dataset to develop a HOL keyword tagger model. This work is a new beginning for HOL keyword identification in Dravidian language – Malayalam.   
 pdf  bib  abs   
   Acoustic Analysis of the Fifth Liquid in M  alayalam    
  Punnoose A K    
 This paper investigates the claim of rhoticity of the fifth liquid in Malayalam using various acoustic characteristics. The Malayalam liquid phonemes are analyzed in terms of the smoothness of the pitch window, formants, formant bandwidth, the effect on surrounding vowels, duration, and classification patterns by an unrelated classifier. We report, for the fifth liquid, a slight similarity in terms of pitch smoothness with one of the laterals, similarity with the laterals in terms of F1 for males, and similarity with the laterals and one of the rhotics in terms of F1 for females. The similarity in terms of formant bandwidth between the fifth liquid and the other liquids is inconclusive. Similarly, the effect of the fifth liquid on the surrounding vowels is inconclusive. No similarity is observed between the fifth liquid and the other liquids in phoneme duration. Classification of the fifth liquid section implies higher order signal level similarity with both laterals and rhotics.   
 pdf  bib  abs   
   Improving Reinfocement Learning Agent Training using Text based Guidance: A study using Commands in D  ravidian Languages    
  Nikhil Chowdary Paleti  | Sai Aravind Vadlapudi  | Sai Aashish Menta  | Sai Akshay Menta  | Vishnu Vardhan Gorantla V N S L  | Janakiram Chandu  | Soman K P  | Sachin Kumar S    
 Reinforcement learning (RL) agents have achieved remarkable success in various domains, such as game-playing and protein structure prediction. However, most RL agents rely on exploration to find optimal solutions without explicit guidance. This paper proposes a methodology for training RL agents using text-based instructions in Dravidian Languages, including Telugu, Tamil, and Malayalam along with using the English language. The agents are trained in a modified Lunar Lander environment, where they must follow specific paths to successfully land the lander. The methodology involves collecting a dataset of human demonstrations and textual instructions, encoding the instructions into numerical representations using text-based embeddings, and training RL agents using state-of-the-art algorithms. The results demonstrate that the trained Soft Actor-Critic (SAC) agent can effectively understand and generalize instructions in different languages, outperforming other RL algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG).   
 pdf  bib  abs   
   Findings of the Second Shared Task on Offensive Span Identification from Code-Mixed T  amil- E  nglish Comments    
  Manikandan Ravikiran  | Ananth Ganesh  | Anand Kumar M  | R Rajalakshmi  | Bharathi Raja Chakravarthi    
 Maintaining effective control over offensive content is essential on social media platforms to foster constructive online discussions. Yet, when it comes to code-mixed Dravidian languages, the current prevalence of offensive content moderation is restricted to categorizing entire comments, failing to identify specific portions that contribute to the offensiveness. Such limitation is primarily due to the lack of annotated data and open source systems for offensive spans. To alleviate this issue, in this shared task, we offer a collection of Tamil-English code-mixed social comments that include offensive comments. This paper provides an overview of the released dataset, the algorithms employed, and the outcomes achieved by the systems submitted for this task.   
 pdf  bib  abs   
   Overview of the shared task on Fake News Detection from Social Media Text    
  Malliga S  | Bharathi Raja Chakravarthi  | Kogilavani S V  | Santhiya Pandiyan  | Prasanna Kumar Kumaresan  | Balasubramanian Palani  | Muskaan Singh    
 This document contains the instructions for preparing a manuscript for the proceedings of RANLP 2023. The document itself conforms to its own specifications and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.   
 pdf  bib  abs   
   Findings of the Shared Task on Multimodal Abusive Language Detection and Sentiment Analysis in T  amil and M  alayalam    
  Premjith B  | Jyothish Lal G  | Sowmya V  | Bharathi Raja Chakravarthi  | Rajeswari Natarajan  | Nandhini K  | Abirami Murugappan  | Bharathi B  | Kaushik M  | Prasanth Sn  | Aswin Raj R  | Vijai Simmon S    
 This paper summarizes the shared task on multimodal abusive language detection and sentiment analysis in Dravidian languages as part of the third Workshop on Speech and Language Technologies for Dravidian Languages at RANLP 2023. This shared task provides a platform for researchers worldwide to submit their models on two crucial social media data analysis problems in Dravidian languages - abusive language detection and sentiment analysis. Abusive language detection identifies social media content with abusive information, whereas sentiment analysis refers to the problem of determining the sentiments expressed in a text. This task aims to build models for detecting abusive content and analyzing fine-grained sentiment from multimodal data in Tamil and Malayalam. The multimodal data consists of three modalities - video, audio and text. The datasets for both tasks were prepared by collecting videos from YouTube. Sixty teams participated in both tasks. However, only two teams submitted their results. The submissions were evaluated using macro F1-score.   
 pdf  bib  abs   
   Overview of Shared-task on Abusive Comment Detection in T  amil and T  elugu    
  Ruba Priyadharshini  | Bharathi Raja Chakravarthi  | Malliga S  | Subalalitha Cn  | Kogilavani S V  | Premjith B  | Abirami Murugappan  | Prasanna Kumar Kumaresan    
 This paper discusses the submissions to the shared task on abusive comment detection in Tamil and Telugu codemixed social media text conducted as part of the third Workshop on Speech and Language Technologies for Dravidian Languages at RANLP 20239. The task encourages researchers to develop models to detect the contents containing abusive information in Tamil and Telugu codemixed social media text. The task has three subtasks - abusive comment detection in Tamil, Tamil-English and Telugu-English. The dataset for all the tasks was developed by collecting comments from YouTube. The submitted models were evaluated using macro F1-score, and prepared the rank list accordingly.   
 pdf  bib  abs   
   Revisiting Automatic Speech Recognition for T  amil and H  indi Connected Number Recognition    
  Rahul Mishra  | Senthil Raja Gunaseela Boopathy  | Manikandan Ravikiran  | Shreyas Kulkarni  | Mayurakshi Mukherjee  | Ananth Ganesh  | Kingshuk Banerjee    
 Automatic Speech Recognition and its applications are rising in popularity across applications with reasonable inference results. Recent state-of-the-art approaches, often employ significantly large-scale models to show high accuracy for ASR as a whole but often do not consider detailed analysis of performance across low-resource languages applications. In this preliminary work, we propose to revisit ASR in the context of Connected Number Recognition (CNR). More specifically, we (i) present a new dataset HCNR collected to understand various errors of ASR models for CNR, (ii) establish preliminary benchmark and baseline model for CNR, (iii) explore error mitigation strategies and their after-effects on CNR. In the due process, we also compare with end-to-end large scale ASR models for reference, to show its effectiveness.   
 pdf  bib  abs   
   A  bhi P  aw@ D  ravidian L  ang T  ech: Multimodal Abusive Language Detection and Sentiment Analysis    
  Abhinaba Bala  | Parameswari Krishnamurthy    
 Detecting abusive language in multimodal videos has become a pressing need in ensuring a safe and inclusive online environment. This paper focuses on addressing this challenge through the development of a novel approach for multimodal abusive language detection in Tamil videos and sentiment analysis for Tamil/Malayalam videos. By leveraging state-of-the-art models such as Multiscale Vision Transformers (MViT) for video analysis, OpenL3 for audio analysis, and the bert-base-multilingual-cased model for textual analysis, our proposed framework integrates visual, auditory, and textual features. Through extensive experiments and evaluations, we demonstrate the effectiveness of our model in accurately detecting abusive content and predicting sentiment categories. The limited availability of effective tools for performing these tasks in Dravidian Languages has prompted a new avenue of research in these domains.   
 pdf  bib  abs   
 pdf  bib  abs   
   D  eep B  lue AI  @ D  ravidian L  ang T  ech- RANLP  2023    
  Zhipeng Luo  | Jiahui Wang    
 This paper presents a study on the language understanding of the Dravidian languages. Three specific tasks related to text classification are focused on in this study, including abusive comment detection, sentiment analysis and fake news detection. The paper provides a detailed description of the tasks, including dataset information and task definitions, as well as the model architectures and training details used to tackle them. Finally, the competition results are presented, demonstrating the effectiveness of the proposed approach for handling these challenging NLP tasks in the context of the Dravidian languages.   
 pdf  bib  abs   
 pdf  bib  abs   
   Supernova@ D  ravidian L  ang T  ech 2023@Abusive Comment Detection in T  amil and T  elugu - ( T  amil, T  amil- E  nglish, T  elugu- E  nglish)    
  Ankitha Reddy  | Pranav Moorthi  | Ann Maria Thomas    
 This paper focuses on using Support Vector Machines (SVM) classifiers with TF-IDF feature extraction to classify whether a comment is abusive or not.The paper tries to identify abusive content in regional languages.The dataset analysis presents the distribution of target variables in the Tamil-English, Telugu-English, and Tamil datasets.The methodology section describes the preprocessing steps, including consistency, removal of special characters and emojis, removal of stop words, and stemming of data. Overall, the study contributes to the field of abusive comment detection in Tamil and Telugu languages.   
 pdf  bib  abs   
   A  bhi P  aw@ D  ravidian L  ang T  ech: Abusive Comment Detection in T  amil and T  elugu using Logistic Regression    
  Abhinaba Bala  | Parameswari Krishnamurthy    
 Abusive comments in online platforms have become a significant concern, necessitating the development of effective detection systems. However, limited work has been done in low resource languages, including Dravidian languages. This paper addresses this gap by focusing on abusive comment detection in a dataset containing Tamil, Tamil-English and Telugu-English code-mixed comments. Our methodology involves logistic regression and explores suitable embeddings to enhance the performance of the detection model. Through rigorous experimentation, we identify the most effective combination of logistic regression and embeddings. The results demonstrate the performance of our proposed model, which contributes to the development of robust abusive comment detection systems in low resource language settings. Keywords: Abusive comment detection, Dravidian languages, logistic regression, embeddings, low resource languages, code-mixed dataset.   
 pdf  bib  abs   
   SADT  ech@ D  ravidian L  ang T  ech: Multimodal Sentiment Analysis of T  amil and M  alayalam    
  Abhinav Patil  | Sam Briggs  | Tara Wueger  | Daniel D. O’Connell    
 We present several models for sentiment analysis of multimodal movie reviews in Tamil and Malayalam into 5 separate classes: highly negative, negative, neutral, positive, and highly positive, based on the shared task, “Multimodal Abusive Language Detection and Sentiment Analysis” at RANLP-2023. We use transformer language models to build text and audio embeddings and then compare the performance of multiple classifier models trained on these embeddings: a Multinomial Naive Bayes baseline, a Logistic Regression, a Random Forest, and an SVM. To account for class imbalance, we use both naive resampling and SMOTE. We found that without resampling, the baseline models have the same performance as a naive Majority Class Classifier. However, with resampling, logistic regression and random forest both demonstrate gains over the baseline.   
 pdf  bib  abs   
   MUCSD  @ D  ravidian L  ang T  ech2023: Predicting Sentiment in Social Media Text using Machine Learning Techniques    
  Sharal Coelho  | Asha Hegde  | Pooja Lamani  | Kavya G  | Hosahalli Lakshmaiah Shashirekha    
 User-generated social media texts are a blend of resource-rich languages like English and low-resource Dravidian languages like Tamil, Kannada, Tulu, etc. These texts referred to as code-mixing texts are enriching social media since they are written in two or more languages using either a common language script or various language scripts. However, due to the complex nature of the code-mixed text, in this paper, we - team MUCSD, describe a Machine learning (ML) models submitted to “Sentiment Analysis in Tamil and Tulu” shared task at DravidianLangTech@RANLP 2023. The proposed methodology makes use of ML models such as Linear Support Vector Classifier (LinearSVC), LR, and ensemble model (LR, DT, and SVM) to perform SA in Tamil and Tulu languages. The proposed LinearSVC model’s predictions submitted to the shared tasks, obtained 8th and 9th rank for Tamil-English and Tulu-English respectively.   
 pdf  bib  abs   
   Designing a Metalanguage of Differences Between Translations: A Case Study for E  nglish-to- J  apanese Translation    
  Tomono Honda  | Atsushi Fujita  | Mayuka Yamamoto  | Kyo Kageura    
 In both the translation industry and translation education, analytic and systematic assessment of translations plays a vital role. However, due to lack of a scheme for describing differences between translations, such assessment has been realized only in an ad-hoc manner. There is prior work on a scheme for describing differences between translations, but it has coverage and objectivity issues. To alleviate these issues and realize more fine-grained analyses, we developed an improved scheme by referring to diverse types of translations and adopting hierarchical linguistic units for analysis, taking English-to-Japanese translation as an example.   
 pdf  bib  abs   
   The 2023 R  epro NLP  Shared Task on Reproducibility of Evaluations in NLP  : Overview and Results    
  Anya Belz  | Craig Thomson    
 This paper presents an overview of, and the results from, the 2023 Shared Task on Reproducibility of Evaluations in NLP (ReproNLP’23), following on from two previous shared tasks on reproducibility of evaluations in NLG, ReproGen’21 and ReproGen’22. This shared task series forms part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP and machine learning, all against a background of an interest in reproducibility that con- tinues to grow in the two fields. This paper describes the ReproNLP’23 shared task, summarises results from the reproduction studies submitted, and provides comparative analysis of the results.   
 pdf  bib  abs   
   Some lessons learned reproducing human evaluation of a data-to-text system    
  Javier González Corbelle  | Jose Alonso  | Alberto Bugarín-Diz    
 This paper presents a human evaluation reproduction study regarding the data-to-text generation task. The evaluation focuses in counting the supported and contradicting facts generated by a neural data-to-text model with a macro planning stage. The model is tested generating sport summaries for the ROTOWIRE dataset. We first describe the approach to reproduction that is agreed in the context of the ReproHum project. Then, we detail the entire configuration of the original human evaluation and the adaptations that had to be made to reproduce such an evaluation. Finally, we compare the reproduction results with those reported in the paper that was taken as reference.   
 pdf  bib  abs   
   Unveiling NLG  Human-Evaluation Reproducibility: Lessons Learned and Key Insights from Participating in the R  epro NLP  Challenge    
  Lewis Watson  | Dimitra Gkatzia    
 Human evaluation is crucial for NLG systems as it provides a reliable assessment of the quality, effectiveness, and utility of generated language outputs. However, concerns about the reproducibility of such evaluations have emerged, casting doubt on the reliability and generalisability of reported results. In this paper, we present the findings of a reproducibility study on a data-to-text system, conducted under two conditions: (1) replicating the original setup as closely as possible with evaluators from AMT, and (2) replicating the original human evaluation but this time, utilising evaluators with a background in academia. Our experiments show that there is a loss of statistical significance between the original and reproduction studies, i.e. the human evaluation results are not reproducible. In addition, we found that employing local participants led to more robust results. We finally discuss lessons learned, addressing the challenges and best practices for ensuring reproducibility in NLG human evaluations.   
 pdf  bib  abs   
   Challenges in Reproducing Human Evaluation Results for Role-Oriented Dialogue Summarization    
  Takumi Ito  | Qixiang Fang  | Pablo Mosteiro  | Albert Gatt  | Kees van Deemter    
 There is a growing concern regarding the reproducibility of human evaluation studies in NLP. As part of the ReproHum campaign, we conducted a study to assess the reproducibility of a recent human evaluation study in NLP. Specifically, we attempted to reproduce a human evaluation of a novel approach to enhance Role-Oriented Dialogue Summarization by considering the influence of role interactions. Despite our best efforts to adhere to the reported setup, we were unable to reproduce the statistical results as presented in the original paper. While no contradictory evidence was found, our study raises questions about the validity of the reported statistical significance results, and/or the comprehensiveness with which the original study was reported. In this paper, we provide a comprehensive account of our reproduction study, detailing the methodologies employed, data collection, and analysis procedures. We discuss the implications of our findings for the broader issue of reproducibility in NLP research. Our findings serve as a cautionary reminder of the challenges in conducting reproducible human evaluations and prompt further discussions within the NLP community.   
 pdf  bib  abs   
   With a Little Help from the Authors: Reproducing Human Evaluation of an MT  Error Detector    
  Ondrej Platek  | Mateusz Lango  | Ondrej Dusek    
 This work presents our efforts to reproduce the results of the human evaluation experiment presented in the paper of Vamvas and Sennrich (2022), which evaluated an automatic system detecting over- and undertranslations (translations containing more or less information than the original) in machine translation (MT) outputs. Despite the high quality of the documentation and code provided by the authors, we discuss some problems we found in reproducing the exact experimental setup and offer recommendations for improving reproducibility. Our replicated results generally confirm the conclusions of the original study, but in some cases statistically significant differences were observed, suggesting a high variability of human annotation.   
 pdf  bib  abs   
   Reproduction of Human Evaluations in: “It’s not Rocket Science: Interpreting Figurative Language in Narratives”    
  Saad Mahamood    
 We describe in this paper an attempt to reproduce some of the human of evaluation results from the paper “It’s not Rocket Science: Interpreting Figurative Language in Narratives”. In particular, we describe the methodology used to reproduce the chosen human evaluation, the challenges faced, and the results that were gathered. We will also make some recommendations on the learnings obtained from this reproduction attempt and what improvements are needed to enable more robust reproductions of future NLP human evaluations.   
   up   pdf (full)   
   An Exploration of Zero-Shot Natural Language Inference-Based Hate Speech Detection    
  Nerses Yuzbashyan  | Nikolay Banar  | Ilia Markov  | Walter Daelemans    
 Conventional techniques for detecting online hate speech rely on the availability of a sufficient number of annotated instances, which can be costly and time consuming. For this reason, zero-shot or few-shot detection can offer an attractive alternative. In this paper, we explore a zero-shot detection approach based on natural language inference (NLI) models. Since the performance of the models in this approach depends heavily on the choice of a hypothesis, our goal is to determine which factors affect the quality of detection. We conducted a set of experiments with three NLI models and four hate speech datasets. We demonstrate that a zero-shot NLI-based approach is competitive with approaches that require supervised learning, yet they are highly sensitive to the choice of hypothesis. In addition, our experiments indicate that the results for a set of hypotheses on different model-data pairs are positively correlated, and that the correlation is higher for different datasets when using the same model than it is for different models when using the same dataset. These results suggest that if we find a hypothesis that works well for a specific model and domain or for a specific type of hate speech, we can use that hypothesis with the same model also within a different domain. While, another model might require different suitable hypotheses in order to demonstrate high performance.   
 pdf  bib  abs   
   E  nglish2 BSL  : A Rule-Based System for Translating E  nglish into B  ritish S  ign L  anguage    
  Phoebe Alexandra Pinney  | Riza Batista-Navarro    
 British Sign Language (BSL) is a complex language with its own vocabulary and grammatical structure, separate from English. Despite its long-standing and widespread use by Deaf communities within the UK, thus far, there have been no effective tools for translating written English into BSL. This overt lack of available resources made learning the language highly inaccessible for most people, exacerbating the communication barrier between hearing and Deaf individuals. This paper introduces a rule-based translation system, designed with the ambitious aim of creating the first web application that is not only able to translate sentences in written English into a BSL video output, but can also serve as a learning aid to empower the development of BSL proficiency.   
 pdf  bib  abs   
   Multilingual Models for Sentiment and Abusive Language Detection for D  ravidian Languages    
  Anand Kumar M    
 This paper presents the TFIDF based LSTM and Hierarchical Attention Networks (HAN) for code-mixed abusive comment detection and sentiment analysis for Dravidian languages. The traditional TF-IDF-based techniques have out- performed the Hierarchical Attention models in both the sentiment analysis and abusive language detection tasks. The Tulu sentiment analysis system demonstrated better performance for the Positive and Neutral classes, whereas the Tamil sentiment analysis system exhibited lower performance overall. This highlights the need for more balanced datasets and additional research to enhance the accuracy of sentiment analysis in the Tamil language. In terms of abusive language detection, the TF-IDF-LSTM models generally outperformed the Hierarchical Attention models. However, the mixed models displayed better performance for specific classes such as “Homophobia” and “Xenophobia.” This implies that considering both code-mixed and original script data can offer a different perspective for research in social media analysis.   
 pdf  bib  abs   
   Overview of the Second Shared Task on Speech Recognition for Vulnerable Individuals in T  amil    
  Bharathi B  | Bharathi Raja Chakravarthi  | Subalalitha Cn  | Sripriya Natarajan  | Rajeswari Natarajan  | S Suhasini  | Swetha Valli    
 This paper manifest the overview of the shared task on Speech Recognition for Vulnerable individuals in Tamil(LT-EDI-ACL2023). Task is provided with an Tamil dataset, which is collected from elderly people of three different genders, male, female and transgender. The audio samples were recorded from the public locations like hospitals, markets, vegetable shop, etc. The dataset is released in two phase, training and testing phase. The partcipants were asked to use different models and methods to handle audio signals and submit the result as transcription of the test samples given. The result submitted by the participants was evaluated using WER (Word Error Rate). The participants used the transformer-based model for automatic speech recognition. The results and different pre-trained transformer based models used by the participants is discussed in this overview paper.   
 pdf  bib  abs   
   Evaluating the Impact of Stereotypes and Language Combinations on Gender Bias Occurrence in NMT  Generic Systems    
  Bertille Triboulet  | Pierrette Bouillon    
 Machine translation, and more specifically neural machine translation (NMT), have been proven to be subject to gender bias in recent years. Many studies have focused on evaluating and reducing this phenomenon, mainly through the analysis of occupational nouns’ translation for the same type of language combinations. In this paper, we reproduce a similar test set than in previous studies to investigate the influence of stereotypes and language combinations’ nature (formed with English, French and Italian) on gender bias occurrence in NMT. Similarly to previous studies, we confirm stereotypes as a major source of gender bias, especially in female contexts, while observing bias even in language combinations traditionally less examined.   
 pdf  bib  abs   
   K  austubh S  hared T  ask@ LT  - EDI  2023: Homophobia-Transphobia Detection in Social Media Comments with NLPAUG  -driven Data Augmentation    
  Kaustubh Lande  | Rahul Ponnusamy  | Prasanna Kumar Kumaresan  | Bharathi Raja Chakravarthi    
 pdf  bib  abs   
   J  udith J  eyafreeda@ LT  - EDI  -2023: Using GPT  model for recognition of Homophobia/Transphobia detection from social media    
  Judith Jeyafreeda Andrew    
 Homophobia and Transphobia is defined as hatred or discomfort towards Gay, Lesbian, Transgender or Bisexual people. With the increase in social media, communication has become free and easy. This also means that people can also express hatred and discomfort towards others. Studies have shown that these can cause mental health issues. Thus detection and masking/removal of these comments from the social media platforms can help with understanding and improving the mental health of LGBTQ+ people. In this paper, GPT2 is used to detect homophobic and/or transphobic comments in social media comments. The comments used in this paper are from five (English, Spanish, Tamil, Malayalam and Hindi) languages. The results show that detecting comments in English language is easier when compared to the other languages.   
 pdf  bib  abs   
   iicteam@ LT  - EDI  -2023: Leveraging pre-trained Transformers for Fine-Grained Depression Level Detection in Social Media    
  Vajratiya Vajrobol  | Nitisha Aggarwal  | Karanpreet Singh    
 pdf  bib  abs   
   JA  - NLP  @ LT  - EDI  -2023: Empowering Mental Health Assessment: A R  o BERT  a-Based Approach for Depression Detection    
  Jyoti Kumari  | Abhinav Kumar    
 pdf  bib  abs   
   cantnlp@ LT  - EDI  -2023: Homophobia/Transphobia Detection in Social Media Comments using Spatio-Temporally Retrained Language Models    
  Sidney Wong  | Matthew Durward  | Benjamin Adams  | Jonathan Dunn    
 This paper describes our multiclass classification system developed as part of the LT-EDI@RANLP-2023 shared task. We used a BERT-based language model to detect homophobic and transphobic content in social media comments across five language conditions: English, Spanish, Hindi, Malayalam, and Tamil. We retrained a transformer-based cross-language pretrained language model, XLM-RoBERTa, with spatially and temporally relevant social media language data. We found the inclusion of this spatio-temporal data improved the classification performance for all language and task conditions when compared with the baseline. We also retrained a subset of models with simulated script-mixed social media language data with varied performance. The results from the current study suggests that transformer-based language classification systems are sensitive to register-specific and language-specific retraining.   
 pdf  bib  abs   
   NLP  _ CHRISTINE  @ LT  - EDI  -2023: R  o BERT  a & D  e BERT  a Fine-tuning for Detecting Signs of Depression from Social Media Text    
  Christina Christodoulou    
 pdf  bib  abs   
   IIITDWD  @ LT  - EDI  -2023 Unveiling Depression: Using pre-trained language models for Harnessing Domain-Specific Features and Context Information    
  Shankar Biradar  | Sunil Saumya  | Sanjana Kavatagi    
 pdf  bib  abs   
   CIMAT  - NLP  @ LT  - EDI  -2023: Finegrain Depression Detection by Multiple Binary Problems Approach    
  María de Jesús García Santiago  | Fernando Sánchez Vega  | Adrián Pastor López Monroy    
 This work described the work of the team CIMAT-NLP on the Shared task of Detecting Signs of Depression from Social Media Text at LT-EDI@RANLP 2023, which consists of depression classification on three levels: “not depression”, “moderate” depression and “severe” depression on text from social media. In this work, we proposed two approaches: (1) a transformer model which can handle big text without truncation of its length, and (2) an ensemble of six binary Bag of Words. Our team placed fourth in the competition and found that models trained with our approaches could place second   
 pdf  bib  abs   
   SIS  @ LT  - EDI  -2023: Detecting Signs of Depression from Social Media Text    
  Sulaksha B K  | Shruti Krishnaveni S  | Ivana Steeve  | Monica Jenefer B    
 Various biological, genetic, psychological or social factors that feature a target oriented life with chronic stress and frequent traumatic experiences, lead to pessimism and apathy. The massive scale of depression should be dealt with as a disease rather than a ‘phase’ that is neglected by the majority. However, not a lot of people are aware of depression and its impact. Depression is a serious issue that should be treated in the right way. Many people dealing with depression do not realize that they have it due to the lack of awareness. This paper aims to address this issue with a tool built on the blocks of machine learning. This model analyzes the public social media texts and detects the signs of depression under three labels namely “not depressed”, “moderately depressed”, and “severely depressed” with high accuracy. The ensembled model uses three learners namely Multi-Layered Perceptron, Support Vector Machine and Multinomial Naive Bayes Classifier. The distinctive feature in this model is that it uses Artificial Neural Networks, Classifiers, Regression and Voting Classifiers to compute the final result or output.   
 pdf  bib  abs   
   TEAM  BIAS  BUSTERS  @ LT  - EDI  -2023: Detecting Signs of Depression with Generative Pretrained Transformers    
  Andrew Nedilko    
 pdf  bib  abs   
   T  ech SSN  1 at LT  - EDI  -2023: Depression Detection and Classification using BERT  Model for Social Media Texts    
  Venkatasai Ojus Yenumulapalli  | Vijai Aravindh R  | Rajalakshmi Sivanaiah  | Angel Deborah S    
 pdf  bib  abs   
   SANBAR  @ LT  - EDI  -2023:Automatic Speech Recognition: vulnerable old-aged and transgender people in T  amil    
  Saranya S  | Bharathi B    
 pdf  bib  abs   
   ASR  _ SSN  _ CSE  @ LTEDI  - 2023: Pretrained Transformer based Automatic Speech Recognition system for Elderly People    
  Suhasini S  | Bharathi B    
 pdf  bib  abs   
   SSNT  ech2@ LT  - EDI  -2023: Homophobia/Transphobia Detection in Social Media Comments Using Linear Classification Techniques    
  Vaidhegi D  | Priya M  | Rajalakshmi Sivanaiah  | Angel Deborah S  | Mirnalinee ThankaNadar    
   IJS  @ LT  - EDI  : Ensemble Approaches to Detect Signs of Depression from Social Media Text    
  Jaya Caporusso  | Thi Hong Hanh Tran  | Senja Pollak    
 This paper presents our ensembling solutions for detecting signs of depression in social media text, as part of the Shared Task at LT-EDI@RANLP 2023. By leveraging social media posts in English, the task involves the development of a system to accurately classify them as presenting signs of depression of one of three levels: “severe”, “moderate”, and “not depressed”. We verify the hypothesis that combining contextual information from a language model with local domain-specific features can improve the classifier’s performance. We do so by evaluating: (1) two global classifiers (support vector machine and logistic regression); (2) contextual information from language models; and (3) the ensembling results.   
 pdf  bib  abs   
   VEL  @ LT  - EDI  -2023: Automatic Detection of Hope Speech in B  ulgarian Language using Embedding Techniques    
  Rahul Ponnusamy  | Malliga S  | Sajeetha Thavareesan  | Ruba Priyadharshini  | Bharathi Raja Chakravarthi    
   Cordyceps@ LT  - EDI  : Depression Detection with R  eddit and Self-training    
  Dean Ninalga    
 Depression is debilitating, and not uncommon. Indeed, studies of excessive social media users show correlations with depression, ADHD, and other mental health concerns. Given that there is a large number of people with excessive social media usage, then there is a significant population of potentially undiagnosed users and posts that they create. In this paper, we propose a depression detection system using a semi-supervised learning technique. Namely, we use a trained model to classify a large number of unlabelled social media posts from Reddit, then use these generated labels to train a more powerful classifier. We demonstrate our framework on Detecting Signs of Depression from Social Media Text - LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.   
 pdf  bib  abs   
   T  ech W  hiz@ LT  - EDI  -2023: Transformer Models to Detect Levels of Depression from Social Media Text    
  Madhumitha M  | Jerin Mahibha C  | Thenmozhi D.    
 pdf  bib  abs   
   CSE  _ SPEECH  @ LT  - EDI  -2023 A  utomatic Speech Recognition vulnerable old-aged and transgender people in T  amil    
  Varsha Balaji  | Archana Jp  | Bharathi B    
 pdf  bib  abs   
   VTUBGM  @ LT  - EDI  -2023: Hope Speech Identification using Layered Differential Training of ULMF  it    
  Sanjana M. Kavatagi  | Rashmi R. Rachh  | Shankar S. Biradar    
 pdf  bib  abs   
   ML  & AI  _ IIITR  anchi@ LT  - EDI  -2023: Identification of Hope Speech of Y  ou T  ube comments in Mixed Languages    
  Kirti Kumari  | Shirish Shekhar Jha  | Zarikunte Kunal Dayanand  | Praneesh Sharma    
 Hope speech analysis refers to the examination and evaluation of speeches or messages that aim to instill hope, inspire optimism, and motivate individuals or communities. It involves analyzing the content, language, rhetorical devices, and delivery techniques used in a speech to understand how it conveys hope and its potential impact on the audience. The objective of this study is to classify the given text comments as Hope Speech or Not Hope Speech. The provided dataset consists of YouTube comments in four languages: English, Hindi, Spanish, Bulgarian; with pre-defined classifications. Our approach involved pre-processing the dataset and using the TF-IDF (Term Frequency-Inverse Document Frequency) method.   
 pdf  bib  abs   
   ML  & AI  _ IIITR  anchi@ LT  - EDI  -2023: Hybrid Model for Text Classification for Identification of Various Types of Depression    
  Kirti Kumari  | Shirish Shekhar Jha  | Zarikunte Kunal Dayanand  | Praneesh Sharma    
 pdf  bib  abs   
   T  ech SSN  4@ LT  - EDI  -2023: Depression Sign Detection in Social Media Postings using D  istil BERT  Model    
  Krupa Elizabeth Thannickal  | Sanmati P  | Rajalakshmi Sivanaiah  | Angel Deborah S    
 pdf  bib  abs   
   The Mavericks@ LT  - EDI  -2023: Detection of signs of Depression from social Media Texts using Navie Bayse approach    
  Sathvika V S  | Vaishnavi Vaishnavi S  | Angel Deborah S  | Rajalakshmi Sivanaiah  | Mirnalinee ThankaNadar    
 pdf  bib  abs   
   hate-alert@ LT  - EDI  -2023: Hope Speech Detection Using Transformer-Based Models    
  Mithun Das  | Shubhankar Barman  | Subhadeep Chatterjee    
 Social media platforms have become integral to our daily lives, facilitating instant sharing of thoughts and ideas. While these platforms often host inspiring, motivational, and positive content, the research community has recognized the significance of such messages by labeling them as “hope speech”. In light of this, we delve into the detection of hope speech on social media platforms. Specifically, we explore various transformer-based model setups for the LT-EDI shared task at RANLP 2023. We observe that the performance of the models varies across languages. Overall, the finetuned m-BERT model showcases the best performance among all the models across languages. Our models secured the first position in Bulgarian and Hindi languages and achieved the third position for the Spanish language in the respective task.   
 pdf  bib  abs   
   TERCET  @ LT  - EDI  -2023: Hope Speech Detection for Equality, Diversity, and Inclusion    
  Priyadharshini Thandavamurthi  | Samyuktaa Sivakumar  | Shwetha Sureshnathan  | Thenmozhi D.  | Bharathi B  | Gayathri Gl    
   Interns@ LT  - EDI  : Detecting Signs of Depression from Social Media Text    
  Koushik L  | Hariharan R. L  | Anand Kumar M    
 This submission presents our approach for depression detection in social media text. The methodology includes data collection, preprocessing - SMOTE, feature extraction/selection - TF-IDF and Glove, model development- SVM, CNN and Bi-LSTM, training, evaluation, optimisation, and validation. The proposed methodology aims to contribute to the accurate detection of depression.   
 pdf  bib  abs   
   Tercet@ LT  - EDI  -2023: Homophobia/Transphobia Detection in social media comment    
  Shwetha Sureshnathan  | Samyuktaa Sivakumar  | Priyadharshini Thandavamurthi  | Thenmozhi D.  | Bharathi B  | Kiruthika Chandrasekaran    
 pdf  bib  abs   
   D  eep L  earning B  rasil@ LT  - EDI  -2023: Exploring Deep Learning Techniques for Detecting Depression in Social Media Text    
  Eduardo Garcia  | Juliana Gomes  | Adalberto Ferreira Barbosa Junior  | Cardeque Henrique Bittes de Alvarenga Borges  | Nadia Félix Felipe da Silva    
 In this paper, we delineate the strategy employed by our team, DeepLearningBrasil, which secured us the first place in the shared task DepSign-LT-EDI@RANLP-2023 with the advantage of 2.4%. The task was to classify social media texts into three distinct levels of depression - “not depressed,” “moderately depressed,” and “severely depressed.” Leveraging the power of the RoBERTa and DeBERTa models, we further pre-trained them on a collected Reddit dataset, specifically curated from mental health-related Reddit’s communities (Subreddits), leading to an enhanced understanding of nuanced mental health discourse. To address lengthy textual data, we introduced truncation techniques that retained the essence of the content by focusing on its beginnings and endings. Our model was robust against unbalanced data by incorporating sample weights into the loss. Cross-validation and ensemble techniques were then employed to combine our k-fold trained models, delivering an optimal solution. The accompanying code is made available for transparency and further development.   
 pdf  bib  abs   
   MUCS  @ LT  - EDI  2023: Learning Approaches for Hope Speech Detection in Social Media Text    
  Asha Hegde  | Kavya G  | Sharal Coelho  | Hosahalli Lakshmaiah Shashirekha    
 Hope plays a significant role in shaping human thoughts and actions and hope content has received limited attention in the realm of social media data analysis. The exploration of hope content helps to uncover the valuable insights into users’ aspirations, expectations, and emotional states. By delving into the analysis of hope content on social media platforms, researchers and analysts can gain a deeper understanding of how hope influences individuals’ behaviors, decisions, and overall well-being in the digital age. However, this area is rarely explored even for resource-high languages. To address the identification of hope text in social media platforms, this paper describes the models submitted by the team MUCS to “Hope Speech Detection for Equality, Diversity, and Inclusion (LT-EDI)” shared task organized at Recent Advances in Natural Language Processing (RANLP) - 2023. This shared task aims to classify a comment/post in English and code-mixed texts in three languages, namely, Bulgarian, Spanish, and Hindi into one of the two predefined categories, namely, “Hope speech” and “Non Hope speech”. Two models, namely: i) Hope_BERT - Linear Support Vector Classifier (LinearSVC) model trained by combining Bidirectional Encoder Representations from Transformers (BERT) embeddings and Term Frequency-Inverse Document Frequency (TF-IDF) of character n-grams with word boundary (char_wb) for English and ii) Hope_mBERT - LinearSVC model trained by combining Multilingual BERT (mBERT) embeddings and TF-IDF of char_wb for Bulgarian, Spanish, and Hindi code-mixed texts are proposed for the shared task to classify the given text into Hope or Non-Hope categories. The proposed models obtained 1st, 1st, 2nd, and 5th ranks for Spanish, Bulgarian, Hindi, and English texts respectively.   
 pdf  bib  abs   
   MUCS  @ LT  - EDI  2023: Homophobic/Transphobic Content Detection in Social Media Text using m BERT     
  Asha Hegde  | Kavya G  | Sharal Coelho  | Hosahalli Lakshmaiah Shashirekha    
 Homophobic/Transphobic (H/T) content includes hate speech, discrimination text, and abusive comments against Gay, Lesbian, Bisexual, Transgender, Queer, and Intersex (LGBTQ) individuals. With the increase in user generated text in social media, there has been an increase in code-mixed H/T content, which poses challenges for efficient analysis and detection of H/T content on social media. The complex nature of code-mixed text necessitates the development of advanced tools and techniques to effectively tackle this issue in social media platforms. To tackle this issue, in this paper, we - team MUCS, describe the transformer based models submitted to “Homophobia/Transphobia Detection in social media comments” shared task in Language Technology for Equality, Diversity and Inclusion (LT-EDI) at Recent Advances in Natural Language Processing (RANLP)-2023. The proposed methodology makes use of resampling the training data to handle the data imbalance and this resampled data is used to fine-tune the Multilingual Bidirectional Encoder Representations from Transformers (mBERT) models. These models obtained 11th, 5th, 3rd, 3rd, and 7th ranks for English, Tamil, Malayalam, Spanish, and Hindi respectively in Task A and 8th, 2nd, and 2nd ranks for English, Tamil, and Malayalam respectively in Task B.   
 pdf  bib  abs   
   MUCS  @ LT  - EDI  2023: Detecting Signs of Depression in Social Media Text    
  Sharal Coelho  | Asha Hegde  | Kavya G  | Hosahalli Lakshmaiah Shashirekha    
 pdf  bib  abs   
   Flamingos_python@ LT  - EDI  -2023: An Ensemble Model to Detect Severity of Depression    
  Abirami P S  | Amritha S  | Pavithra Meganathan  | Jerin Mahibha C    
   Machine translation, translation errors, and adequacy: S  panish- E  nglish vs. S  panish- R  omanian    
  Laura Monguilod  | Bianca Vitalaru    
 This paper has two objectives: 1. To analyse the adequacy of using neural machine translation (NMT) for the translation of health information (from Spanish into English and Romanian) used in Spanish public health campaigns; and 2. To compare results considering these two linguistic combinations. Results show that post-editing is essential to improve the quality of the translations for both language combinations since they cannot be used as a primary resource for informing foreign users without post-editing. Moreover, Romanian translations require more post-editing. However, using NMT for informative texts combined with human post-editing can be used as a strategy to benefit from the potential of MT while at the same time ensuring the quality of the public service translations depending on the language combination and on the amount of time allotted for the task.   
 pdf  bib  abs   
   Cross-Lingual Idiom Sense Clustering in G  erman and E  nglish    
  Mohammed Absar    
 Idioms are expressions with non-literal and non-compositional meanings. For this reason, they pose a unique challenge for various NLP tasks including Machine Translation and Sentiment Analysis. In this paper, we propose an approach to clustering idioms in different languages by their sense. We leverage pre-trained cross-lingual transformer models and fine-tune them to produce cross-lingual vector representations of idioms according to their sense.   
 pdf  bib  abs   
   Leveraging Large Language Models to Extract Terminology    
  Julie Giguere    
 Large Language Models (LLMs) have brought us efficient tools for various natural language processing (NLP) tasks. This paper explores the application of LLMs for extracting domain-specific terms from textual data. We will present the advantages and limitations of using LLMs for this task and will highlight the significant improvements they offer over traditional terminology extraction methods such as rule-based and statistical approaches.   
 pdf  bib  abs   
   Cross-lingual Mediation: Readability Effects    
  Maria Kunilovskaya  | Ruslan Mitkov  | Eveline Wandl-Vogt    
 This paper explores the readability of translated and interpreted texts compared to the original source texts and target language texts in the same domain. It was shown in the literature that translated and interpreted texts could exhibit lexical and syntactic properties that make them simpler, and hence, easier to process than their sources or comparable non-translations. In translation, this effect is attributed to the tendency to simplify and disambiguate the message. In interpreting, it can be enhanced by the temporal and cognitive constraints. We use readability annotations from the Newsela corpus to formulate a number of classification and regression tasks and fine-tune a multilingual pre-trained model on these tasks, obtaining models that can differentiate between complex and simple sentences. Then, the models are applied to predict the readability of sources, targets, and comparable target language originals in a zero-shot manner. Our test data – parallel and comparable – come from English-German bidirectional interpreting and translation subsets from the Europarl corpus. The results confirm the difference in readability between translated/interpreted targets against sentences in standard originally-authored source and target languages. Besides, we find consistent differences between the translation directions in the English-German language pair.   
 pdf  bib  abs   
   Automatic Text Simplification for People with Cognitive Disabilities: Resource Creation within the C  lear T  ext Project    
  Isabel Espinosa-Zaragoza  | José Abreu-Salas  | Paloma Moreda  | Manuel Palomar    
 This paper presents the ongoing work conducted within the ClearText project, specifically focusing on the resource creation for the simplification of Spanish for people with cognitive disabilities. These resources include the CLEARSIM corpus and the Simple.Text tool. On the one hand, a description of the corpus compilation process with the help of APSA is detailed along with information regarding whether these texts are bronze, silver or gold standard simplification versions from the original text. The goal to reach is 18,000 texts in total by the end of the project. On the other hand, we aim to explore Large Language Models (LLMs) in a sequence-to-sequence setup for text simplification at the document level. Therefore, the tool’s objectives, technical aspects, and the preliminary results derived from early experimentation are also presented. The initial results are subject to improvement, given that experimentation is in a very preliminary stage. Despite showcasing flaws inherent to generative models (e.g. hallucinations, repetitive text), we examine the resolutions (or lack thereof) of complex linguistic phenomena that can be learned from the corpus. These issues will be addressed throughout the remainder of this project. The expected positive results from this project that will impact society are three-fold in nature: scientific-technical, social, and economic.   
 pdf  bib  abs   
   Towards Sentence-level Text Readability Assessment for F  rench    
  Duy Van Ngo  | Yannick Parmentier    
 In this paper, we report on some experiments aimed at exploring the relation between document-level and sentence-level readability assessment for French. These were run on an open-source tailored corpus, which was automatically created by aggregating various sources from children’s literature. On top of providing the research community with a freely available corpus, we report on sentence readability scores obtained when applying both classical approaches (aka readability formulas) and state-of-the-art deep learning techniques (e.g. fine-tuning of large language models). Results show a relatively strong correlation between document-level and sentence-level readability, suggesting ways to reduce the cost of building annotated sentence-level readability datasets.   
 pdf  bib  abs   
   LSL  lama: Fine-Tuned LL  a MA  for Lexical Simplification    
  Anthony Baez  | Horacio Saggion    
 Generative Large Language Models (LLMs), such as GPT-3, have become increasingly effective and versatile in natural language processing (NLP) tasks. One such task is Lexical Simplification, where state-of-the-art methods involve complex, multi-step processes which can use both deep learning and non-deep learning processes. LLaMA, an LLM with full research access, holds unique potential for the adaption of the entire LS pipeline. This paper details the process of fine-tuning LLaMA to create LSLlama, which performs comparably to previous LS baseline models LSBert and UniHD.   
 pdf  bib  abs   
   On Operations in Automatic Text Simplification    
  Rémi Cardon  | Adrien Bibal    
 This paper explores the literature of automatic text simplification (ATS) centered on the notion of operations. Operations are the processed of applying certain modifications to a given text in order to transform it. In ATS, the intent of the transformation is to simplify the text. This paper overviews and structures the domain by showing how operations are defined and how they are exploited. We extensively discuss the most recent works on this notion and perform preliminary experiments to automatize operations recognition with large language models (LLMs). Through our overview of the literature and the preliminary experiment with LLMs, this paper provides insights on the topic that can help lead to new directions in ATS research.   
 pdf  bib  abs
6. RANLP_2 conference:
Submissions 
  Sponsors 
  RANLP History 
  Contacts 
 Call for shared tasks proposals: September 2024   
 Call for workshop proposals: December 2024   
 Tharindu Ranasinghe, Lancaster University, UK (Shared tasks Co-Chair)  
 Saad Ezzini, Lancaster University, UK (Sponsorship Chair and Shared tasks Co-Chair)  
  RANLP SJR Rank  :  
 The RANLP Proceedings are indexed by Scopus. The SJR-rank can be checked here  .  
    Scroll to Top
7. RANLP_3 conference:
Submissions 
  Sponsors 
  RANLP History 
  Contacts
8. RCIS_0 conference:
The 17th International Conference on Research Challenges in Information Science  
 23 - 26 May 2023, Corfu, Greece  
 Toggle navigation        
  Forum 
  Doctoral Consortium 
  Research Projects @ RCIS Track 
  Journal-First Papers 
  Committees | Conference Chairs 
 Latest News   
 May 23 2023  : RCIS 2023 conference is now ready to open, welcome!  
 May 18 2023  : The details for the RCIS 2023 sightseeing tour have been updated!   
 May 18 2023  : The details for the RCIS 2023 sightseeing tour have been updated!   
 April 28 2023  : The gala dinner location is now announced!   
 April 25 2023  : The list of accepted papers (full conference, forum, doctoral consortium, journal - first) is now published!   
 April 24 2023  : The Program of the Conference is now announced!   
 March 21 2023  : RCIS 2023 Program Board met in Paris and notifications of papers for the main conference and forum have already been sent! Stay tuned for the programme!   
 February 23 2023  : Extension on the dates for the call for Research Projects - February 24, 2023  New date: March 13, 2023    
  January 26 2023  :    We consider that the authors need more time. We would like to change the deadlines from: Deadline 29.01.2023  to the following.  
  Abstracts (*mandatory*): February 5th  
  Full Papers: February 10th    
 January 23 2022  : Call for Papers - Submission  , Deadline 29.01.2023  -> February 5th 2023 (abstracts - mandatory), February 10th 2023 (Full Papers)    
 October 21 2022  : Conference dates: 23 - 26 May.  
  Place: Corfu, Greece. Hosting from Ionian University  and NMSLab  from the Department of informatics  .   
 October 20 2022  : The New Website of RCIS is ready!  
  Contact   
    Organizers  
 The Conference - Research Challenges in Information Science   
 Research Challenges in Information Science   
  The 17th International Conference, RCIS 2023, Corfu, Greece, 23-26 May, 2023   
  Proceedings  
 Scope and Topics   
 In the age of connectivity, we experience technological components functioning and communicating in new, expanded ways. Large and interconnected information and communication systems create, aggregate, analyze, communicate, transform and generally process data from various sources to serve a range of applications, services and systems. This spectrum of connectivity enables new opportunities, challenges, issues and risks, which call for investigation and consideration by practitioners, researchers, and other stakeholders. RCIS 2023 will welcome scientists, researchers, engineers and practitioners from a wide range of information science fields and will bring them together to provide opportunities for knowledge sharing and dissemination towards connected information systems and services. Hence, this year, RCIS 2023 has chosen the theme Information Science and the Connected World to be in the center of its reflections and discussions.  
 RCIS welcomes submissions from the whole spectrum of the information science field. The list of themes and topics includes, but is not limited to:  
 A1. Information Systems and their Engineering | A2. User-Oriented Approaches 
  Venue   
 RCIS 2023 will be held in the Corfu, organized by Networks, Multimedia and Security Systems Laboratory (NMSLab) of the Department of the Informatics of the Ionian University.  
 Past News   
 October 21 2022  : Call for Papers - Submission  , Deadline 29.01.2023.    
 October 21 2022  : Conference dates: 23 - 26 May.  
  Place: Corfu, Greece. Hosting from Ionian University  and NMSLab  from the Department of informatics  .   
 October 20 2022  : The New Website of RCIS is ready!  
  International Conference on Research Challenges in Information Science 2023  
 Sponsors
9. RCIS_1 conference:
The 17th International Conference on Research Challenges in Information Science  
 23 - 26 May 2023, Corfu, Greece  
 Toggle navigation        
  Forum 
  Doctoral Consortium 
  Research Projects @ RCIS Track 
  Journal-First Papers 
  Committees | Conference Chairs 
 All times depicted below are local GR times | Go to the Detailed Program    
   Tuesday   
 Tuesday, 23 May 2023 | Wednesday, 24 May 2023 | Thursday, 25 May 2023 | Friday, 26 May 2023 
 Please note that the registration will be done at the Department of Informatics   
  International Conference on Research Challenges in Information Science 2023  
 Sponsors
10. RCIS_2 conference:
This book constitutes the proceedings of the 17th International Conference on Research Challenges in Information Science   
  DOWNLOAD FILE   
 Author / Uploaded 
  Selmin Nurcan 
  Andreas L. Opdahl 
 This book constitutes the proceedings of the 17th International Conference on Research Challenges in Information Science  
 This book constitutes the revised selected papers of the 15th International Conference, MEDES 2023, Heraklion, Crete, Gr  
 This book constitutes the refereed proceedings of the 20th International Conference on The Semantic Web, ESWC 2023, held

output:1. RAMiCS_0 information:
2. RAMiCS_1 information:
3. RAMiCS_2 information:
4. RANLP_0 information:
5. RANLP_1 information:
6. RANLP_2 information:
7. RANLP_3 information:
8. RCIS_0 information:
9. RCIS_1 information:
10. RCIS_2 information:
