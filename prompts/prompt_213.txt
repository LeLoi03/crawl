input:
1. MSWIM_1 conference:
26 th  MSWIM 2023     
 Call for Papers   
  MSWiM 2023 is the 26th Annual International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems. MSWiM is an international forum dedicated to in-depth discussion of Wireless and Mobile systems, networks, algorithms and applications, with an emphasis on rigorous performance evaluation. MSWiM is a highly selective conference with a long track record of publishing innovative ideas and breakthroughs. MSWIM 2023 will be held in Montreal, Canada.   
 Authors are encouraged to submit full papers presenting new research related to the theory or practice of all aspects of modeling, analysis and simulation of mobile and wireless systems. Submitted papers must not have been published elsewhere nor currently be under review by another conference or journal.   
 Topics of Interest   
  AI and Federated Learning models for wireless and mobile networks 
  Paper Submission and Publication   
  High-quality original papers are solicited. Papers must be unpublished and must not be submitted for publication elsewhere. All papers will be reviewed by Technical Program Committee members and other experts active in the field to ensure high quality and relevance to the conference. Short papers will be included in the technical program to complement mature results and foster discussion and exchange of novel ideas at an early stage. More detailed instructions for paper submission will be provided soon. Accepted papers will appear in the conference proceedings to be published by ACM Press.  
  Paper Registration Deadline: July 2nd, 2023  
  Paper Submission Deadline: June 21st, 2023  July 8th, 2023   
  Notification of Acceptance: August 15th, 2023  
  Camera Ready version due: September 5th, 2023
2. MSWIM_2 conference:
and Simulation of Wireless and Mobile Systems  
  October 29th - November 2nd, 2023   
 Montreal, Canada   
 LATEST NEWS!    
 Paper Submission Deadline: June 21st, 2023  July 8th, 2023    
  Notification of Acceptance: August 15th, 2023   
  Camera-ready: September 5th, 2023   
  Registration Deadline: September 12th, 2023   
 Note: Extended versions of selected papers will be considered for  
  Sponsors: | LATEST NEWS!    
 Paper Submission Deadline: June 21st, 2023  July 8th, 2023    
  Notification of Acceptance: August 15th, 2023   
  Camera-ready: September 5th, 2023   
  Registration Deadline: September 12th, 2023   
 Note: Extended versions of selected papers will be considered for  
 LATEST NEWS!    
 Paper Submission Deadline: June 21st, 2023  July 8th, 2023    
  Notification of Acceptance: August 15th, 2023   
  Camera-ready: September 5th, 2023   
  Registration Deadline: September 12th, 2023   
 Note: Extended versions of selected papers will be considered for
3. MSWIM_3 conference:
MSWiM'23 RISING STAR Award    
  The RISING STAR Researcher Award
4. MT SUMMIT_0 conference:
Invited Talks & Panel   
 Workshops   
 Research Track accepted paper list   
 Users Track accepted paper list 
  Venue | Accommodation   
 Conference Venue 
  FAQ 
  Organization | Organizing Committee   
 MT Summit History   
 About AAMT 
  Calls | Call for Papers   
  Whova Guidelines 
   Welcome to MT Summit 2023!  
 Loading...
5. MT SUMMIT_1 conference:
Invited Talks & Panel   
 Workshops   
 Research Track accepted paper list   
 Users Track accepted paper list 
  Venue | Accommodation   
 Conference Venue 
  FAQ 
  Organization | Organizing Committee   
 MT Summit History   
 About AAMT 
  Calls | Call for Papers   
 ===================================================================    
 MT Summit 2023    
 2nd Call for Papers and Presentations    
 ===================================================================    
 With the coming of the MT Summit XIX, the 19th Machine Translation Summit, held from September 4 to 8, 2023, we are pleased to announce that we are putting out a call for papers and presentations, as well as for proposals for workshops and tutorials.    
 COVID-19 has forced many international conferences to go fully online, and this was no different for the previous MT Summit. This time, however, we’re looking to provide both real opportunities for face-to-face discussions and for encounters that take place online.    
 The conference will take place in the convention center located on the Cotai Strip, which is at the heart of modern Macao’s accommodation, event, and entertainment offerings. We will also continue to monitor the pandemic situation closely to ensure the safety of participants, and, if necessary, will convert to a completely virtual format.    
 The MT Summit XIX is organized by the Asia-Pacific Association for Machine Translation. This biennial MT Summit brings together the Asian-Pacific (AAMT), American (AMTA), and European (EAMT) branches of the International Association for Machine Translation (IAMT), creating an invaluable opportunity to discuss the issues of the day with a truly global gathering of all the stakeholders of machine translation. This includes researchers of MT and translation, developers, providers, translators, and general users. For researchers, MT Summit XIX provides a unique context in which to share their latest results with colleagues, as well as an opportunity to understand real-world user requirements. MT providers, translators, and general users in both business and government settings will benefit from updates on leading-edge R&D in Machine Translation and have a chance to present and discuss their own use cases. In all of these contexts, we strongly encourage and hope that students will also make submissions and participate.    
 Important Dates for Papers:    
 Important dates (***extended***) for both Research and Users Tracks  .   
 Submission deadline: 
  o     Monday, 15 May 2023 (Research and Users)    
 Notification of acceptance: 
  o     Monday, 19 June 2023 (Users)     
 o     Monday, 3 July 2023 (Research)    
 Full paper submission: 
  o   Monday, 10 July 2023 (Users)   
 Final “camera-ready” versions: 
  o   Monday, 24 July 2023 (Research and Users)    
 The submission and “camera-ready” deadline time zone for all of the above dates is “Anywhere on Earth” (UTC–12).    
 * Presentation video and a PDF file (slides or poster):   
 Thursday, 10 August 2023 (Research, Users, all workshops) 
  The length of a video is 20 minutes or less. 
  Upload link will be e-mailed to the authors. 
 IMPORTANT NOTE   : Due to the hybrid nature of the conference, all papers, presentations, tutorials, and workshops will be recorded without exception. Any submission will be understood by the MT Summit Organizing Committee as giving your tacit permission for the organizers to create a video recording of your presentation and make it available to conference attendees, and potentially to members of AMTA, AAMT, or EAMT as well. We also request that all remote representations be pre-recorded.    
 Conference Tracks    
 The conference will feature two main tracks – Research  and Users  , each dedicated to a respective area in machine translation research and user research & experiences. The final presentation format is either oral or poster.    
 Guidelines for submission to the tracks of the conference are as follows:    
 Research Track    
 Chairs: Masao Utiyama and Rui Wang (  mt-summit-2023-research-track@googlegroups.com    )    
 We invite original, substantial, and unpublished research in all aspects of machine translation (MT). We seek submissions across the entire spectrum of MT-related research, but with a particular focus on MT Summit’s strength: the close interaction between researchers and practitioners who are looking to apply the latest MT technology to their tasks. Topics of interest include but are not limited to:    
 Advances in data-driven MT (e.g. neural, statistical) 
  Lexicon acquisition and integration into MT 
  Detecting and preventing Catastrophic errors in Translation 
  Best practices in annotation for Translation 
  Submission Instructions:    
 Papers should not be longer than 10 pages of content (for references, unlimited number of pages is allowed). The papers must follow the MT Summit 2023 style guides (  PDF version    ,  LaTeX version    ,  MS Word version    , and  Overleaf template    ) and be submitted in PDF format. To allow for blind reviewing, please do not include author names and affiliations within the paper and avoid obvious self-references.    
 Publication    
 Final versions of papers must be submitted by the final “camera-ready” date. All Research track papers will be included in the conference proceedings, which will be made publicly available on the MT Summit website. Papers will also be hosted individually on the ACL Anthology website.    
 Users Track    
 Chairs: Masaru Yamada and Félix do Carmo (  mt-summit-2023-users-track@googlegroups.com    )    
 This track calls for participation of innovative and unpublished papers and presentations describing studies on general conditions of usage of MT products, both by end users, who consume its raw products for purposes like gisting, as well as by translators and other professional services, who use MT for producing quality translations by the process of post-editing.    
 In addition, we are looking for papers/reports on the status of MT literacy and education, aiming to enlighten users on the proper use of MT, and on MT use cases in specific domains, including local government, public services, academia and schools. Studies related to positive outcomes brought by the use of MT, but also to its shortcomings and limitations, risks, ethical concerns and others related to the use of MT outside of research labs are also welcome to this track. Submissions by MT technology and service providers should present analyses of innovative MT use cases, rather than focus on projects, products or service offerings.    
  Adoption of MT by diverse populations of users, across countries, demographics, cultural and social | s | pec | t | rums 
 Submission instructions    
 Publication    
 If the abstract or the paper is accepted, final versions of papers, abstracts, or slide decks must be submitted by the final camera-ready date for publication, in PDF format, via the SUBMISSION WEBSITE indicated above. All papers and presentations received by the camera-ready deadline will be published in the conference proceedings and on the ACL Anthology website.    
 As indicated in the call for papers, we anticipate high-quality submissions for the User Track, addressing topics such as machine translation (MT) literacy and education, domain-specific MT use cases, and research on the positive outcomes, limitations, risks, and ethical concerns related to MT usage. We have modified the standard call for papers process, and as such, we request abstract submissions by May 1st. Full papers should be submitted by July 10th to be included in the conference proceedings.The conference will be held in a hybrid format, and we hope that a significant number of attendees will be able to gather in Macau for stimulating post-presentation discussions.    
 Additionally, we are currently organizing a range of engaging events to take place during the main conference.
6. MT SUMMIT_2 conference:
EAMT 
  IAMT 
  MT Summit 
  SIGMT 
  SIGSLT 
 Events 
  MT Summit 2023 
 MT Summit 2023  
 Machine Translation Summit  
  The Machine Translation Summit ( MT Summit 2023  ) took place in Macau Special Administrative Region, CN from 04 September to 08 September, 2023.  
 MT Summit 2023 was organised by AMTA  .  
  Machine Translation (MT) Summit is an international conference, which brings together people from the academic and commercial world developing MT and users of MT systems. The conference overviews state-of-the-art technologies in MT, its major contemporary trends, and practical applications.  
 Location  
  Important Dates  
 Workshops and tutorials submission deadline | 17 April 
 Papers submission deadline | 15 May 
 Notification of acceptance of users papers | 19 June 
 Notification of acceptance of research papers | 03 July 
 Full paper submission of users papers | 10 July 
 Final “camera-ready” versions of users and research papers | 24 July 
 Keynote speakers  
  Prof. Akiko Sakamoto (Kansai University) 
  Workshops  
 WAT 2023  
 The Workshop on Asian Translation (WAT) is an open machine translation evaluation campaign focusing on Asian languages. WAT gathers and shares the resources and knowledge of Asian language translation to understand the problems to be solved for the practical use of machine translation technologies among all Asian countries.  
  WAT 2023   
 ALT 2023：Ancient Language Translation Workshop  
 This workshop aims to provide an opportunity for practitioners and scholars of interest to learn about the challenges and latest developments in the field of machine translation for ancient languages. Participants will engage in discussions and hands-on activities to develop a deeper understanding of the field and the techniques used to address the unique challenges posed by translating texts written in ancient languages.  
  ancientnlp.com/alt2023/   
  Chairs:  Masaru Yamada, Félix do Carmo [email protected]   
    Call for workshops and tutorials  
 Workshops and tutorials will take place on 4 September and 5 September, 2023.  
 Chairs:  Thepchai Supnithi, Jiajun Zhang [email protected]  [email protected]   
    Workshops  
  ↑   
 Want to learn more about MT Summit 2023?  
 Search the community for MT Summit 2023    
 Ask a question about MT Summit 2023    
   Edited on  by  .
7. MT SUMMIT_3 conference:
Github 
 Machine Translation Summit (2023)  
   Multiloop Incremental Bootstrapping for Low-Resource Machine Translation    
  Wuying Liu  | Wei Li  | Lin Wang    
 Due to the scarcity of high-quality bilingual sentence pairs, some deep-learning-based machine translation algorithms cannot achieve better performance in low-resource machine translation. On this basis, we are committed to integrating the ideas of machine learning algorithm improvement and data augmentation, propose a novel multiloop incremental bootstrapping framework, and design the corresponding semi-supervised learning algorithm. This framework is a meta-frame independent of specific machine translation algorithms. This algorithm makes full use of bilingual seed data of appropriate scale and super-large-scale monolingual data to expand bilingual sentence pair data incrementally, and trains machine translation models step by step to improve the translation quality. The experimental results of neural machine translation on multiple language pairs prove that our proposed framework can make use of continuous monolingual data to raise itself. Its effectiveness is not only reflected in the easy implementation of state-of-the-art low-resource machine translation, but also in the practical option to quickly establish precise domain machine translation systems.   
 pdf  bib  abs   
   Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables    
  Ali Araabi  | Vlad Niculae  | Christof Monz    
 Despite the tremendous success of Neural Machine Translation (NMT), its performance on low- resource language pairs still remains subpar, partly due to the limited ability to handle previously unseen inputs, i.e., generalization. In this paper, we propose a method called Joint Dropout, that addresses the challenge of low-resource neural machine translation by substituting phrases with variables, resulting in significant enhancement of compositionality, which is a key aspect of generalization. We observe a substantial improvement in translation quality for language pairs with minimal resources, as seen in BLEU and Direct Assessment scores. Furthermore, we conduct an error analysis, and find Joint Dropout to also enhance generalizability of low-resource NMT in terms of robustness and adaptability across different domains.   
 pdf  bib  abs   
   A Study of Multilingual versus Meta-Learning for Language Model Pre-Training for Adaptation to Unseen Low Resource Languages    
  Jyotsana Khatri  | Rudra Murthy  | Amar Prakash Azad  | Pushpak Bhattacharyya    
 In this paper, we compare two approaches to train a multilingual language model: (i) simple multilingual learning using data-mixing, and (ii) meta-learning. We examine the performance of these models by extending them to unseen language pairs and further finetune them for the task of unsupervised NMT. We perform several experiments with varying amounts of data and give a comparative analysis of the approaches. We observe that both approaches give a comparable performance, and meta-learning gives slightly better results in a few cases of low amounts of data. For Oriya-Punjabi language pair, meta-learning performs better than multilingual learning when using 2M, and 3M sentences.   
 pdf  bib  abs   
   Data Augmentation with Diversified Rephrasing for Low-Resource Neural Machine Translation    
  Yuan Gao  | Feng Hou  | Huia Jahnke  | Ruili Wang    
 Data augmentation is an effective way to enhance the performance of neural machine translation models, especially for low-resource languages. Existing data augmentation methods are either at a token level or a sentence level. The data augmented using token level methods lack syntactic diversity and may alter original meanings. Sentence level methods usually generate low-quality source sentences that are not semantically paired with the original target sentences. In this paper, we propose a novel data augmentation method to generate diverse, high-quality and meaning-preserved new instances. Our method leverages high-quality translation models trained with high-resource languages to rephrase an original sentence by translating it into an intermediate language and then back to the original language. Through this process, the high-performing translation models guarantee the quality of the rephrased sentences, and the syntactic knowledge from the intermediate language can bring syntactic diversity to the rephrased sentences. Experimental results show our method can enhance the performance in various low-resource machine translation tasks. Moreover, by combining our method with other techniques that facilitate NMT, we can yield even better results.   
 pdf  bib  abs   
   A Dual Reinforcement Method for Data Augmentation using Middle Sentences for Machine Translation    
  Wenyi Tang  | Yves Lepage    
 This paper presents an approach to enhance the quality of machine translation by leveraging middle sentences as pivot points and employing dual reinforcement learning. Conventional methods for generating parallel sentence pairs for machine translation rely on parallel corpora, which may be scarce, resulting in limitations in translation quality. In contrast, our proposed method entails training two machine translation models in opposite directions, utilizing the middle sentence as a bridge for a virtuous feedback loop between the two models. This feedback loop resembles reinforcement learning, facilitating the models to make informed decisions based on mutual feedback. Experimental results substantiate that our proposed method significantly improves machine translation quality.   
 pdf  bib  abs   
   Perturbation-based QE  : An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation    
  Tu Anh Dinh  | Jan Niehues    
 Quality Estimation (QE) is the task of predicting the quality of Machine Translation (MT) system output, without using any gold-standard translation references. State-of-the-art QE models are supervised: they require human-labeled quality of some MT system output on some datasets for training, making them domain-dependent and MT-system-dependent. There has been research on unsupervised QE, which requires glass-box access to the MT systems, or parallel MT data to generate synthetic errors for training QE models. In this paper, we present Perturbation-based QE - a word-level Quality Estimation approach that works simply by analyzing MT system output on perturbed input source sentences. Our approach is unsupervised, explainable, and can evaluate any type of blackbox MT systems, including the currently prominent large language models (LLMs) with opaque internal processes. For language directions with no labeled QE data, our approach has similar or better performance than the zero-shot supervised approach on the WMT21 shared task. Our approach is better at detecting gender bias and word-sense-disambiguation errors in translation than supervised QE, indicating its robustness to out-of-domain usage. The performance gap is larger when detecting errors on a nontraditional translation-prompting LLM, indicating that our approach is more generalizable to different MT systems. We give examples demonstrating our approach’s explainability power, where it shows which input source words have influence on a certain MT output word.   
 pdf  bib  abs   
   Enhancing Translation of M  yanmar Sign Language by Transfer Learning and Self-Training    
  Hlaing Myat Nwe  | Kiyoaki Shirai  | Natthawut Kertkeidkachorn  | Thanaruk Theeramunkong  | Ye Kyaw Thu  | Thepchai Supnithi  | Natsuda Kaothanthong    
 This paper proposes a method to develop a machine translation (MT) system from Myanmar Sign Language (MSL) to Myanmar Written Language (MWL) and vice versa for the deaf community. Translation of MSL is a difficult task since only a small amount of a parallel corpus between MSL and MWL is available. To address the challenge for MT of the low-resource language, transfer learning is applied. An MT model is trained first for a high-resource language pair, American Sign Language (ASL) and English, then it is used as an initial model to train an MT model between MSL and MWL. The mT5 model is used as a base MT model in this transfer learning. Additionally, a self-training technique is applied to generate synthetic translation pairs of MSL and MWL from a large monolingual MWL corpus. Furthermore, since the segmentation of a sentence is required as preprocessing of MT for the Myanmar language, several segmentation schemes are empirically compared. Results of experiments show that both transfer learning and self-training can enhance the performance of the translation between MSL and MWL compared with a baseline model fine-tuned from a small MSL-MWL parallel corpus only.   
 pdf  bib  abs   
   Improving Embedding Transfer for Low-Resource Machine Translation    
  Van Hien Tran  | Chenchen Ding  | Hideki Tanaka  | Masao Utiyama    
 Low-resource machine translation (LRMT) poses a substantial challenge due to the scarcity of parallel training data. This paper introduces a new method to improve the transfer of the embedding layer from the Parent model to the Child model in LRMT, utilizing trained token embeddings in the Parent model’s high-resource vocabulary. Our approach involves projecting all tokens into a shared semantic space and measuring the semantic similarity between tokens in the low-resource and high-resource languages. These measures are then utilized to initialize token representations in the Child model’s low-resource vocabulary. We evaluated our approach on three benchmark datasets of low-resource language pairs: Myanmar-English, Indonesian-English, and Turkish-English. The experimental results demonstrate that our method outperforms previous methods regarding translation quality. Additionally, our approach is computationally efficient, leading to reduced training time compared to prior works.   
 pdf  bib  abs   
   A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation    
  Ramakrishna Appicharla  | Baban Gain  | Santanu Pal  | Asif Ekbal    
 Recent studies have shown that the multi-encoder models are agnostic to the choice of context and the context encoder generates noise which helps in the improvement of the models in terms of BLEU score. In this paper, we further explore this idea by evaluating with context-aware pronoun translation test set by training multi-encoder models trained on three different context settings viz,  previous two sentences, random two sentences, and a mix of both as context. Specifically, we evaluate the models on the ContraPro test set to study how different contexts affect pronoun translation accuracy. The results show that the model can perform well on the ContraPro test set even when the context is random. We also analyze the source representations to study whether the context encoder is generating noise or not. Our analysis shows that the context encoder is providing sufficient information to learn discourse-level information. Additionally, we observe that mixing the selected context (the previous two sentences in this case) and the random context is generally better than the other settings.   
 pdf  bib  abs   
   Bad MT  Systems are Good for Quality Estimation    
  Iryna Tryhubyshyn  | Aleš Tamchyna  | Ondřej Bojar    
 Quality estimation (QE) is the task of predicting quality of outputs produced by machine translation (MT) systems. Currently, the highest-performing QE systems are supervised and require training on data with golden quality scores. In this paper, we investigate the impact of the quality of the underlying MT outputs on the performance of QE systems. We find that QE models trained on datasets with lower-quality translations often outperform those trained on higher-quality data. We also demonstrate that good performance can be achieved by using a mix of data from different MT systems.   
 pdf  bib  abs   
   Benchmarking Dialectal A  rabic- T  urkish Machine Translation    
  Hasan Alkheder  | Houda Bouamor  | Nizar Habash  | Ahmet Zengin    
 Due to the significant influx of Syrian refugees in Turkey in recent years, the Syrian Arabic dialect has become increasingly prevalent in certain regions of Turkey. Developing a machine translation system between Turkish and Syrian Arabic would be crucial in facilitating communication between the Turkish and Syrian communities in these regions, which can have a positive impact on various domains such as politics, trade, and humanitarian aid. Such a system would also contribute positively to the growing Arab-focused tourism industry in Turkey. In this paper, we present the first research effort exploring translation between Syrian Arabic and Turkish. We use a set of 2,000 parallel sentences from the MADAR corpus containing 25 different city dialects from different cities across the Arab world, in addition to Modern Standard Arabic (MSA), English, and French. Additionally, we explore the translation performance into Turkish from other Arabic dialects and compare the results to the performance achieved when translating from Syrian Arabic. We build our MADAR-Turk data set by manually translating the set of 2,000 sentences from the Damascus dialect of Syria to Turkish with the help of two native Arabic speakers from Syria who are also highly fluent in Turkish. We evaluate the quality of the translations and report the results achieved. We make this first-of-a-kind data set publicly available to support research in machine translation between these important but less studied language pairs.   
 pdf  bib  abs   
   Context-aware Neural Machine Translation for E  nglish- J  apanese Business Scene Dialogues    
  Sumire Honda  | Patrick Fernandes  | Chrysoula Zerva    
 Despite the remarkable advancements in machine translation, the current sentence-level paradigm faces challenges when dealing with highly-contextual languages like Japanese. In this paper, we explore how context-awareness can improve the performance of the current Neural Machine Translation (NMT) models for English-Japanese business dialogues translation, and what kind of context provides meaningful information to improve translation. As business dialogue involves complex discourse phenomena but offers scarce training resources, we adapted a pretrained mBART model, finetuning on multi-sentence dialogue data, which allows us to experiment with different contexts. We investigate the impact of larger context sizes and propose novel context tokens encoding extra-sentential information, such as speaker turn and scene type. We make use of Conditional Cross-Mutual Information (CXMI) to explore how much of the context the model uses and generalise CXMI to study the impact of the extra sentential context. Overall, we find that models leverage both preceding sentences and extra-sentential context (with CXMI increasing with context size) and we provide a more focused analysis on honorifics translation. Regarding translation quality, increased source-side context paired with scene and speaker information improves the model performance compared to previous work and our context-agnostic baselines, measured in BLEU and COMET metrics.   
 pdf  bib  abs   
   Target Language Monolingual Translation Memory based NMT  by Cross-lingual Retrieval of Similar Translations and Reranking    
  Takuya Tamura  | Xiaotian Wang  | Takehito Utsuro  | Masaaki Nagata    
 Retrieve-edit-rerank is a text generation framework composed of three steps: retrieving for sentences using the input sentence as a query, generating multiple output sentence candidates, and selecting the final output sentence from these candidates. This simple approach has outperformed other existing and more complex methods. This paper focuses on the retrieving and the reranking steps. In the retrieving step, we propose retrieving similar target language sentences from a target language monolingual translation memory using language-independent sentence embeddings generated by mSBERT or LaBSE. We demonstrate that this approach significantly outperforms existing methods that use monolingual inter-sentence similarity measures such as edit distance, which is only applicable to a parallel translation memory. In the reranking step, we propose a new reranking score for selecting the best sentences, which considers both the log-likelihood of each candidate and the sentence embeddings based similarity between the input and the candidate. We evaluated the proposed method for English-to-Japanese translation on the ASPEC and English-to-French translation on the EU Bookshop Corpus (EUBC). The proposed method significantly exceeded the baseline in BLEU score, especially observing a 1.4-point improvement in the EUBC dataset over the original Retrieve-Edit-Rerank method.   
 pdf  bib  abs   
   Towards Zero-Shot Multilingual Poetry Translation    
  Wai Lei Song  | Haoyun Xu  | Derek F. Wong  | Runzhe Zhan  | Lidia S. Chao  | Shanshan Wang    
 The application of machine translation in the field of poetry has always presented significant challenges. Conventional machine translation techniques are inadequate for capturing and translating the unique style of poetry. The absence of a parallel poetry corpus and the distinctive structure of poetry further restrict the effectiveness of traditional methods. This paper introduces a zero-shot method that is capable of translating poetry style without the need for a large-scale training corpus. Specifically, we treat poetry translation as a standard machine translation problem and subsequently inject the poetry style upon completion of the translation process. Our injection model only requires back-translation and easily obtainable monolingual data, making it a low-cost solution. We conducted experiments on three translation directions and presented automatic and human evaluations, demonstrating that our proposed method outperforms existing online systems and other competitive baselines. These results validate the feasibility and potential of our proposed approach and provide new prospects for poetry translation.   
 pdf  bib  abs   
   Leveraging Highly Accurate Word Alignment for Low Resource Translation by Pretrained Multilingual Model    
  Jingyi Zhu  | Minato Kondo  | Takuya Tamura  | Takehito Utsuro  | Masaaki Nagata    
 Recently, there has been a growing interest in pretraining models in the field of natural language processing. As opposed to training models from scratch, pretrained models have been shown to produce superior results in low-resource translation tasks. In this paper, we introduced the use of pretrained seq2seq models for preordering and translation tasks. We utilized manual word alignment data and mBERT-based generated word alignment data for training preordering and compared the effectiveness of various types of mT5 and mBART models for preordering. For the translation task, we chose mBART as our baseline model and evaluated several input manners. Our approach was evaluated on the Asian Language Treebank dataset, consisting of 20,000 parallel data in Japanese, English and Hindi, where Japanese is either on the source or target side. We also used in-house 3,000 parallel data in Chinese and Japanese. The results indicated that mT5-large trained with manual word alignment achieved a preordering performance exceeding 0.9 RIBES score on Ja-En and Ja-Zh pairs. Moreover, our proposed approach significantly outperformed the baseline model in most translation directions of Ja-En, Ja-Zh, and Ja-Hi pairs in at least one of BLEU/COMET scores.   
 pdf  bib  abs   
   Pivot Translation for Zero-resource Language Pairs Based on a Multilingual Pretrained Model    
  Kenji Imamura  | Masao Utiyama  | Eiichiro Sumita    
 A multilingual translation model enables a single model to handle multiple languages. However, the translation qualities of unlearned language pairs (i.e., zero-shot translation qualities) are still poor. By contrast, pivot translation translates source texts into target ones via a pivot language such as English, thus enabling machine translation without parallel texts between the source and target languages. In this paper, we perform pivot translation using a multilingual model and compare it with direct translation. We improve the translation quality without using parallel texts of direct translation by fine-tuning the model with machine-translated pseudo-translations. We also discuss what type of parallel texts are suitable for effectively improving the translation quality in multilingual pivot translation.   
 pdf  bib  abs   
   Negative Lexical Constraints in Neural Machine Translation    
  Josef Jon  | Dusan Varis  | Michal Novák  | João Paulo Aires  | Ondřej Bojar    
 This paper explores negative lexical constraining in English to Czech neural machine translation. Negative lexical constraining is used to prohibit certain words or expressions in the translation produced by the NMT model. We compared various methods based on modifying either the decoding process or the training data. The comparison was performed on two tasks: paraphrasing and feedback-based translation refinement. We also studied how the methods “evade” the constraints, meaning that the disallowed expression is still present in the output, but in a changed form, most interestingly the case where a different surface form (for example different inflection) is produced. We propose a way to mitigate the issue through training with stemmed negative constraints, so that the ability of the model to induce different forms of a word might be used to prohibit the usage of all possible forms of the constraint. This helps to some extent, but the problem still persists in many cases.   
 pdf  bib  abs   
   A Filtering Approach to Object Region Detection in Multimodal Machine Translation    
  Ali Hatami  | Paul Buitelaar  | Mihael Arcan    
 Recent studies in Multimodal Machine Translation (MMT) have explored the use of visual information in a multimodal setting to analyze its redundancy with textual information. The aim of this work is to develop a more effective approach to incorporating relevant visual information into the translation process and improve the overall performance of MMT models. This paper proposes an object-level filtering approach in Multimodal Machine Translation, where the approach is applied to object regions extracted from an image to filter out irrelevant objects based on the image captions to be translated. Using the filtered image helps the model to consider only relevant objects and their relative locations to each other. Different matching methods, including string matching and word embeddings, are employed to identify relevant objects. Gaussian blurring is used to soften irrelevant objects from the image and to evaluate the effect of object filtering on translation quality. The performance of the filtering approaches was evaluated on the Multi30K dataset in English to German, French, and Czech translations, based on BLEU, ChrF2, and TER metrics.   
   up   pdf (full)   
   Technology Preparedness and Translator Training: Implications for Pedagogy    
  Hari Venkatesan    
 With increasing acknowledgement of enhanced quality now achievable by Machine Translation, new possibilities have emerged in translation, both vis-à-vis division of labour between human and machine in the translation process and acceptability of lower quality of language in exchange for efficiency. This paper presents surveys of four cohorts of post-graduate students of translation from the University of Macau to see if perceived trainee awareness and preparedness has kept pace with these possibilities. It is found that trainees across the years generally lack confidence in their perceived awareness, are hesitant in employing MT, and show definite reservations when reconsidering issues such as quality and division of labour. While the size of respondents is small, it is interesting to note that the awareness and preparedness mentioned above are found to be similar across the four years. The implication for training is that technology be fully integrated into the translation process in order to provide trainees with a template/framework to handle diverse situations, particularly those that require offering translations of a lower quality with a short turnaround time. The focus here is on Chinese-English translation, but the discussion may find resonance with other language pairs. Keywords Translator training, Computer-Assisted Translation, Machine Translation, translation pedagogy, Chinese-English translation   
 pdf  bib  abs   
   Human-in-the-loop Machine Translation with Large Language Model    
  Xinyi Yang  | Runzhe Zhan  | Derek F. Wong  | Junchao Wu  | Lidia S. Chao    
 The large language model (LLM) has garnered significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation. In this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM’s translation through in-context learning. The human-machine interactions generated in this pipeline are also stored in an external database to expand the in-context retrieval database, enabling us to leverage human supervision in an offline setting. We evaluate the proposed pipeline using the GPT-3.5-turbo API on five domain-specific benchmarks for German-English translation. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance compared to direct translation instructions. Additionally, we discuss the experimental results from the following perspectives: 1) the effectiveness of different in-context retrieval methods; 2) the construction of a retrieval database under low-resource scenarios; 3) the observed differences across selected domains; 4) the quantitative analysis of sentence-level and word-level statistics; and 5) the qualitative analysis of representative translation cases.   
 pdf  bib  abs   
   Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution    
  Akshat Dewan  | Michal Ziemski  | Henri Meylan  | Lorenzo Concina  | Bruno Pouliquen    
 This paper presents an end-to-end solution for the creation of fully automated conference meeting transcripts and their machine translations into various languages. This tool has been developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. Beyond describing data collection and fine-tuning, resulting in a highly customized and robust system, this paper describes the architecture and evolution of the technical components as well as highlights the business impact and benefits from the user side. We also point out particular challenges in the evolution and adoption of the system and how the new approach created a new product and replaced existing established workflows in conference management documentation.   
 pdf  bib  abs   
   Optimizing Machine Translation through Prompt Engineering: An Investigation into C  hat GPT  ’s Customizability    
  Masaru Yamada    
 This paper explores the influence of integrating the purpose of the translation and the target audience into prompts on the quality of translations produced by ChatGPT. Drawing on previous translation studies, industry practices, and ISO standards, the research underscores the significance of the pre-production phase in the translation process. The study reveals that the inclusion of suitable prompts in large-scale language models like ChatGPT can yield flexible translations, a feat yet to be realized by conventional Ma-chine Translation (MT). The research scrutinizes the changes in translation quality when prompts are used to generate translations that meet specific conditions. The evaluation is conducted from a practicing translator’s viewpoint, both subjectively and qualitatively, supplemented by the use of OpenAI’s word embedding API for cosine similarity calculations. The findings suggest that the integration of the purpose and target audience into prompts can indeed modify the generated translations, generally enhancing the translation quality by industry standards. The study also demonstrates the practical application of the “good translation” concept, particularly in the context of marketing documents and culturally dependent idioms.   
 pdf  bib  abs   
   Challenges of Human vs Machine Translation of Emotion-Loaded C  hinese Microblog Texts    
  Shenbin Qian  | Constantin Orăsan  | Félix do Carmo  | Diptesh Kanojia    
 This paper attempts to identify challenges professional translators face when translating emotion-loaded texts as well as errors machine translation (MT) makes when translating this content. We invited ten Chinese-English translators to translate thirty posts of a Chinese microblog, and interviewed them about the challenges encountered during translation and the problems they believe MT might have. Further, we analysed more than five-thousand automatic translations of microblog posts to observe problems in MT outputs. We establish that the most challenging problem for human translators is emotion-carrying words, which translators also consider as a problem for MT. Analysis of MT outputs shows that this is also the most common source of MT errors. We also find that what is challenging for MT, such as non-standard writing, is not necessarily an issue for humans. Our work contributes to a better understanding of the challenges for the translation of microblog posts by humans and MT, caused by different forms of expression of emotion.   
   up   pdf (full)   
   Do Not Discard – Extracting Useful Fragments from Low-Quality Parallel Data to Improve Machine Translation    
  Steinþór Steingrímsson  | Pintu Lohar  | Hrafn Loftsson  | Andy Way    
 When parallel corpora are preprocessed for machine translation (MT) training, a part of the parallel data is commonly discarded and deemed non-parallel due to odd-length ratio, overlapping text in source and target sentences or failing some other form of a semantic equivalency test. For language pairs with limited parallel resources, this can be costly as in such cases modest amounts of acceptable data may be useful to help build MT systems that generate higher quality translations. In this paper, we refine parallel corpora for two language pairs, English–Bengali and English–Icelandic, by extracting sub-sentence fragments from sentence pairs that would otherwise have been discarded, in order to increase recall when compiling training data. We find that by including the fragments, translation quality of NMT systems trained on the data improves significantly when translating from English to Bengali and from English to Icelandic.   
 pdf  bib  abs   
 pdf  bib  abs   
   Findings of the C  o C  o4 MT  2023 Shared Task on Corpus Construction for Machine Translation    
  Ananya Ganesh  | Marine Carpuat  | William Chen  | Katharina Kann  | Constantine Lignos  | John E. Ortega  | Jonne Saleva  | Shabnam Tafreshi  | Rodolfo Zevallos    
 This paper provides an overview of the first shared task on choosing beneficial instances for machine translation, conducted as part of the CoCo4MT 2023 Workshop at MTSummit. This shared task was motivated by the need to make the data annotation process for machine translation more efficient, particularly for low-resource languages for which collecting human translations may be difficult or expensive. The task involved developing methods for selecting the most beneficial instances for training a machine translation system without access to an existing parallel dataset in the target language, such that the best selected instances can then be manually translated. Two teams participated in the shared task, namely the Williams team and the AST team. Submissions were evaluated by training a machine translation model on each submission’s chosen instances, and comparing their performance with the chRF++ score. The system that ranked first is by the Williams team, that finds representative instances by clustering the training data.   
 pdf  bib  abs   
   W  illiams College’s Submission for the C  oco4 MT  2023 Shared Task    
  Alex Root  | Mark Hopkins    
 Professional translation is expensive. As a consequence, when developing a translation system in the absence of a pre-existing parallel corpus, it is important to strategically choose sentences to have professionally translated for the training corpus. In our contribution to the Coco4MT 2023 Shared Task, we explore how sentence embeddings can be leveraged to choose an impactful set of sentences to translate. Based on six language pairs of the JHU Bible corpus, we demonstrate that a technique based on SimCSE embeddings outperforms a competitive suite of baselines.   
 pdf  bib  abs   
   The AST  Submission for the C  o C  o4 MT  2023 Shared Task on Corpus Construction for Low-Resource Machine Translation    
  Steinþór Steingrímsson    
   The Ups and Downs of Training R  o BERT  a-based models on Smaller Datasets for Translation Tasks from Classical C  hinese into Modern Standard M  andarin and M  odern E  nglish    
  Stuart Michael McManus  | Roslin Liu  | Yuji Li  | Leo Tam  | Stephanie Qiu  | Letian Yu    
 The paper presents an investigation into the effectiveness of pre-trained language models, Siku-RoBERTa and RoBERTa, for Classical Chinese to Modern Standard Mandarin and Classical Chinese to English translation tasks. The English translation model resulted in unsatisfactory performance due to the small dataset, while the Modern Standard Mandarin model gave reasonable results.   
 pdf  bib  abs   
   Pre-trained Model In A  ncient- C  hinese-to- M  odern- C  hinese Machine Translation    
  Jiahui Wang  | Xuqin Zhang  | Jiahuan Li  | Shujian Huang    
 This paper presents an analysis of the pre-trained Transformer model Neural Machine Translation (NMT) for the Ancient-Chinese-to-Modern-Chinese machine translation task.   
 pdf  bib  abs   
 pdf  bib  abs   
   Istic Neural Machine Translation System for E  va H  an 2023    
  Ningyuan Deng  | Shuao Guo  | Yanqing He    
 This paper presents the system architecture and the technique details adopted by Institute of Scientific and Technical Information of China (ISTIC) in the evaluation of First Conference on EvaHan(2023). In this evaluation, ISTIC participated in two tasks of Ancient Chinese Machine Translation: Ancient Chinese to Modern Chinese and Ancient Chinese to English. The paper mainly elaborates the model framework and data processing methods adopted in ISTIC’s system. Finally a comparison and analysis of different machine translation systems are also given.   
 pdf  bib  abs   
   BIT  - ACT  : An A  ncient C  hinese Translation System Using Data Augmentation    
  Li Zeng  | Yanzhi Tian  | Yingyu Shan  | Yuhang Guo    
 This paper describes a translation model for ancient Chinese to modern Chinese and English for the Evahan 2023 competition, a subtask of the Ancient Language Translation 2023 challenge. During the training of our model, we applied various data augmentation techniques and used SiKu-RoBERTa as part of our model architecture. The results indicate that back translation improves the model’s performance, but double back translation introduces noise and harms the model’s performance. Fine-tuning on the original dataset can be helpful in solving the issue.   
 pdf  bib  abs   
   Translating A  ncient C  hinese to M  odern C  hinese at Scale: A Large Language Model-based Approach    
  Jiahuan Cao  | Dezhi Peng  | Yongxin Shi  | Zongyuan Jiang  | Lianwen Jin    
 Recently, the emergence of large language models (LLMs) has provided powerful foundation models for a wide range of natural language processing (NLP) tasks. However, the vast majority of the pre-training corpus for most existing LLMs is in English, resulting in their Chinese proficiency falling far behind that of English. Furthermore, ancient Chinese has a much larger vocabulary and less available corpus than modern Chinese, which significantly challenges the generalization capacity of existing LLMs. In this paper, we investigate the Ancient-Chinese-to-Modern-Chinese (A2M) translation using LLMs including LLaMA and Ziya. Specifically, to improve the understanding of Chinese texts, we explore the vocabulary expansion and incremental pre-training methods based on existing pre-trained LLMs. Subsequently, a large-scale A2M translation dataset with 4M pairs is utilized to finetune the LLMs.Experimental results demonstrate the effectiveness of the proposed method, especially with Ziya-13B, in translating ancient Chinese to modern Chinese. Moreover,we deeply analyze the performance of various LLMs with different strategies, which we believe can benefit further research on LLM-based A2M approaches.   
   up   pdf (full)   
 pdf  bib  abs   
   BITS  - P  at WAT  2023: Improving I  ndic Language Multimodal Translation by Image Augmentation using Diffusion Models    
  Amulya Dash  | Hrithik Raj Gupta  | Yashvardhan Sharma    
 This paper describes the proposed system for mutlimodal machine translation. We have participated in multimodal translation tasks for English into three Indic languages: Hindi, Bengali, and Malayalam. We leverage the inherent richness of multimodal data to bridge the gap of ambiguity in translation. We fine-tuned the ‘No Language Left Behind’ (NLLB) machine translation model for multimodal translation, further enhancing the model accuracy by image data augmentation using latent diffusion. Our submission achieves the best BLEU score for English-Hindi, English-Bengali, and English-Malayalam language pairs for both Evaluation and Challenge test sets.   
 pdf  bib  abs   
   O  dia G  en AI  ’s Participation at WAT  2023    
  Sk Shahid  | Guneet Singh Kohli  | Sambit Sekhar  | Debasish Dhal  | Adit Sharma  | Shubhendra Kushwaha  | Shantipriya Parida  | Stig-Arne Grönroos  | Satya Ranjan Dash
8. MTNS_0 conference:
Department of Mathematics    
 25th International Symposium on Mathematical Theory of Networks and Systems (MTNS 2022):  
  12-16 September 2022, Bayreuth, Germany  
 menu bar  Mobile Menu  Close 
  Home 
  Overview 
  Important dates 
  Author and speaker information | Close 
  Back 
  Author and speaker information 
  Overview 
  Remote presentations 
 25th International Symposium on  
  Mathematical Theory of Networks and Systems  
  MTNS 2022  
 Bayreuth, Germany, 12-16 September 2022  
  The full papers presented at MTNS 2022 are available on IFAC-PapersOnLine  . The extended abstracts are available here  .  
 Lars Grüne (General Chair), Birgit Jacob and Karl Worthmann (Programme co-Chairs)  
 flyer for the MTNS 2024 (JPG)   
  flyer for the MTNS 2022 (PDF)   
  poster for the MTNS 2022 (PDF) 
  MTNS was co-sponsored by IFAC, DFG, Oberfrankenstiftung, and Bechtle AG    
 Webmaster: Dr. Robert Baier
9. MTNS_1 conference:
Log in 
 Mathematical Theory of Networks and Systems (in cooperation with IFAC) - 25th MTNS 2022  
 Info      
  Past Conferences 
  Organizer Guide 
  Author Guide 
  PaperCept 
  Conference Application 
  IFAC Release Form for Speakers 
  Editor's Guide 
  IFAC Young Author Conference Award 
  IFAC Webpage template (and hosting) 
  IFACx labeled conferences 2023 
  Guidelines for IFAC Conference Awards 
  IFACx labeled conferences 2024 
  Mathematical Theory of Networks and Systems (in cooperation with IFAC) - 25th MTNS 2022 
 Sitemap   
 Past  Conferences   
 Organizer Guide   
 Author Guide   
 PaperCept   
 Conference Application     
 Imprint  | © 2024 International Federation of Automatic Control. All Rights Reserved.  
 IFAC - the International Federation of Automatic Control - values the privacy of its members, affiliates and visitors to its website and is strongly committed to each visitor's right to privacy. By using IFAC's website, you express your acceptance of IFAC's privacy policy. For details of its privacy policy please follow this link  .
10. MTNS_2 conference:
EECI Graduate School on Control (www.eeci-igsc.eu )  
 MO2. March, 4-8, Rome Italy. Dissipativity in Optimal Control - Turnpikes, Predictive | Control, and Uncertainty (Lars Grune, Timm Faulwasser) 
  MO3. April, 8-12, Besançon France. Modeling and control of distributed parameter systems:the Port Hamiltonian Approach (Yann Le Gorrec, Hans Zwart) 
  M10. May, 20-24, Montpellier France. Control Of Biological Systems: From The Cell to the environment (Denis Dochain) 
  M18. July, 01-05, Dubrovnik. Control and Machine Learning (Martin Lazar, Enrique Zuazua) 
  December 2024   
 IEEE CSS Conference on Decision and Control December 16-19, Milan Italy 
  Invited session on Flow systems (Shu-Xia Tang and Mamamdou Diagne) 
  September 2024   
 IFAC Workshop MICNON September 4-6, Lyon, France 
  August 2024   
 Workshop “Methods and Algorithms for the Control of Complex Systems”. August 27-29, 2024, Banyuls, France 
  Symposium MTNS, August 19-23, Cambridge, UK 
  July 2024   
 American Control Conference, July 6-12, Toronto Canada 
  June 2024   
 European Control Conference June 25-28, Stockolm, Sweden 
  IFAC Wokshop on Lagrangian and Hamiltonian Methods for Non Linear Control LHMNC24 June 10-12, Besançon France 
  May 2024   
 2nd Brig Workshop on Dissipativity in Systems and Control, May 21-24, 2024, Brig, Switzerland 
  --------------------------------------------------------------------------------------------------------------------------------------   
 Past events   
 July 2023    
 22nd IFAC World Congress, 9-13 July 2023, Yokohama, Japan. 
  Invited open track: | Recent trends in modeling, simulation and control of Distributed Parameter Systems | organized by Y. Le Gorrec and H. Ramirez 
  June 2023    
 American Control Conference, May 31 - June 2, 2023, San Diego, CA, USA. 
  January 2023    
 IFAC Symposium on Nonlinear Control Systems - 12th NOLCOS 2022™, 4-6 January | Canberra, Australia. 
  December 2022    
 IEEE Conference on Decision and Control, 6-9 December, Cancun, Mexico. 
  September 2022    
 IFAC Symposium on Nonlinear Control Systems - 12th NOLCOS 2022™ Canberra, AUSTRALIA, 6-8 September 2022. Postponed to January 2023 
  IFAC-IEEE CSS Workshop on Control of systems Governed by Partial Differential Equations (CPDE2022), 7-9 September 2022, Kiel, Germany 
  25th International Symposium on Mathematical Theory of Networks and Systems (MTNS 2022): 12-16 September 2022, Bayreuth, Germany 
  July 2022    
 European Control Conference (in cooperation with IFAC) - ECC 2022 London, UNITED KINGDOM, 13-15 July 2022. 
  4th IFAC Workshop on Thermodynamics Foundations of Mathematical Systems Theory, TFMST 2022, July 24-27, 2022, Montreal, Canada. 
  TU Vienna/IFAC International Conference on Mathematical Modelling - 10th MATHMOD 2022 Vienna, AUSTRIA, 16-18 July 2022 
  June 2022    
 American Control Conference (in cooperation with IFAC) - ACC 2022, Atlanta, GA, USA 8-10 June 2022. 
  IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems - 13th DYCOPS 2022™ Busan, REPUBLIC OF KOREA, 14-17 June. 
  April 2022    
   March 2022    
 Spring  School  on Theory and Applications of Port-  Hamiltonian  Systems  Frauenchiemsee  ,  20 – 25 March 2022. www.mw.tum.de/rt/phs2022

output:1. MSWIM_1 information:
2. MSWIM_2 information:
3. MSWIM_3 information:
4. MT SUMMIT_0 information:
5. MT SUMMIT_1 information:
6. MT SUMMIT_2 information:
7. MT SUMMIT_3 information:
8. MTNS_0 information:
9. MTNS_1 information:
10. MTNS_2 information:
