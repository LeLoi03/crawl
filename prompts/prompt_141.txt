input:
1. ICNC_2 conference:
Latest Additions 
 19th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD 2023)  
 Title | 19th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD 2023) 
 Event date | 29 Jul 2023  to end of  31 Aug 2023 
 Location | Harbin, China
2. ICNC_3 conference:
Feature papers represent the most advanced research with significant potential for high impact in the field. A Feature Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for future research directions and describes possible research applications.  
 Feature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive positive feedback from the reviewers.  
 Editor’s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. Editors select a small number of articles recently published in the journal that they believe will be particularly interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the most exciting work published in the various research areas of the journal.  
 Original Submission Date Received:  .  
 You seem to have javascript disabled. Please note that many of the page functionalities won't work as expected without javascript enabled.   
 Title / Keyword   
 Author / Affiliation / Email   
 Journal   
 Article Type   
 All Article Types  Article  Review  Communication  Editorial  Abstract  Book Review  Brief Communication  Brief Report  Case Report  Clinicopathological Challenge  Comment  Commentary  Concept Paper  Conference Report  Correction  Creative  Data Descriptor  Discussion  Entry  Essay  Expression of Concern  Extended Abstract  Field Guide  Guidelines  Hypothesis  Interesting Images  Letter  New Book Received  Obituary  Opinion  Perspective  Proceeding Paper  Project Report  Protocol  Registered Report  Reply  Retraction  Short Note  Study Protocol  Systematic Review  Technical Note  Tutorial  Viewpoint     
 Advanced Search     
 Technologies    
 Special Issues    
 Selected Papers from ICNC-FSKD 2023 Conference    
   Submit to Technologies   Review for Technologies   Propose a Special Issue    
 arrow_forward_ios  Forthcoming issue | arrow_forward_ios  Current issue 
  Vol. 12 (2024) 
  Vol. 11 (2023) 
  Vol. 10 (2022) 
 clear     
 Selected Papers from ICNC-FSKD 2023 Conference  
 Special Issue Editors 
  Special Issue Information 
  Guest Editors   
 Manuscript Submission Information   
 Submitted manuscripts should not have been published previously, nor be under consideration for publication elsewhere (except conference proceedings papers). All manuscripts are thoroughly refereed through a single-blind peer-review process. A guide for authors and other relevant information for submission of manuscripts is available on the Instructions for Authors  page. Technologies   is an international peer-reviewed open access monthly journal published by MDPI.  
  e-Book format: Special Issues with more than 10 articles can be published as dedicated e-books, ensuring wide and rapid dissemination. 
  Further information on MDPI's Special Issue polices can be found  here  .  
  Published Papers (1 paper)  
  Order results   
 (This article belongs to the Special Issue Selected Papers from ICNC-FSKD 2023 Conference  )  
 ►  ▼  Show Figures     
 Disclaimer  Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.   
 Terms and Conditions  Privacy Policy
3. ICNP_0 conference:
Call for workshops 
  Travel Grants 
  Submission | Paper submission 
  Camera-ready 
  Program | Conference Program 
  Accepted Papers 
  Sponsors 
  IEEE ICNP 2023   
 The 31st IEEE International Conference on Network Protocols  
  Reykjavik, Iceland, October 10-13, 2023 Follow @IEEE_ICNP     
 ICNP 2023, the 31st annual edition of the IEEE International Conference on Network Protocols, is the premier conference covering all aspects of network protocol research, including design, analysis, specification verification, implementation, and performance.  
 ICNP 2023 will be held physically in-person in Reykjavik, Iceland between October 10 and October 13, 2023. The workshops will be held on the 10th of October and the main conference will take place on the 11th-13th October.  
 Click here  to find out more for past ICNPs.  
 News  
 October 13 2023: ICNP Conference participants 
 October 13 2023: Best paper (see image below) was | Enabling Load Balancing for Lossless Datacenters | by Jinbin Hu (Hong Kong University of Science and Technology); Chaoliang Zeng (Hong Kong University of Science and Technology); Zilong Wang (Hong Kong University of Science and Technology); Junxue Zhang (Hong Kong University of Science and Technology); Kun Guo (FuZhou University); Hong Xu (Chinese University of Hong Kong); Jiawei Huang (Central South University); Kai Chen (Hong Kong University of Science and Technology); 
 October 11 2023: The conference banquet will be held at Iðnó (Idno), next to the pond and City Hall. See | program | for more information. 
  September 3 2023: Non-author registration extended to Octber 1 2023 
  September 3 2023: Author registration extended to September 4 2023 
  August 27 2023: Added | conditionally accepted papers | . 
  August 25 2023: Added | keynote | speaker Prof. Jim Kurose (University of Massachusetts Amherst). 
  August 21 2023: August 31 2023 is the | Camera ready | deadline. 
  August 17 2023: August 31 2023 is the deadline for | student travel grants | . 
  May 22 2023: Added TAISEN workshop 
  April 28 2023: Added workshops page and submission link 
  April 18 2023: Added call for posters/demos 
  Conference Venue 
  Reyjkavik offers a great number and variety of accomodations in hotels and short term rentals. The conference venue is quite central. Please visit our | Accommodation | page for more information. 
 Important Dates  
 Notification of acceptance | July 31, 2023 
 Camera ready version | August 31, 2023 
 Sponsors  
 Gold  
 © 2023 ICNP Organizing Committee  
 Last updated: 10/15/2023
4. ICNP_2 conference:
ICNP    
 IEEE International Conference on Network Protocols |  
 ICNP Steering Committee | ICNP Advisory Board 
 Kevin Almeroth UC-Santa Barbara | Mostafa Ammar, Georgia Tech 
 Ken Calvert, Univ of Kentucky | Simon Lam, University of Texas-Austin 
 Year | Dates | Venue | General Chair(s) | TPC Chair(s) | Papers 
 2023 | October 10 - 13 | Reykjavik, Iceland | Gísli Hjálmtýsson, Cormac Sreenan | Kate Lin, Yong Liu | TBA 
 2022 | October 30 - November 2 | Lexington, Kentucky, USA | Zongming Fei, Simone Silvestri | Deep Medhi, Dan Pei | TBA
5. ICNP_3 conference:
Call for workshops 
  Travel Grants 
  Submission | Paper submission 
  Camera-ready 
  Program | Conference Program 
  Accepted Papers 
 Call for Papers  
 The 31st IEEE International Conference on Network Protocols (ICNP 2023)  
  Reykjavik, Iceland, October 10-13, 2023 Follow @IEEE_ICNP     
 ICNP, the IEEE International Conference on Network Protocols, is the premier conference covering all aspects of network protocol research, including design, analysis, specification, verification, implementation and performance. ICNP 2023 will be held in Reykjavík, Iceland between October 10-13, 2023.  
 Scope  
 The conference is soliciting paper submissions with significant research contributions to the field of network protocol  research. Both experimental studies as well as formal investigations are equally welcome. Topics traditionally of interest include, but are not limited to:  
 All aspects of network protocol research including design, specification, verification implementation, measurement, testing, and analysis. 
  Protocols for cellular networks, data center networks, Internet of Things, sensor networks social networks, software-defined networks, vehicular networks, and wireless networks. 
  Contributions to other areas of data communications to the extent that they articulate a strong connection to network protocols, for example by discussing potential implications on the design or performance of certain types of network protocols. 
  Authors who are unsure whether or not their submissions might fit the scope of the conference are welcome to contact the Program Committee Co-Chairs. .  
 Paper Formatting  
 Paper submissions should adhere to the IEEE Conference formatting requirements using the templates available here  , and should not exceed 10 pages excluding  references. It is the authors' responsibility to produce readable submissions that comply with the formatting constraints. Violating the formatting requirements to squeeze in additional materials will result in a submission being returned without being reviewed.  
 Submission and Anonymity Policy  
 Papers submitted to the conference will be reviewed through a double-blind review process, where the identities of the authors are withheld from the reviewers (and that of the reviewers from the authors). Achieving this goal requires some care to, on the one hand, preserve the anonymity of your submission and on the other hand ensuring proper coverage of related past work, including your own. While this requirement may seem challenging, the few basic steps listed below will go a long way towards achieving the desired outcome.  
 Remove all authors' identifiers, e.g., names, emails, and affiliations, from the title page. 
  Remove acknowledgments of and identifiers of funding sources. 
  Use anonymous names for your files, as source file names are often embedded in the final output you generate, and therefore accessible as comments. 
  When referring to your own related work, refer to it in the third person as you would with any other related work by another author. 
  Besides anonymizing your submission, double-blind reviewing also imposes additional requirements on both authors and reviewers. Specifically, while it is permissible for authors to give local talks on their work and release their paper on a non-peer-reviewed location, e.g., an institutional repository or even arXiv, care should be exercised to limit public exposure as much as possible. This requirement includes refraining from advertising the work on mailing lists and public forums, and in general limiting as much as possible the odds that program committee members be exposed to the work and the authors’ identity. Conversely, program committee members will be advised to neither actively seek to “reverse engineer” the authors’ identity, nor to directly share with other program committee members any such information they may have acquired. All questions regarding possible breaches of the anonymity covenant that underlies the double-blind review process will be adjudicated by the Program Committee co-Chairs.  
 All papers will be provisionally accepted and the final acceptance of any paper will be subject to shepherding by a member of the program committee.  
 Submission and Plagiarism Policy  
 Papers must present original contributions and can neither be previously published nor under review by another conference or journal. Papers containing plagiarized materials will be subject to the IEEE plagiarism policy  and possible penalties and will be rejected without being reviewed.  
 Best Paper Award and Fast-Track Journal Publication  
 One of the accepted papers will be selected for the Best Paper award, and will be fast tracked to the IEEE/ACM Transactions on Networking, with a streamlined journal review process.  
 Presentation and Registration Requirements  
 ICNP 2023 will be held physically in-person in Reykjavík, Iceland. IEEE ICNP requires that at least one of the authors of any accepted paper must register for the conference at the full rate and be available to present the paper at the conference. Any paper that is not presented by an author of the paper will be withdrawn from the proceedings and thus from IEEE Xplore.  
 Technical Program Chairs  
 Kate Ching-Ju Lin  (National Yang Ming Chiao Tung University, Taiwan)  
 Notification of acceptance | July 31, 2023 
 Camera ready version | August 31, 2023 
 Sponsors  
 Gold  
 © 2023 ICNP Organizing Committee  
 Last updated: 9/24/2023
6. ICOCI_0 conference:
ABOUT  CALL FOR PAPER  PUBLICATION  REGISTRATION  SUBMISSION    
 ABOUT  CALL FOR PAPER  PUBLICATION  REGISTRATION  SUBMISSION    
  9 th   International Conference    
  on Computing and Informatics (ICOCI 2023)   
  "Nurturing an inclusive digital society for a sustainable nation"    
  13-14 September 2023 | Sama-sama Hotel KLIA, Kuala Lumpur, MALAYSIA    
  9  th   International Conference    
  on Computing and Informatics (ICOCI 2023)   
  Nurturing an Inclusive Digital Society for a Sustainable Nation     
  13 & 14 September 2023 | Sama-sama Hotel KLIA, Kuala Lumpur, MALAYSIA    
  9 th   International Conference    
  on Computing and Informatics (ICOCI 2023)   
  "Nurturing an inclusive digital society for a sustainable nation"    
  13-14 September 2023 | Sama-Sama Hotel KLIA, Kuala Lumpur, MALAYSIA    
 Organized by:    
 Highlights  
  Indexing   
   Call for paper  ( poster  )  
   Fees & Registration   
   How to submit?   
 Updates   
 - 5th December 2023 - Conference Publications [Publications]   
  - 9th September 2023 - Programme Book [download]   
  - 7th August 2023 - Presentation Schedule [download]   
  - 1st August 2023 - Online Payment Guideline [download]   
  - 15th May 2023 - Submission date is extended to 31st May 2023  
  - 29th April 2023 - Submission date is extended to 15th May 2023  
  - 6th April 2023 - Submission date is extended to 30th April 2023  
  - 5th April 2023 - Keynote address by Prof. Atreyi Kankanhalli  on 14th September 2023 is confirmed  
  - 4th April 2023 - The venue is confirmed at Sama-sama Hotel KLIA, Kuala Lumpur, Malaysia   
  - 2nd April 2023 - Keynote address by Prof. Alan Dix  on 13th September 2023 is confirmed  
  - 13th March 2023 - Conference Proceeding will be published in Communications in Computer and Information Science (CCIS) by Springer Nature   
  - 15th Jan 2023 - Submission Guideline [ click here  ] , Paper Template [ click here  ]  
  - 13th Oct 2022 - Launching of the ICOCI 2023's website  
  - 14th Oct 2022 - Call for collaborators [ brochure  ]  
  - 14th Oct 2022 - Call for International Technical Committee [ brochure  | registration form  ]  
 About  ICOCI 2023  
 Life today has changed drastically as a result of Covid-19. Many aspects of life have shifted from the traditional ways to automation using electronic and communication devices. Digitalization is the main keyword emphasized everywhere to keep society resilient and adaptive to disasters. As a result, a digital society emerges, in which people use information and communication technology at home, work, school, and socialize to keep going with the businesses and life.  
 “ … a society that over-rides differences of race, gender, class, generation, and geography, and ensures inclusion, equality of opportunity as well as capability of all members of the society to determine an agreed set of social institutions that govern social interaction.”   
 As the digital society emerges, the term is extended to the society’s current values and behaviours. Therefore, the United Nations of Economic and Social Commission for Asia and The Pacific ( ESCAP  ) promotes an “inclusive digital society” focusing on making the society e-resilient in recovering from the disaster.  
 From the aspect of computing and informatics, a question arises: What are the features of technology that would enable an inclusive digital society? Specifically, how the major computing fields of study like artificial intelligence, IoT, and cybersecurity would assist in achieving an “inclusive digital society”? Therefore, ICOCI 2023 would like to invite you to discuss this topic and share your thoughts and works. Together, we could nurture an inclusive digital society for the sustainability of our nation and the universe.  
 Programme Schedule  
  Provost's Chair Professor, School of Computing, National University of Singapore   
 History of ICOCI  
 ICOCI has been a successful platform for knowledge sharing for more than 15 years. Thank you for your participation and support.  
   Digital Transformation and Innovative Technologies for Sustainable Cities and Communities   
 2023 | 9 th  ICOCI  
   Nurturing an Inclusive Digital Society for a Sustainable Nation   
  13-14 September 2023, Sama-sama Hotel, KLIA, Kuala Lumpur, Malaysia 
 ICOCI 2023 Committee  
 PATRON   
  Prof. Dr. Mohd. Foad Sakdan  
  Suwannit Chareen Chit Sop Chit  
  PAPER WORK & PROCEEDINGS   
  Assoc. Prof. Dr. Nur Haryani Zakaria  
  Assoc. Prof. Dr. Husniza Husni  
  List of Members   
 Call for Paper  
 Nurturing an Inclusive Digital Society for a Sustainable Nation   
 ICOCI 2023 invites researchers and practitioners to submit manuscripts in the field of computing that can foster an inclusive digital society toward a sustainable nation. The scope of the topic is as follows (but not limited to)  
 Topics of interest  
 Intelligent Systems 
 Important Dates  
 Full Paper  
  Submission | : | 31 st  May 2023 
 Notification of  
  Acceptance | : | Starting 15 th  July 2023 
 Camera Ready | : | 30 th  July 2023 
 Registration &  
  Payment Deadline | : | 15 th  August 2023 
 Conference Date | : | 13- 14 th  September 2023 
 Submission  
  Click for Guideline   
   For enquiry (icoci@uum.edu.my)   
 Submission  
 Format for paper submission  
 Authors must prepare the manuscript using Microsoft Word. The manuscript should be written between 4,000 to 6,000 words or a minimum of 12 pages. However, the manuscript shall not exceed 14 pages at the maximum. It should strictly follow the ICOCI2023 Template  . Please upload and submit TWO(2) manuscript files (1) A manuscript with complete authors’ names and their affiliations and the acknowledgement, and (2) A manuscript WITHOUT authors’ names, affiliations, and acknowledgement for the double-blind review process. All submissions are subjected to a double blind review process. The double blind review is a normal standard practice applies in many conferences. The main reason is to conceal the identity of the authors to the reviewers in order to avoid any bias judgement during the reviewing process. Read the ICOCI Authors Guideline  and please comply to the Springer's Guideline  .  
 The followings are several guidelines for preparing a double blind paper:  
 In the place of the authors' names on the front page, please | do not put authors details | . 
  Avoid from referring to your university, campus or faculty by name, use generalities. 
  Please | avoid statement | like 'In the previous work, we...' 
  Please | omit the acknowledgment | section. This section can be added in the final version of your paper. 
 Submissions containing any of these information will be returned for modification. Once you have received the acceptance notification from the reviewer committee, you can always add the above information in your paper.  
 How to submit  
 Authors can submit their papers through our online submission system starting from 1 January 2023    
  == Submission System ==   
 Pre-review checklist  
 In general, all manuscript submitted will be vetted by ICOCI Technical Committee for quality. Please ensure that your manuscript fulfils the following requirements:  
  1. Title – relate with the conference theme.  
  2. Structures – Abstract, Keywords, Introduction, Related Work, Methodology, Results, Discussion, Conclusion.  
  6. References – please check the formatting in the Springer template given.  
  7. Acknowledgement – please include funding agencies (if any).  
  8. Submit TWO (2) versions of files; (1) With author names and affiliations, (2) Without author names and affiliations.  
 Paper Template  
 Use these template to prepare your article.  
 Authors must make the payment by 15 th  August 2023  or your article will not be published in the proceedings.  
 Standard rate | registration fees (presenters) include conference admission, | e-proceedings | and | e-program book | . 
  Each presenter (author) is allowed to register for only | ONE additional | paper and will receive only one set of conference materials. 
  Accommodation fee is not included | . You have to arrange your own accommodation. 
  There will be no refund for withdrawal made after 15 th  August 2023. A management fee of 6% from the total fee will be charged for refund made before or by 15 th  August 2023. 
 How to pay  
 Important:  Please forward a scanned copy of the bank slip/telegraphic transfer receipt to ICOCI 2023 Secretariat (icoci@uum.edu.my and cc to anizamd@uum.edu.my) for us to verify the transaction. Please kindly write down the participant's name, date and time of the transfer and country & city of origin.  
 Upon receiving the registration form and full payment, the Secretariat will send a letter of confirmation.  
 Local presenters/participants  
  Select the "CONFERENCE/TRAINING" on the left menu. 
  Select the option "PARTICIPANT FEES-CONFERENCE" on the Payment Type. 
  Select the option "THE 9th INTERNATIONAL CONFERENCE ON COMPUTING AND INFORMATICS 2023" on the Payment For. 
  Write "PARTICIPANT FEE: ICOCI 2023 (PAPER ID: XX)" on the Payment Description. 
  OR scan the QR-code  Local Order   
  Select the "CONFERENCE/TRAINING" on the left menu. 
  Select the option "PARTICIPANT FEES-CONFERENCE" on the Payment Type. 
  Select the option "THE 9th INTERNATIONAL CONFERENCE ON COMPUTING AND INFORMATICS 2023" on the Payment For. 
  Write "PARTICIPANT FEE: ICOCI 2023 (PAPER ID: XX)" on the Payment Description. 
  OR scan the QR-code    
  Local   
   International  Submit Evidence  
  icoci@uum.edu.my    
 Publication  
 Indexing  
 ICOCI published papers in conference proceedings and journals  
   2023  
 Publications of Previous Conferences  
  ICOCI 2023   
  About   
   The Team   
 Call for...   
  Paper Submission  ( Poster  )  
   Reviewer Application   
   Authors' Registration   
 Contact   
 ICOCI Secretariat,  
  School of Computing,  
  Universiti Utara Malaysia,  
   icoci@uum.edu.my
7. ICOCI_1 conference:
School of Computing  
  CALL FOR PAPERS - 9th International Conference on Computing & Informatics 2023  
 Join ICOCI 2023 scheduled on 13-14 September 2023 in Kuala Lumpur.    
 - Prof. Alan Dix and Prof. Atreyi Kankanhalli   will deliver a keynote address.  
  - Accepted papers will be published in Communications in Computer and Information Science (CCIS) by Springer Nature.  
 Submit your papers before 15 April 2023.
8. ICOCI_2 conference:
This book constitutes the revised selected papers of the 9th International conference, ICOCI 2023, held in Kuala Lumpur,   
 This book constitutes the revised selected papers of the 9th International conference, ICOCI 2023, held in Kuala Lumpur,  
 This book constitutes the refereed proceedings of the 8th International Conference on Advances in Visual Informatics, IV  
 Author / Uploaded 
  Nur Haryani Zakaria (editor) 
  Nur Suhaili Mansor (editor) 
 Table of contents :  
  Preface  
  ICOCI 2023 Committee  
  Contents – Part I  
  Contents – Part II  
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  Nur Haryani Zakaria Nur Suhaili Mansor Husniza Husni Fathey Mohammed (Eds.)  
  Communications in Computer and Information Science  
  Computing and Informatics 9th International Conference, ICOCI 2023 Kuala Lumpur, Malaysia, September 13–14, 2023 Revised Selected Papers, Part I  
  Nur Haryani Zakaria · Nur Suhaili Mansor · Husniza Husni · Fathey Mohammed Editors  
  Computing and Informatics 9th International Conference, ICOCI 2023 Kuala Lumpur, Malaysia, September 13–14, 2023 Revised Selected Papers, Part I  
  Editors Nur Haryani Zakaria Universiti Utara Malaysia Sintok, Malaysia  
  ICOCI 2023 Committee  
  Patron Mohd. Foad Sakdan  
  viii  
  ICOCI 2023 Committee  
  Secretariat Alawiyah Abd Wahab Nur Azzah Abu Bakar  
  Intel, Argentina Universiti Kebangsaan Malaysia, Malaysia University Ferhat Abbas Setif 1, Algeria University Ferhat Abbas Setif 1, Algeria University Ferhat Abbas Setif 1, Algeria Flextronics International (Flex), Austria City University, Bangladesh Institute of Applied Physics and Computational Mathematics, China University of Hong Kong, China Amity University, Noida, India Chitkara University, India Chitkara University, India Chitkara University, India Chitkara University, India  
  ICOCI 2023 Committee  
  Nazeer Unnisa Qurishi Ali M. Abdulshahed Pooja Gupta Prateek Agrawal Gulfam Ahamad Prashant Johri M. A. Ansari Swagata Dey Venkatesh Gauri Shankar Bali Devi Vikas Kamra Lalit Kumar R. Raja Subramanian Shrddha Sagar Vikram Kumar Susama Bagchi P. Sardar Maran Amit Kumar Mishra Ade Novia Maulana Apri Siswanto Abdullah Tito Sugiharto Rio Andriyat Krisdiawan Erlan Darmawan Evizal Abdul Kadir Yeffry Handoko Putra Waleed Khalid Al-Hadban Athraa Jasim Mohammed Suhaib Kh. Hamed Mohammed Rashad Baker Firas Mahmood Mustafa Zakho Khalid Shaker Arwa Alqudsi Ramadi Hussein K. Almulla Roberto Vergallo Mohd Nor Akmal Khalid  
  x  
  ICOCI 2023 Committee  
  Mustafa Ali Abuzaraida Bhagyashree S. R. Mohd Hasbullah Omar Rubijesmin Abdul Latif Mohd Helmy Abd Wahab Husna Sarirah Husin Aida Zamnah Zainal Abidin Mohammed Gamal Alsamman Quah Wei Boon Ihsan Ali Abdulrazak Yahya Saleh Rajina R. Mohamed Dalilah Binti Abdullah Shahrinaz Ismail Ruhaya Ab. Aziz Syahrul Fahmy Nooraida Samsudin Norhafizah Ismail Noormadinah Allias Zainab Attar Bashi Ashikin Ali Roziyani Setik Siti Fairuz Nurr Sadikan Safyzan Salim Marwan Nafea Irny Suzila Ishak Abdul Majid Soomro Nor Masharah Husain Nur Intan Raihana Ruhaiyem Nik Zulkarnaen Khidzir Nadilah Mohd Ralim Kavikumar Jacob Fawad Salam Khan Muhammad Abdulrazaaq Thanoon Mohammad Jassim Mohammad Muazam Ali  
  Misurata University, Libya ATME College of Engineering, Libya Universiti Utara Malaysia, Malaysia Universiti Tenaga Nasional, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Kuala Lumpur Malaysian Institute of Information Technology, Malaysia Asia Pacific University of Technology & Innovation, Malaysia Universiti Utara Malaysia, Malaysia Ministry of Higher Education, Malaysia University of Malaya, Malaysia Universiti Malaysia Sarawak, Malaysia Universiti Tenaga Nasional, Malaysia Universiti Kuala Lumpur, Malaysia Albukhary International University, Malaysia Universiti Tun Hussain Onn Malaysia, Malaysia University College TATI, Malaysia University College TATI, Malaysia Politeknik Mersing, Malaysia Tunku Abdul Rahman University of Management and Technology, Malaysia International Islamic University Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Selangor, Malaysia Universiti Teknologi MARA, Malaysia Universiti Kuala Lumpur British Malaysian Institute, Malaysia University of Nottingham Malaysia, Malaysia Universiti Selangor, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Pendidikan Sultan Idris, Malaysia Universiti Sains Malaysia, Malaysia Universiti Malaysia Kelantan, Malaysia Universiti Kuala Lumpur, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Kebangsaan Malaysia, Malaysia Universiti Kebangsaan Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia  
  ICOCI 2023 Committee  
  Khairol Amali Ahmad Juliana Aida Abu Bakar Mohd Nizam Omar Waqas Ahmed Shakiroh Khamis Habiba Akter Noris Mohd Norowi Siti Munirah Mohd Sulaiman Mahzan Shahidatul Arfah Baharudin Pantea Keikhosrokiani Renugah Rengasamy Khalid Hussain Massudi Mahmuddin Mahmood Abdullah Bazel Masitah Ghazali Norhanisha Yusof Saiful Bakhtiar Osman Azliza Mohd Ali Norhasyimatul Naquiah Ghazali Yusmadi Yah Jusoh Jasni Ahmad Azlin Nordin Abdullah Al-Sakkaf Kamsiah Mohamed Mudiana Mokhsin Suhaimi Abd-Latif Sani Salisu Ijaz Ahmad Ghaith Abdulsattar Al-Kubaisi Qamar Ul Islam Abdulrazak F. Shahatha Al-Mashhadani Muhammad Kashif Shaikh Mir Jamal Ud Din Najia Saher Tasneem Mohammad Ameen Duridi  
  xii  
  ICOCI 2023 Committee  
  Krzysztof Marian Tomiczek Abdullah Hussein Al-Ghushami Abayomi Abdultaofeek Fathima Musfira Ameer Mohammed Ahmed Taiye Sasalak Tongkaw Abdulfattah Esmail Hasan Abdullah Ba Alawi Mehmet Nergiz Huseyin First Ismail Rakip Karas Mehmet Sirac Ozerdem Evi Indriasari Mansor Hamzah Alaidaros Munya Saleh Ba Matraf Abdullah Almogahed Abdulaziz Yahya Yahya Al-Nahari Ridhima Rani Hani Mizhir Magid Rohaida Romli Shaymah Akram Yasear Mohd Hafizul Afifi Abdullah  
  duties. It must comply with the leadership’s, rules, regulations, and direction. Finally, if all the actors can overcome the obstacles, they will benefit as determined, and the actor-network mission will be complete. This OPP cannot perfectly unite all actors in the translation process, which results in the actor network’s mission not being fully completed. The imperfections in completing the mission of the actor-network will be analyzed to find the factors that support and hinder the completion of the mission of the actor-network.  
  7 Interessement Moment In the problematization process, the actors involved agreed to be included in the network. In practice, it is not guaranteed that they are all consistent in the network of actors, some of whom may think they are not worthy of joining because of their identity. Therefore, it takes a second moment, namely interressement. Several tools were used to establish a balance of power that favored each actor in overcoming obstacles to passing the OPP. Referring to the network of cyber security actors at JSC, the instruments needed for the moment of interest are standardizations, Standard Operating Procedures, Laws and regulations, policies, and leader instructions. Standardization is an instrument needed by non-human actors, especially in electronic systems for public services. It acts as a liaison between the focus actor and other actors and helps the focus actor in coercing and controlling other actors to perform the required activities in the actor network. Standard Operating Procedures are necessary tools for human and non-human actors, especially in software and hardware maintenance. If the network actor does not have this device, there is no good communication between the security system, hardware, and users. In other words, this device can help non-human actors to have a universal language to interact or communicate in the actor-network. Laws and regulations are other significant instruments needed by a network of actors. Laws and regulations are the main handle of the focus actor (JSC Director) to lock all human actors in position. Like laws and regulations, policies are another crucial instrument a network of actors needs. Policies are very effective to use in imposing the interests of various actors in the network. Lead instruction is another tool used to conduct the interessement of several actors in the network.  
  8 Enrollment Moment Enrollment is the third moment of translation, which refers to a set of strategies as actorfocused efforts to define and link various roles that allow other actors to be enrolled. This process relates to the transforming or moving intangible things such as ideas, concepts, and plans into something tangible in the real world [17]. The analysis of enrollment moments in this study is summarized in Table 2.  
  Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh {asadullah.al.galib,humaion.kabir.mehedi, ehsanur.rahman.rhyth}@g.bracu.ac.bd  
  Abstract. The heart of any substantial search engine is a crawler. A crawler is a program that collects web pages by following links from one web page to the next. Due to our complete dependence on search engines for finding information and insights into every aspect of human endeavors, from finding cat videos to the deep mysteries of the universe, we tend to overlook the enormous complexities of today’s search engines powered by the web crawlers to index and aggregate everything found on the internet. The sheer scale and technological innovation that enabled the vast body of knowledge on the internet to be indexed and easily accessible upon queries is constantly evolving. In this paper, we look at the current state of the massive apparatus of crawling the internet, specifically focusing on deep web crawling, given the explosion of information behind an interface that cannot be extracted from raw text. We also explore distributed search engines and the way forward for finding information in the age of large language models like ChatGPT or Bard. Our primary goal is to explore the junction of large-scale web crawling and search engines in an integrative approach to identify the emerging challenges and scopes in massive data where recent advancements in AI upend traditional means of information retrieval. Finally, we present the design of a new asynchronous crawler that can extract information from any domain into a structured format. Keywords: Web Crawling · Crawler · Distributed Systems · Search Engines · Deep Web Crawling · Large Language Models · Asynchronous Crawler  
  A. Al Galib et al.  
  Even though crawling and related technologies have gone through tremendous innovations in the past decade, the ever-expanding and fluid nature of the internet means crawling tools and techniques need to stay on top of the current trends of the web to collect and aggregate information. Due to the explosion of various web technologies, most of the data on the internet now sits behind various search forms that can only be accessed using search queries. This adds another layer of complexity for the crawlers. Along with the current technological advancements in the field of large-scale web crawling, we also focus on deep web crawling, where crawlers need to interact with various web forms to access information. We explore and analyze research materials with a primary focus on old and new crawling techniques capable of withstanding the ever-growing data space. Distributed crawling techniques built on open-source tools show great promise for a future of decentralized indexing of vast amounts of data. We look at the current status of distributed search engines and the challenges it faces in the era of big data. To consider the disruptive AI conversational agents like ChatGPT and Bard and to understand their implications on how people search for information on the web, we explore how these agents can upend the status quo of current search engines. Finally, we are proposing a new asynchronous web crawling technique that can be employed by small to medium organizations for domain-agnostic crawling needs that require data to be extracted from web pages with similar content architectures and stored in specific formats. The Introduction describes the motivation and overall targets of the paper. In the Related Work section, we will explore previous survey works related to web crawling techniques and various architectures of distributed search engines. In the next section, Crawling, we will take a detailed look at various current crawling techniques and different scaling issues that crawlers need to deal with. In the Distributed Search Engine section, we will go through various architectural challenges and prospects of distributed search engines. In the section, AI Conversational Agents, we will discuss the implications of ChatGPT and other conversational AI agents for the domain of search engines and how we acquire information from the web. The section, Domain-Agnostic Asynchronous Crawler, proposes a new asynchronous crawler suitable for crawling sites of any domain, all of which share a common architecture. Finally, we will conclude with the utility and prospects of crawling in the age of AI and generated content.  
  19  
  perform against traditional graph crawling techniques. The authors introduce a type of preferential crawler, Board Forum Crawling, designed explicitly for crawling forum sites efficiently. Traditional breadth-first crawling for forum sites struggles with spider-trap due to duplicate pages and noisy links for various user actions. In their preferential or topical crawler, authors describe a structured approach to crawling all the post pages on a forum site. The authors of [7] describe the challenges and learnings of a highperformance web crawler, Mercator, that is distributed, scalable, and extensible. In, the authors propose a distributed search engine based on a cooperative model, where local search engines reduce the update interval time as compared to a centralized search engine. Each local search engine maintains a local index by accessing files locally. A Hadoop-based distributed search engine is proposed in [4]. Due to the ever evolving and constantly growing internet size, a distributed search engine is proposed to reduce query time. Hadoop consists of two dimensions, the storage dimension and the processing dimension. The proposed system uses Hadoop Distributed File System or HDFS for storage, and for indexing and processing users’ requests, MapReduce is used. This personalized distributed search engine presents an end-to-end model where crawling, extracting, indexing, and fetching results for users’ queries are implemented in Hadoop. A detailed explanation of various crawlers as well as a comparison among crawlers in terms of applicability, usability, and scalability, can be found in [2].  
  3 Crawling The essential components of a web crawler are - a list of URLs to start crawling from, an aggregator that collects web pages using the URLs from the initial list, and a parser that parses the content of web pages and adds new URLs in the initial list. The crawler keeps repeating this process until the crawling list is empty or some other thresholds are achieved [4]. In this section, we will focus on the recent advancements in the crawling field along with the challenges that arise as the size of the data grows. 3.1 Types of Crawlers Based on the scope and type of information collected during crawling, crawlers can be divided into multiple categories, such as universal crawlers, topical crawlers, forum crawlers, and hidden or deep web crawlers [4]. As the internet grows rapidly, the vast majority of the world’s accumulated information can be found hidden behind millions of databases that are only accessible through search panels or forms. To collect data from these hidden parts of the internet, a new type of crawler has emerged, called hidden or deep web crawlers. These crawlers need to generate queries for the search interfaces to acquire information buried in various databases and storage systems. For the indexed data to be useful, it needs to stay updated as time passes. Instead of visiting new web pages, Incremental crawlers update existing and already crawled sites and remove various redundancies to increase storage efficiencies [8]. Running multiple instances of the crawler in a distributed architecture to minimize traffic load and scale horizontally is a required attribute of modern web crawlers [1].  
  A. Al Galib et al.  
  3.2 Techniques This section will explore emerging technologies and approaches for distributed and hidden web crawlers. In a stand-alone crawler module, also known as centralized crawling, a single instance performs all required steps of crawling. While this approach is easy to implement and maintain, given the sheer scale of data, this approach is only suitable for simple use cases [9]. In a distributed system, each crawler instance is a complete crawling module equipped with the necessary tools to perform the end-to-end crawling task. Depending on how these individual modules are managed and run, distributed crawling can be divided into master-worker and peer-to-peer crawling. Distributed crawling is slightly different from parallel crawling. In parallel crawling, individual crawler instances reside in the same LAN, whereas crawler instances in the distributed architecture are scattered across different geographical locations [9]. The hybrid crawling architecture tries to combine the simplicity of centralized crawling with the scalability of distributed crawling. In this approach, URL queue management is centralized, and content fetching from the URLs is distributed across many instances [9]. In the master-worker mode of distributed crawling, a master node performs an orchestrator’s job, where crawling tasks are managed and assigned to worker nodes to perform the actual crawling tasks. The master node manages the global URL list of pages to visit and assigns URLs to each crawler instance [10]. The master node can perform load balancing to avoid overwhelming crawler instances. In a peer-to-peer architecture, crawler instances independently discover and visit web pages without a master node. Some disadvantages of this approach include - a lack of load balancing where one crawler may be downloading a huge number of pages, whereas other crawlers may not have a sufficient number of URLs to crawl data from. 3.3 Distributed Crawling Architectures Some recent implementations of distributed crawling use various open-source crawling frameworks and combine those with the power of cloud services to build robust and scalable systems. Some of these approaches are described below: Scrapy and Redis: Scrapy is an open-source crawling and scraping framework. Redis is an efficient, in-memory, and key-value data store that is used to manage the message queues used to assign tasks to crawler instances. The Scrapy-Redis distributed component can be used to crawl sites with semi-structured information efficiently [10, 11]. Container Clustering: This approach uses a docker container cluster that hosts the crawler instances. Kubernetes is used to orchestrate the clusters of distributed containers. Apache Kafka is used as the communication medium for the crawling instances [10]. Apache Nutch: Apache Nutch is an open-source, powerful, highly scalable, and configurable web crawler that can be customized in various ways to handle all sorts of web pages found on the internet. It uses Hadoop for data processing [12].  
  Large Scale Web Crawling and Distributed Search Engines  
  A. Al Galib et al.  
  online users’ privacy and data protection are of major concern. It is now possible to deanonymize people using sophisticated crawling techniques even if their real identity is hidden behind some authentication protocols [18]. As crawling tools get more efficient and competent in discovering data in the deep web, users’ privacy and, in some cases, their safety due to their political or social activism can be threatened using mass surveillance tools powered by large-scale invasive crawling. Despite these major challenges, many optimization techniques can be applied to increase the efficiency of the crawlers. To identify more effective search panels or user interfaces for deep web crawling, advanced machine learning models can be applied to infer the usability of search interfaces or forms. In deep web crawling, AI can generate better queries to extract more data with few queries. Emerging cloud services could be used to make distributed crawling more efficient and cost-effective. To counter the unfair use of personal data and protect people’s privacy, there should be proper regulations to prevent both private and public sectors from aggressive and indiscriminate crawling without respecting Robots Exclusion Protocols.  
  23  
  Apache Lucene: Lucene is a high-performance indexing and search engine library. Many websites use Lucene to implement their internal search engines. It indexes text documents and then, upon query, generates ranked search results from the indexed content [22]. For the data structure, Lucene uses an inverted index. To provide the auto-complete feature, Lucene uses an n-gram tokenizer. Elasticsearch: Elasticsearch is another open-source distributed analytics and search engine built on Lucene [23]. It provides REST APIs to index and search relevant documents using highly configurable and advanced queries. Due to its distributed architecture using clustering of nodes, Elasticsearch can scale horizontally and rebalance indexes as necessary. It uses JSON as its document storage type [23].  
  4.3 Challenges and Prospects Even though the idea of an open and censor-free web is very promising, there are major technical and practical challenges while running a highly scalable, distributed, and faulttolerant search engine at the internet scale. Current implementations of P2P text-based searching using DHT works efficiently for a fraction of the actual size of the web. Two main issues regarding decentralized search engines are available storage on individual nodes considering the ever-expanding nature of the internet, and constraints on network bandwidth used during full-text searching on a P2P network [21]. Another challenge is reducing search queries’ response time, given that multiple nodes with indexed data need to be queried before a result can be sent to the users. Since no central server controls the addition of new nodes, some adversarial entities can manipulate the crawled index and ranking of search results [19]. Recent advancements in blockchain technologies can be used to optimize different aspects of a distributed search engine, such as crawling, indexing, and storage [19]. Extensive research is also needed to prevent attacks on the P2P system from adversarial players.  
  5 AI Conversational Agents The emergence of ChatGPT has taken the internet by storm. Since it was published in November 2022, ChatGPT has become one of the most popular sites on the web in just a couple of months [24]. ChatGPT is based on GPT-3, a large language model [17] trained on petabytes of data to produce human-like text. 5.1 Challenges to Traditional Search Engines People have been using ChatGPT to generate text and as an interactive search engine to find information. While search engines return a list of web pages, these AI conversational agents provide information in a way that humans are more comfortable with. We need to keep in mind that the purpose of search engines is to parse users’ queries, find relevant pages or documents, and finally rank the search results to provide high-quality responses  
  24  
  A. Al Galib et al.  
  that are beneficial to the users. Large language models generate a sequence of words with a starting prompt. Moreover, the models cannot access the most recent data since their training and cannot provide all sources that played a role in generating certain content. The appeal of these conversational agents in finding information on the web stems from the fact that search engines cannot combine information from multiple sources and then aggregate it to produce a coherent and factually correct answer. Whereas these agents answer questions like another human expert would [25]. Due to the lack of reference materials for generated content, it is more difficult to ascertain the authenticity of generated content by these models. 5.2 Societal and Ethical Impacts The world is already plagued by propaganda and disinformation abundant on social media sites. Since these models are trained on human-generated text in the first place, they may have inherent biases due to the training datasets. Producing disinformation will be much easier with human-like text and will exacerbate the already fragile social and political divides worldwide. Any task that is about generating text on a given topic now needs to be re-examined and re-evaluated. Various professional roles in the domain of content generation, whether article writers or programmers, will be transformed significantly. Prompt engineering, in other words, providing the correct starting sequence of words to generate the best possible output, will be a key skill in the coming days. Exams and evaluation criteria of all sorts need to be rethought in light of the ubiquity of these language models. 5.3 Future Prospects A key area of research that needs to take place is to retain the sources of information generated by the language models and provide them as references to the users. Since people will be using these AI agents to find information on the web, further improvements can be introduced to incorporate recent events in the generated content and references. Much more attention should be given to de-bias the training datasets and shielding the models from being tricked into generating harmful and dangerous content.  
  N. Katuk et al.  
  performed using a hash function that is embedded in the second component of the model. It guarantees that data is safe from alteration or corruption and that access and analysis may be done in a secure manner. It is essential in deepfake films, which may be used to edit video to fabricate scenarios [9]. The integrity of the footage is protected by utilizing a hash function to detect any data manipulation or alteration. Additionally, it ensures that the data can be securely accessed and analyzed without being altered or corrupted. The approach is anticipated to assist organizations by enhancing data retrieval abilities and archiving processes, which will lead to increased security and better incident reaction times.  
  N. Katuk et al.  
  demonstrates the basic need for a system architecture to ensure critical surveillance data is stored securely and efficiently accessible when needed. The second component of the data archiving model focuses on the archiving algorithm, which performs several critical tasks to ensure that only relevant data are stored in the cloud. The algorithm reads the video feed, identifies humans, captures the scene, cuts and stores it in a new image file, creates metadata about the individual, and stores it along with the image file, with an index table created to enable quick and easy retrieval of the archived data. By incorporating integrity checks using hash functions, which produce a distinct digital fingerprint, or hash value, for each data file, the third component addresses the security issue and ensures that the highest levels of data security and reliability are maintained. Last but not least, the data schema is a crucial part of the data archiving architecture for cloud-based video surveillance systems. It outlines the metadata and other pertinent information, as well as the structure of the stored data. The efficiency and efficacy of the surveillance system are increased when the archived data is readily available and retrievable for authorized users thanks to a well-designed data structure. Modern video surveillance operations require a system architecture for video archiving of surveillance equipment on the cloud. Different devices, front-end and back-end applications, communication routes, storage options, and system modules must all be supported by the architecture. The gadgets often consist of cameras, sensors, and other monitoring tools that record and provide data to the system, including video. The incoming data must be captured, processed, and sent to the back end for archiving by the frontend application. The back-end program is in charge of archiving the data in the cloud and giving authorized users access to search and retrieval capability. The data transmission requires channels across a virtual private network (VPN) between the devices, the back end, and the cloud. Data preservation and retrieval solutions are essential for system modules including person detection, scene cropping, indexing, and metadata development. Critical surveillance data may be saved safely and made easily accessible when needed with the aid of a well-designed system architecture for video archiving surveillance systems on the cloud. The system architecture for video archiving of surveillance systems in the cloud is shown in Fig. 1.  
  Fig. 1. System architecture for cloud-based video archiving of surveillance systems  
  N. Katuk et al.  
  hash functions into the data archiving strategy, organizations may guarantee that their crucial surveillance data is accurate and safe while enabling quick and easy retrieval. Algorithm 2 Data integrity checking using a hash function. 1. Retrieve the information and picture data for each image file kept in the cloud. 2. Utilizing a hash function to determine the image data's hash value. 3. Verify that the hash value generated matches the hash value recorded in the metadata. 4. The data is still intact if the hash values line up. Transfer to the following file. 5. If the hash values are different, the data has been tampered with or corrupted. Notify the system administrator and place the file in quarantine. 6. Repeat steps 1-5 for all cloud-stored image files.  
  The data structure, which includes tables to store the information produced by the algorithm, is the fourth component. For example, a table is needed to store information about the video feeds, like the original file name, time, and frame number. Next, a table is needed to store information about the humans in the video, like their physical characteristics, movements, and scene. Next, the metadata requires a separate table to capture information about the new image file, including about the humans in the scene. This table would also include fields for storing the hash values of both the metadata and the image file. Finally, an index table is necessary to retrieve the archived data quickly, with a searchable database of the archived data. Each table in the data schema would have a primary key field to identify each record uniquely. Additionally, there would be foreign key relationships between the tables to ensure that data is appropriately linked and can be accessed and analysed meaningfully. Tables 1, 2, 3 and 4 list the four tables that make up the proposed data schema: Video_Feed, Human_Identification, Metadata, and Index_Table. The original file name, frame number, and time of the video feed are all stored in the Video_Feed table. The Human_Identification database keeps track of details about the people in the video, such as their appearance, how they move, and the scene in which they are present. The new image file’s metadata, including details about the people in the scene and the hash values of the metadata and image file, are stored in the metadata table. The Index_Table, which offers a searchable database of the saved data and enables simple retrieval, is the final component. Table 1. Video_Feed. Column Name  
  Data Type  
  Fig. 4. The data in the table for the humans’ identification with a hash value.  
  the retrieval module is beyond the scope of this paper, as it could involve data mining and machine learning techniques, integrating the integrity check module with it ensures a robust system for data verification. The process of the integrity check module consists of several steps. First, the hash value is calculated using SHA-1 for the given image files, representing the selected frame of the video footage. Next, the stored hash value in the Human_Identification table, calculated during the archiving process, is retrieved. These two hash values are then compared to determine the integrity of the data. If the hash values match, the data is confirmed intact, ensuring the system’s reliability. However, if the hash values do not match, the data has likely been corrupted or tampered with. When combined with the retrieval module, this process fortifies the security and trustworthiness of video surveillance systems, paving the way for a more secure digital landscape. Figure 5 shows the function for calculating the hash value using SHA-1.  
  Fig. 5. The function for calculating hash value using SHA-1.  
  Abstract. Blockchain technology is a distributed digital ledger in a decentralized network that offers immutability, security, and transparency in various applications among digital societies. The consensus mechanism is the defining technology behind the security and performance of the Blockchain system. Under the Industrial Revolution 4.0, blockchain has been considered for integration into supply chain business as an innovative solution to tackle the challenges of traceability, transparency, lack of trust, and data counterfeiting in digital supply chain management. A private permissioned Blockchain is the most suitable type of Blockchain for Supply Chain Management (SCM) as it promises better performance with high throughput and low latency. However, private Blockchains that use the Byzantine Fault Tolerance (BFT) consensus mechanism have low-security capabilities and are more vulnerable to cyber-attacks triggered by malicious nodes. In this paper, we outline the research challenges from the security aspect towards the integration of Blockchain with SCM. Then we design an approach for a private Blockchain-based Supply Chain with security capabilities by proposing an enhancement consensus model to the BFT consensus mechanism for identifying and terminating malicious nodes in the consensus process. The performance of the proposed approach will be validated experimentally and compared against Practical Byzantine Fault Tolerance (PBFT). The proposed approach is expected to prevent security attacks on the consensus mechanism, thereby improving the security and performance of the Blockchain system. Keywords: Byzantine Fault Tolerance · Consensus Algorithm · Supply Chain  
  Kota Samarahan, Sarawak, Malaysia  
  Abstract. Phishing attacks have emerged as a major problem in the digital world due to a rising trend in their frequency. While various approaches have been developed to detect and prevent phishing attacks, a definitive solution to the problem has yet to be discovered. This study discusses automated anti-phishing systems while analyzing and comparing various anti-phishing strategies using exploratory research. Traditional, machine learning, and deep learning-based anti-phishing systems are discussed in the article. The study highlights the use of Artificial Intelligence (AI) based systems, particularly utilizing methods such as Convolutional Neural Networks, Support Vector Machines, and Recurrent Neural Networks. These AI-based approaches dominate the current trend in the field. This study could potentially be helpful for researchers who wish to delve deeper into the topic of automated phishing detection and prevention systems with a comprehensive review. It is advised to carry out further research to investigate the strengths and limitations of different methods and algorithms used in automated anti-phishing systems to understand their performance and effectiveness better. Keywords: Phishing · Anti-phishing · Machine Learning · Deep Learning  
  M. A. A. Aziz et al.  
  As the advancements in internet technology progresses, so does the sophistication of phishing attacks that malicious actors carry out. These attacks pose a growing threat to individuals and organizations; as they evolve, they become progressively more challenging to detect and employ diverse approaches. In order to effectively counter phishing threats, the development of automated anti-phishing systems must keep pace with the advancements in current technologies. Therefore, it becomes crucial to identify the effectiveness and efficiency of advanced automated anti-phishing. This study aims to identify common methods for automated phishing detection and prevention and comprehensively review the related literature. In order to do this, two research questions were constructed to drive the study, which are; (1) what are the available automated methods for detecting and preventing phishing, and (2) how efficient are these common detection and prevention methods. The research aims to investigate the use of automated detection and prevention systems in combating phishing attacks. It will thoroughly review existing literature and studies in the field, including scientific papers, articles, and reports. The objective of the review is to understand the various technologies and algorithms used in these systems and evaluate their effectiveness in detecting and preventing phishing attacks. The research will also identify current systems’ limitations and challenges and determine areas for future research. It is important to note that the research is limited to a literature review and does not involve primary research or experimentation. The information available in the literature may be influenced by the current state of research on the subject and may affect the scope of the research. This research is significant as it addresses the growing need for effective solutions to combat the increasing progression of phishing attacks. The research findings can contribute to the community by identifying suitable anti-phishing systems and providing insights to the limitations and challenges faced by current systems. It can also serve as a reference for future studies in the field and benefit other researchers who are interested in the topic. The significance of the research lies in its support to the development of methods for countering phishing attempts and provides information on their challenges and limitations. Additionally, it contributes to the body of information regarding the development of automated systems that can successfully detect and stop phishing attacks. This article is organized as follows; the next section will discuss the literature review, the methodology, the results and findings will be presented, and a discussion and conclusion.  
  M. A. A. Aziz et al.  
  4.2 Various Methods of Implementing Anti-phishing Systems According to the second objective of the research, this section examines the various methods of implementing anti-phishing systems and presents a brief evaluation of their results. [21] make use of both random forest and naive bayes machine learning algorithms and deep learning-based methods such as convolutional neural networks and long-short term memory networks. Similarly, [22] discusses an automated anti-phishing system that uses convolutional neural networks and recurrent neural networks. Other studies such as [23–30] emphasize the use of multiple machine learning algorithms including decision tree-based algorithms, support vector machines, and natural language processing to classify and identify phishing content in emails and URLs of websites. [30] proposed an anti-phishing system that leverages deep learning to classify the phishing content, and machine learning algorithms to identify phishing content based on the homographs of the website domain name. These studies’ results suggest that using machine learning and deep learning algorithms is a promising approach for anti-phishing systems. Of the 22 studies reviewed, only one proposed a non-machine and deep learning-based method. The advantage of using AI-based anti-phishing systems is the ability to identify new and unknown threats, a significant challenge for traditional detection methods. However, the performance of these methods varies depending on the specific models and techniques used. Some studies have found that deep learning algorithms outperform traditional machine learning methods in terms of malware detection rate and false positive rate, while others have found that a combination of machine learning and deep learning methods is more effective. As a result, the use of a combination of machine learning and deep learning has become widely adopted in the development of anti-phishing systems. It is important to note that the results of these studies may vary depending on the specific dataset and evaluation metrics used. Furthermore, the constantly evolving nature of phishing attacks requires the development of new and innovative anti-phishing systems that can adapt to changing threats. As a result, further research is needed to improve anti-phishing systems’ performance and accuracy.  
  Abstract. Cloud computing is a pay-as-you-go business model that offers elastic remote data storage, and computing resources have become necessary due to the emergence of big data. After data outsourcing to the cloud, cloud users lose control over data and are always concerned about data privacy and security in adopting the cloud service model. So, to ensure remote data integrity, a trusted auditor can make auditing tasks according to the users’ request, which is helpful to release auditing overheads on a user device and meaningfully improve the scalability of cloud services. Although numerous data auditing techniques have been designed with TPA so far, these techniques need to improve on data security and efficiency issues. First, these techniques cannot authenticate block indices, so the server can produce valid proof without an original data block to pass the audit process. Second, existing approaches do not include position fields, so the server can replace the tampered data block with a healthy one to pass the audit phase. To overcome these issues, this paper introduces a new public data authentication scheme, ERPDA. The proposed technique incorporated a newly designed Merkle Tree (MT) based structure, Sequence and Position-based Tree (SPT) that minimises computation complexity to find nodes in data audit and avoid data replacement attacks. The experimental outputs showed that our suggested technique is effective with the comparative data auditing techniques in computation overheads, and the security is proved under the random model. Keywords: Cloud Computing · Third-party Auditing · Proof of Data Possession  
  2 Literature Review Juels and Kaliske [16] presented the first evidence of the data retrievability technique. The sentinel (block masking) approach was used to hide the value in the standard data block so that the server cannot differentiate hashed data blocks. The users may download and confirm the accuracy of data simultaneously. They use symmetric encryption methodology to secure user data, which imposes little computation overhead. However, it is limited to the number of requested blocks as the pre-processed Message Authentication Codes (MACs) are used in the audit phase. Also, these schemes support static data and cannot support public audits.  
  Abstract. In today’s world, cybersecurity is critical in the field of information technology. With the rise of cyber-attacks, including ransomware attacks, protecting user data has become a top priority. Despite the various strategies employed by governments and companies to counteract cybercrime, ransomware continues to be a major concern. Therefore, there is a need to detect and obfuscate viruses in a better way. This immutable impact on the target is what recognizes ransomware attacks from traditional malware. Ransomware attacks are expected to become more problematic in the future. Attackers might use new encryption methods or obfuscation techniques to make ransomware detection and analysis a difficult job. To protect against such attacks, organizations and users employ various tools, guidelines, security guards, and best practices. However, despite these efforts, cyber-attacks have increased exponentially in recent years. Among the most devastating of these attacks is ransomware, which can encrypt user files or lock their devices’ interfaces, rendering them unusable. This research paper provides a valuable resource for researchers, practitioners, and policymakers seeking to enhance their understanding of ransomware detection and mitigation. It also examines defense tactics, such as system backups and network breakdowns, which can help mitigate the impact of an attack. Finally, the paper considers upcoming challenges in the field of cybersecurity and the importance of staying vigilant in protecting against cyber threats. Keywords: Cyberattack · Cybersecurity · Ransomware detection · Ransomware mitigation  
  M. u. Rehman et al.  
  1.3 Major Problem Previous systematic reviews of ransomware have mainly focused on its impact in specialized industries such as healthcare, and government organizations neglecting the fact that ransomware is not limited to specific domains. To address this limitation, this paper proposes a comprehensive evaluation of the detecting and mitigating of ransomware, serving as a starting point for further research. Furthermore, the paper discusses existing methods for detecting ransomware, analyzing their pros and cons. Lastly, prevention tools for ransomware attacks are discussed, providing valuable insights for organizations looking to enhance their security measures against ransomware threats. 1.4 Study Objectives The aim of this study is to examine prior research, consolidate its findings, and concentrate on analyzing ransomware attacks, risks, mitigation, and prevention methods to control ransomware attacks. The study also aims to provide recommendations for the use of these techniques and tools, as well as identify areas for future research in this field. Ultimately, the objective would be to contribute to the development of more effective strategies for mitigating the impact of ransomware attacks. To achieve this goal, three research questions have been formulated, as shown in Table 1. Table 1. Formulated Questions and discussion Research Question  
  Discussion  
  1.5 Contribution and Structure This systematic literature review provides a valuable resource for individuals seeking to advance their knowledge in ransomware attacks and cyber security. By synthesizing previous research, it builds upon existing knowledge and makes new research, as discussed in Table 1. • Our review identified 31 papers that are relevant to the topics of cyber security and ransomware threats and detection. This set of studies can serve as a resource for other researchers who seek to further investigate these areas. • Organize and classify different methods of ransomware attacks into a specific taxonomy. • We investigated the conditions utilized for evaluating defense, detection, mitigation, and prevention techniques against ransomware attacks. • We identified available research data for a future analysis of ransomware and provided guidelines to assist in further research in this field. The structure of this paper unfolds as follows: Sect. 2 explains the methodology employed to systematically select primary studies for our comprehensive analysis. In Sect. 3, we present the outcomes derived from our scrutiny of the selected primary research studies. Finally, Sect. 4 serves as the result of our research efforts, offering conclusions drawn from our findings and suggesting recommendations for future investigations.  
  2 Methodology The research methodology section of this paper describes the systematic approach taken to look at previous studies about prospective ransomware attacks and their corresponding detection systems. Article offer details on the inclusion and exclusion criteria used to choose relevant research, also describe how we locate articles, papers, books, and journals about ransomware attacks. 2.1 Source Material The study utilized a specific search engine and focused on entering relevant keywords to ensure the retrieval of primary research that would address the research questions. The selected keywords were carefully chosen to optimize the development of relevant findings. Boolean operators were limited to AND and OR. The search terms used were: (insert the specific keywords used). (“ransom” OR “ransom-ware” OR “ransomware” OR “Mal-ware” OR “Malware” OR “ransomware attacks”) AND “information security” (“ransomware” OR “ransom” OR “Malware AND (“security” OR “cybersecurity” OR “cyber-security”). In the first phase, the task to be performed for the quality of research is to undertake an exhaustive literature search. Therefore, a search was conducted using six different electronic libraries namely IEEE Xplore, Science Direct, ACM, Springer, Web of Science, and Google Scholar to search for the relevant materials.  
  the “anywhere in the article” option was used. For Web of Science, the search was limited to the “subject” parameter. The search included a variety of publication types, such as journal articles, book sections, working papers, conference papers, dissertations, and reports. Advanced search filters were used to refine search results, including past 13 years, document types, and English language. New keywords like “cyber risk” and “challenges and analysis” were added. A slimming approach was used to analyze articles, removing duplicates, and considering only English-language textual sources. 30 journal articles were selected for the literature study, as shown in Fig. 4. 2.2 Inclusion and Exclusion Criteria A systematic literature review requires empirical evidence from case studies, new ransomware attacks, and advancements in ransomware mitigation technologies. Englishwritten, peer-reviewed studies must meet standards, and only updated ones within recent years are considered. Google Scholar results may not meet standards, so all results are evaluated for compliance (Table 2). Table 2. Inclusion and exclusion criteria for primary studies Inclusion Criteria  
  Exclusion Criteria  
  Governmental documents and blogs should not be included in the article  
  The article must be a peer-reviewed paper published in a journal or conference proceedings  
  non-English publications  
  M. u. Rehman et al.  
  The evaluation process was modeled after similar literature reviews. To evaluate the effectiveness of randomly selected papers, a specific quality assessment procedure was implemented. Step 1: Ransomware: The article should discuss multiple forms of ransomware attacks or security breaches and offer insightful commentary on a specific issue. Step 2: Perspective: The research’s objectives and conclusions should be properly contextualized to ensure a comprehensive understanding of the study. Step 3: Ransomware detection Strategy: Study must provide enough information to show how technology is used to detect attacks and answer research questions, including specific tools and techniques used for detection and mitigation. Step 4: Defense context: The document should explain the security issue to help answer research questions, including its nature, potential consequences, and challenges in addressing it. Step 5: Security measures: The application of diverse security measures to alleviate several types of ransomware attacks. Step 6: Data Recovery: Specifics on data collection, measurement, and reporting must be provided to assess accuracy. 2.5 Data Extraction The data completeness and accuracy of articles were assessed by extracting data from quality-approved papers. The technique was tested on a preliminary investigation before being applied to the full set of research. Data was categorized and entered into a spreadsheet using the following categories. Context Data: Information involving the study’s performed objectives. Qualitative Data: The author’s findings and opinions. Quantitative Data: Information collected through tests and research has been used in the study.  
  Key Qualitative  
  [26] The article covers the methodology and threats of Petya ransomware, as well as strategies for awareness and mitigation [27] Healthcare companies can improve system defense through user-focused tactics like simulation and training on proper computer and network application usage [19] [25] The paper covers the impact of ransomware attacks on cloud service users and providers and proposes mitigating tactics.[28] [29] To provide the decryption key for encrypted user data, hackers often demand a ransom or payment, typically in the form of digital currencies [19] The paper stresses the importance of a written information security program mandated by Massachusetts law or other security frameworks [30] Memory forensics was conducted on volatile memory dumps of virtual machines using the Volatility framework for analysis [9] The report introduces Net Converse, a machine learning study for detecting ransomware network traffic reliably [18] The article proposes DNA act-Ran, a digital DNA sequencing engine that uses machine learning to detect ransomware, utilizing frequency vectors and design limitations for digital sequencing  
  Type of research effects Mitigation  
  3.3 What Are the Most Common Tactics and Techniques Used by Ransomware Attackers and How Can These Be Thwarted? Ransomware attackers commonly use social engineering, phishing, and software vulnerabilities to gain access to systems and demand payment [30]. To thwart these attacks, user education, software patching, data backups, network segmentation, and access controls can help prevent these attacks and limit their impact.  
  4 Mitigation and Prevention Techniques of Ransomware Preventing ransomware is crucial to protect against its damaging effects on individuals and corporations. In case of infection, data recovery can be challenging and may require the help of a trusted specialist. Pre-encryption mitigation refers to the security measures taken before the encryption process to minimize the risk of security breaches.  
  Identify the simulator or programming language used  
  Fig. 1. Paper Selection Process  
  G. Guntoro and M. N. B. Omar  
  3.4 Study Selection Specific inclusion and exclusion criteria are used to select the primary studies [25]. The inclusion criteria are as follows: 1) articles must be published in journals. 2) articles are selected based on the journal’s impact factor, limited to Q1-Q3. 3) papers on the subject of IDS primarily compare algorithms or techniques. On the other hand, the exclusion criteria are as follows: 1) Research not written in English. 2) Literature review studies. 3) Studies without substantial IDS validation. 4) Research addressing intrusion methods and datasets unrelated to IDS contexts. 5) Studies not centered on relevant subjects. 3.5 Data Extraction To address the research questions, data collection from primary studies is necessary. Following that, we will perform data extraction using the collected data. We will extract the IDS research area (to address RQ1), IDS techniques (to address RQ2), IDS datasets (to address RQ3), IDS methodologies (to address RQ3, RQ4, RQ5), and IDS simulators (to address RQ6). 3.6 Study Quality Assessment and Data Synthesis Assessing the quality of studies is essential to enhance the understanding of synthesized findings and solidify conclusions. The primary aim of data synthesis is to provide comprehensive responses to all research inquiries. This data is organized based on the research question. It is then visualized using pie charts, bar graphs, and tables. 3.7 Threat Validation There is a potential threat to this review’s reliability. This occurs because the paper search solely entails manually reviewing the titles of all journal articles. Therefore, specific papers might have yet to undergo comprehensive screening for inclusion in this study.  
  March 2023, as indicated by the selected primary research, include Python (20), Matlab (20), Weka (4), Sucirata (1), Rapidminer (1), C# (1), and Java (1). This information is depicted in Fig. 4.  
  Fig. 4. Simulators Used  
  (de Carvalho Bertoli et al. 2023a)  
  2023  
  (Abu Alghanam et al. 2023)  
  2023  
  Abstract. Wireless Sensor Networks (WSNs) consist of numerous affordable, energy-efficient, compact wireless sensors. These sensors are designed to collect, process, and communicate data from their surrounding environment. Several energy-efficient protocols have been created specifically for WSNs to optimize data transfer rates and prolong network lifespan. Multi-channel protocols in WSN are one of the ways to optimize efficiency and enable seamless communication between nodes, thereby reducing interference and minimizing packet loss through multiple channels. Despite their numerous advantages in data sensing and monitoring, various attacks can pose a threat to a WSN. There are several types of attacks that a WSN may encounter, including spoofing, eavesdropping, jamming, sinkhole attacks, wormhole attacks, black hole attacks, Sybil attacks, and DoS attacks. One of the strategies for enhancing security in WSNs is implementing a cross-layer intrusion detection system (IDS) that can detect initial indicators of attacks that target vulnerabilities across multiple WSN layers. This paper reviews the existing IDS at each layer and the challenges in an energy-efficient cross-layer IDS for WSN in terms of the attacks and IDS approaches. Keywords: Cross-layer IDS · Wireless Sensor Network · Multi-channel protocol  
  can lessen the impacts of interference, enhancing network effectiveness, stability, and link dependability, minimizing latency, and reducing total energy usage. This, however, creates another issue. A wireless sensor network is susceptible to several various attacks. Due to several flaws and, most crucially, the data involved, wireless sensor networks are continually vulnerable to serious attacks. Typically, the nodes in a WSN are tiny, battery-operated gadgets containing sensors, microcontrollers, and communication transcribers. Due to the node’s limited resources, wireless sensor networks are susceptible to various threats that may jeopardize the security and integrity of the data. Nevertheless, WSNs are susceptible to risks despite the various benefits they offer regarding data sensing and monitoring. These risk factors include those caused by memory limitations, unreliable communication, higher communication latency, unattended network operation, deployment in an environment prone to attacks and scalability. Some of these attacks, such as random multi-channel jamming attacks that interfere with radio frequencies on wireless communication channels and cause channel congestion, are intended to take down the network. The challenge may be that random multi-channel jamming attacks are difficult to detect and eliminate due to their random jamming behaviors. Attackers have complete discretion over the time and the specific channels to jam. Other attacks aim to eavesdrop on communications. Others are made to introduce erroneous data into the network. This poses a danger to real-time, reliable WSNs. Security in WSNs is, therefore a difficult problem since it depends on the way to evaluate the reliability of sensor data. Numerous studies on intrusion detection in WSNs have been done in recent years [1–5]. Intrusion detection is used to detect unauthorized activity in a system. It works well as a security measure to defend WSNs against intrusion. There have been a few studies on the security of WSNs. However, they have mostly emphasized attack prevention instead of attack detection. This is an important study area since an attacker who can go undetected might cause significant damage or disruption. Although several intrusion detection systems have been developed to support WSNs, the majority of these systems only work at one layer of the Open Systems Interconnection (OSI) model. Several proposed intrusion detection systems are based on a cross-layer approach. They comprise the physical, data link, and network layers that contribute to cross-layer intrusion detection systems (IDS) design. By detecting the attackers across multiple layers, cross-layer IDS secures the WSN. The rest of the paper is organized as follows. Section 2 highlights various attacks and challenges associated with WSN at each layer. Section 3 presents and compares recent existing work in cross-layer IDS in WSN. Section 4 discusses the challenges and future directions on cross-layer IDS, and Sect. 5 concludes the paper.  
  2 Related Work 2.1 Wireless Sensor Network Cross-layer Protocols WSNs are networks of many inexpensive, low-power, small wireless sensors. The sensors can gather, analyze, and transmit data from their environment. WSNs have gotten a lot of attention from several application sectors because of their capabilities, including  
  N. Nordin and M. S. Mohd Pozi  
  military surveillance, industrial monitoring, target tracking, and environment monitoring. Numerous energy-efficient protocols have been developed for WSNs to maximize throughputs while extending the lifetime of the networks through the Medium Access Control (MAC) and routing protocols, power consumption, and energy harvesting. The protocols are a vital aspect of WSN communication. The protocols determine the allocation of channel resources among the network’s nodes in a way that maximizes efficiency, manages channel constraint, and ensures that nodes communicate simultaneously in single or multiple channels effectively to reduce interference which leads to packet drop. The WSNs are susceptible to attacks due to the extensive nature of node dispersion and the hardware limitation of the nodes. Numerous studies on single-channel WSN protocols such as LEACH [6], RPL [7] and multi-channel protocols such as Chrysso [8] and MiCMAC [9] that interface to the MAC and the network layers, as well as MCRP [10], that interfaces to the MAC, network, and application layers, have been conducted. The real-time nature of MCRP’s multichannel processing enables it to adjust to any location’s local interference. MCRP is a cross-layer protocol that is decentralized and centrally controlled to reduce interference without knowing where the channels are occupied in advance. In order to effectively use the spectrum, MCRP considers all channels that are accessible and transmits on a number of them. This generality makes it possible for better channels to be selected based on the location the sensor nodes are deployed. As a result, the protocol reduces the impact of interference, improving network efficiency, stability, and link reliability. While MCRP exhibits promising results in terms of improved resilience to interference, significantly higher throughput, and link stability, extending the lifetime of WSNs, it is vulnerable to numerous attacks because security was not considered. The protocol is more susceptible to attacks due to the cross-layer attributes and usage of several channels which are necessary for proper data transmission and reception. Thus, the intrusion detection system is a potential approach to detect attacks. 2.2 Intrusion Detection Systems The limitations of sensor nodes in WSNs prevent traditional IDSs from being directly implemented in WSNs. To resolve this issue, various IDSs have been proposed for WSNs. Due to its IDS mechanism and the high processing demands of the algorithms of the IDs, several extended protocols have negatively impacted the network’s energy. An IDS tracks traffic data that may be used to spot and prevent intrusions that compromise the privacy, integrity, and accessibility of an information system. An IDS is a term for software or hardware devices that monitor networks for cyberattacks from inside or outside and trigger an alert.  
  Fig. 1. Fundamental IDS architecture  
  Application Eavesdropping, false data injection, spoofing and altering routing attack, malicious code attack, repudiation attack, DoS attack  
  Other attacks on all the layers are listed in Table 1 [1–5]. These cyber-attacks have a variety of objectives, including stealing, altering, hacking, and flooding the targeted nodes with excessive packets to deplete the sensors’ battery power and disconnect them from the network, making them unusable and hindering them from sensing or routing traffic. The performance, effectiveness, and reliability of communication may suffer as a result of these attacks. To overcome these problems, effective security mechanisms, such as well-defined detection and mitigation procedures, must be put in place. As a result, intrusion detection methods to protect against such attacks are becoming increasingly important. An intrusion detection system (IDS) is a promising solution to identify intrusions in WSNs. However, the IDSs in WSNs face new challenges due to the characteristics of WSNs, thus, there is a need for an IDS to work interoperability across the layers.  
  3.2 Cross-layer Intrusion Detection Systems Due to the numerous characteristics of sensor networks, such as their limited battery power supply, poor bandwidth support, self-organizing nature, and dependence on other nodes, there is a significant risk of security attacks in all OSI model layers. A single or a series of attacks may be made. Several specific attacks occur at regular intervals, such as blackhole attacks, rushing attacks, and flooding attacks. It has been noticed that circumstances may result in several attacks rather than a single attack. As a result, it is  
  Packet delivery ratio, energy consumption, RSSI, bad packet ratios  
  Bengag et al. (2023) [23]  
  Fuzzy logic system  
  Trust value  
  Kumar et al. (2023) [30]  
  Trust model and verification  
  routing mechanism and make it resistant to both insider and external attackers, it might be expanded to add security features such as with an IDS. WSNs use energy to gather information about their surroundings, process it, and send the resulting data. The IDSs must therefore use the least amount of energy feasible to leave enough for the WSN’s vital operations. IDSs are crucial for the security of WSNs, and those created for them need to have specific features like low power usage. The success of an IDS in a WSN depends on the way it affects the network’s energy usage as a WSN is resource constrained. Maintaining a network over its lifespan is one of the biggest issues in WSNs, so energy efficiency in IDSs is equally important. WSN sensor nodes have limited storage capacity. Therefore, it is challenging to meet the need to store attack signatures in sensor nodes. In order to create an IDS in the WSN to identify various sorts of attacks, machine learning techniques were mostly utilized. The drawback of those techniques is that they require more memory to deploy a model to a sensor node and take longer for machine learning algorithms to build and evaluate data sets for WSN. It could be conceivable to develop a hybrid or cloud-based machine learning prototype for carrying out intrusion detection in the WSN to reduce the amount of memory required in the detection techniques. Another point to consider is many of the IDS schemes available do not provide self-defense. It is crucial because certain attackers may frequently generate false alarms by flooding the IDS host with irrelevant traffic. The host can run out of resources as a result, leaving the system open to intrusions. IDS’s ability to protect itself is thus desirable.  
  5 Conclusions WSNs face numerous cyberattacks that pose risks to the network’s availability, privacy, control, and reliability. These attacks exploit the vulnerable nature of nodes deployed in hazardous and remote environments, where they often remain unattended, unable to protect the information flow physically. As a result, there is an increased likelihood of node compromise, leading to decreased network security and protection. It is crucial to implement robust security measures to safeguard these networks against breaches and assaults. One effective approach is the adoption of a cross-layer intrusion detection system, which provides comprehensive protection across multiple WSN layers. This paper reviews the existing IDS at each of the layers and cross-layers for WSN in terms of the attacks and approaches. Cross-layer IDS can detect early signs of advanced attacks exploiting multiple layers’ vulnerabilities. They reduce evasion techniques by analyzing data from multiple layers, making it harder for attackers to evade detection. However, it’s important to consider WSN’s limited resources and constraints when designing and implementing cross-layer IDS. Thus, a more energy-efficient cross-layer IDS for WSN needs to be developed and improved from the existing IDS.  
  Cross-layer Based Intrusion Detection System  
  Kedah, Malaysia [email protected]  2 Department of Computer Science, College of Computer Science and Engineering, Al-Ahgaff University, Hadhramaut, Yemen  
  Abstract. The social networks and news ecosystem provide valuable social information, however, the rise of deceptive content such as fake news generated by social media users, poses an increasing threat to the propagation and diffusion of fake news over the social network and among users. Low-quality news and misinformation spread on social media had negative impacts on individuals and society. Hence, it is essential to detect fake news to ensure the spread of accurate and truthful information. To address this problem, a new approach using Binary Bat Algorithm (BBA) for fake news detection (FND) on Twitter data is proposed in this paper. Twitter data usually generates massive feature space which might consist of irrelevant features that could jeopardize the subsequent process. The proposed FND approach involves four stages, namely data collection, pre-processing, feature extraction, and fake news detection. The proposed techniques are tested on PHEME dataset, and the experimental results are measured in term average of Precision (PR), Recall (R), F-measure (F), and Accuracy (ACC). The experimental results show that the BBA algorithm has outperformed the Social Spider Optimization (SSO) algorithm. Thus, BBA is a promising solution for solving high-dimensionality feature space in fake news Twitter data. Keywords: Fake news detection · Binary Bat Algorithm · Social Spider Optimization · Feature Selection · Text Mining  
  can be used to identify patterns in language use that may be associated with fake news. Like NER, POS is dependent on existing lexicons. Besides the above studies, in some fake news detection models, several researchers have omitted the feature selection phase that caused poor performance since the training data can be biased and overwhelmed which ultimately hinders the subsequent process. Hence, this paper proposes a feature-based optimization approach for fake news detection on social media using BBA and SSO with K-Means clustering. The approach involves extracting relevant features from Twitter data, and optimizing these features before clustering them with K-Means. The resulting optimized features are to train a machine learning model for fake news detection. The proposed approach aims to achieve high accuracy in fake news detection. The remainder of this paper is organized as follows. Section two provides related works in clustering analysis. The proposed feature-based optimization approach for fake news detection on social media using K-Means clustering is presented in section three. Section four describes the experimental setup and the results obtained. Finally, section five presents the conclusion and future work.  
  2 Related Works in Clustering Analysis Cluster analysis is a technique for finding regions in n-dimensional space with large concentrations of data. These regions are called “clusters”. Data are sorted into groups in clustering analysis based on predetermined principles. Its function is to categorize the data so that there is a significant degree of similarity within classes and a minor degree of similarity between classes. Although there are many different types of clustering algorithms available right now, each one has unique properties and applications. In general, there are different perspectives in categorizing the data, such as: a) Partition method: The partition method is an iterative relocation algorithm that reduce the clustering criteria by reallocating the data points between clusters until convergence occurred. One of the common algorithms under this method is K- Means [4]. b) Hierarchical method: Hierarchical clustering is an algorithm that builds a hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left. The results of hierarchical clustering can be shown using a dendrogram. The issue with this method is the emerging and splitting of clusters are complex and errors generated cannot be revised. Moreover, this method is difficult to handle complex datasets. c) Density-based method: Density-based Clustering method is based on determining regions where points are concentrated and those regions where the data points are separated by vacant or sparse regions [5]. Points that do not belong to any cluster are assigned as noise/outliers. One of the disadvantages of density-based clustering is it unable to handle high-dimensional datasets. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is representational of the density-based clustering method.  
  F. K. Ahmad et al.  
  3.2 Phase II Data Pre-processing Data pre-processing is a crucial step in text mining, where the raw data is prepared for subsequent processes. In general, there are some common techniques used in data preprocessing such as (1) tokenization, (2) stop word removal, (3) stemming and lemmatization, (4) lowercasing, removing special characters and punctuation and (5) spell checking and correction. Some of these techniques can be applied to the raw dataset depending on its need and the requirement of the task at hand. At the stage of data pre-processing, several techniques have been applied and some parameters are adjusted to obtain optimal results. In this study, the TextBlob library is used to tokenize the words into features. TextBlob is a Python library for processing textual data. 3.3 Phase III Feature Extraction Once the data is cleaned, the next phase is text feature extraction. Text feature extraction is the process to transform the pre-processed data into vectorization space which can be used as input for the machine learning model. The main aim of this phase is to reduce the high dimensionality features to more efficient and meaningful vector space. Generally, there are three common feature extraction techniques as explained below: 1. Bag of words: Bags of words is a representation of text according to its occurrence in the document. This technique counts the words and represents them based on their frequencies in a given document or corpus. The matrix created represents unique features however, this technique may lead to a spare matrix that often hinders the downstream process. 2. Term frequency-inverse document frequency (TF-IDF): TF-IDF is a technique based on a weighting mechanism that calculates the importance of words in a document or corpus. This technique measures the frequency of words and inverse document frequency of words across the documents [6]. 3. Word embedding: Word embedding is a technique with a dense vector representation that captures the semantic relationships between words. Word embedding is usually trained with a large corpus and produces a small number of feature dimensionality in comparison to a bag-of-words or TF-IDF. In this study, the TF-IDF technique is used to extract feature vectors. Two factors are measured, which are frequency (TF) and inverse document frequency (IDF). TF measures how many times the term appears in the document while the IDF indicates the scarcity of words across the documents. Formulas to calculate TF is given in Eq. (1) while Eq. (2) shows the IDF formula. (1)  
  tf i,j= ni,j  
  Fig. 2. Feature space generated using the TF-IDF  
  This paper aims to automate the discovery of the number of clusters and their respective centroids using BBA. The following section describes the K-Means clustering and BBA proposed in this study. Binary Bat Algorithm (BBA). BBA is a meta-heuristic technique that is inspired by the behaviors of bats. This technique used the echo property of bats as a medium of communication in determining its prey [7–9]. Generally, BBA involves four phases as given below:  
  Description  
  Step 3 Recalculation: Recalculate the mean of all data points to assure all data points are assigned to the respective centroid Step 4 Repetition of Step 2 and Step 3 until convergence occurred: Reallocating data points to their nearest centroid and recalculate the centroids until no data point changes its assigned cluster Step 5 The final k centroids and the cluster assignments for each data point are determined  
  The k-means algorithm intends to reduce the within-cluster sum of squares, by measuring the distance between the data points to their assigned centroids. Data points that are close to assigned centroids will be grouped in the same cluster. In this study, we have used the elbow method or silhouette in determining the number of clusters, k.  
  Abstract. Arabic people use Arabic dialects on social media platforms to express their opinions and connect. Due to the absence of standard rules or grammar, Arabic dialects are more challenging for NLP tools to analyze than standard Arabic. While most review studies in this field have focused on highly indexed databases such as Scopus, Web of Science, and IEEE, these databases are not accessible to many Arabic researchers in Arabic countries due to financial constraints. This review study explores recent research and studies published in different databases to address this gap. The study identifies the most common sentiment analysis approaches, preprocessing and feature extraction techniques, and classification and evaluation techniques used in this field. The authors found that Twitter is the most commonly utilized source for researchers to collect their datasets, and machine learning approaches are the most commonly used for sentiment analysis in Arabic dialects. Overall, this study provides valuable insights into the challenges and opportunities for sentiment analysis in Arabic dialects. Keywords: Sentiment Analysis · Arabic Language · Arabic Dialects · Machine Learning · Classification  
  A. Habberrih and M. A. Abuzaraida  
  and education, whereas DA is an informal version of Arabic [2]. DA is used in daily life for communication. DA is different from country to country and even from city to city. However, there are six popular types of dialectal Arabic, namely Maghrebi, spoken in northern Africa, Khaliji spoken in the Arab Gulf area, Shami spoken in Jordan, Lebanon, Palestine, and Syria, Egyptian spoken in Egypt, Sudanese, and Iraqi spoken in Sudan and Iraq [4]. The Arabic language poses several challenges in SA, are addressed in [6, 11] and a study that focused on Saudi dialects, as described in [7, 10, 20]. One of the key complexities in dealing with dialectical Arabic is the absence of standard rules or grammar. Regarding sentence structure and morphology, Arabic sentences can start with a noun phrase, verb, or nominal phrase. Moreover, Arabic exhibits numerous syntactic variations within all sentence types. Additionally, dialect natives may express their opinions using different dialects and slang words and abbreviations. Furthermore, repetition of letters may be used to show emotion and emphasis, leading to spelling errors. Many Arabic individuals use DA to convey their opinions on social media platforms like Twitter and Facebook. Consequently, many researchers have focused on studying DA rather than MSA [8, 20]. The main objective of this study is to examine the most notable research conducted on DA and emphasize the available datasets, common preprocessing techniques, and Machine Learning (ML) approaches commonly used to classify DA sentiment.  
  2 Methodology This study conducts a comprehensive survey for the previous studies in Arabic dialect sentiment analysis. Here, the techniques used by the past studies in each phase of processing the Arabic sentences are highlighted. These phases are listed: Arabic dialect datasets, preprocess, feature extraction, and classification techniques. The search strategy of this study is explained in Sect. 2.1. 2.1 Search Strategy Many recent studies have been carried out to present a systematic review in this field, as evidenced by the works of [2, 4, 7, 9]. However, these studies have exclusively employed highly indexed databases such as Scopus, Web of Science, and IEEE. Notably, publishing in these indexed databases is not preferred for many Arabic researchers in Arabic countries, mainly due to financial constraints. The primary reason for this reluctance is the lack of financial support, discouraging researchers from investing heavily in publication costs. Consequently, many researchers publish their work in local or non-indexed journals, even if their papers represent high-quality research. Against this backdrop, this study aims to conduct a comprehensive survey to review as many studies as possible instead of solely focusing on limited databases. The scope of this study is limited to cover the ML and Hybrid approaches of SA.  
  Sentiment Analysis of Arabic Dialects: A Review Study  
  A. Habberrih and M. A. Abuzaraida  
  After the preprocessing process, the next step is feature extraction, which involves identifying the most effective features for the sentiment analysis process and removing irrelevant, redundant, and noisy data. This step reduces the dimensionality of the feature space and the processing time, ultimately improving the efficiency and effectiveness of the analysis [17]. Feature frequency (FF), Term Frequency-Inverse Document Frequency (TF-IDF), feature presence (FP), Word Embedding (WE), Part of Speech (POS), and N-grams methods are the most commonly used methods for the features extraction phase [16]. 2.4 Classification Techniques The process of SA can be achieved through three primary approaches: Lexicon-Based (LB), Machine Learning (ML), and Hybrid approaches. The Lexicon-Based approach can be divided into two techniques: Corpus-based and Dictionary-based. Lexicon-Based techniques analyze a sentiment lexicon, a compilation of words with corresponding positive, negative, or neutral polarity labels [20]. However, this approach is excluded from this study due to its structure which is completely different from the ML and Hybrid approaches. The ML approach involves the training of models on pre-labeled data, and it is categorized into three approaches: Supervised, Unsupervised, and Semi-supervised learning. The Supervised learning technique, also known as Classification or Regression, requires two subsets of data: a training set of labeled data and a testing set. The accuracy of this technique is dependent on the training set and the algorithm used. Unsupervised learning, also known as Clustering, is used for unlabeled data and aims to create clusters of data points, with similar points grouped in the same cluster and dissimilar points in different clusters. Semi-supervised techniques combine the advantages of both Supervised and Unsupervised techniques. The third approach used in SA is the Hybrid approach, which combines the Lexicon-Based and Machine Learning approaches [21]. Furthermore, a comparative study of some of the works invested in Arabic dialect sentiment analysis is presented in Table 1. Note that N/A means Not Available; DC: Data Cleaning; SWR: Stop-words removal; W2V: Word2Vec; NER: Named Entity Recognition; E: Evaluation; BR: Best Results; SR: Some Results.  
  4 Conclusion and Future Work Sentiment analysis is a significant application of NLP due to the rich source of information provided by textual data. However, developing NLP models and tools for Arabic dialects is challenging, given the limited written resources for many of these dialects. Unlike MSA, which has a rich corpus of written resources such as news articles, books, and academic papers, Arabic dialects often lack standard written forms and are primarily used in spoken communication. This study has investigated the studies that have been recently published and revealed that most researchers collect their datasets from Twitter, Facebook, and YouTube. Furthermore, studies were predominantly applied to dialects spoken in Saudi Arabia, Tunisia, Iraq, Algeria, and Sudan, with fewer studies applied to Libyan, Syrian, and Yemeni dialects. Machine learning approaches were the most utilized in sentiment analysis of Arabic dialects, with the best results obtained using this approach—however, most papers employed Arabic NLP tools rather than building specific tools for the studied dialect. Future work will aim to build a sentiment analysis model and compare different machine learning classifiers for the Libyan dialect.  
  Total Publications (TP)  
  Conference Paper  
  Percentage (%)  
  Citations per Paper  
  12.08  
  Figure 3 depicts a visualization map that demonstrates the connections among author keywords, citations by documents, and bibliographic coupling by authors. The varying colors, font sizes, and thickness of the connecting lines signify the strength of the relationships between the keywords. Keywords that share the same color are frequently listed together, indicating their close association and tendency to co-occur. For example, the diagram shows that E-Participation, Social Media, Social Networking (online), and Electronic Participation are highly related. Additionally, after excluding the core keyword (E-Participation) specified in the search query, the keywords with the highest occurrences are “Government Data Processing”, “E-government”, “Decision-Making”, “Public Policy”, and “Electronic Participation”. This figure is helpful for researchers to visualize the relationships between different keywords in the field of E-Participation research and to identify potential research topics or themes.  
  H. Awang et al.  
  Fig. 3. Network visualization map of the author keywords.  
  S. O. Haroon-Sulyman et al.  
  2 Review of Related Literature 2.1 Algorithmic Features In recent times, researchers have examined the simultaneous use of models such as Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) [3], Convolutional Neural Network (CNN) with Bidirectional Long Short-term Memory (BiLSTM) [10], Long Short-term Memory (LSTM) with Bidirectional Long Short-term Memory (BiLSTM) [11, 12] to mitigate text data vanishing gradient issues. CNNs commonly used in image data contain spatial features that, when combined with LSTM, are effective in handling sequential and temporal text data; this sometimes results in a potential model for the vanishing gradient. CNNs can extract local text features, while LSTMs capture contextual features and long-term dependencies. However, as much as this approach has shown promising results in research, its limitations still lie in unsuitability for large-scale datasets as it requires time and multiple network layers, leading to model complexity and lack of interpretability. [3] used CNN and LSTM for sentence classification, which is an NLP approach. The study observed that even though the model could learn longterm dependencies, their experiments show that the model performance is affected by dataset size, classifiers, and gradients vanishing. A similar study [10] used CNN and BiLSTM deep neural networks for a text sequence classification task. The model was able to extract semantics, which is an essential feature in long text sequences, though the delay was encountered in the processing time due to the sequential process in the BiLSTM architecture. Similarly, the sentiment analysis task was performed using deep networks. Authors observed that the proposed framework is prone to limitations in cases of imbalanced or text bias in capturing diversity [11]. To improve performance, especially in the case of imbalanced datasets, some studies used multiple embedding techniques in addition to the network layers. [13] proposed a model that combines CNN and LSTM with word2vec and GloVe word embedding representations for sentence-level classification. Their model was able to capture different various representations of word embedding in the input layer. The results showed that it outperformed the models with a single embedding technique though it is timeconsuming and may not be efficient for large datasets. Also, [14] proposed a model on CNN-LSTM with an attention mechanism, where the attention mechanism is useful to focus and capture relevant information. However, its use is dedicated to the specific study, which may not be suitable in all cases. Other algorithmic features that have been commonly used in research to mitigate the vanishing gradient problem is the combination of activation functions [15–24] such as Rectified Linear Unit (ReLU), sigmoid, tanh, and SoftMax function, which has been used to improve the deep learning training by involving them in one or more of the network layers. A commonly used activation function is ReLU which has been used in various NLP tasks [15–18, 25]. Studies show that its major challenge is the dying ReLU which causes neurons to stop learning and eventually die. 2.2 Regularization Technique This has been widely used as a means of mitigating the vanishing gradient challenge with techniques such as dropout [13, 26, 27], early stopping [13, 15, 28, 29], batch  
  Systematic Literature Review and Bibliometric Analysis  
  Fig. 1. PRISMA flow diagram  
  Step 2 VOSviewer is a valuable tool for researchers in bibliometric analysis. It helps visualize and understand complex networks from scholarly literature and identify research trends and gaps, thereby making informed decisions regarding collaboration, resource allocation, and future research directions surrounding the DNN and vanishing gradient issue. Combining these analytical approaches aims to ensure quality information can be reproduced from the study. Furthermore, the Scopus database was chosen to gather relevant bibliographical data due to its functionality in generating a wide range of results.  
  Systematic Literature Review and Bibliometric Analysis  
  Microsoft Excel was also used to collect data in structured form for a rigorous review of literature related to “vanishing gradients” and “text data.” This has enabled easy identification of patterns from the existing studies and suggests new ideas to overcome the challenges for further research. 3.2 Data Analysis Process This is important to achieve the aim of this study and guide further research. The data analysis includes literature selection, information extraction, information synthesis, and bibliometric analysis. Table 1 further shows the top 10 most cited documents related to DNN and their various NLP applications. It is observed that the most common NLP applications focus on short-text sentence tasks such as sentiment analysis and text classification. It also provided insights into the various techniques for handling the vanishing gradient problem. As a result, the information provided in the table should be valuable to researchers in this domain and serve as a reference point for future work. Table 1. Top 10 highly cited documents No  
  Author/Title  
  Task performed  
  No  
  Author/Title  
  4  
  Table 1. (continued) No  
  Author/Title  
  Task performed  
  Figures 2, 3, 4 give insights into exploring the selected text data. The next section will discuss observations and trends obtained from this data. Cluster 1 centers on text data, classification, and sentiment analysis. It provides potential insights into methodologies for analyzing and classifying textual data and provides insights into exploring the performance evaluation, methodologies for exploring transfer learning, and experimental outcomes analyses within the dataset. Cluster 2 This cluster highlights deep learning, recurrent neural networks RNNs, and NLP. It offers insights into the practical application of deep learning models, RNN architectures, and NLP techniques relevant to the dataset.  
  4 Findings The clusters and their associated terms provide an overview of the various approaches and techniques employed in research to address the vanishing gradient issue in text data. It is, however, important to note that this review paper serves as only a starting point in the analysis of this challenge. There is a need for further research in terms of other domain–specific contextual analysis for a more explicit solution to the vanishing gradient issue in text data.  
  Systematic Literature Review and Bibliometric Analysis  
  2 Offensive Words in the Malay Language English language datasets are the focus of most current research compared to other languages [11]. English, Spanish and French is recognized as high-resource languages [12]. In contrast, Malay is one of the low-resource languages [14]. High-resource language refers to numerous datasets that are currently available, and low-resource language refers to limited datasets [13]. Malay is spoken in Malaysia, Indonesia, Singapore, and Brunei. Despite their shared basis and incredible similarities, Malay dialects vary in each country [14]. Due to its low-resource nature, there are very few datasets related to cyberbullying in the Malay language [15]. Most recent studies on cyberbullying detection utilize Twitter datasets in the Indonesian language [2, 16–18].  
  N. Ismail et al.  
  Due to the close similarity of the Malay language used in Malaysia and Indonesia, this research adopted the guideline of determining offensive words from the research conducted by Ibrohim and Budi [18]. Offensive words are usually associated with conditions, animals, astral beings, objects, parts of a body, family members, activities, or professions. Table 1 explains further about the categories of offensive words referenced in the Indonesian language, which are also relevant to the Malay language in Malaysia. Table 1. Categories of offensive words in Indonesian and Malay languages. Category  
  Conditions  
  4 Conclusions This paper presented a new test collection for cyberbullying and language in the Malaysian language. Additionally, this paper outlined the methodology to build a test collection that includes a series of offensive language written by Instagram users. A series of baseline experiments were performed to evaluate the model performance and provided a report.  
  A Test Dataset of Offensive Malay Language  
  This paper also discussed the types of offensive language identified in the Malay language used in Malaysia. Due to the limited number of cyberbullying datasets for the Malaysian language, this new collection can be used as a reference to foster research related to cyberbullying in Malaysia. Future works aims to extend this research by exploring cyberbullying behaviour in the bystanders’ comments and building a corpus specifically for Instagram in the Malay language. This research would be beneficial for facilitating cyberbullying research in Malaysia by providing a complete corpus in the Malay language. Furthermore, it is also important in supporting society’s awareness of their roles in the effort to curb cyberbullying as well as for the authorities to witness the cyberbullying incidents in Malaysia.  
  2 Related Work A topic modelling experiment based on user comments on social media is shown in the study [3]. The author conducted an experiment using two datasets from Yahoo and Tokyo Electric Power Company (TEPCO), which covered the most popular news stories and video streaming comments, respectively. LDA was used to implement topic clustering based on topic modelling. This experiment received 15,000 comments throughout the same period. The results of the modelling are displayed in Fig. 1.  
  Fig. 1. Modelling’s outcome  
  In the [4] study, Twitter social media was searched for information about the Covid19 epidemic from March 3rd to March 31st. This author copied ten thousand tweets. The outcomes of clustering based on latent semantic analysis produced more clusters than clustering based on LDA. Because the largest cluster would show the day of trend, it was used as a comparison. The total number of confirmed cases in each nation and worldwide are shown in Fig. 2 [4].  
  Fig. 2. Number of confirmed cases  
  This study [5] focuses on the two-step problem of extracting semantically relevant themes and trend analysis of these subjects from a large temporal text corpus utilising an end-to-end unsupervised technique. The author first created word clouds based on the frequency of terms in each cluster of abstract text. As a result, terms that were less prevalent and significant to the cluster were deleted from word clouds. The author generated word clouds based on the TF-IDF scores of phrases belonging to a cluster. The TF-IDF based on a word cloud of four distinct clusters is displayed in Fig. 3 [5].  
  Fig. 3. TF-IDF based on a word cloud of 4 different clusters  
  Fig. 4. Process of LDA  
  In [7], a new spammer classification method is based on the LDA topic model. This technique captures the spamming essence by retrieving global and local data regarding topic distribution patterns. Clustering based on online spam detection is proposed to discover spammers that appear to be posting legal tweets but are difficult to identify using existing spammer categorization methods. The examination of the K-means technique produces an accurate result, and it also identifies spammers on social media. A past study from [8] explained the process of document clustering by using Kmeans and K-medoids algorithm (see Table 1.). This study focuses on hundreds of documents collected from Entertainment, Literature, Sports, Political, and Zoology. The authors implement the K-means algorithm on the WEKA tool and K-medoids on the Java platform. Both results of each algorithm are compared to get the best cluster. Based on Table 1, each cluster defines documents of a particular domain topic. According to the result, the authors conclude that the K-means algorithm is more efficient than the clusters obtained from the K-medoids algorithm [8]. The following word clustering experiment, carried out by [9], focuses on grouping Chinese words using LDA and K-means in accordance with five categories: politics, economics, culture, people’s livelihood, and science and technology. The Latent Dirichlet Allocation (LDA) algorithm and the k-means clustering algorithm are combined in a novel approach that is put forth in this work. The highest probability of each topic is picked as the centroids of k-means after some topics are retrieved using LDA. In the final stage, the K-means algorithm is employed to group every word in the text [9]. The authors calculate the K-means centroids using the LDA algorithm findings and utilise the Chinese word similarity calculation method to calculate the distance between the  
  TikTok Video Cluster Analysis Based on Trending Topic  
  Abstract. We live in an information world where visual data undergo exponential growth within a very short time window. With diverging content diversity, we simply have no capacity to keep track of those data. While short video platforms (such as TikTok™ or YouTube Shorts™) can helped users viewing relevant videos within the shortest time possible, those videos might have misleading information, primarily if it is derived from long videos. Here, we analyzed several short videos (in terms of movie trailers) from YouTube and established a correlation between one movie trailer and the classified movie genre based on the emotion found in the trailer. This paper contributes to (1) an efficient framework to process the movie trailer and (2) a correlation analysis between the movie trailer and movie genre. We found that every movie genre can be represented by two unique emotions. Keywords: movie analytic · face detection · emotion recognition · video summarization  
  2 Related Works The use of electronic nose machine learning for identifying the various characteristics of foods or drinks was covered in a number of literature studies. Conventional techniques like high performance liquid chromatography (HPLC), microbiological cell counting, mass spectrometry, and gas and liquid chromatography are cumbersome because they require time-consuming and laborious sample processing [7]. To run these conventional methods, skilled personnel were also required. Therefore, e-nose technology has been used in the food industry due to its ease, cost effectiveness, and close connection to sensory panels. In a number of areas of food safety assessment, electronic noses have recently become known as potential tools for quick, early detection of contamination and defects in the food production chain. By identifying its distinctive components and studying its chemical components, an e-nose may recognize an odour [8, 9]. The electronic nose (E-nose) is a group of electronic gas sensors that can detect volatile compounds in the headspaces of food product samples with high sensitivity and selectivity [10]. It resembles the human sense of smell to some extent [10–12]. Figure 1 shows the correlation of human nose with electronic nose. According to Fig. 1 above, gas sensors and sensing materials were used in place of the odour receptor cells to mimic a biological olfactory system. A computational algorithm, an artificial neural network, and data analysis software are used in place  
  E-Nose: Spoiled Food Detection Embedded Device Using Machine Learning  
  Fig. 2. Food Samples in Fresh Condition: (a) Sup Daging, (b) Nasi Goreng Daging, (c) Ayam Masak Kunyit, (d) Ayam Masak Lemak, (e) Ayam Goreng Berempah and (f) Sawi Masak Air.  
  3.2 Experimental Setup The food’s emissions of gases can be detected using four (4) gas sensors. The data from the sensor to the computer in this study is processed and converted using an Arduino microcontroller and a data collection programme called PLX-DAQ. The list of sensors utilized in this investigation was provided in Table 1 below, along with the types of gases the sensors detected. 3.3 Data Analysis Methods The present study involved the utilization of Support Vector Machines (SVM) and kNearest Neighbors (k-NN) in the analysis of data gathered from gas sensors, as well as the classification of the degree of food contamination. The datasets utilized in this study were characterized by an imbalance in the amounts of dependent variables, which may result in inaccurate classification algorithm performance due to unevenly distributed class label characteristics. To address this issue, oversampling and undersampling techniques were  
  E-Nose: Spoiled Food Detection Embedded Device Using Machine Learning  
  Based on the summarized outcomes presented in Table 2, it is apparent that the level of accuracy in food classification varies between different food samples. This difference is due to the various ways of cooking the food samples such as frying, cooking proteins or vegetables in soups and cooking proteins or vegetables using coconut milk. These different cooking methods can also influence the spoilage rate of the food samples. Hence, it can be inferred that the accuracy of classification outcomes is impacted by the techniques employed for preparing the food samples.  
  Education University, Amman, Jordan [email protected]  2 InterNetWorks Research Lab, School of Computing, Universiti Utara Malaysia, Kedah, Malaysia 3 Data Management and Software Solution Research Lab, School of Computing, Universiti Utara Malaysia, Kedah, Malaysia  
  Abstract. The routing protocol known as RPL is employed in low power and lossy networks. It makes use of an objective function (OF) to establish a Destination Oriented Directed Acyclic Graph (DODAG) and ascertain the most suitable parental candidate or trip route. Nevertheless, the task of identifying a suitable OF in Low Power and Lossy Networks (LLN) is a significant challenge. The RPL was intentionally designed to possess a high degree of flexibility, allowing for the construction of routing topologies without imposing any specific routing metric or constraint. This design choice was made to accommodate the diverse range of LLN that exist. This study provides a critical overview of recent literature pertaining to the topic of RPL and specifically focuses on the many strategies aimed at enhancing OF inside the RPL protocol. The objective of this study is to provide an analysis of relevant endeavors, including the development of innovative metrics and the application of fuzzy logic techniques in the combination of OF metrics. Furthermore, this paper discusses the recommended augmentation strategies, as well as constraints and future development directions. The research community can employ the findings to gain a deeper comprehension of objective functions and improve the performance of RPL in the face of security problems. Keywords: RPL · OF enhancement · IoT · RPL performance · Fuzzy Logic  
  L. Al-Qaisi et al.  
  devices utilized on IoT [2]. Hence, an efficient routing protocol in this setting is both a requirement and a difficult problem to study [3]. A new IPv6 protocol, termed Routing Protocol for Low Power and Lossy Networks (RPL), has been suggested by the IETF ROLL working group to address the needs of low-power and lossy networks (LLN) [4]. As a promising protocol, RPL offers various benefits for tiny devices, that it can adapt to and manage shifts in network architecture and implementation, which is one of its primary strengths. It is also employed to address issues unique to LLN, such as traffic congestion [5], imbalanced consumed energy [6], and load balancing [7]. Yet, choosing the best route to the destination still represents a significant challenge for RPL [8]. Consequently, numerous research studies have addressed this concern and recommended various enhancements for the best parent/best path [9]. For advanced IoT applications to succeed, it’s essential to select a path between sensors that is both fast and has minimal data loss [10]. The routing quality can be enhanced by employing a suitable objective function (OF). Yet, several scenarios, including network scalability, mobility [11], security [12, 13], and topology changes, might pose challenges for applications built on LLNs. More packet loss, shorter network life, higher overhead, and higher energy consumption are all possible outcomes of certain worst-case scenarios [10–13]. This article will focus on OF enhancements. The OF in routing protocols finds the optimum path to a destination. A good route meets power consumption, network durability, convergence speed, and connection quality parameters like ETX and PDR. The rapid development of OF attracts LLN researchers. This paper discusses the most important efforts to assess and enhance the objective function, prompted by the lack of earlier RPL objective function surveys. This research critically evaluates OF modification methods for RPL routing service improvement. RPL should be explained thoroughly. Discussing OF modification approaches. Then fuzzy logic is discussed. The rest of the paper is organized: Prelims are explained in Sect. 2. The paper’s methodology is in Sect. 3. Modification procedures are in Sect. 4. The results are thoroughly discussed in Sect. 5. Section 6 discusses directions and opportunities.  
  2 The Objective Function in RPL It’s one of the most important parts of RPL because it helps build the topology using the Directed Acyclic Graphs (DAG) concept and the distance vector technique to create a tree-like structure, or Destination Oriented Directed Acyclic Graph (DODAG), which regulates links between reachable nodes [14]. DODAG root and RPL routers exchange data between source and destination nodes [15]. RPL’s DODAG construction process depends heavily on the networks OF. Route selection also affects network efficiency. The metrics and limitations used to establish the ideal path from a node to the root can improve or damage network performance [16]. 2.1 Standard Objective Functions (oF) RPL defines two standards OF, Objective Function zero (OF0) and Minimum Rank with Hysteresis Objective Function (MRHOF).  
  L. Al-Qaisi et al.  
  network-condition-based routing flexibility. Metrics give RPL extensions a complete network picture and enable intelligent routing decisions that improve energy efficiency, dependability, latency, and service quality. RPL routes data in resource constrained and lossy IoT environments using many variables. 2.2 Routing Metrics RPL was ratified by the IETF ROLL working group as an IPv6 routing protocol to meet LLN lossy link and limited node requirements. RPL supports point-to-point, multipointto-point, and point-to-multipoint topologies [9]. RPL updates routing topology and information using four ICMPv6 control packets. DODAG Information Object (DIO) preserves node rating and root distance before finding best parent. Second, send DAO-containing up-ward traffic to parents. Third, joinable nodes receive a DODAG Information Solicitation for DIO messages. Finally, the DAO receiver confirms receipt with a DAO-ACK message [13]. LLN have distinct behavioral characteristics compared to wired and ad hoc networks. The most notable is that RPL is being used as the major routing protocol in these networks. Because of the OF, this protocol offers tremendous freedom in choosing routing metrics [25]. Routing metrics ensure path cost evaluation and the least restrictive path selection. Certain RPL implementations require multiple routing metrics and limitations, whereas others require only one [26]. Routing metrics can be static or dynamic, focus on the link or the node, emphasize quality or quantity, etc. However, routing metrics and constraints are different. The routing protocol may consider Both of these factors when determining the best route to take. In order to avoid potentially problematic links, a routing protocol may take advantage of a routing constraint. A routing metric selects its path according to the links that guarantee a certain level of reliability. The requirements for RPL implementation will determine which metrics or constraints will be imposed. The routing metrics also need to consider the network’s dynamic nature. LLN networks’ link or node metrics are dynamic and subject to change as the network functions. Here, we’ll look at the residual energy as a node metric for choosing a route. The network nodes’ inability to maintain their energy reserves gradually decreases their remaining power. Thus, the path calculation using this metric shift as the measure itself shifts and evolves. Both node and link metrics are explained as per [27, 28] and [29]. 1. Node Metrics: Hope Count: a common wireless network routing metric It is deployed for measuring network path length. Energy: reports network node power consumption. Location and distance from the sink may cause some nodes to lose energy faster than others. 2. Link Metrics: Throughput: amount of data to be exchanged between nodes over the network in a certain timeframe. More throughput means better performance. Latency: time to transport data across the network. The latency of a network is measured in milliseconds, and lower latency indicates a quicker reaction time.  
  An Analysis of Objective Function Modification Approaches  
  Expected Transmission Count (ETX): checks network reliability. It shows how many transmissions the destination needs to confirm data receipt. The root is best reached via the lowest ETX path. ETX’s high value shows the network’s instability. RSSI/LQI: The physical layer may precisely set a network’s signal, frequency, voltage, etc. RSSI and LQI are the most common radio link estimators. RSSI checks received frequency signals as a radio transceiver. Thus, a greater RSSI indicates a stronger radio signal and closer destination. LQI rates link reliability from 0 to 7.  
  2.2 Related Work The research by Shamantha, Shetty, and Rai [15] focuses on conducting sentiment analysis of tweets and reviews shared on Twitter. Its objective is to categorize opinions expressed in the text as positive, negative, or neutral. ML classifiers, including Naïve Bayes, Random Forest, and Support Vector Machine (SVM), are employed to assess sentiments using specific keywords. The theoretical framework of this work is rooted in sentiment analysis, a subset of natural language processing (NLP). The methodology blends techniques from NLP and ML to carry out sentiment analysis, involving the identification of particular keywords in tweets and reviews to discern expressed sentiment. Classifier performance is measured in terms of accuracy, precision, and processing time. Additionally, feature selection is applied to pinpoint the most relevant aspects for sentiment analysis. The results indicate that the NB Classifier outperforms the other two classifiers in both accuracy and speed. Moreover, concerning sentiment model performance, the study suggests that employing binary labels for sentiment analysis of tweets or reviews can be a beneficial approach for gauging overall sentiment as it simplifies classification and reduces complexity. Similarly, with binary labels, text sentiment can be categorized as either positive or negative, facilitating result interpretation. Additionally, binary labels prove valuable when text sentiment isn’t distinctly positive or negative, enabling a more nuanced sentiment analysis. As well, Wang and Wang [16] emphasize the need to safeguard plant biodiversity, which necessitates the identification of plant species. However, traditional plant species identification is challenging for the general public and even experts. In response, the study presents a few-shot learning method for leaf classification with a small sample size based on the Siamese network framework. The neural network architecture in this study consists of two identical subnetworks that share weights and distinguish between similar and dissimilar inputs. To extract features from two distinct images, a Siamese network structure is used, involving a parallel two-way Convolutional Neural Network (CNN). The learned metric space is then used for leaf classification with a K-Nearest Neighbor (KNN) classifier. The loss function used to generate this metric space aims to place analogous leaf samples close together and different leaf samples far apart. Additionally, the study proposes a Spatial Structure Optimizer (SSO) method to enhance leaf classification accuracy. The proposed method is evaluated on three datasets (Flavia, Swedish, and Leafsnap). Despite the limited size of supervised samples, it achieves high classification accuracy. The proposed method’s effectiveness is evaluated using the average classification accuracy as a performance metric and it achieves high accuracy with a small size of supervised samples. The use of the SSO method further enhances the accuracy of leaf classification. However, the study does not provide a comparison of the proposed method with other state-of-the-art methods for leaf classification, which may limit its applicability. Despite this limitation, the proposed method shows promise as a solution for leaf classification with a small sample size. The use of binary labels in the Siamese network suggests a recommendation for future studies.  
  Abstract. Mooring (Thin) lines are fabricated of polyester ropes, steel wire ropes, and chains. These are considered the essential components which are used to secure offshore marine vessels and floating facilities by keeping them in a fixed place and resisting external loads. However, the failure of any mooring lines because of anomalies can cause severe consequences including financial losses, loss of life, and harm to the environment. Thus, it is essential to determine the anomalies in mooring lines beforehand to ascertain reliable and safe offshore mooring operations. This paper furnishes a comprehensive review of various types of anomalies in mooring lines with their underlying causes, and risk mitigation tactics. Furthermore, the types of mooring lines including polyester ropes, chain, and steel wire ropes have been discussed with their advantages and disadvantages. Additionally, the real-time consequences of failure in mooring lines are explored which occur due to the anomalies in the mooring lines including but not limited to environmental damage, vessel drift, and collision. In order to reduce the risks associated with mooring line anomalies, this review concludes by summarizing the major findings and emphasizing the significance of proactive monitoring and maintenance. Keywords: Anomalies in Mooring Lines · Mooring Systems · Mooring (Thin) Line Failure  
  1 Introduction Mooring systems consist of mooring (Thin) lines that are used to keep floating structures and the offshore vessel stationary in deep water during the unloading of the hydrocarbon production. These offshore vessels shape like ships or boats. The mooring lines are disseminated into polyester or fiber ropes, chain, and steel wire ropes, and such types are utilized to fix a floating vessel in one place by connecting the mooring lines to the vessel which is further anchored to the seafloor [1]. However, mooring systems are used to prevent the offshore floating structure from drifting and to keep the vessels fixed from being affected due to the external force that offshore waves, currents, and strong winds may cause. Mooring systems fall under numerous categories which are considered based on the length, floating structure type, offshore water depth, and external sea conditions. Besides, many factors are involved in the design of the mooring system such as seabed constitution, length of the mooring ropes, and the strength of the anchors and weight [2]. However, the maintenance and the proper installation of the mooring systems are critical for protecting the marine environment and keeping the vessel safe [3]. Mooring lines have great importance in the offshore marine environment and failure of any line in the mooring system due to anomalies in the mooring lines can cause severe consequences. The anomalies in the mooring lines induce a substantial risk to the safety of the floating facilities. These anomalies are posed by numerous factors that may include poor maintenance, broken wires or chains, corrosion, and loose connections [4]. These issues can lead to a loss of stability or position of the vessel or structure, increasing the risk of collisions or damage to equipment, and posing a threat to the safety of personnel onboard. Besides, if a mooring line fails due to the existence of an anomaly in the mooring systems, it can cause the vessel or offshore structure to drift, potentially colliding with other vessels, structures, or even shorelines, resulting in severe damage or loss of life [4, 5]. Additionally, failure in the mooring system can cause hydrocarbon spills, leading to environmental damage and financial losses. Furthermore, the failure of a single mooring line can result in increased tension on the remaining mooring lines, potentially causing them to fail as well, leading to a catastrophic situation. Therefore, it is essential to regularly inspect, monitor and maintain mooring lines and address anomalies promptly to prevent any potential safety or environmental risks caused by the failure of the mooring system [6]. Before monitoring and addressing the anomalies in mooring lines, it is crucial to identify the different types of anomalies in mooring lines with their causes and risk mitigation strategies [4]. However, no comprehensive review paper defines the various anomalies in different types of mooring lines except [1, 7], along with their underlying causes and prevention. Besides, no real-time consequences of failure in mooring lines have been discussed in the literature except for a few instances in [8]. Therefore, this paper comprehensively reviews anomalies in different types of mooring lines, including polyester ropes, chains, and steel wire ropes, along with their underlying causes and risk mitigation strategies, as part of mooring systems. Additionally, the paper explores the real-time consequences of mooring line failure due to anomalies in the offshore marine environment, including vessel drift, collision, and environmental damage.  
  T. K. Khatri et al.  
  Furthermore, these objectives have been accomplished by an extensive analysis of research articles, existing studies, publications, and reports associated with the various types of mooring lines and their anomalies, causes, risk tactics, and real-life consequences of line failure. However, the said comprehensive information has been gathered by searching prestigious databases, conference proceedings, academic journals, and industry reports which have been found available through online sources and other technical websites. To the best of our knowledge, this paper represents the first attempt to comprehensively review the different anomalies in various types of mooring lines with their underlying causes and risk mitigation strategies while also discussing their real-time reported consequences of failure. This review paper is structured into the following sections. Section 2 describes the anomalies in mooring systems, their underlying causes, and risk mitigation strategies. A comparative analysis is also depicted in this section based on the advantages and drawbacks of each type of mooring line. The real-time reported consequences of failure in mooring lines are demonstrated in Sect. 3 with some examples. Finally, the entire study is concluded in Sect. 4.  
  2 Anomalies in Mooring Lines Mooring (Thin) lines normally comprise polyester or fiber ropes, steel wire ropes, and the chain. Each form of these mooring lines is thoroughly discussed in the following subsections with respect to the different types of anomalies in various sorts of mooring lines, their underlying causes, and the risk prevention tactics. Properly implementing the risk mitigation tactics can prevent the mooring lines from failing and may also assure the marine vessels’ reliable and safe mooring operation. Furthermore, Table 1 presents a comparative analysis aimed at selecting the most suitable mooring lines solution from among all available types, based on their respective advantages, and disadvantages in the context of mooring systems. 2.1 Anomalies in Polyester Mooring Ropes Fiber ropes are utilized in mooring operations in a wide range because of their durability, ability to withstand abrasion, and great strength. Even so, these ropes may still fail in a case when not maintained in the right manner or if they are experienced with specific anomalies. The following subsections discuss typical anomalies by which the polyester ropes are caused to fail when the mooring operations are carried out. Cut or Abrasion Damage. Abrasion takes place when the polyester ropes are rubbed in contact with the seafloor, other fiber ropes, or hull. It induces the fibers in the ropes to undergo scratches and wear which may result in undermining the structure of the ropes and can be susceptible to failure or breakage in the mooring rope. Abrasion in ropes can be caused by various factors, including but not limited to the rough sea, strong current, and acute edges of the surface by which the rope is rubbed [9]. Furthermore, some examples of damage in the fiber ropes are shown in Fig. 1. To mitigate the risk of such failure, obviating contact with abrasive surfaces or rough seas is essential. Besides the ropes need to be cautiously inspected to identify the damage  
  Anomalies in Mooring (Thin) Lines  
  or wear signs. In case of abrasion is observed, it must be either replaced with new material or completely removed to assure the unity of the rope. Thorough storage of the rope aids in precluding abrasion by storing the rope on a reel or drum from being tangled and avoiding rubbing the rope in contact with other surfaces [10]. UV Degradation. The exposure of sunlight to the polyester rope causes the fibers of the mooring rope to collapse overtime because of ultraviolet (UV) radiation in the sunlight that breaks down the fibers of the mooring rope which result in deterioration of the rope strength and may be susceptible to fail [11]. To mitigate the risk of such type of failure, placing the mooring ropes away from sunlight is significant when these are not in function. Chemical Damage. The damage to polyester mooring ropes is done when the ropes are in contact with solvent and oil chemicals which causes the fibers of the ropes to become weak and increases the chances of the rope failing [12]. The risk of such failure can be mitigated by obviating the leakage and spilling of oil close to mooring ropes. Besides, the ropes should be stored in a dry and well-ventilated area when they are not functioning.  
  Fig. 1. Polyester rope cut or abrasion damages [13].  
  Knotting. The fibers in the polyester ropes are compressed and deformed when the rope is knotted. Over time, this anomaly weakens the rope and makes it more susceptible to failure [14]. The risk of such an anomaly can be mitigated by avoiding the knot that may create unneeded stress on the rope [15]. Manufacturing Defects. The structure of the rope can be weakened due to the manufacturing defects in the polyester ropes. These defects may contain wear spots, defects  
  T. K. Khatri et al.  
  in the construction of the rope, or incompatibility in the fibers [16]. The risk of these defects can be mitigated by utilizing good quality mooring ropes that are constructed and produced by good manufacturers [15, 16]. Heat Damage. Polyester mooring lines are caused by heat damage which fails the mooring rope. When a rope is subjected to extreme temperatures, heat damage happens. Hot surfaces induce this anomaly, the friction generated by the motion of the fiber rope through equipping and the exposure to flames [7, 17]. It is crucial to place the ropes away from the heat origins to mitigate the heat damage anomaly in the fiber mooring rope. Besides, the ropes must be avoided in close contact with the hot surfaces and flames. In addition, it is very important to inspect the rope regularly to determine the signs of heat damage which may include melting of the fibers. To reduce the risk of failure, the rope must be replaced immediately if heat dam-age is suspected [18].  
  2.2 Anomalies in Steel Wire Mooring Ropes Steel wire ropes are utilized in mooring operations in a wide range because of their durability and great strength. Even so, these ropes may still fail in a case when not maintained in the right manner or if they are experienced with specific anomalies. The following subsections discuss typical anomalies by which the steel wire ropes are caused to fail when the mooring operations are carried out. Corrosion Fatigue. Coronary fatigue happens when steel wire is brought out to a corrosive environment and undergoes cyclic loading during mooring operations. Multiple factors can spread the small cracks in the wires and lead to the wire rope failure [1, 7]. The risk of corrosion fatigue can be mitigated by regularly inspecting the steel wire ropes to get the sign of corrosion and obviate revealing the mooring lines to extravagant loads. Steel Corrosion Cracking. Steel corrosion cracking happens when the steel wire ropes are subjected to high tension. Steel corrosion cracking causes cracks in the steel wire ropes when the mooring lines are exposed to high loads for an extensive delay [19]. To mitigate the risk of steel corrosion cracking, it is significant to determine the corrosion signs by regular inspection and avoid disclosing the line under high tension [20]. Human Error. Humans can also be part of the failure of steel wire mooring ropes. Wrong storage and handling of the mooring lines, absence of regular inspection of the mooring lines, and improper training of the staff are the factors that may lead to problems that enhance the chances of failure in the mooring line [1]. Establishing accurate processes, inspection, and monitoring procedures is crucial to reduce the risk of human error. Furthermore, proper training and supervision are necessary for the staff who perform mooring operations [20]. Vibration. When the frequency of vibration coincides with the natural frequency of the steel wire then the steel wire is more likely to become worn out and break. Over time, the lines are caused to fail [20]. The risk of such anomaly can be mitigated by measuring the length and the load of the mooring lines properly and obviating the reveal of mooring lines to the high frequency of vibrations [21].  
  Anomalies in Mooring (Thin) Lines  
  Environmental Factors. Exposure to sunlight that causes to generate ultraviolet radiation may weaken the steel wire mooring ropes. Over time, it may enhance the chances of failure in steel wire rope [7, 20]. To prevent environmental degradation, inspecting the mooring lines for signs of damage regularly and storing the lines in a cool, dry, and protected environment when not in use [20]. Creep. Creep occurs when the wire is under a constant load for an extended period, which can cause slow, permanent deformation of the wire [21]. To prevent creep, it is important to properly size the mooring lines and avoid exposing them to excessive loads for extended periods [1]. Manufacturing Defects. Manufacturing defects can occur during the wire drawing process, heat treatment, or quality control procedures. These defects can cause weaknesses in the wire, leading to failure over time [7]. To prevent manufacturing defects, it is important to work with reputable manufacturers and inspect the mooring lines for any signs of defects before using them [20]. Overloading and Broken Wires. Overloading occurs when the mooring lines are subjected to loads that exceed their capacity. This can cause the wire to stretch, deform, or even break. Very few amounts of broken wires in a steel rope at termination show high tension. This may be because of inaccurate fixing of the steel rope at the endpoints, managed badly at the time of recovery and deployment, some fatigue, or the due to overloading [22]. The local damage is caused by broken wires which are grouped in a neighboring strand or one strand. This is considered the worst situation when finding such a breakage of steel wire rope and this constraint can disturb the load balance that is conveyed by steel rope strands as shown in Fig. 2. To prevent overloading, it is important to properly size the mooring lines for the vessel and operating conditions and to avoid exposing the lines to extreme weather conditions or other sources of excessive loading [22].  
  Fig. 2. Broken wires at wire rope endpoint [1].  
  2.3 Anomalies in Mooring Chains Mooring chains are commonly used in mooring operations due to their high strength and durability. However, these chains can still fail if they are not properly maintained or if they are subjected to certain anomalies. Some of the common anomalies that can cause mooring chains to fail during mooring operations include:  
  T. K. Khatri et al.  
  Corrosion. Corrosion is a chemical reaction between metal and its environments, such as saltwater or moisture. Corrosion can lead to the loss of metal mass, weakening the chain’s structural integrity, and eventually causing it to fail. Corrosion can be accelerated by factors such as high humidity, exposure to saltwater, and extreme temperatures. Regular inspection and maintenance of the mooring chains are essential to detect and address any corrosion promptly [23]. Chemical reactions between the material and the environment can cause rust and corrosion. Expanding marine life may also increase the need for new mooring lines to pre-vent failure [7]. The mooring chains are typically found with corrosion in the splash zone as demonstrated in Fig. 3 (a and b). It is very belligerent to have a corrosion rate greater than 1 mm per year by relying on the temperate of the seafloor and the quality [1].  
  Fig. 3. Mooring chain with dense corrosion [1].  
  Fatigue. Fatigue failure occurs when a material is subjected to repeated loading and unloading. This repeated stress can cause small cracks to develop in the material, which can eventually grow into larger cracks that weaken the chain and cause it to fail. The risk of fatigue failure can be reduced by ensuring that the mooring chains are designed and manufactured to withstand repeated stress cycles and that the loading is distributed evenly across all chain links [23]. Overloading. Overloading occurs when the mooring chain is subjected to loads that exceed its maximum capacity. This can cause the chain to deform, stretch, or even break. Overloading can occur due to factors such as high winds, waves, or improper mooring techniques. To mitigate the risk of such anomaly, mooring chains are properly managed for the expected tension concerning their design, installation, and size to function within their great capacity [24]. Wear and Tear. Another anomaly known as wear and tear contributes to failure in the mooring chain. This type of anomaly occurs due to the friction of the chain tied to the seabed, other chain links, or the mooring chain connected to the Catenary Anchor Leg Mooring (CALM) mooring buoy [1]. It causes the materials to wear down, which reduces the diameter of the chain, loss of the metal, and finally leads to failure in the mooring chain [7]. Such anomaly can be mitigated by properly classifying the chain loads and it can be done by regularly inspecting the mooring chain to identify the signs of wear and tear in the form of reduction in the chain diameter and distortion. Furthermore, the proper maintenance and lubrication of the chain can also help to reduce friction and wear and tear [23]. In addition, it is important to monitor the mooring environment and take appropriate measures to reduce the risk of wear and tear, such as using a protective covering over the chain in areas where it is likely to come into contact with other surfaces or reducing the intensity of loading on the chain by using multiple mooring points [24]. Abrasions. Abrasion can occur when the mooring chain is subjected to contact with other surfaces or when the chain is bent and flexed. Over time, abrasion can weaken the chain and make it more susceptible to failure [7, 25]. Besides, Sediments on the seafloor can be abrasive, and friction can erode the chain when it encounters the bottom of the ocean [1].  
  Fig. 5. Ground touching mooring chain with one side material loss [1].  
  The mooring chain can also fail due to ground touching region as shown in Fig. 5 and such causes are considered seafloor abrasion [1]. To prevent abrasion, it is important  
  T. K. Khatri et al.  
  to ensure that the mooring lines are properly sized, positioned, and secured to avoid contact with other surfaces [24]. Deformation. Deformation occurs when the mooring chain is subjected to excessive bending or torsional stress, causing it to deform or bend out of shape. This can weaken the chain and eventually lead to failure [22]. Deformation is caused by overloading, and environmental conditions (waves, strong wind, and current). It can also be due to not properly installing the mooring system during the operation [1, 7]. The deformation risk can be reduced by checking the proper installation of the mooring chains that will be utilized for bearing the high loads, which is limited to their design capacity. Besides, causes of deformation can also be addressed promptly by regularly monitoring the mooring chains to diagnose the deformation-affected factors and then perform the required maintenance to fix them [4]. Chain Links. Chain links anomaly concerned with the failure in mooring chains which is caused by wear and tear, overloading, wear and tear and most often it can be found due to manufacturing faults. A failure of a chain link causes the adjacent links of the chains to fail and finally result in the failure of the entire mooring chain [23]. These anomalies can be mitigated by monitoring the mooring chain regularly to confront the signs of the above-mentioned factors and fixing them appropriately through proper maintenance of the mooring chains where the faults actually exist. Additionally, assurance of the designed mooring chain, manufacturing materials, and knowing the installation base for the expected load to be borne by mooring chains can aid in minimizing the risk of failure of chain links and deformation and results in ensuring secured and reliable mooring operations [4]. Improper Handling. The failure of morning chains also happens due to improper installation and handling them inappropriately. Several examples lead to improper handling anomalies. These include distortion or weakening of the chain [23], letting the chain fall to the surface can induce cracks in the chains, and improper handling of the mooring chain throughout the storage, installation, and transportation. All of these contribute to damage in the chain and finally cause mooring chains to fail [24]. The risk of such anomaly can be reduced by following the appropriate procedures and thumb rules, including properly giving up the components, keeping the mooring chain secure, and obviating the high tension and load that causes the chains to bend over time [24]. Additionally, the mooring chains must be inspected and monitored before and during the commencement of the mooring operations to eliminate the damages through repairing or completely replacing the chains that may turn to fail the mooring chains after a long [4, 26].  
  Anomalies in Mooring (Thin) Lines  
  Can withstand high loads and tension  
  Can be prone to deformation and failure due to repeated bending and straightening  
  Resistant to UV radiation and chemical degradation  
  4 Conclusion Mooring (Thin) lines are considered essential to secure offshore marine vessels and floating facilities by keeping them in a fixed place and resisting external loads. However, the failure of any mooring line because of anomalies can cause severe consequences including financial losses, loss of life, and harm to the environment. This review paper has comprehensively reviewed various types of anomalies in mooring lines with their underlying causes and risk mitigation tactics. Besides, the real-time consequences of  
  Anomalies in Mooring (Thin) Lines  
  Malaysia [email protected]   
  Abstract. Community College adhered to the Ministry of Higher Education is well known for its lifelong learning prospects and distinctively offers a short course about the community surrounding the institute. To date, the primary focus in the academic and industrial realms is on descriptive and predictive analytics. Nevertheless, prescriptive analytics, which seeks to find the best course of action for the future, has increasingly garnered research interest. Meanwhile, the analysis will be used to implement actionable plans to help in decision-making that can benefit the institution as well as the officers concerned. This paper investigates the problem arising by using analytical methods in elevating short course enrolment in Seberang Jaya Community College. Upon completion with the usage of Market Basket Analysis (MBA) techniques integrating the descriptive and predictive analysis, results obtained are established thoroughly with specific details that were to attain cluster insights based on the participant’s interest that leads to non-mainstream courses related to the college credential-expertise program. Course modelling proposal for participants’ enrolment through MBA that leads to output produced for Lift Parameter, uses specific rules that have higher lift and confidence that participants tend to join Kursus Penyelenggaraan Komputer (consequents) when they joined Kursus Rangkaian Komputer (Antecedents). In looking at the association rules, it seems that both these courses are highly considered to be enrolled. Keywords: Community College · data analytics · lifelong learning · short courses  
  Fig. 1. The relationship between confidence at 0.9 and lift for Unsupervised rules  
  7 Conclusion This paper introduces association rule mining (ARM) and its application in developer turnover, with a focus on OSS projects like blockchain. It covers ARM’s key concepts and the Apriori algorithm for generating rules. Past ARM applications are mentioned, along with a counseling example. However, the paper stresses the need for caution in interpreting ARM’s discovered rules, as they serve as exploratory findings requiring validation by domain experts. Its primary objective is to demonstrate ARM’s value as a potent data analysis tool for researchers in various fields.  
  N. S. Mansor et al.  
  1 Introduction Geospatial technologies is a field of study in which multispectral refers to using multiple electromagnetic radiation wavelengths or spectral bands for analysis and mapping [1]. Satellite image classification is a technique that involves grouping pixels with similar radiance or digital number values across various image bands or data channels [2]. In addition, applying different statistical learning techniques has become instrumental in extracting valuable information from remote sensing data [3]. Multispectral data in geospatial typically involves the integration of remote sensing imagery with spatial data layers, allowing for a more comprehensive understanding of geographic phenomena [4]. Multispectral remote sensing images are crucial in various fields, including environmental monitoring, agriculture, forestry, urban planning, disaster assessment, and resource exploration [5]. They enable researchers, scientists, and decision-makers to gain insights into the Earth’s features, changes over time, and environmental conditions by leveraging different materials’ distinct spectral signatures and interactions of other materials with electromagnetic radiation. Prior studies used traditional methods of handling multispectral remote sensing images, such as pixel-based methods [6] and object-based methods [7]. However, the conventional method faces challenges in complex applications due to distributional assumptions and the nature of the input data image [8]. Older approaches also have limitations in accurately classifying and interpreting multispectral data due to difficulties extracting complex characteristics [9]. Recent studies have shown that intelligent computing systems, such as machine learning (ML) tools like the Random Forests, K-Nearest Neighbors, and Neural Networks, offer interesting classification task results but face challenges capturing intricate relationships within images and requiring substantial training data, leading to higher computational cost [10, 11]. Support vector machine (SVM) with polynomial kernels can effectively replace conventional statistical methods in handling multispectral remote sensing image classification problems [2]. Although this method requires more processing resources, it has been found to have potential overfitting and inefficiency for large datasets. This study aims to improve the existing methods in handling multispectral remote sensing image classification problems; perhaps a new approach is needed, one that requires less computing power and provides more accurate results. This study utilizes Python libraries and data science tools such as Jupyter Notebook to perform SVM classification with a radial basis function kernel approach, addressing a gap in the existing literature. Figures 1 and 2 depict satellite images of the study area, while Figs. 3 and 4 illustrate the ground truth data for training samples and compare them with the data. Generating supervised training sample datasets necessitates carefully collecting ground truth data through surveys using georeferenced satellite image data.  
  Support Vector Machine for Satellite Images Classification  
  Abstract. In software development, test cases are stored for later use, such as retesting or regression testing. Optimization is one of the approaches used in regression testing, particularly test case prioritization (TCP). TCP aims to rapidly uncover defects during software development. Existing TCP methods lack reliability and suffer from efficiency and effectiveness due to insufficient evaluation, reproducibility, and benchmarking. Currently, no existing TCP framework is integrated with the hybrid PSO-ABC optimization method. This paper aims to introduce a TCP framework that includes five factors, namely fault detection and severity, test case dependency, clustered test cases, and test input, which are used to prioritize test cases. The process starts by determining three factors from the literature (i.e., fault detection and severity, as well as clustered test cases) and contributing the other two (i.e., test case dependency and test input) to seek better optimization. Historical data from previous runs regarding these TCP factors were extracted and stored for analysis. The proposed TCP framework was verified by ten experts, and it was learned that this framework received positive feedback. TCP is closely related to the longevity of software since it can ensure that systems remain reliable, dependable, and maintainable over time. By identifying and prioritizing essential test cases, developers can focus their testing efforts on the areas of the system that are most likely to be affected by changes. Keywords: Test Case Prioritization · Swarm Intelligence · Multi-objective optimization  
  To find faults by gaining coverage of other aspects related to [16–18] software criteria  
  Table 1 summarizes the current TCP methods in the literature, their aims, and related studies. The code-based TCP methods are broadly popular, in which their focus is concentrated on achieving maximum code coverage for revealing defects. Fault-based TCP methods mainly focus on fault-proneness, where fault history, impacts, and probability are utilized. Requirement-based TCP techniques attain coverage of stakeholder requirements to build confidence in software functionality. On the contrary, employing multiple factors in optimizing test cases is gaining popularity due to their performance. Finally, several TCP methods apply different aspects of the information related to software development for test optimization. With the increasing use of multi-objective optimization to solve NP-hard problems, TCP has been treated as a multi-factor problem in the research community [19, 20]. On the contrary, using a single factor for optimizing test cases restricts the ability of the optimized test cases to locate faults and minimizes the flexibility of the technique against the increased complexity of regression testing and other practical considerations [2, 22]. So, multi-objective optimization methods in TCP received wide acceptance and popularity in the research society because they surpassed single optimization methods  
  A Regression Test Case Prioritization Framework  
  in several aspects, such as their capability to cope with the increasing complexity of test case optimization and tackle two or more objectives for optimization [22–25]. Also, their capability to accelerate fault detection ability, maximize coverage criteria, minimize cost, provide better performance in industrial case studies, and provide better distributions of the weighted average percent of faults detected (APFD) metric values makes them widely acceptable [22, 26, 27]. This paper aims to contribute four aspects to the body of knowledge, as follows: • A new method for optimizing test cases through multiple factors mainly focuses on testware without considering the source code in optimizing test cases. • New optimizing factors were introduced to the literature in the prioritization activity. • A new implementation of the hybrid swarm algorithm in TCP coped with tackling the optimization process. • A new weighted objective (fitness) function was formulated to handle these factors for the optimization process. This paper proposes a new TCP framework in which five factors, namely fault detection and severity, test case dependency, clustered test cases, and test input, are considered for optimizing test cases. The framework uses a hybrid swarm algorithm to tackle the multi-objective optimization process. This proposed framework aims to enhance the efficiency and effectiveness of the testing process by involving different factors and focusing on the testware artifacts and their historical data. The rest of this paper is organized as follows: Sect. 2 discusses the related works of literature. Section 3 elaborates on the proposed work, and Sect. 5 concludes this paper and highlights directions for future research.  
  Abstract. The increasing proliferation of intelligent mobile devices and the subsequent surge in data traffic have placed a burden on the current Internet infrastructure. To address this challenge, Named Data Networking (NDN) has emerged as a promising future Internet architecture. NDN aims to address the evolving patterns of Internet traffic by providing inherent support for consumer mobility through innetwork caching. This approach enhances content availability while minimizing delays. However, producer mobility in NDN raises numerous challenges, including Interest packet loss, Interest retransmission, high signalling costs, and unnecessary bandwidth consumption. This research explores and critically analyses the most widely used approaches for managing producer mobility in NDN. This paper introduces an innovative immobile anchor-based mobility mechanism designed to address the challenges associated with producer mobility in NDN. The immobile refers to the fixed nature of the anchor router, which is strategically placed within the network topology to facilitate the management of producer mobility. This immobile anchor router serves as a centralized control point for caching and redirecting Interest packets during producer handoff processes, thereby mitigating packet loss and optimizing bandwidth usage. The focal point of this novel approach is to reduce the repercussions of producer mobility on network performance. Its aim is to minimize factors like packet loss, signalling overhead, and bandwidth usage, with the ultimate goal of enhancing the overall efficiency of NDN-based networks. Keywords: Handoff · Mobility Management · Producer Mobility · Information-centric networking · Named data networking  
  Fig. 2. Assistance of consumer mobility in NDN.  
  The producer sends the requested content in the form of data packets to the consumer via the Interest reverse path. The consumer disconnects from CR3 and connects to CR4 during the packet exchange, which is known as handoff. The consumer handoff process disrupts ongoing communication. Surprisingly, due to the in-router caching ability the remaining contents already caches at the junctional router CR2. When a consumer relocates to a new location CR4, it sends the Interest for remaining content to the junctional router CR2 and retrieves it. As a result of the in-router caching feature, the NDN inherently supports consumer mobility and improves the content availability [12].  
  5 Producer Mobility Assistance In NDN paradigm, a producer implies to a content source that supplies content in response to consumer data demand. Every producer has its a unique name prefix to distinguishes it from others, and the FIB in the NDN data structure maintains the list of these different name prefixes. The FIB also determines the best path to access content with the least amount of effort. Typically, a consumer generates an Interest packet as of CR1 to the producer’s location at CR3 to retrieve content. However, if the producer physically relocates from CR3 to CR4 router, the consumer’s new incoming Interest packets continue to follow the producer’s CR3 old location router. This can result in lost Interest packets at CR3 router due to producer inaccessibility and inadequate stored contents, as presented in Fig 3.  
  In NDN paradigm, when a producer relocates, the consumer constantly forwards the Interest packet in an attempt to locate the required content. However, this approach results in excessive loss of Interest packets, higher Interest retransmission rate, unnecessary bandwidth utilization, and extensive handoff latency. As a result, producer is not capable of fully supporting the mobility in NDN environment [5, 17]. Additionally, the current router-level protocol, Listen First Broadcast Later (LFBL), is not entirely capable of addressing the issues of losing Interest packets and constant Interest retransmission [13].  
  6 Classification of Producer Diverse Mobility Approaches Various solutions have been proposed by researchers to address the issues related to producer mobility in the NDN paradigm. These solutions have been categorized into specific approaches based on their characteristics and advantages. 6.1 Indirection-Based Mobility Approach (IMA) To enable support for producer mobility in the NDN paradigm, the Indirection-based Mobility Approach (IMA) introduces a Home Agent (HA) router in the network. The HA is responsible for maintaining information regarding the content prefix and its location. The content prefix identifies the producer, while the location indicates its physical position. During the handoff process, the HA redirects packets towards the producer’s new location [14]. As illustrated in Fig 4, When a producer moves from CR3 to CR4, it sends binding information to update its new location and name prefix to the HA router, which acknowledges the information. Consequently, when an incoming Interest packet reaches CR3, it is redirected to the HA, which in turn forwards it to the producer at CR4 using a triangular routing path. Triangular communication uses encapsulation and decapsulation in order to exchange Interest and data packet. However, during the handoff process Interest packets experiences loss and excessive Interest transmissions. Further, it uses the triangular routing after the handoff process which may result in high signalling and extra overhead for each packet.  
  Towards a Sustainable Digital Society  
  Fig. 7. Control data plane split-based mobility approach in NDN.  
  In CDPSMA, when a producer initiates a handoff process from CR3, it sends a deregistered query to the RH to remove its location information. At the same time, upcoming consumer Interest packets are cached at the RP. When the producer handoffs to CR4, it sends a register query to the RH to inform it of its new location information. The RH then forwards the producer’s new location information to the RP, which redirects the consumer Interest packets to the producer’s new location at CR4. However, this approach can result in high signalling to manage producer mobility. Additionally, frequent producer movements towards different CRs may cause redirection of consumer Interest packets towards an outdated route, leading to Interest packet loss and retransmission. An analysis of different mechanisms for supporting producer mobility, such as IMA, MMA, LISMA, and CDPSMA, shows that each approach has its strengths and weaknesses. IMA supports mobility through the use of a Home Agent (HA), but it may suffer from high signalling issues and packet encapsulation challenges in large networks. MMA avoids packet encapsulation but experience problems with Interest packet loss, high latency, and signalling overhead. LISMA addresses packet loss but may not be suitable for large networks and may not provide support in certain network scenarios. CDPSMA reduces the negative impact of mobility through monitoring but can also experience high overhead and signalling. Despite these approaches’ potential benefits, there is still a need for a feasible solution that addresses their limitations. This research proposes a new mechanism for controlling producer mobility that overcomes the challenges presented by existing approaches. Table 1 summarizes the analysis of the various producer mobility support mechanisms, including their benefits and drawbacks, and lists unresolved issues.  
  7 Proposed Producer Mobility Management Mechanism By critical analysis of previous approaches some notable factors have been identified such as packet loss, high signalling, extra bandwidth usage and excessive Interest retransmission. In order to overcome the impact of the identified factors, this research proposed a mechanism that will be able to control the associated issues due to producer handoff mobility. The proposed mechanism design consists of mobility packet and immobile  
  High overhead Long Handoff latency Intence handoff singnaling Consume extra bandwidth  
  anchor router. The mobility packet is responsible for updating the producer’s location information in the network, while the immobile anchor router manages the flow of Interest packets during the producer’s handoff mobility process, ensuring minimal packet loss. Moreover, the proposed design modifies the normal NDN forwarding plane. By modifying the normal NDN forwarding plane, the proposed mechanism effectively distinguishes between mobility packets and Interest data packets. This differentiation enables the network to prioritize mobility updates and maintain efficient routing during the handoff process, thus minimizing disruptions to ongoing data exchanges. By referring to Fig. 8, when producer moves from CR3 to CR4, the consumer Interest is unable to retrieve content from producer due to handoff process. During the handoff process, the consumer Interest redirected to CR2 which work as an immobile anchor. The immobile anchor buffers the redirected Interest packets. Meanwhile, when producer relocate and connects to CR4, it sends Mobility Notification (MN) packet to immobile anchor to broadcast its location in the network. The broadcast of MN packet updates the FIBs in the network router to inform about the producer new location. When the location updated, the immobile anchor redirects the cached  
  Towards a Sustainable Digital Society  
  Interest packets towards the producer location. The producer sends the data packets by following the Interest revers path towards the consumer. The new incoming Interest from consumer follows the optimal path due to update in the FIB. The consumer Interest moves from CR1 to CR2 and further moves to producer router CR4. In response, the producer forwards the data packets towards the consumer but in the reverse manner. As a result, the proposed mechanism effectively reduces Interest packet loss, signalling overhead, bandwidth usage, and excessive Interest retransmissions.  
  Fig. 8. Proposed producer mobility management mechanism.  
  The new incoming consumer Interest packets follow the optimal path, due to update in the FIB. The consumer Interest packets move from CR1 to CR2 and further moves to producer router CR4. In response, the producer forwards the data packets towards the consumer by following the Interest packets path in reverse manner. In this way, the proposed mechanism reduces the Interest packet loss, high signalling, extra bandwidth usage, and excessive Interest retransmission.  
  8 Message Flow Figure 9 illustrates the message exchange between a consumer and a producer under the proposed mechanism. It is assumed that the consumer and producer are already in communication with each other. To retrieve data packets, the consumer at CR1 sends an Interest packet (/pic.jpg) to the producer’s (IRL.my) router at CR3. The producer responds to the Interest packet with a data packet, which is divided into versions and several segments corresponding to the requested Interest packets (IRL.my/pic.jpg/v1/sn, where n = 1, 2, 3, 4……). The Interest path from the consumer location towards the producer is (Consumer/CR1/CR3/Producer), while the data path follows the Interest packet path in a reverse way (Producer/CR3/CR1/Consumer). During the communication, the producer decides to move and subsequently disconnects from CR3. While the producer is in the process of handoff, the consumer sends another Interest packet direct towards the producer’s router at CR3, following the path (Consumer/CR1/CR3). Since the producer is not available at CR3, the consumer’s Interest packet is redirected to the immobile anchor router, CR2, following the path (Consumer/CR1/CR3/CR2).  
  Fig. 9. The interest data stream in proposed mobility management mechanism.  
  The CR2 caches the redirected Interest packets. Meanwhile, the producer completes its handoff process by connecting to CR4 and sends the Mobility Notification (MN) packet to CR2. The MN packets updates the producer location towards the network and provides the new route towards producer location. Once the producer updates its location in the network, the cached Interest packets at CR2 are forwarded to producer router CR4 with path Consumer/CR1/CR3/CR2/CR4/Producer). In response, the producer sends the data packets to consumer by following the Interest reverse path (Producer/CR4/CR2/CR3/CR1/Consumer). Furthermore, after the producer’s location update, new incoming consumer Interest packets follow the updated optimal path (Consumer/CR1/CR2/CR4/Producer) and the data path (Producer/CR4/CR2/CR1/Consumer), ensuring efficient routing and minimizing latency. As a result, the associated concerns during the producer mobility are handled by the proposed mechanism. Additionally, the proposed mechanism provides an effective solution that can control excessive Interest packet loss, high signalling, Interest retransmission, and extra bandwidth consumption compared to existing approaches such as IMA, MMA, LISMA, and CDPSMA.  
  9 Conclusion The most interesting NDN producer mobility approaches have been addressed in this study. All of these approaches offer various ways to control producer mobility, that each have fundamental design and unique properties. However, following a rigorous examination of each approach, we draw attention to a number of problems, including losing of Interest packet, constant Interest retransmission, prolonged handover latency, and an inefficient routing path. To address these challenges, we propose a producer mobility management mechanism, which is expected to effectively resolve the identified issues  
  Abstract. Named Data Networking (NDN) is the most remarkable initiative of Information-Centric Network (ICN) to improve overall network performance. With its data-centric architecture and forwarding philosophy, NDN natively addressed consumer mobility. However, the producer mobility problem remains a challenging issue in NDN architecture. Among the critical issues of producer mobility are handover latency, Interest packet loss, and Interest retransmission. This paper classifies existing producer mobility solutions into rendezvous, anchorbased, and anchor-less approaches. Despite the efforts of many researchers poured into solving these issues, there is still room for improvement. This paper proposes a producer mobility management mechanism based on a proactive approach to managing producer mobility in NDN. The proposed mechanism proactively evaluates the handover time of the producer and sends the mobility notification packet to inform about the producer’s movement. In the meantime, a new Interest packet for the moving producer will be buffered on the router. Thus, the proposed mechanism aims to reduce the handover latency and packet loss. Keywords: Named Data Networking · Producer Mobility · Consumer Mobility  
  N. H. A. Zukri et al.  
  An emerging novel future Internet called the Information-Centric Network (ICN) is committed to the development of decentralized networks. With its new structure, ICN replaces IP addressing with the idea of addressing the content for network addressing. Respective research communities keep an eye on this subject matter. Thus, introducing some new architectures, namely Data-Oriented Network Architectural (DONA) [4], Content-Centric Networks (CCN) or Named Data Networks (NDN) [5], PublishSubscribe Internet Technology (PURSUIT) [6], and Network of Information (NetInf) [7]. NDN receives immense attention from the ICN architectural research group among these architectures. Future Internet architecture Named Data Networking (NDN) has caused a paradigm shift in network communication from point-to-point to name delivery. In addition, NDN depends on the stateful forwarding plane for datagram delivery. The adoption of the NDN forwarding plane indeed supports consumer mobility. If the consumer is relocated to a new attached content router (CR), no signaling from the network is required to handle consumer mobility. Instead, Interest retransmission is adequate for successful communication. However, producer mobility is a more complex problem that impacts overall communication. This is because the routers’ Forwarding Information Base (FIB) needs to be updated once the producer reconnects with the new attached content router. Failure to update FIB will affect the search for the desired Data. Ultimately, it results in high-Interest packet loss, high-Interest retransmission, increased bandwidth demand, and high handoff latency [8, 9]. Further, the aforementioned issues will restrain the work for building a sustainable and more inclusive digital future. Therefore, this paper introduces a producer mobility management mechanism design in NDN using a proactive approach. The proposed mechanism will handle the mobility of the producer by analyzing the producer’s mobility status and informing the router and consumer to take necessary action, such as delaying the Interest transmission. Managing the producer mobility before the handover process based on a proactive approach would minimize the handover latency and interest retransmission. Also, it aims to alleviate other critical issues, such as high signaling and Interest packet loss issues. The remainder of this paper is organized as follows. Section 2 presents the mobility in Named Data Networking. Section 3 discusses the producer mobility approaches. Section 4 shows the proposed design and the conclusion in Sect. 5.  
  2 Mobility in Named Data Networking One of the auspicious Information-centric networking (ICN) approaches to address the challenges of the future Internet is NDN [10]. The NDN relies on named data, namebased routing, and in-network caching to distribute content efficiently and improve network bandwidth. Additionally, the NDN facilitates consumer mobility easily and has incorporated security for every piece of Data. The essence of NDN comes from CCN [11, 12]. This can be shown in NDN architectural and protocol operations. Besides that, NDN has a modular and extensible codebase. As NDN architecture changes the Internet architecture model, the network communication changes to retrieve content. Any communication in NDN is based on two kinds of packets: Interest and Data. A Data packet is a named sequence of bytes. A name is a  
  N. H. A. Zukri et al.  
  As shown in Fig. 1, the consumer can resubmit any outstanding or expired Interests if they relocate while the network gets the desired data. By doing so, the intermediate router can update the path to the consumer’s current location. If the old and new paths cross, the consumer will retrieve the previously requested Data from a router’s cache without propagating further. Given this NDN nature, the consumer effortlessly gets the remaining content from these routers to fulfill its content needs. For the next request, the consumer can efficiently get the remaining Data packets only by re-issuing Interest again for similar content [13, 14], and it can be done within the minimum delay. In conclusion, the consumer mobility issue is inherently solved through cached contents at router space or resending the Interest packet to the intermediate router after the completion of consumer mobility [15]. 2.2 Producer Mobility In NDN, consumer issue Interest messages carrying the name of the requested information object. Although the name of the content is separated from the location, the hierarchical name comprises the location of the content such as the example ‘uum/edu/my/ahsgs/homepage.pic’. The NDN directly coupled the location with the content name, adding them to FIB in the routing protocol [15]. Consequently, whenever the consumer delivers an Interest packet, the connected router determines the path by looking up the FIB for a suitable producer (content source). Generally, FIB contains the entries of long prefixes address for the next-hop or content source. This allows it to determine the optimal path to access content and further the Interest. The producer satisfies the consumer’s content needs by sending the desired content from its location to the consumer. Whenever the producer shifts and connects with the other CR, the producer must update the FIB of all routers in the NDN network. Otherwise, Interest reaches the old CR to retrieve the content. Sometimes, it takes quite a long time to get a new CR. Due to the unavailability of the producer, no content is available for the consumer [16]. Furthermore, the network will suffer from high overhead, packet loss, and long handover latency [8]. Ultimately, the consumer lost communication until the producer informed its new name prefix in the network. Figure 2 illustrates the producer mobility in NDN. To control mobility in ad-hoc networks, NDN uses the Listen First Broadcast Later (LFBL) protocol. LFBL floods the Interest packet to the serving routers and broadcasts name prefixes across all the serving routers after the producer handoff process is completed [17, 18]. The decisions for prefix announcements are made depending on the availability of the wireless channel. If no other node has sent a matching data packet, the router forwards the packet. However, the LFBL protocol generates high signaling and has no mechanism to recover the dropped and incoming Interest packets sent at the previous attached CR. Furthermore, producer mobility is not natively supported due to the unavailability of content locators and failures in content access through the routing system. The second NDN solution to minimize the producer mobility effect is caching content at the router.  
  Enabling a Sustainable and Inclusive Digital Future  
  Fig. 3. Operation of a Rendezvous Approach [8].  
  3.2 Anchor-Based Approach Similar to the idea of MobileIP in TCP/IP, the anchor-based solution made use of an anchor node called Home Agent (HA). HA keeps tracking the position information of relocated mobile nodes. Figure 4 shows a consumer resubmitting the Interest to HA and finding a new location for the producer. Then, HA transmits the Interest directly to the producer. Various schemes [16, 22] introduce the notification method about the producer’s relocation to HA. Although these schemes are simple and reduce the handover latency, this approach is vulnerable because of heavily dependent on a single node (HA). One of the drawbacks is the transmission of all packets via the anchor node will lead to the bottleneck effect [23]. 3.3 Anchor-Less Approach The anchor-less approach allows a producer to update its new location information in the network without relying on third-party [24, 25], as shown in Fig. 5. Upon receiving the notification, routers update the FIB table with the latest location information. Since FIB is keeping up to date, the Interest can be submitted to the current location of the producer. Thus, no specific node like a rendezvous point or HA is required to forward the packet. [26] apply this strategy and update FIBs of associated routers to facilitate forwarding the Interest to the node. However, the disadvantage of this scheme lies in a scenario in which the producers are relocating regularly, which frequently updating the FIB of the connected routers may trigger traffic [23].  
  Enabling a Sustainable and Inclusive Digital Future  
  Fig. 4. Operation of the Anchor-based Approach.  
  In [27], the producer announces the leaving status of the network, including the previous attached CR. The producer’s frequent movement hampered the propagation of the concurrent FIB updates. This scheme provides a fast and lightweight handover but incurs high signaling due to extra packets sent for the notification from the leaving producer.  
  Fig. 5. Operation of the Anchor-less Approach.  
  High traffic, high signaling  
  4 Proposed Mechanism In this section, we present the mechanism that describes the main ideas in addressing the issues of long handover latency, high-Interest packet loss, and Interest retransmission. Based on these issues, Fig. 6 illustrates the proposed proactive producer mobility management mechanism. Despite the exchange of Interest and Data packet, this mechanism proposed three schemes for normal communication in NDN which are: 4.1 Determine the Producer’s Mobility Handover Time In order to reduce handover latency, the mechanism shall proactively determine the producer’s leaving time. This research overcomes the issue by calculating the minutes’ handover begins. Every few seconds, the scheme will trigger the sensor to detect the Received Signal Strength (RSS) of the producer with CR1. If RSS is lower than the threshold (th), the scheme will calculate the producer’s mobility handover time. It is important to determine the handover time so that the method to minimize the producer’s mobility effect would be implemented sooner. 4.2 Send Mobility Packet After determining the producer’s handover time, it is necessary to inform the consumer that the producer is leaving the router. The mobility packet acts as a notification signal to the consumer that contains information about the producer’s mobility. The consumer may delay the Interest transmission to the same producer upon receiving the packet. The Interest retransmission is delayed until the consumer receives further notice about the producer’s new location.  
  Enabling a Sustainable and Inclusive Digital Future  
  The proposed mechanism will be measured based on handoff latency, interest packet loss, data packet delivery, and throughput for the evaluation. The formula for each parameter will be studied. Results from the simulation will be accumulated and compared with the OPMSS [25] and MAP-Me [27].  
  5 Conclusion The NDN architecture is key to building a more inclusive and sustainable future Internet. Nevertheless, the NDN does not provide enough assistance for mobile producers, particularly if they lose their connections. Given the severe effects of mobility towards communication in the network, much research has been done, and these research works are categorized into various approaches. However, after thoroughly investigating previous research, we have identified several problems, such as high signaling, high packet loss, and long handover latency. Thus, this paper proposed a proactive producer mobility management mechanism to solve such problems. The contribution of this paper is twofold. First, we look at mobility in NDN and producer mobility approaches. The approaches are categorized into rendezvous, anchor-based, and anchor-less approaches. All papers are being analyzed and criticized, respectively. By comparing both approaches, we can identify the best fit for designing the proactive mobility management mechanism. Second, we design the proactive producer mobility management mechanism by incorporating three strategies: determining the producer’s mobility handover time, sending the mobility packet, and buffer Interest. The proposed mechanism is expected to provide the mobility solution to support producer mobility in NDN architecture by minimizing the Interest retransmission and handoff latency.  
  Enabling a Sustainable and Inclusive Digital Future  
  2.3 Eligibility and Exclusion Criteria Several eligibility and exclusion criteria are determined. The first criterion, Literature type, specifies that only research articles published in journals and proceedings will be eligible for inclusion. These articles are typically peer-reviewed and provide original research findings, making them more reliable and valuable for research purposes. The exclusion criteria under this criterion include systematic reviews, book series, books, chapters in books, and conference proceeding books. The second criterion, Language, specifies that only articles published in English will be considered. This criterion ensures that the selected articles can be easily understood and reduces the potential for languagerelated bias in the selection process. The final criterion is Timeline. Since the GitHub Copilot is a new tool, selecting articles is unlimited. All articles published in 2023 and the years before, have been selected. This criterion ensures that all articles related to the GitHub Copilot were chosen (see Table 1). Table 1. The inclusion and exclusion criteria. Criterion  
  Eligibility  
  Exclusion  
  Literature type Journal (research articles), proceeding Journal (systematic review), (research articles) proceeding (systematic review), book series, book, chapter in book, conference proceeding book Language  
  English  
  Timeline  
  Any time < 2023  
  None  
  2.4 Systematic Review Process The systematic review was conducted in April 2023 and comprised four distinct stages. The first stage involved the identification of relevant keywords for the search process. Since GitHub Copilot is a unique keyword, “GitHub Copilot” was used. Following a meticulous process, 12 duplicated articles were eliminated. The second stage was the screening process, which excluded two articles due to their status as conference proceedings, leaving 26 eligible articles for review. In the third stage, the eligibility criteria were applied to the full articles, excluding nine articles that focused on topics other than GitHub Copilot. Finally, the review’s last stage yielded 15 articles suitable for qualitative analysis, as presented in Fig. 1.  
  2.5 Data Abstraction and Analysis The remaining articles were assessed and analyzed. Efforts were concentrated on studies that met the predetermined inclusion criteria, which ensured that the analysis focused on relevant sources of evidence. To extract relevant data, the team initially read through the article abstracts, followed by a more in-depth analysis of the full articles to identify significant trends related to GitHub Copilot. Qualitative analysis was performed using content analysis techniques, which allowed the team to identify relevant themes and patterns in the data. This rigorous and systematic approach to data analysis ensured that the study’s findings were grounded in the available evidence and provided a robust synthesis of the research on GitHub Copilot.  
  3 Results Table 2 shows the recent trends of GitHub Copilot research. There are four main areas studied by researchers in 2022 and 2023: developer productivity, code quality, code security and education.  
  The Recent Trends of Research on GitHub Copilot  
  √ √  
  Nguyen and Nadi (2022) [11] Al Madi (2022) [2] Siddiq et al. (2022) [17] Pearce et al. (2022) [13] Finnie-Ansley et al. (2023) [6] Denny et al. (2023) [3] Wermelinger (2023) [23]  
  √ √ √ √ √ √ √  
  3.1 Developer Productivity The study [9] aimed to assess the performance of GitHub Copilot, an automatic program synthesis tool, and compare it with genetic programming approaches on common program synthesis benchmark problems. The results revealed that both approaches had similar performance on benchmark problems. However, genetic programming approaches still needed to be mature enough to support practical software development due to their reliance on expensive hand-labelled training cases, long execution times, and bloated and hard-to-understand generated code. The researchers suggested that future work on program synthesis with genetic programming should prioritize improving execution  
  insight into Copilot’s potential for programming education and to highlight the need for instructors to adapt their teaching practices accordingly. The findings indicate that while Copilot can generate correct and understandable code, it cannot replace the process of learning programming. Therefore, educators must incorporate Copilot into their pedagogical strategies judiciously.  
  4 Discussion GitHub Copilot is an AI-powered programming tool that has gained attention for its ability to generate code automatically. Several studies have explored Copilot’s performance, productivity, and usability in different contexts. One study compared Copilot with genetic programming approaches on common program synthesis benchmark problems, concluding that both approaches had similar performance but that genetic programming approaches still needed to be mature enough for practical software development. Another study investigated the impact of Copilot on user productivity and found that the rate of acceptance of suggestions was the primary driver of productivity. The third study compared Copilot with human pair programming and found that while Copilot increased productivity, the quality of generated code was inferior. The fourth study revealed that Copilot provided a useful starting point for programmers but needed help understanding, editing, and debugging generated code snippets. Several studies have recently assessed the code quality generated by GitHub Copilot, which assists programmers by generating code based on natural language descriptions of desired functionality. The studies have used various methods to evaluate the correctness and understandability of the generated code and have generally found that Copilot holds significant promise as a programming tool, generating valid code with high success rates. However, the studies also identify potential shortcomings, such as generating code that could be further simplified and relying on undefined helper methods. Further assessments and improvements are necessary to optimize Copilot’s performance in generating entirely accurate code that meets all requirements. Using machine learning-based code generation models, such as GitHub Copilot, raises ethical and security concerns. Several recent studies highlight the potential for such models to generate vulnerable code and the need for careful selection and scrutiny of training data to minimize risks. To address these concerns, researchers have introduced SecurityEval, a dataset for evaluating the security of code generation models, and CoProtector, a prototype aimed at safeguarding open-source code from misuse and breaches. While Copilot’s performance varies considerably based on the diversity of weaknesses, prompts, and domains, the studies emphasize the importance of vetting generated code for security vulnerabilities to prevent potential breaches. The studies explore using OpenAI’s Codex machine learning model in programming education through its implementation as the GitHub Copilot plugin. They investigate Copilot’s impact on the learning process, its ability to generate original code, and its performance on diverse programming problems. The studies show that Copilot has the potential to support students in completing programming assignments and exams and can promote equitable access to high-quality programming education. However, the studies also suggest that Copilot cannot replace the process of learning programming, and  
  Abstract. With infinite apps and online services, future Internet architecture will face new challenges and consequences, such as scalability, dependability, suitable mobility, and security. Internet use has changed spectacularly from one-way communication to content distribution, as much content is generated every minute. Blockchain and Named Data Networking (NDN) are two cutting-edge technologies on the verge of revolutionizing how we use the Internet. Blockchain is a decentralized ledger technology allowing users to store and share data securely. On the other hand, NDN is a new way of networking that focuses on content instead of location. Combining blockchain and NDN can create a safer, more efficient, and more decentralized internet. Blockchain can provide tamper-proof data records, and NDN can deliver content to users efficiently and securely. This paper emphasizes the importance of research in the field of blockchain over Named Data networks. It highlights the advantages of combining blockchain with NDN and discusses the difficulties and open research questions related to the use of blockchain over NDN. Also, the potential impact of blockchain over NDN on the future of the Internet as it can create a safer, more efficient, and more decentralized Internet. Keywords: Blockchain · Content Distribution · Content-Centric Network · Future Internet  
  The original Internet design is elegant and powerful due to its hourglass architecture. The narrow waist of the hourglass represents the core network layer, which implements the essential features necessary for global interconnectedness. This small size has been essential to the development of the Internet, as it has freed higher- and lower-layer technologies from unnecessary limitations. The NDN design also has a narrow waist, but it differs from the IP architecture in a fundamental way [3]. The use of data names in NDN allows for more efficient and scalable content delivery, as well as improved security and privacy. Data names are hierarchical, making finding and retrieving content easy. They are also self-describing, which means they contain information about the content, such as its type, size, and last modified date. Routers can use this information to make more informed decisions about how to route data packets [4]. Integrating blockchain technology with NDN has recently gained much attention in the research community. Scholars have highlighted the potential benefits of this integration, such as improved security, privacy, and scalability. NDN can also fulfill the needs of blockchain applications by providing a secure and efficient way to store and transfer data. This study focuses on integrating NDN and blockchain technology and discusses the potential benefits of this integration [5, 6]. The use of blockchain over NDN will be examined in this paper. The document layout is as follows: We’ll present the Methodology in Sect. 2 and the Background review in Sect. 3. A review of the latest studies of blockchain over NDN will be covered in Sect. 4. The discussions on the value of blockchain over NDN will be covered in Sect. 5. Open challenges will be presented in Sect. 6 and Sect. 7 will conclude the paper.  
  Fig. 2. Articles from different resources.  
  3 Background First Section Blockchain and named data networking (NDN) are two emerging technologies that have the potential to revolutionize the way we interact with the Internet. Blockchain is a distributed ledger technology that can securely record and share data, while NDN is a new networking paradigm focusing on content rather than location. In this section, we will provide a background on blockchain and NDN. We will then discuss the potential benefits of combining these two technologies [6]. 3.1 NDN Forwarding Plane NDN architectural design proposes two packet types: interest packets and data packets. NDN users can access the data by subscribing to an Interest packet, which requests a content object and returns it as a Data packet; both packets contain the name of the content object. The Forwarding Information Base (FIB), the Pending Interest Table (PIT), and the Content Store are the three significant Data structures that an NDN router must maintain (CS) [3]. The FIB of an NDN router is often similar to the FIB of an IP router, with the architectural difference that it holds name prefixes rather than IP address prefixes. By doing this, the name prefixes may be sent to various interfaces. Each PIT section keeps a record of the Interest’s name, arriving interface(s), and forwarding interface(s) that it has been passed to (like a history table), when a router receives an interest packet, it first checks the Content Store (CS) to see whether there is any matching Data. The CS provides short-term in-network storage (caching) of the incoming Data packet. The interface from which the Interest is coming receives the data immediately [7]. On the other hand, if the name matches, the interest will proceed to look up the PIT entries. Suppose the name already appears in the PIT. In that case, it may be a duplicate Interest that has to be discarded or a retransmitted Interest from the same customer sent via a different outgoing interface (or an Interest from an alternate consumer requesting the same Data). This causes the PIT to check the nonce of the Interest and update the existing PIT record with the number of incoming interfaces. This effectively creates a  
  Blockchain Over Named Data Networking Architecture: A Review  
  multicast tree for users requesting the same Data simultaneously. If the Interest name does not already exist in the PIT, it is added to the PIT and sent to the FIB, where the forwarding plane module will handle it. When a Data packet comes at this stage, the PIT is checked using the packet’s name. The router delivers the Data packet to the interface(s) from which the Interest arrived and deletes the PIT entrance if there is a match in the PIT entrance. Data packets then frequently follow the interests’ backward routes. The Data packet is discarded or cached in the CS if no match is detected. Every Interest has an associated lifespan that the consumer determines; if an Interest is unsatisfied before its lifetime expires, a PIT item is removed. Nevertheless, an NDN router may keep a Data packet in the CS due to the signature’s uniqueness and the dependability of the caching strategy. Even so, future interests can be satisfied using the data packets stored in the CS [8]. 3.2 Blockchain Framework Most modern blockchain systems use a common framework that was first established in Bitcoin and Ethereum and may be organized generally into four levels [9] as follow: Application Layer: By utilizing smart contracts, the decentralized applications of blockchain technology, such as supply chain management, identity management, and notarial services, have grown to include the application layer, which is used for cryptocurrency transfers. Data Layer: A blockchain framework’s data layer contains structures for consensus, data transfer, and ledger maintenance. Blocks are connected via hash references; however, block architectures may vary. Consensus Layer: The shared ledger is created via a process followed by the consensus layer in blockchain nodes, with copies on each node. Consistency requires agreement on how transactions should be executed in order. For quicker processing, transactions are organized into blocks, and the block sequence is chosen instead. The PoW consensus, used by many blockchain systems, is infamous for its lengthy transaction times and lack of transactional finality because of ledger forking. Researchers are looking into novel consensus methods to get over these restrictions and support a range of blockchain technology use cases. Transport Layer: The blockchain network’s transport layer defines how transactions are recorded to the ledger and how blockchain data is propagated. Public blockchains like Bitcoin and Ethereum use a P2P overlay to transfer data items to all nodes from a single source. The transport layer of the blockchain network controls how transactions are recorded and propagated.  
  4 Blockchain over Named Data Networks Most research on Named Data Networks and Blockchain technology has been done independently. Table 1 shows recent years that adopted Blockchain over NDN in various fields, including security and privacy, networks, the Internet of Things, mobility, and  
  2022 Security Presents hierarchical identity-based cryptography (HIBC) and blockchain-based security method for NDN  
  [16] 2023 Security Proposed efficient and secure auditing of data transmission behavior for NDN IIoT networks [17] 2023 Security Proposed a NACDA approach for data verification in NDN, which improved the considerable delays brought on by the extremely dynamic nature of vehicle networks [18] 2023 Security Proposed a decentralized data authentication mechanism based on blockchain technology [19] 2023 Trust  
  Build mechanics trust between vehicles using NDN to route data efficiently, and blockchain to record transactions securely  
  [20] 2022 Security proposed a system called BIoVN, to secure IoV over NDN [21] 2023 Routing Present a new data dissemination protocol called A-C is based on the NDN forwarding [6]  
  2022 Routing Proposed a deployment of named data networking (NDN) at the network layer of the blockchain to provide differentiated QoS assurance  
  [22] 2023 Routing Proposed a framework called AFFIRM for generating, validating, storing, and retrieving mobility data in Web3 applications [23] 2022 Trust  
  Present a trust management system is to allow well-behaved peers to gain a good reputation  
  Proposes a proof-of-trust-based data authentication system for blockchains in NDN  
  [26] 2022 Routing Proposed access control system based on NFT enables NDN routers to forward ciphertext data [27] 2022 Routing This paper proposes integrating blockchain and NDN to improve document content storage (continued)  
  Contribution  
  [28] 2023 Security Proposed a CCN-based secure content delivery scheme for V2G networks [29] 2022 Security Proposes a security architecture for NDN based on a consortium blockchain and bootstrapping procedures  
  The authors [16] In the Industrial Internet of Things (IIoT), the study provides a simple transmission behavior audit scheme for Named Data Networking (NDN). The blockchain-based system makes it possible to audit data transmission behavior in NDN networks safely and effectively. It consists of three basic parts: a lightweight auditor for gathering and submitting records to the blockchain, a blockchain-based audit system for managing records, and a data packet for carrying audit records. The findings show that NDN networks can effectively detect malicious activities and have high throughput and low latency. IN [17] proposed Naming-Based Access Control and Decentralized Authorization (NACDA) system addresses challenges in data verification in dynamic vehicular net works by enabling secure and flexible data sharing on the Named Data Network (NDN) using Identity-Based Encryption with Wildcard Key Derivation (WKD-IBE) and blockchain. A new mechanism has been proposed in [18] to provide a decentralized data authentication mechanism based on blockchain technology that is both efficient and straightforward. A new framework proposed in [19] uses VSNs to build trust between vehicles, NDN to route data efficiently, and blockchain to record transactions securely. The framework is designed to be P2P, meaning that vehicles can trade energy directly with each other without needing central authority. While in [20] the authors proposed another system called BIoVN, which is a combination of blockchain technology and named data networking (NDN) for the Internet of Vehicles (IoV). The purpose of this system is to improve the security of vehicular communications over NDN. The authors in [21] introduce the Named Data Networking (NDN) and Erasure Coding (EC)-based A-C data distribution protocol. The protocol uses a two-layer NDNbased publication-subscription mechanism to maximize bandwidth efficiency and speed up data dissemination. It focuses on the prompt distribution of blocks and transactions in blockchain systems, which is crucial for consensus, effectiveness, and security. The A-C protocol improves data transmission efficiency and security in blockchain systems, reduces data redundancy, and addresses shortcomings of flooding-based gossip protocols. A deployment of named data networking (NDN) at the network layer of the blockchain to provide differentiated QoS assurance is proposed in [6]. It discussed the use of window sliding and forwarding strategies to speed up packet processing and meet the delay requirements of delay-sensitive packets. Also, a blockchain framework called AFFIRM for generating, validating, storing, and retrieving mobility data in Web3 applications. This framework enables nearby devices to self-organize as a fog network  
  and collaboratively train machine learning algorithms locally to securely generate, validate, store, and retrieve mobility data via consensus leveraging Information Centric Networking as the underlying architecture [22]. Author in [24] proposes a novel encryption-based data access control scheme for Named Data Networking (NDN) using Role-Based Encryption (RBE). The scheme ensures efficient data access control over hierarchical content, making it suitable for large-scale content-centric applications like Netflix. The study [25] proposes a proof-oftrust-based data authentication system for blockchains. The technique collects votes from a group of nodes to distribute and store items in the cache memory. The suggested system provides a fresh data authentication option for the upcoming Internet environment while attempting to address difficulties with tainted cache memory. In [26] smart contracts are used to distribute AttributeNFT and AccessNFT, a proposed access control system based on Non-Fungible Token (NFT) that enables NDN routers to forward ciphertext data packets only to authorized users, assuring data security and secure distribution. To increase document content distribution, security, and network speed, research in [27] suggests fusing blockchain technology with Named Data Networks (NDN). Three key contributions are made in the paper’s proposal for a CCN-based Three key contributions are made in the paper’s proposal for a CCN-based secure content delivery scheme for V2G networks: in-network caching for quick content delivery; a contract theory-based incentive scheme to entice vehicle participation, and the proof of authority consensus algorithm for secure content delivery and network trust [28]. Authors in [29] Used a symmetric-key-based authenticated encryption technique and a one-way hash chain for source authentication, this article suggests a security architecture for NDN that is based on a consortium blockchain and bootstrapping procedures. In [30] proposed CPA detection and prevention mechanism includes a threshold-based content caching system, a blockchain system for privacy, and an extension of NDN to push-based content dissemination.  
  5 Discussion With the goal of replacing TCP/IP at the network layer, adopting blockchain technology over NDN offers special benefits and applications that will benefit both the blockchain community and established online services. [5]. By focusing on network-level connectivity and adopting “data-driven authenticity” to assure the security of the data’s source, blockchain over NDN prioritizes data over location and ensures real decentralization. Researchers are interested in how specific technologies are emerging. Data retrieval is efficient using NDN, and data security is ensured via blockchain. Some scholars believe using blockchain technology for the current IP would be unwise. Instead, using blockchain technology over NDN may lead to more effective performance [6, 11]. NDN, a hypothetical future Internet architecture, can support blockchain technology, offering a dependable way to maintain databases without central authority. Blockchain over NDN fixes IP network problems and provides a decentralized system, making connecting nodes and synchronizing data simpler. Trust models can be centralized or decentralized; a prior method involved a central credit authority to collect and disseminate reputation values, but this method still entailed communication costs [15].  
  4 Classification of Creative Industry: Cross Country Comparison A comparative analysis was conducted to find differences between the classification of the creative industry in a few countries. It was also referred to by the World Intellectual Property Organization (WIPO), United Nations Conference on Trade and Development (UNCTAD), European (EU) Commission, and United Nations Educational, Scientific and Cultural Organization (UNESCO). The countries include Malaysia, Thailand,  
  Author Index  
  Author Index  
  Author Index
9. ICOCI_3 conference:
This book constitutes the revised selected papers of the 9th International conference, ICOCI 2023, held in Kuala Lumpur,   
 This book constitutes the revised selected papers of the 9th International conference, ICOCI 2023, held in Kuala Lumpur,  
 Author / Uploaded 
  Nur Haryani Zakaria (editor) 
  Nur Suhaili Mansor (editor) 
 Table of contents :  
  Preface  
  ICOCI 2023 Committee  
  Contents – Part II  
  Contents – Part I  
  1 Introduction  
  2 Background  
  2.1 Final Project Management Application  
  2.2 Social Representation Theory  
  2.3 Shame  
  2.4 Risk  
  2.5 The Mechanics of Trust Model  
  2.6 Unified Theory of Acceptance and Use of Technology (UTAUT)  
  3 Research Design  
  3.1 Proposed Hypothesis  
  2.2 ICT Services in Agribusiness  
  3 Theoretical Framework and Hypotheses Formulation  
  3.1 Technology Acceptance Model (TAM)  
  3.2 Hypothesis  
  4 Research Methods  
  5 Conclusion and Future Works  
  References  
  Author Index   
 Citation preview   
  Nur Haryani Zakaria Nur Suhaili Mansor Husniza Husni Fathey Mohammed (Eds.)  
  Communications in Computer and Information Science  
  Computing and Informatics 9th International Conference, ICOCI 2023 Kuala Lumpur, Malaysia, September 13–14, 2023 Revised Selected Papers, Part II  
  Nur Haryani Zakaria · Nur Suhaili Mansor · Husniza Husni · Fathey Mohammed Editors  
  Computing and Informatics 9th International Conference, ICOCI 2023 Kuala Lumpur, Malaysia, September 13–14, 2023 Revised Selected Papers, Part II  
  Editors Nur Haryani Zakaria Universiti Utara Malaysia Sintok, Malaysia  
  ICOCI 2023 Committee  
  Patron Mohd. Foad Sakdan  
  viii  
  ICOCI 2023 Committee  
  Secretariat Alawiyah Abd Wahab Nur Azzah Abu Bakar  
  Intel, Argentina Universiti Kebangsaan Malaysia, Malaysia University Ferhat Abbas Setif 1, Algeria University Ferhat Abbas Setif 1, Algeria University Ferhat Abbas Setif 1, Algeria Flextronics International (Flex), Austria City University, Bangladesh Institute of Applied Physics and Computational Mathematics, China University of Hong Kong, China Amity University, Noida, India Chitkara University, India Chitkara University, India Chitkara University, India Chitkara University, India  
  ICOCI 2023 Committee  
  Nazeer Unnisa Qurishi Ali M. Abdulshahed Pooja Gupta Prateek Agrawal Gulfam Ahamad Prashant Johri M. A. Ansari Swagata Dey Venkatesh Gauri Shankar Bali Devi Vikas Kamra Lalit Kumar R. Raja Subramanian Shrddha Sagar Vikram Kumar Susama Bagchi P. Sardar Maran Amit Kumar Mishra Ade Novia Maulana Apri Siswanto Abdullah Tito Sugiharto Rio Andriyat Krisdiawan Erlan Darmawan Evizal Abdul Kadir Yeffry Handoko Putra Waleed Khalid Al-Hadban Athraa Jasim Mohammed Suhaib Kh. Hamed Mohammed Rashad Baker Firas Mahmood Mustafa Zakho Khalid Shaker Arwa Alqudsi Ramadi Hussein K. Almulla Roberto Vergallo Mohd Nor Akmal Khalid  
  x  
  ICOCI 2023 Committee  
  Mustafa Ali Abuzaraida Bhagyashree S. R. Mohd Hasbullah Omar Rubijesmin Abdul Latif Mohd Helmy Abd Wahab Husna Sarirah Husin Aida Zamnah Zainal Abidin Mohammed Gamal Alsamman Quah Wei Boon Ihsan Ali Abdulrazak Yahya Saleh Rajina R. Mohamed Dalilah Binti Abdullah Shahrinaz Ismail Ruhaya Ab. Aziz Syahrul Fahmy Nooraida Samsudin Norhafizah Ismail Noormadinah Allias Zainab Attar Bashi Ashikin Ali Roziyani Setik Siti Fairuz Nurr Sadikan Safyzan Salim Marwan Nafea Irny Suzila Ishak Abdul Majid Soomro Nor Masharah Husain Nur Intan Raihana Ruhaiyem Nik Zulkarnaen Khidzir Nadilah Mohd Ralim Kavikumar Jacob Fawad Salam Khan Muhammad Abdulrazaaq Thanoon Mohammad Jassim Mohammad Muazam Ali  
  Misurata University, Libya ATME College of Engineering, Libya Universiti Utara Malaysia, Malaysia Universiti Tenaga Nasional, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Kuala Lumpur Malaysian Institute of Information Technology, Malaysia Asia Pacific University of Technology & Innovation, Malaysia Universiti Utara Malaysia, Malaysia Ministry of Higher Education, Malaysia University of Malaya, Malaysia Universiti Malaysia Sarawak, Malaysia Universiti Tenaga Nasional, Malaysia Universiti Kuala Lumpur, Malaysia Albukhary International University, Malaysia Universiti Tun Hussain Onn Malaysia, Malaysia University College TATI, Malaysia University College TATI, Malaysia Politeknik Mersing, Malaysia Tunku Abdul Rahman University of Management and Technology, Malaysia International Islamic University Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Selangor, Malaysia Universiti Teknologi MARA, Malaysia Universiti Kuala Lumpur British Malaysian Institute, Malaysia University of Nottingham Malaysia, Malaysia Universiti Selangor, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Pendidikan Sultan Idris, Malaysia Universiti Sains Malaysia, Malaysia Universiti Malaysia Kelantan, Malaysia Universiti Kuala Lumpur, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Kebangsaan Malaysia, Malaysia Universiti Kebangsaan Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia  
  ICOCI 2023 Committee  
  Khairol Amali Ahmad Juliana Aida Abu Bakar Mohd Nizam Omar Waqas Ahmed Shakiroh Khamis Habiba Akter Noris Mohd Norowi Siti Munirah Mohd Sulaiman Mahzan Shahidatul Arfah Baharudin Pantea Keikhosrokiani Renugah Rengasamy Khalid Hussain Massudi Mahmuddin Mahmood Abdullah Bazel Masitah Ghazali Norhanisha Yusof Saiful Bakhtiar Osman Azliza Mohd Ali Norhasyimatul Naquiah Ghazali Yusmadi Yah Jusoh Jasni Ahmad Azlin Nordin Abdullah Al-Sakkaf Kamsiah Mohamed Mudiana Mokhsin Suhaimi Abd-Latif Sani Salisu Ijaz Ahmad Ghaith Abdulsattar Al-Kubaisi Qamar Ul Islam Abdulrazak F. Shahatha Al-Mashhadani Muhammad Kashif Shaikh Mir Jamal Ud Din Najia Saher Tasneem Mohammad Ameen Duridi  
  xii  
  ICOCI 2023 Committee  
  Krzysztof Marian Tomiczek Abdullah Hussein Al-Ghushami Abayomi Abdultaofeek Fathima Musfira Ameer Mohammed Ahmed Taiye Sasalak Tongkaw Abdulfattah Esmail Hasan Abdullah Ba Alawi Mehmet Nergiz Huseyin First Ismail Rakip Karas Mehmet Sirac Ozerdem Evi Indriasari Mansor Hamzah Alaidaros Munya Saleh Ba Matraf Abdullah Almogahed Abdulaziz Yahya Yahya Al-Nahari Ridhima Rani Hani Mizhir Magid Rohaida Romli Shaymah Akram Yasear Mohd Hafizul Afifi Abdullah  
  are examined in Sect. 4. Lastly, Sect. 5 concludes the discussion and provides direction for future studies.  
  2 Related Work 2.1 Particulate Matter Prediction Models Air pollution is a major environmental concern that significantly impacts public health and the economy. Various techniques have been employed to mitigate air pollution and its related impacts. One approach uses Particulate Matter (PM2.5) prediction models, mathematical and statistical tools to forecast the intensity of fine particulate matter in the air [21]. These models utilize data from various sources, such as satellite imagery, meteorological information, and ground-based measurements of PM2.5 concentrations, to create a spatial and temporal understanding of air pollution. PM2.5 can be emitted from primary sources, such as transportation and industrial activities, or form through secondary sources through chemical reactions with gaseous pollutants. Machine learning models have also been used for PM2.5 prediction, showing promising results. Some of the popular machine learning models employed for PM2.5 prediction are Artificial Neural Networks (ANNs) [1, 8], Support Vector Machines (SVMs) [22, 23], Random Forests (RFs) [24, 25], and Gradient Boosting Machines (GBMs). These models utilize data inputs such as meteorological observations, land use patterns, and satellite images that can help predict PM2.5 concentrations more accurately. It’s worth noting that selecting a model to use in a specific case should be based on the nature of available data and specific prediction goals. Traditional statistical and Machine learning models (ML) have been used in PM2.5 prediction, but they differ in their approaches and assumptions. Conventional statistical models are typically based on predefined algorithms developed using regression analysis or other statistical techniques. These models require that the data meet certain statistical assumptions such as linearity, homoscedasticity, and normal distribution. In contrast, ML models are built on machine learning algorithms that do not involve explicit statistical assumptions. Instead, they learn from data, identifying patterns and relationships without being specifically programmed. They can handle higher-dimensional data, non-linear relationships, and complex data structures. Unlike traditional statistical models, ML models perform better in PM2.5 prediction because they can learn non-linear relationships and assumptions [26]. However, ML models may be considered black-box models that often provide no explanation of how the result was derived and may be difficult to interpret and diagnose. Traditional models are more straightforward, better understand the underlying statistical assumptions, and can be useful in interpreting the nature of the relationship between the predictor and response variable. 2.2 Support Vector Regression Support Vector Regression is a widely held machine learning algorithm to analyze regression based on the Support Vector Machine (SVM) algorithm. It was introduced by Vapnik  
  6  
  Y. Yusof and I. S. Maijama’a  
  properties were analyzed using SPSS, and it was learned that the descriptive features are not linear with the target feature. Following this, the data was also normalized using MinMax scaler due to skewness of the target feature. 3.3 Optimized SVR-ABO Figure 1 illustrates the adapted algorithm for predicting the dataset’s air quality. The input for the deployment is the training data, while the output is the optimal values for SVR and the values of the evaluation measures.  
  Fig. 1. Flow Chart of SVR-ABO  
  M. A. Bazel et al.  
  change, transparent, secure, reliable, and trustworthy in public and private environments [6]. The healthcare sector is recognized as one of the sectors that is a possible beneficiary of BCT adoption [7]. Current healthcare systems confront numerous issues, including security, interoperability, privacy, lengthy processes, delays in diagnosis and treatment, difficulties in sharing information, high operational expenses, data control, and data ownership [8, 9]. Health information is fragmented and challenging to exchange due to diverse standards and formats [10]. Institutions often resist sharing data due to privacy concerns and fears of providing competitors with an advantage. The healthcare data ecosystem is excessively fragmented to meet modern patients’ pressing needs [11]. As stakeholders are encouraged to retain their records, verifying their authenticity and accuracy becomes difficult. This situation leads to substantial server maintenance and security costs [10, 12, 13]. BCT offers a promising solution to these problems [14, 15]. It can address the exchange, integrity, confidentiality, privacy, and interoperability issues inherent in healthcare systems [10, 16]. BCT enables sharing a patient’s medical data with international parties while granting patients control over their information [8, 17]. Due to the high sensitivity of medical data, the use of BCT in healthcare facilitates the secure transfer of patient medical records and enhances healthcare data security and transparency [14, 18]. If BCT is developed appropriately, it can lead to a revolution in healthcare and reshape it to make a stable, trustworthy, protected, and sustainable digital ecosystem for better-quality health data management [19, 20]. BCT is considered one of the top technology trends for health IT, notwithstanding the widespread notion that it is still in its infancy [21]. Despite the great potential that BCT brings to the healthcare industry, the adoption rate is significantly low, with a lack of empirical evidence [19, 22, 23]. However, little research has been done to discover the factors influencing BCT adoption in healthcare organizations [24]. Current academic literature on BCT adoption found that there are research shortages in terms of the factors impacting healthcare decision-makers to adopt BCT, and these factors require additional exploration [7, 14, 15, 25]. Therefore, to deliver the maximum benefit of BCT applications in the healthcare context, more research is required to identify the factors influencing hospitals’ adoption of BCT, which motivated our study. This pilot study aims to verify the validity and reliability of the instrument developed to identify the factors influencing BCT adoption in Malaysian hospitals. The questionnaire was developed by adapting Technology-Organizational-Environment (TOE) framework and related studies constructs’ items. To the best of our knowledge, this study represents one of the earliest attempts to investigate the adoption of BCT in Malaysian hospitals at an organizational level. The healthcare sector in Malaysia is subject to rigorous regulations and stands out for its unique approach to providing services. With a vision to become a digital-first nation, Malaysia’s healthcare industry is experiencing rapid growth, with an emphasis on transferring and developing technology to foster innovations, and BCT is one of the leading technologies driving this growth [26, 27] The government of Malaysia is interested in capitalizing on the assets of the country’s current healthcare system to develop a long-term system that is equitable, effective, and efficient as well as adaptive to the quickly changing environment [27, 28]. Consequently,  
  The Factors Influencing Blockchain Adoption in Hospitals  
  17  
  the healthcare industry must closely monitor changing technology adoption patterns to understand BCT’s possible effects [29]. This paper is planned as follows: The theoretical foundation and proposed model are described in Sect. 2; the methodology used is explained in Sect. 3. The results are presented and discussed in Sect. 4. Lastly, the conclusion is given in Sect. 5.  
  Based on the results obtained from the three methods described above, compelling evidence supports the discriminant validity of the research constructs proposed in the model. Consequently, the measurements have been successfully validated by analyzing construct reliability and validity.  
  5 Conclusion Blockchain technology has the possibility to improve the privacy, security, and authenticity of data, and solve the main issues in the current healthcare sector. However, its adoption is low and several factors may influence the decision. Encouraging healthcare institutions in Malaysia to implement and promote BCT requires an understanding of the factors influencing hospitals’ adoption of BCT. This study validated the instrument developed to examine the impact of technology trust, cost-effectiveness, organization readiness, top management support, competitor pressure, and government support on hospitals’ intention to adopt BCT. Measurements were modified from previous studies to fit the present investigation better. The instrument’s content validity was assessed by obtaining feedback from five academic staff and information system researchers. A pilot test was conducted to examine the reliability and validity of the instrument. The results have provided preliminary support for the model constructs and instruments utilized to evaluate the adoption of BCT in Malaysian hospitals. The reliability of the model constructs was established, with all constructs exhibiting composite reliability and Cronbach’s alpha coefficients that exceeded the recommended threshold. The convergent and discriminant validity analysis exhibited that all constructs have a satisfactory level of validity. As a result, the questionnaire is valid for use in large-scale data collection. In the context of hospitals’ adoption of BCTs, this study is one of the few to evaluate the reliability and validity of a questionnaire based on a TOE framework. This might contribute to a better understanding of BCT adoption in developing countries’ healthcare institutions, specifically Malaysia.  
  the media and popular press as a tool for measuring young people’s intentions to engage in secure behavior related to cloud services [29]. Furthermore, an empirical investigation was conducted to explore the factors influencing employee information security behavior and analyze the relationship between attitude, subjective norm, and perceived behavioral control using related theories and models [26]. 2.2 Factors Contributing to Online Game Addiction Figure 1 depicts the factors that contribute to online game addiction as suggested by Yaacob et al. [2], which include Salience, Tolerance, Mood Modification, Problems, Conflict, Withdrawal and Relapse. Such factors categorize individuals with a strong inclination towards online game addiction, who are often driven by escapism and the desire to escape their real-life circumstances. It also indicates a strong propensity to develop addictive behaviors while engaging in online gaming, which may lead to negative habits, such as stress, anxiety and other distractions.  
  Fig. 1. Factors contributing to online game addiction.  
  6 Conclusion and Future Work The study sheds light on the importance of considering protection motivation and the severity of online game addiction in fostering secure intention behavior among adolescents. The findings suggest that protection motivation can positively influence secure intention behavior, while the severity of online game addiction may have a detrimental impact on it. Therefore, interventions addressing both these factors could be essential in promoting safe and responsible online behavior. The findings also highlight the importance of education programs aimed at raising awareness about the risks associated with cybercognition and secure intention behavior. Such programs can play a crucial role in enhancing online safety practices among adolescents. Additionally, counseling and therapy programs may be necessary for individuals exhibiting high levels of online game addiction severity. These programs can address the root causes of addiction and encourage the development of healthy coping mechanisms to ensure better overall well-being. Overall, the study provides valuable insights into the relationship between protection motivation, the severity of online game addiction and secure intention behavior. The findings hold potential to guide the development of interventions that promote safe and responsible online behavior while addressing the negative consequences of online game addiction. Nonetheless, there is still ample room for further research in this field. Future work can build upon these findings and further explore the complex relationships between these variables. Conducting longitudinal studies that track changes in protection motivation, the severity of online game addiction and secure intention behavior over time can provide a more nuanced understanding of these relationships. This can help to identify the specific factors that contribute to changes in these variables over time, leading to the development of targeted interventions. Cross-cultural studies should also be considered to expand the scope of this research. While this study focused on a specific population of late adolescent students in Malaysia, exploring these relationships in different cultural contexts can uncover cultural factors that might influence them. This can lead to the development of culturally appropriate interventions since such studies  
  Abstract. This research paper presents the design and development process of the M-Thyroid Care app, a mobile application prototype aimed at facilitating virtual video call-based consultations for thyroid clinics using design-based research. The app offers a convenient and accessible platform for remote consultations, enabling patients and healthcare providers, especially doctors, to engage through video calls. Additional features like chat functionality and file-sharing capabilities are incorporated to facilitate seamless communication and information exchange between patients and healthcare providers. The motivation behind developing M-Thyroid Care stems from the need to address the challenges associated with unurgent illness physical visits, specifically focusing on follow-up appointments for thyroid disorder patients. It is important to note that the app is not intended to replace physical visits but to assist clinics and healthcare providers in managing their time more effectively, allowing for more in-clinic consultations, particularly for new thyroid disorder patients. By eliminating the need for physical visits, M-Thyroid Care offers several benefits. Patients can save on travel, parking, childcare, and other related expenses, resulting in enhanced convenience and cost-effectiveness. Moreover, reducing physical visits can reduce traffic congestion and emissions, aligning with sustainable environmental practices. In summary, M-Thyroid Care represents a significant step towards leveraging mobile technology to improve thyroid healthcare consultations. The insights gained from the design and development process of M-Thyroid Care contribute to the broader understanding of designing mobile healthcare apps for remote consultations, serving as a valuable resource for future app development in telemedicine and mobile healthcare. Keywords: M-Thyroid Care · Mobile Healthcare App · Virtual Consultations  
  telemedicine apps specific to Malaysia have been developed to facilitate virtual consultations, including DoctorOnCall, BookDoc, MyDoc, HealthMetrics, Teleme, MySejahtera, Naluri, and PingDoc. These apps offer a range of features such as video consultations, chat functionality, e-prescriptions, and access to medical records, providing convenient healthcare services for patients. However, among these apps, MyDoc, developed by MyDoc Pte Ltd, a Singapore-based digital healthcare company, stands out due to its comprehensive platform that can be accessed through iOS, Android, and webbased platforms. In addition to virtual consultations and medical record access, MyDoc includes appointment booking services, health monitoring, medication reminders, and health tips, offering a holistic approach to healthcare management. While other appointment apps in Malaysia provide convenient ways to find and book appointments with healthcare providers, the specific scenario of thyroid clinics presents unique challenges. In thyroid clinics, doctors determine the appointment dates and times, and any available doctors may see patients. Therefore, there is a need to design and develop a dedicated app like M-Thyroid Care with a specialized appointment booking feature to streamline the process and ensure efficient scheduling and management of appointments, addressing the specific needs of thyroid clinics. By incorporating tailored features and functionalities, M-Thyroid Care can enhance the overall patient experience and optimize healthcare delivery in thyroid care.  
  A. H. M. Shabli et al.  
  4 Results This paper focuses primarily on the second phase of the design-based research process, which involves the creation of a solution for the M-Thyroid Care application. During this crucial phase, a mobile application called “M-Thyroid Care” was designed and developed specifically for Android mobile devices. This application’s main goal is to improve the entire experience for both doctors and patients by efficiently addressing the practical issues that thyroid clinics face. 4.1 M-Thyroid Care Application Design M-Thyroid Care has been meticulously designed using a user flow process to establish a clear and structured sequence of steps and interactions for both doctors and patients when utilizing the mobile application. This user flow process serves to illustrate the different paths and decision points that users encounter while pursuing specific goals within the app. Each step in the user flow corresponds to a distinct screen or action that users can engage with during their interaction with the application. The user flow process of M-Thyroid Care encompasses three key stages: login and registration, new appointments, and appointment management. Login and Registration. For the login and registration, the user flow process is illustrated in Fig. 1. The login process is started by the user by launching the M-Thyroid Care app. The user must input their registered information, including their email address and password, in the appropriate boxes on the login screen in order to log in. The user then taps the “Log-in” button after entering the required information. The app then compares the entered credentials with the user data stored in the system to validate them and gives the user access to their main screen if the credentials are legitimate and match the records. This main screen is especially suited to the user’s function within the app, whether they are a patient or a doctor. Users who do not already have an account can register by choosing the appropriate registration option. The user is prompted during the registration process to enter the necessary information, including their name, email address, and preferred password. Once the user submits the registration form, the app starts a phase for email validation. A user can start the password reset procedure in the M-Thyroid Care app by clicking the “Reset Password” link. The software asks the user to input their registered email address when they click the “Reset Password” option. The app validates the email address after receiving it before starting the password reset process. The user’s registered email address is then sent an email with a link to reset their password. The user can access the password reset page on the designated web page by clicking the provided reset link in the email after getting it. The user can then type in and confirm a new password. The app validates the password reset and enables users to log in using their updated credentials once the new password has been successfully set. New Appointment. For the new appointment, the user flow process is illustrated in Fig. 2. In the M-Thyroid Care app, doctors can set up new appointments with their patients by navigating to the current appointment screen. They can perform a patient search, select a time and date, and enter any relevant information. The patient receives an email  
  Designing and Developing M-Thyroid Care  
  Fig. 1. User Flow Process for Login and Registration  
  notification with the appointment information after it has been scheduled. On the app’s current screen, the patient can also see the appointment details. This procedure guarantees effective communication and makes it simple for doctors to arrange virtual consultations with their patients while keeping patients updated on their upcoming appointments.  
  Fig. 2. User Flow Process for User Appointment and Appointment Details  
  In addition to the email and password fields, the interface includes a “Forgot Password” tool to aid users with password recovery. This option is usually given as a distinct button or a clickable hyperlink. Upon selection, users are redirected to a password recovery page where they can initiate the process of resetting their password. This process commonly involves sending an email notification to the registered email address containing instructions for resetting the password and regaining access to the account.  
  Fig. 3. M-Thyroid Care Login Interface  
  Fig. 5. M-Thyroid Care Appointment Details Screen  
  The screen design emphasizes user-friendliness and clarity, enabling patients to access and comprehend their appointment details effortlessly. Patients can readily view the scheduled time and date, ensuring they are well-informed about their upcoming medical consultations. Additionally, any remarks or additional information the doctor provides are displayed, offering patients important instructions or details about their appointments. Distinct from patients, doctors possess additional privileges within the Appointment Details screen. They can modify the appointment status to “Ready” to signal their availability and readiness for the appointment. This status change serves as an indication for patients that their doctor is prepared to engage in a video call or consultation. The M-Thyroid Care application utilizes the WebRTC (Web Real-Time Communication) protocol to facilitate video calls between patients and doctors. WebRTC is a technology framework that enables real-time communication over web browsers without the need for external plugins or software installations. M-Thyroid Care’s integration of WebRTC provides users with a seamless and secure video call experience. WebRTC makes use of a number of standardized communication protocols, including the RealTime Transport Protocol (RTP), the Session Description Protocol (SDP), and the Interactive Connectivity Establishment (ICE). These protocols collaborate to build and maintain  
  Designing and Developing M-Thyroid Care  
  a direct peer-to-peer connection between the patient’s and doctor’s devices, enabling realtime audio and video transmission. There are various stages to integrating video calls utilizing WebRTC within the M-Thyroid Care application. To begin, the application initiates the connection procedure when a doctor sets the appointment status to “Ready” on the Appointment Details screen, indicating their availability for a video call. The SDP protocol is used by the patient’s and doctor’s devices to exchange session descriptions, which include information on media capabilities, network addresses, and encryption settings. Following that, the ICE protocol is used to connect the devices, even in the presence of Network Address Translators (NATs) or firewalls. ICE employs several strategies, such as STUN (Session Traversal Utilities for NAT) and TURN (Traversal Using Relays around NAT), to select the best potential communication network path and solve network traversal challenges. Once the connection is established, the WebRTC framework allows for the real-time transmission of audio and video streams. The RTP protocol is used to packetize and transfer media data between the patient and the doctor, assuring timely delivery. WebRTC also enables advanced features such as adaptive bitrate control and echo cancellation, which improve the quality and performance of the video conversation. From a security standpoint, WebRTC’s built-in encryption techniques ensure the privacy and integrity of the video call from a security aspect. Secure transport protocols, such as Datagram Transport Layer Security (DTLS), are used to encrypt media streams, preventing eavesdropping and unwanted access to the communication. Moreover, patients and doctors can also use a menu button at the bottom of the screen. This menu button provides a variety of functions to improve the user experience. Users can attach photos, files, and videos to appointments, allowing for the effective sharing of medical reports, test results, and other associated materials. This feature encourages patients and doctors to communicate and collaborate effectively, allowing for a thorough grasp of the patient’s situation. In addition, the Appointment Details screen has a chat button, which allows patients and doctors to communicate in real time. This chat functionality promotes effective and convenient communication by allowing users to clear up any doubts, ask questions, or provide extra information about the visit or the patient’s condition. Following the conclusion of a video call session between a patient and a doctor within the M-Thyroid Care application, the doctor is prompted to set the appointment status to “Completed”, indicating the end of the appointment. This action is an important step in the workflow to ensure proper record-keeping and maintenance of the patient’s medical history. Following the completion of the appointment, the session details are made available in the Appointment History screen, allowing for comprehensive and organized documentation. By setting the appointment as “Completed”, the doctor confirms that the session has ended satisfactorily and that all necessary medical interactions have happened. This is an important step in the appointment lifecycle since it marks the change from an ongoing consultation to a closed session. It allows both the doctor and the patient to clearly realize that the appointment’s objectives have been reached. The Appointment History screen is a repository for past appointment details, ensuring a complete record of the patient’s medical history. This screen gives users, including doctors and patients, access to a list of completed appointments in chronological order. Each appointment entry in the history comprises pertinent information such as the date  
  epochs. Our robust model validation methodology incorporates multiple performance metrics.  
  2 Problem Statement The Covid-19 pandemic has significantly impacted global public health, necessitating effective diagnosis [11]. However, despite some relief from the contagious virus, healthcare institutions worldwide still face a significant number of positive cases. In addition, interpreting chest X-rays for Covid-19 detection presents challenges, with previous studies relying on limited data and radiologists’ expertise [8, 12, 13, 17]. Therefore, there is a critical need for more accurate, efficient, and automated methods utilizing diverse and sufficient data. This paper addresses this issue by employing two distinct chest Xray datasets for Covid-19 detection, classifying the data into COVID (+) and COVID (−), and creating separate train and test sets. The model is trained using TensorFlow, Keras, and a deep learning convolutional neural network (CNN) to enhance diagnostic capabilities.  
  4.1 Dataset and Justification This academic study employs two distinct chest X-ray (CXR) image datasets for training and evaluating a deep-learning model for Covid-19 detection Fig. 2. Using two datasets is crucial to ensure diverse representation, enhance model generalization, and validate its performance.  
  Fig. 2. Final selected dataset consisted of images of Covid-19 positive and negative cases, which were used to train and test the model.  
  [email protected]   
  Abstract. BacaDisleksia is an application specifically designed for children with dyslexia learning to read. The application aims to facilitate dyslexic children and ease their reading by carefully considering the Human-Computer Interaction and Interaction Design fundamentals that could facilitate them to read better. However, the design of BacaDisleksia is yet to be empirically confirmed. Therefore, a usability testing was conducted using Tobii eye-tracker to further examine its design. Six dyslexic children as participants were involved in the testing revealing design issues related to BacaDisleksia that can be improved based on eye-tracking data such as heat maps and gaze plots. As a results, this paper presents the eye-tracking usability findings that could inform dyslexia-friendlier design decisions for any application with a similar aim as BacaDisleksia. Such design decisions are crucial in digital innovation to provide better digital solutions for dyslexia and other learners with reading difficulties, in line with one of UNESCO’s aims for having the technology to support inclusivity for children with disabilities, including learning disabilities such as dyslexia. Keywords: Eye-tracking usability · human-computer interaction · interaction design · dyslexia  
  H. Husni et al.  
  BacaDisleksia is an application designed and developed with the aim to ease the reading process by offering the children an interactive, self-assessed intervention for a stress-free reading session. Its fundamentals are based on three Interaction Design (IxD) dimensions [2] – text (1D), visual representation (2D), and behavior (5D) – leaving the other two dimensions untouched, i.e., space (3D) and time (4D). The three dimensions are selected based on the needs of children with dyslexia when it comes to an application for reading, as most of the interaction covers text, visual representation, and the application’s behavior. The other two dimensions are not considered, as each child requires a different time and reads at an individual pace (4D). The interaction with BacaDisleksia is mainly using a mouse and very minimal use of keyboards. Since 3D is concerned with tangible means of control, it is not included, as any changes or improved aspects concern this dimension. Besides IxD, the Human-Computer Interaction (HCI) interaction model also serves as its base for the design, following the classic [3] interaction model. This application aims to provide an interactive reading tool that can facilitate children’s reading better by introducing features that could ease learning to read, such as Irlen color theory [4] for the reading background. In this paper, we explore the potential of using an eye tracker in a usability study with dyslexic children to uncover design decisions we could have missed when designing and developing the application. Such intricate eye-tracking data could potentially be considered to improve further the design aspects, not only on the user interface but also the interaction that comes with it. What are the usability issues that can be uncovered from BacaDisleksia? How does the eye-tracking help in conducting usability testing with children with dyslexia? What design suggestions can be made to improve the interaction and user experience for the children?  
  2 Eye-Tracking for Dyslexia 2.1 Dyslexia Dyslexia is a neurological learning disability that particularly hinders a child’s reading abilities [5, 6]. Despite having normal intelligence, these children frequently read at substantially lower levels than expected. People with dyslexia often struggle with spelling, reading comprehension, and the ability to learn a second language. Nonetheless, these issues are independent of their overall IQ level, as dyslexia is an unforeseen reading challenge for a child who is intelligent enough to be a far better reader. However, children with dyslexia who happen to be slow readers often, paradoxically, are very fast and creative thinkers with strong reasoning [6]. Dyslexia can be inherited in some families, and recent studies have identified a number of genes that may predispose an individual to develop dyslexia. The main focus of remediation for dyslexia should be on the specific learning problems of affected individuals. The usual course is to modify teaching methods, tools, and the educational environment to meet the specific needs of the individual with dyslexia. That said, a good interactive tool should be designed and developed to suit dyslexics to assist their learning. A study by Lebeniˇcnik [7] spells out that the key problems experienced by dyslexic users are confusing page layout, unclear navigation, poor color  
  Eye-Tracking Usability Data of BacaDisleksia  
  before the start of the test. At the beginning of the test, the eye gaze of each participant is calibrated. The calibration process may take quite some time to complete. Participants were advised to follow the following regulation to ensure successful calibration and avoid further issues during eye tracking. The regulations are no glasses, no hat, no physical objects that could potentially get between the eye and the eye-tracker, sitting relatively straight in the chair, does not fidget or move too much, and remains at about 20 to 23 inches from the monitor at all times, and remain positioned in the middle of the monitor at all times. 3.3 The Tasks Five tasks have been identified based on the main functions and purpose of BacaDisleksia as listed in Table 1. With the five tasks, the participants were asked to express their thoughts during the test, following the Thinking Out Loud method. The premise is that users keep up a running commentary to say what they think as they attempt their tasks. This can help to better understand usability issues and suggest solutions to the problems faced. However, the tester has to facilitate the process by asking probing questions, considering that the participants are children with limited expression of thoughts and actions. Table 1. The tasks specified for the usability test using an eye tracker. No.  
  Tasks  
  Fig. 6. Gaze plot and heat map for P2 Task 3.  
  In this study, the usage of an eye tracker in the usability of BacaDisleksia gives a better understanding and comprehension of the usability problems of BacaDisleksia towards dyslexic children. BacaDisleksia is designed and developed to assist dyslexic children to learn to read with their own preferences on the color of the font, the background, and the levels of difficulties, presumably reducing their cognitive load while attempting to read. To determine its usability issues, the evaluation was made to understand problems encountered during the children’s learning session thus improving the design. In addition to the traditional usability study such as the study of effectiveness, efficiency, and satisfaction, the usage of eye-tracker provides data visualization in the form of heatmaps and gaze plots. These two visualization data indicate the amount of time and the focus of the eye gaze spent and its movement. The significant intensity of the heatmap accumulated can be observed from participant P5 upon completing Task 5. The participant spent a lot of time at the word, causing the color intensity to darken thus showing the area of eye focus. The significant gaze plot that shows a lot of eye movement can be found in participant P2 task 5. The eye movement of the participant shows that she encountered problems when trying to find the color palette to change the reading background colour. The color palette’s position is located at the bottom side of the interface and besides the word level difficulties (denoted by three facial expressions). Participant P2 eye movement went all over the user interface rather than going straight to the designated palette to complete the task given. The user interface design may need to be reconsidered; for example, moving the color palette to the side of the user interface as the focus of the children may be  
  Eye-Tracking Usability Data of BacaDisleksia  
  distracted by the facial expressions (level of difficulties icons) at the bottom. Besides, the left side of the interface is not as crowded as the bottom side. Although the children successfully completed all tasks, they were having difficulty operating the software because most dyslexic children are not good at using computers or have less experience using a computer. Most of them are novice users and rarely use computers. This can be shown when some of the children had difficulty typing their names, perhaps due to them having dyslexia and thus making it harder for them to find the alphabet. The arrangement of the alphabet on the keyboard is one of the reasons for the delay in completing the task. The children are used to the sequence of ‘ABCs’ that they need to find the alphabet one at a time. They often encounter typing errors and confusion to differentiate the alphabet, such as ‘D’ and ‘O’. One of the children took almost one minute just to log in to the application even though her name only contains five alphabets. She obviously had difficulty differentiating the alphabet. She confused ‘D’ with ‘O’, ‘A’ with ‘R’, and ‘M with ‘N’. However, most children only encountered difficulty operating the computer while logging in. After logging in, they did not have much difficulty. They can operate the software fairly well. This was proven for participant P5. Even though she had difficulty during login which is the first task, she managed to perform well for the subsequent tasks. She understood the task well and knew how to operate BacaDisleksia application with minimal error. This can also be observed in “Find the word ‘bapa’ and spell it outloud” task (Task 4). All the children managed to complete the task on average at the same time. The dyslexic children find the word ‘bapa’, which only has four alphabets in a two-syllable word. Compared to Task 5, which is to find the words ‘sayang’, although it is a two-syllable word, the second syllable ‘yang’ imposed some difficulty to some of the children comes with the diphthong (‘ng’), making it more challenging to spell and read correctly. Thus, more time is needed to find, identify, and read the word. Therefore, the design of how the word should be presented could be considered to assist dyslexic children in reading longer, more difficult words.  
  5 Design Decisions and Recommendation The eye tracking data, provides valuable and insightful design suggestions for the improvement of BacaDisleksia. Of course, the fundamentals remain, but the user interface could be improved further for better interaction, providing enhanced reading experience. Firstly, the namespace in the login interface. As the blank space to fill in their name use a feature that can fill in unlimited alphabet, by limiting the alphabet, children can only enter their nickname in the space which can prevent them from entering their full name thus reducing the error and can improve their emotional distress. There is obviously the pros and cons to it – on the positive note, it is easier for them to spell out short names rather the long ones; on the other hand, it could also be a platform for them to learn to spell their full name. Anyway, it is a matter of choice and what objective we are aiming for. Technically, both can be easily accomplished. Secondly, the representation of the word. Having different levels of difficulty with the word is a great idea, but how the word is represented is also an important element  
  H. Husni et al.  
  to assist dyslexic children. In addition to different syllables having different colors, adding a hyphen in between two or more syllables can also help children who are suffering from severe dyslexia, as suggested by [17, 18]. This would assist in terms of its visual representation, which further enhanced the separation of the syllables in a word and between alphabets. Dyslexic children find crowded words jumpy, making reading difficult and stressful [19]. Eliminating or reducing stress and cognitive load is important for their successful design. Thirdly, the placement of the color palette for choosing the background color. This feature is considered one of the important factors leading to correct spelling and reading. Based on the eye-tracking data, the current position at the bottom of the user interface has caused some distractions to the children. This is probably due to the somewhat crowded space at the bottom of the user interface with many icons and features. Instead of putting it at the bottom of the page, the background color could be place near the font color (or syllable color) selection located at the right side of the user interface. Re-positioning this color palette would satisfy one of the Gestalt Principles as it groups similar, related features, thus reducing cognitive load in terms of interaction. Such design suggestions could be mapped and summarized into suitable Interaction Design (IxD) dimensions for design recommendation. The IxD dimensions considered include text (1D), visual representation (2D), and behavior (5D). There are also potential considerations of careful design of time (3D) and space (4D) to enhance the learning experience for the children using BacaDisleksia. However, based on the findings and new insights obtained from the eye-tracking usability conducted, much of the design decisions and recommendations fall under 2D and 5D, leaving 1D not so much affected. This suggests improving the representation of the words and the layout of the user interface following Gestalt Principle and mapping it back to what children with dyslexia really need to read better and more accurately using an application. The login user interface suggests considering the third IxD dimension, i.e., time (3D), towards improving the interaction. The time recorded to accomplish each task, as depicted in Table 2, could also point out areas of the interaction that can be further improved. By providing the mapping of IxD dimensions and existing principles in HCI, having the characteristics of children with dyslexia and their specific needs serving as the foundation of such inclusive design, a reading application such as BacaDisleksia could be leveraged as an assistive technology for facilitating reading to these struggling readers. This could lead to positive development in inclusive design for children with learning disabilities, as careful consideration is crucial, and adaptation to such specific needs requires careful design, understanding, and empathy.  
  6 Conclusion Eye-tracking usability could uncover unnoticed issues that potentially lead to insightful design improvement. The eye movement, eye gaze, and the children’s focus can be evaluated with the tasks prepared for them. The study results are presented in the form of heat maps and gaze plots. From the testing, all of the children completed all the tasks given even though a few children took longer to complete the task. The heat map and gaze plot of these participants were analyzed and compared to children with little  
  [email protected]  2 University of Information Technology and Communications, Baghdad, Iraq  
  Abstract. Attention Deficit Hyperactivity Disorder (ADHD) is a frequent neurodevelopmental disorder affecting children and adults, which is routinely diagnosed based on subjective observations and behavioural assessments. Recent advancements in neuroimaging, particularly in resting-state functional magnetic resonance imaging (rs-fMRI), have provided a better understanding of the functional brain network impairments linked to ADHD. The human brain naturally consists of resting-state networks (RSNs) that are spatially distinct and functionally homogenous. Therefore, identifying ADHD biomarkers using the human brain’s RSNs is a promising approach. In order to make accurate statistical inferences in brain science, it is necessary to utilize brain atlases for localizing network-ofinterest (NoIs). However, locating the spatial components of these RSNs using human brain functional atlases poses challenges due to a lack of disease-specific atlases and atlases concordance issues. This research (1) conducts a study and addresses six RSNs that are frequently referenced in ADHD literature: (Auditory-, Cognitive Control-, Dorsal Attention-, Default Mode-, Sensorimotor-, and Ventral Attention-) Networks (2) Introduces a framework that attempts to enhance the generation of ADHD-specific brain reference, named “Hexa-Net”; This comprehensive approach may improve the reliability and applicability of ADHD studies to a fresh level via segregating and integrating the brain into (NoIs) by evaluating predetermined brain atlases. We hypothesize that the Hexa-Net Model can offer a more precise and unbiased method for identifying ADHD-related impairments. As a result, this framework serves as a practical guide for analyzing biomarkers from rs-fMRI scans to aid in diagnosing ADHD. Keywords: ADHD identification Framework · Brain Networks Analysis · Functional Brain atlases · Resting-state networks · Hexa-Net Model  
  Fig. 2. This image clarify that the parcellations defined by different atlases lack consistency and is adopted from [18] to visualize the “Brain Atlases Concordance Problem”: The Superior Temporal gyrus defined at different ways by four different atlases (from left to right AAL1, CerebrA, Hammersmith and Harvard Oxford (HO)).  
  Insufficient attention has been given to standardizing brain atlases [21]. As far as researchers know, no ADHD-specific brain functional atlas or reference is obtained from rs-fMRI data; thus, neuroimaging studies targeted at comprehending the functional connectivity and cognitive performance of the ADHD brain may be restricted without a frame of reference. One challenge arises when different catalogues/atlases assign the same label to RSNs despite their distinct spatial composition. To overcome these challenges with the target of creating an ADHD-specific brain reference, this work suggested an approach in the context of unifying and integrating multiple brain networks from predefined brain atlases into one model, this work proposes a systematic framework for conducting a methodical evaluation of several functional brain parcellations regardless of their generating approaches and the number of  
  D. A. Al-Ubaidi et al.  
  delineated networks to identify and label multi-source RSNs into single brain reference accurately. An introduction to ADHD, its diagnostic criteria, and the contribution of neuroimaging of the disorder diagnosis was provided above; then, the rest of the paper is organized as follows: Sect. 2. Dives into the state of arts on methods identified ADHD-based (RSNs) and the commonly used pre-computed brain atlases. Then Sect. 2 offers a schematic framework for selecting a parcellation tailored to ADHD brain investigations. Additionally, in Sect. 3, the researcher discusses how to use the suggested framework could standardize brain RSNs identification in ADHD research. Furthermore, Sect. 4 concludes the work.  
  2 Related Works 2.1 The Role of RSNs in Identifying ADHD Functional and structural connectivity alterations have been documented in various neuropsychiatric conditions [22]. Evidence revealed that individuals with ADHD experience dysregulation of neural networks (i.e., across multiple brain regions) as DAN, DMN, and AUN, not just specific ones, as proofed by extensive research in this field [23, 24]. This section explores the question of how many networks are thought to have dysfunctional behaviour in ADHD. The exact delineation of brain regions and networks is an essential concept in neuroscience; function, connectivity, architecture, and topography are crucial in defining these regions [22, 25]. Variability in network nomenclature across studies limits the integration of findings. Recent research indicates that ADHD may arise from alterations in brain connectivity patterns, particularly in functional connectivity networks (RSNs) among different brain regions that coordinate cognition and activity [24]. Acknowledging that various atlases may utilize different terminology when describing identical anatomical regions and (RSNs) is essential. In depth, the Cognitive Control Network (CCN) simultaneously known as Fronto-Parietal Control Network (FPCN) or Executive Control Network (ECN) as stated in [26, 27], while Yeo, Krienen [28] used the term Ventral Attention Network (VAN) which is referred to Salience Network (SN) in [29] and Cingulo-Opercular Network in [17]. Furthermore, “Affective or Motivational Network” is sometimes referred to as “Limbic Network”; Sect. 4 provides more information about this challenge. Another issue is that RSNs are frequently categorized by authors who often exhibit biases that arise from their subjective understanding of the network’s spatial or functional configurations. Finally, some atlases offer individual maps for subdivisions within the same RSN, and they require integration to represent the complete RSN; For instance, some scholars have employed the Sensorimotor Network (SMN) to signify the incorporated Sensorimotor and Auditory Networks (AUN). Conversely, others have deemed it appropriate to split it into two discrete (RSNs): the primary SMN and the Auditory Network (AUN) [17]. Three theories have been introduced here, from the perspective of RSNs, to advance the study of ADHD Brain Connectivity. These theories are based on previous studies and aim to build upon existing knowledge:  
  Hexa-Net Framework: A Fresh ADHD-Specific Model  
  7–17  
  Spectral clustering algorithms; the number of final parcels depends on the stability of clustering algorithm;  
  [28] (continued)  
  N. A. M. Sharif et al.  
  that could interpret the visual world, and retinal fundus image is one of the implemented AI imaging subjects [2]. Retinal fundus image captured to define staging of diabetic retinopathy (DR), which is one of the chronic diseases. DR is a complication related to long-term diabetic diseases. It is pathologically affected by prolonged high blood sugar level that causes damage to the blood vessels in the retina. Damaged blood vessels can be seen as enlargement and leakage of the blood vessel. There are two types of DR: nonproliferative (NPDR) and proliferative (PDR). NPDR is a type of DR that is less severe and does not cause symptoms. PDR is a well-known type of DR that creates new fragile blood vessels in the retina [3]. There are a lot of people that have lost their eyesight because of DR. Earlier diagnosis and treatment, DR can be prevented from progressing to blindness. In order to diagnose DR, an ophthalmologist will look for abnormal signs such as haemorrhages, microaneurysms, and hard exudates that appeared on the retinal fundus image [4]. The examination of retinal fundus images is frequently employed in the current day to assist ophthalmologists in gathering essential information about a patient’s retina to detect DR [5]. In Malaysia nowadays, we are facing a shortage of ophthalmologists [6]. With the increasing trend of DR, it burdens ophthalmologists with more workload and fewer staff. Consequently, it contributes to long appointment waiting times and worsened DR staging prognoses. Furthermore, the procedure for treating the DR is usually made through an appointment with an ophthalmologist. Most patients only meet the ophthalmologist when the eye vision has blurring and or even blindness has set in. Problems also appeared as most of the retinal fundus images captured always faced low image quality, such as low contrast, low illumination, and colour inconsistency. Hence, due to aging populations, a shortage of ophthalmologists, and medical sustainability challenges, an efficient and accurate system is needed to make the procedure and the results accurate [7]. Therefore, this system’s existence can help the ophthalmologist make an early diagnosis and prognosis for the patient as the image can be automatically enhanced. This motivational approach may directly increase efficiency and creativity for eye care facilities infrastructure, specifically in rural areas. The research aims to design and construct an AI-based system for eyecare facilities that can improve retinal fundus images for detecting DR patients in rural areas. One of the most significant aspects of screening DR is detecting abnormal fundus images. Some problems, like low contrast and blur image, may seriously affect the diagnostic stages. The authors used three methods for improving the fundus image: Retinex, Contrast Limited Adaptive Histogram Equalization (CLAHE), and Low-Light Enhancement. These methods perform better in handling low-contrast images and blurred images [8]. So, based on this system, ophthalmologists can precisely identify a patient with a DR problem. Hopefully, it will improve systematic DR screening program, human resources, and infrastructure distribution that align with the population’s needs especially in rural areas. This study consists of six topics: introduction, related work, methodology, result and evaluation, and conclusion.  
  An Automated Enhancement System of Diabetic Retinopathy  
  DR classification. The system’s function has been designed and implemented, as well as the system’s interface. The image processing approach is built using MATLAB software. The system has been tested to ensure that there is no bug. HUSM ophthalmologists test the system by employing the system in the last step. 3.1 Image Processing A flow chart of image processing is shown in Fig. 2. First, the fundus camera captured retinal fundus image and converts RGB to Hue Saturation Value (HSV). Retinex, CLAHE, and Low-light enhancement are three types of enhancement methods applied on the converted HSV image. Finally, the visual quality of the enhanced retinal fundus image is measured using the Mean Squared Error (MSE), Peak-Signal Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).  
  Fig. 2. Flow chart of Image Processing.  
  Fig. 12. An overall system performance evaluated by the expert.  
  5 Conclusion and Future Work In this paper, image enhancement methods including Retinex, CLAHE and Low-light were applied to automate enhancing low quality retinal fundus image to support the diagnosis of DR. The results of the experiments demonstrated that DRES (AI-based system) can improve retinal fundus images significantly, using Retinex enhancement method. The results tested in the DRES system showed that Retinex enhancement was the best method. Retinex had the highest performance value compared to CLAHE and Lowlight enhancement. The features using Retinex enhancement such as hard exudates and haemorrhage was clear enhanced rather than original retina image. This study explored different image enhancement techniques for the same dataset to determine the variation of the output image. Although DRES helps overcome issues of related to the lack of ophthalmologists visiting rural areas for elderly and multi-morbid DR patients, the biggest challenge for  
  Kuala Lumpur, Malaysia [email protected]  2 Faculty of Computing, Universiti Teknologi Malaysia, Johor Bahru, Malaysia  
  Abstract. The challenge to dengue prevention lies in sustaining the preventive activity among the community, which commonly occurs periodically, i.e., when there are dengue outbreaks, with health officers under the Communication for Behavioural Impact (COMBI) campaign. In this paper, a mobile application is specifically developed based on the COMBI Behavioural Change model, which focuses on persuading users to adopt and sustain new habits. The application was carefully designed to reflect the stages of behavioural change. Combining both quantitative and qualitative approaches, an experiment was performed over a four-week study with eight participants, and based on the empirical findings, it is found that with careful design, users can be persuaded to take up the new habit. Two traits that are deemed important are understanding the ‘why’, and the trait of being responsible citizens. In addition, at the end of the study, users felt comfortable continuing with the cleaning activities without the application. Thus, more thoughts should be considered for the application to remain relevant in the sustainable phase. Keywords: Dengue Prevention · Persuasive Technology · Behavioural Change  
  Many case studies in tropical and subtropical countries around the world succeed in preventing and controlling dengue through community participation [13–17]. The key to success is the sustainability shown by the community in the activity, and for sustainability to happen, behavioural change must occur, or it would become a periodic event. But behaviour change is not easy to achieve. In Malaysia, many awareness activities have been carried out by the Pejabat Kesihatan Daerah (District Health Office) of every state through COMBI [18, 19]. However, the impact is still very small and unsustainable [20, 21]. Sustainable community participation is paramount to the surveillance system that is a part of the national health information system. By doing so, a harmonized effort across national dengue surveillance systems can inform the critical data of the disease’s burden, which is necessary to assess progress in reaching mortality and morbidity reduction goals [19]. In this study, we designed an experiment to investigate the effect of usage of a mobile application that is designed based on the COMBI Behavioural Change model [22]. In particular, the created features of the application were intended to translate each of the phases we identified for the behavioural change to be initiated, progressed, and sustained. We deployed the application for a four-week user study with 8 participants. Our study explores, in the specific domain of behavioural change, how persuasive applications can create change in user attitudes and sustain it. Both qualitatively and quantitatively, we examine how the features of the application affect the users’ perceptions and awareness of their activities, creating the potential for behaviour change.  
  2 Experiment Design We developed a working prototype mobile application based on the found model derived from self-previous work [22] with 8 users for 4 weeks. In this section, we describe the design of the application and the methods of the experiment. 2.1 System Description 2.1.1 Data In Malaysia, the dengue prevention initiative recommended by COMBI and the Ministry of Health is to spend at least 10 min per week cleaning. Based on this guideline, we collected data on the types of activities performed by each user and the time spent to conduct the activity. We also recorded the images of places involved in the cleaning activity. In addition, to support the eco-system of COMBI at the community level, data on the COMBI team that one joins with its leader are also considered. All recorded data are presented to users, where they can see their performance, either weekly or monthly, i.e., the accumulated time spent on cleaning and the activities performed. If the user is the COMBI leader, he/she can also monitor the accumulated time of his/her members. The ability to review past activities is an important feature as part of a persuasive design strategy [23].  
  Fig. 1. Pre-Contemplation phase (first, second) and Contemplation phase (third) splash screens  
  Contemplation. In this phase, no prevention activity is engaged, but there is already an intention to do so. This phase’s intervention strategies are education, advice, and comparison. The trigger element is the spark. As soon as the Get Started button is clicked, it brings a user to the login screen, where there are options to either sign up with their Google account or proceed with the registration process (Fig. 2 first and second images). Figure 2, the third image, where the user is ready to start, can first explore what he/she does as part of the prevention activity. One can learn more from News, FAQ, and Community pages to set goals and compare how others are doing.  
  Persuading People to Fight Dengue and Sustaining It  
  Fig. 2. Sign up and register as part of the Contemplation phase (first and second images), and the home screen as the Preparation phase (third image)  
  Preparation. User in this phase has started to be involved in the prevention activity, where the intervention strategies are setting goals and comparison. Facilitator is the trigger for this phase. In the set of images in Fig. 3, they show the flow of screens when recording their cleaning activity, by first taking a picture, starting the cleaning time, stopping the cleaning time, and choosing the type of cleaning activity they have performed. Every step of the sequence facilitates the user to accomplish the activity. Competence. In this phase, the user has been consistently involved in prevention activity for a certain amount of time, where the intervention strategies are engagement, communication, and reward. Signal is used as the trigger. Figure 4 shows the community screens where one could connect and engage with other users. Maintenance. The final phase refers to when the user has been consistently involved in prevention activity for a longer period that has broken the old habit. Communication and reward are the intervention strategies applied, and the signal is the trigger. Consistent users can view their past activities (Fig. 5).  
  M. Ghazali et al.  
  4-week Field Experiment. At the beginning of the 4-week study, we created a Telegram group as a platform to touch base with participants. We held an introductory session with the participants via Google Meet to share the aim of the study and provided them with brief information about the application. We informed the participants that we wanted to understand how they would use the application to help fight dengue through cleaning activities. A short survey was distributed at the end of the second week as an interim to gauge the participants’ motivation level. The chronological recorded time and activities for the four weeks may provide evidence of the possible behaviour change on the individual level in a semi-longitudinal manner. We hypothesized the frequent logged time and activity would reflect the participant’s attempt to change his or her habit of dengue prevention. Post-completion Survey. At the end of the 4-week use period, we posted the URL link to the final survey via Telegram. Through the survey, our goal is to evaluate these concerns; (i) user experience with the application and (ii) the transition one had throughout the 5 phases, although we did not explicitly mention the phase to them. In addition, we asked about their current awareness of dengue prevention and whether they will resume this habit.  
  3 Results We obtained the results of the participants at the end of the experiment. Our analysis showed each individual’s persuasion toward behaviour change is varied and gradual. We also present the results of the detailed findings of every phase. 3.1 General Performances Before the study, none of the participants knew much about COMBI cleaning initiatives as part of dengue prevention by the Ministry of Health (MoH) Malaysia. The participants were briefed about the common activities recommended during the introductory session, but they were free to add other cleaning-related activities. Over four weeks, turning over water containers is the most frequent activity. While the least frequent are the bottom four as in Fig. 6, where the participants add these activities to the cleaning activities. Over the course of four weeks, the pattern of the weekly participation is only consistent for three participants: P2, P3, and P6. In addition, what is noticeable from the data (see Fig. 7), all weekly participants did not spend close to 10 min per week as recommended by MoH. Nonetheless, as newcomers who just started, and even more so for the three consistent participants, this can be considered an accomplishment. 3.2 Behavioural Change Phases We are also interested in understanding the experiences the participants went through as they used the application. Without disclosing the phases to the participants, we inquired about their experiences in the final survey as they were at specific stages and the respective application screens.  
  Persuading People to Fight Dengue and Sustaining It  
  Maintenance. Going into the final week, most participants have shown positive changes in their behaviour. There were incidents where they forgot to use the application but continued with the prevention activity. This is a sign of the activity adopted to be sustainable. “…without the app also I will have habits already now as I was doing for 4 weeks. (P3)” “(I) Always want (to) carry out with the apps, but since I just do it straight away, I forget (about) it and just realize after finishing the cleaning. (P8)” To keep it more engaging, another participant wishes the community to be taking a more active role in sharing what they do on the platform. “It was cool to me, and I was eager to see what people posted in the community, I think it would be more fun if we could see people posting their activities to know what they do every week. (P2)” In sustaining the new habit, the participants had mixed reactions to doing the cleaning activities with the application or without. Some who are already comfortable with the cleaning activities do not feel the urge to keep using the application unless more features are added to enhance the experience and make it more impactful. “Yes because my living area had many dengue cases, without the app, it was attractive to me at first since I wanted to learn more about the info to avoid dengue. If there are more updates, news, games, and people involved in the app, I might consider to continue using the app.” (P2) “Yes, I would continue even without the applications as cleanliness is essential to build a safe and healthy environment. However, with the application that is able to record and share the cleaning activities, it is able to spread awareness of dengue diseases in our community, besides motivating them to perform cleaning more. (P4)”  
  4 Discussion The results of our study answered our research question on the role played by an application if properly designed can persuade people and sustain a new habit. We also have discovered other pertinent findings from the study. Here we articulate three remarks derived from the analysis of the result. 4.1 Adopting New Habit Through Persuasion We learned that providing the Why is very essential in the persuasion of behaviour change. This is supported in psychology [24] to get everyone on the same page. Cleaning activities are trivial, but understanding how it impacts dengue prevention gives users a new perspective. We also noticed that self-responsibility is another trait that we found contributed to behaviour change. Seeing how others do and how others can see one’s contribution also play roles in persuading oneself to continue doing the activity.  
  N. F. A. Azmi et al.  
  Heuristic Evaluation, and Remote Usability Testing. Usability testing aims to evaluate digital products’ effectiveness, efficiency, and user satisfaction. All the above methods do not involve eye-tracking to analyse eye fixation to explain human behaviour while using a website. Eye-tracking in a web-usability test is a technique where the eye movement is recorded while the participant performs the tasks given during the test. The eye will not be fixed in one position for too long but normally moves several times per second. The eye-tracking software usually captures the fixation and saccades. The data from the eye tracker are analysed through several methods: gaze replay, gaze plots, and heat maps [3]. The eye-tracking patterns analysed from the data collected will be correlated to the usability problems. Data from eye-tracking can be used to assist in a better understanding of user reactions. The motivation to study the usability of the UUM Student portal is that the students will use it frequently throughout their study period. Problems related to the interface or system identified during the test will help improve the website’s usability. Suggestions can be made to improve and elevate the user experience. This study aims to identify the usability problems of the UUM Student Web Portal through usability testing using eye-tracking data. The second objective is to provide suggestions for improvement to enhance the user experience while using UUM Student Web Portal. This paper discusses the related work in which eye-tracking is used to gain insight into a webpage’s usability. Then, the method, results, and suggestions to improve the usability of the UUM Student Portal are also discussed in this paper.  
  taken to complete a task. The participants spend too much time using the site’s search option to find and register for courses. Thus, the search options must be re-designed to increase Coursera’s efficiency. The participants’ opinions examined the satisfaction of Coursera. Most participants state it was difficult to use, while a group of participants are satisfied with Coursera’s usability. Some factors that reduce user satisfaction are no language support provided and only a part of the website is translated into Turkish. Language support and translation are also essential to improve the usability of the website. In the second part of the study, the eye-tracking study was carried out to support the Coursera usability study. The test took place at the eye-tracking laboratory with three participants who were not involved in the study’s first phase. The participants are also given nine tasks, similar to the first phase tasks. The data from the eye tracker is visualized as heat maps and gaze plots. The data from heat maps and gaze plots shows that the participant did not focus on the search bar, which is the most important function of the task. The statement is made based on the evidence from the heat zones on the heat maps which shows the participants focus more on the menu and content part. These findings suggest that the search bar may need to be re-designed and to be placed in a more visible area. Zardari et al. [6] also use eye tracking and other usability tests to evaluate e-learning portals. Three usability experts were involved in this study for heuristic evaluation using the 10 usability heuristics by Nielsen. From the heuristic evaluation, problems that are related to navigation are detected. For example, when the user browses to any other page, the current page in the navigation bar becomes highlighted to show the current position of the user. But in the portal, the highlighted navigation bar was different from the page that was visible. Nielsen [7] advises that one should create a system that is similar to the real world. For the eye-tracking component of this study, twenty students are involved. The data collected from the eye-tracking session are dynamic gaze maps, gaze fixations and heat maps. The data collected from the eye tracker provides additional information regarding the portal’s user interface (UI) elements. Heat maps showing the higher heat around the menu suggest that the user faced some difficulty to find the desired feature or button. This situation can be associated with the complexity of the interface. It can be concluded as the eye-tracking evaluation can help to locate usability problems regarding its UI. For example, the higher fixations showed signs of confusion and may lead to the discovery of the usability problems of the webpage. Additionally, Goldbergs and Kotval [8] investigate the correlations between eye-tracking metrics and usability problems. Table 1 shows the summary of the eye movement metrics and their related cognitive process or usability problems. Integrating eye trackers in a usability test could enhance the findings. The data visualization from the eye tracking can provide extra information that will lead to the discovery of trends or patterns in eye movement that can suggest the possible usability problems of a website.  
  Number of fixations overall  
  Less efficient search due to sub-optimal layout  
  Repeated fixations  
  Scanpath regularity  
  Search problems due to lack of training or interface layout problems  
  Transition matrix (back and forth between areas)  
  T. S. Li et al.  
  education, finance, and retail [1–5]. Moreover, dashboards have become an indispensable tool for organizations seeking to improve their operational efficiency and elevate their decision-making processes. There has been a growing trend in the adoption of dashboards in the field of education, with universities increasingly utilizing dashboards to facilitate data-driven decisionmaking. Dashboards offer a visual representation of key performance metrics and data, enabling university administrators to identify areas of strength and weakness in various academic and administrative functions. This facilitates effective resource allocation, strategic planning, and decision-making. Furthermore, dashboards can be customized to cater to the specific needs of different stakeholders, such as students, faculty, and administrators. With the increasing availability of data in universities, the trend towards dashboard utilization is expected to continue, with future research focusing on developing more sophisticated dashboard models that incorporate advanced analytics techniques and machine learning algorithms [6–8] to enhance the accuracy and efficiency of data-driven decision-making. The primary objective of higher education institutions such as universities is to provide quality education and produce graduates who are well-rounded. In Malaysia, universities aspire to become centers of excellence for education, leading referral centers for all aspects of education scholarship, and premier resource centers for various studies. However, the current economic situation presents significant challenges for these institutions in terms of achieving their objectives. Despite the challenges, universities in Malaysia are determined to become the world’s top university. To achieve this goal, it is essential to address issues such as the development of a comprehensive decision-making process for students. Students are critical to universities, as they are the fundamental basis of these institutions. Thus, it is vital for universities to prioritize the well-being of their students, considering their function as the primary revenue source, reputation enhancers, key players in the research and innovation framework, future alumni, and deserving beneficiaries of a safe and supportive educational environment, aligned with the social obligations of universities. The top management of universities plays a crucial role in developing appropriate policies and making informed decisions that benefit the students. To address this challenge, higher institutions like universities recognize the need for a platform that can provide the university’s top management with a well-organized and systematic presentation of the university and student data. An innovative approach such as the business intelligence approach is necessary to provide the required perspective to solve the problem at hand. The business intelligence approach encompasses the processes, technologies, and tools that organizations use to collect, analyze, and present data that enables the analysis of information to optimize decisions and performance [9, 10]. The aim of this research paper is to present a new university dashboard model that integrates all essential components of student data on campus, implemented in a higher education institution in Malaysia. This model will provide extensive and regular monitoring of various student data segments, enabling top management to gain a more comprehensive perspective on the students’ condition on campus. To ensure the usability of the dashboard model while balancing the target audience’s needs with technicality and  
  University Student Dashboard  
  information, we will evaluate it using the Datus model. The Datus model is a comprehensive framework that supports organizations in designing and implementing effective dashboards, considering multiple factors such as data sources, data quality, visualization techniques, user requirements, and performance indicators. The remaining sections of this paper are organized as follows: Sect. 2 presents related works to address the issues of existing methods in designing a university dashboard model. Section 3 outlines the proposed dashboard model and its methodology for developing the dashboards. Section 4 presents and discusses the procedure of evaluation and the evaluation results of the proposed models in detail. Finally, Sect. 5 provides concluding remarks, including a discussion of potential future research areas.  
  2 Related Work Mihaela et al. [11] presented a performance dashboard model designed to enhance the efficiency of measuring, monitoring, and managing organizational performance for executives. Their work included a SWOT analysis to implement a performance management system and dashboard for Romanian universities, currently undergoing significant changes to remain competitive in the market. The study focused on the critical components of a performance dashboard that can provide accurate and timely information to university management. Although the proposed dashboard model covered six dimensions of university focus, including research, finance, business processes, staff and workplace satisfaction, student teaching and learning, and faculty, the authors did not provide detailed explanations for refining each component. The student segment, for example, only covers a few parameters such as enrollment, retention rate, student outcomes, and students living on campus, which does not offer a comprehensive view of student life on campus. The dashboard’s requirement was considered weak due to the absence of a user acceptance test model to confirm its relevance. This study proposes a well-constructed requirement for the university’s performance dashboard, including all necessary and accurate information related to students on campus. The proposed dashboard model evaluates and refines each focus of the student through a series of processes with top management, leading to high user acceptance. Erna et al. [12] introduced a tactical dashboard model designed to present program study information at a university in Indonesia. The primary objective of this research was to develop a dashboard model that emphasized accurate presentation of pertinent information based on the university’s requirements. While the proposed dashboard model describes a well-refined process for identifying the appropriate parameters, the refinement process was based on assumptions and not on any specific dashboard development model or collection requirements obtained from targeted audience feedback. As a result, the proposed model is considered inadequate, as it did not receive proper evaluation to ensure it met the needs of the intended audience. Therefore, in this proposed study, the authors aim to incorporate targeted audience feedback to create a stronger foundation for developing a suitable dashboard for top management. Additionally, the study aims to balance the needs of top management with the technicality of the dashboard and the information displayed on it.  
  T. S. Li et al.  
  Mohd Tuah et.al [13] introduced a smart system that uses data analytics and a dashboard to facilitate university students’ self-monitoring, progress tracking, and management of important information related to their final year project. The system was developed using a Rapid Application Development methodology to create the prototype. To ensure that the dashboard meets user requirements, the author utilized Technology Acceptance Model (TAM) to measure system acceptance and user behavior intention in using the proposed system. Although TAM has shown significant impacts, it does not fully account for social and contextual factors that may influence technology adoption. TAM primarily focuses on individual users’ perceptions of the technology’s usefulness and ease of use, ignoring how social context and relationships between users and their peers may affect their adoption behavior. Therefore, an alternative model, Datus, was utilized in this paper, which recognizes social influence and trust as critical factors that may influence technology adoption. Datus considers the influence of interpersonal and social network relationships, providing a more comprehensive view of user behavior and representing user feedback towards the dashboard’s quality and usability more accurately.  
  3 Methodology This section outlines the proposed dashboard model development process, which employs Agile methodology to achieve research objectives. The Agile methodology was chosen for this research project due to its suitability in addressing the challenges of changing requirements from stakeholders. The ability for universities to adapt quickly to the uncertainties and fast-paced changes in business is crucial for improving efficiency and meeting stakeholder expectations [14].  
  Fig. 1. Agile software development methodology  
  The Agile methodology employed in this research involved five stages: brainstorming, design, development, quality assurance, and deployment. These stages are illustrated in Fig. 1, and their particulars are as follows: Brainstorm: In the requirement gathering phase, the university’s top management was involved in three iterations of the process. Meetings were held with various stakeholders to identify the necessary information for the dashboard. The framework draft was subjected to an initial iteration, followed by gathering feedback from the strategic and corporate planning division staff member from a higher education institution, and then underwent subsequent revisions. Further feedback was obtained to create the final framework, which formed the basis for the storyboard design.  
  University Student Dashboard  
  in our lives, it is essential to educate Internet users from a young age about Internet safety issues [2]. By fostering a solid cybersecurity knowledge and practices foundation, young people can better navigate the digital world and reduce their vulnerability to potential threats. In addition, Information Technology (IT)-related organizations particularly need robust cybersecurity awareness [3]. Employees in these organizations should be equipped with the necessary skills and knowledge to identify, prevent, and respond to cyber threats effectively. By implementing comprehensive CSA programs in educational institutions and IT-related organizations, it can foster a culture of security and vigilance that empowers individuals to protect their digital assets actively. Investing in cybersecurity education and awareness benefits individuals and strengthens the overall security posture of organizations and the broader digital ecosystem. Malware awareness is paramount among higher education students, as cybercriminals often target them due to their limited experience and knowledge in dealing with digital threats [4]. In addition, the prevalence of malware attacks on college and university networks has been steadily increasing, with institutions reporting numerous incidents involving ransomware, phishing, and other forms of malicious software [5]. Given the vast amount of sensitive information stored in educational institutions, students must be well-informed about the risks and types of malware and how to protect themselves [2]. Therefore, educational institutions are critical in equipping students with the necessary skills to identify and avoid malware threats [6]. By incorporating malware awareness into their cybersecurity curriculum, higher education institutions can ensure that students develop a solid understanding of the primary attack vectors and the proactive steps they can take to secure their devices and data [4]. Furthermore, regular training sessions and workshops can help students stay up-to-date with the latest malware trends and defense strategies [7]. The lack of malware awareness among students can have severe consequences for individuals and higher learning institutions [8]. On a personal level, students unaware of malware prevention strategies risk exposing sensitive data, such as academic records and financial information, to cyber criminals [6]. Malware attacks can lead to identity theft, financial loss, and damaged reputations, affecting students’ prospects and wellbeing. Higher learning institutions also suffer from the consequences of inadequate malware awareness [9]. Generally, cyberattacks can compromise institutional data, disrupt academic activities, and erode the institution’s credibility. Furthermore, the financial costs of recovering from a cyberattack can be immense, diverting resources from essential academic programs [7]. Given these potential consequences, this study aims to investigate and develop suitable and relevant malware awareness programs for higher-education students. Although organizations conducted awareness programs, limited studies were conducted on the effectiveness of the materials and the methods of conducting these programs. While the importance of malware awareness is recognized, there is a need for more comprehensive research that evaluates the specific materials, resources, and instructional approaches employed in these programs. On the other hand, equipping the students with the knowledge to recognize and prevent cyber threats can foster a more secure digital environment, safeguard personal and institutional data, and promote academic success [10]. Investing in the research and  
  N. Katuk et al.  
  approximately ten to twelve minutes, ensuring a concise and focused material delivery. Subsequently, a ‘Question and Answer’ (Q&A) session was conducted, fostering open dialogue and addressing students’ concerns or queries. Finally, post-test questionnaires containing identical questions to the pre-test were administered to the students after the Q&A session. The post-test evaluated the student’s knowledge and understanding of malware prevention strategies due to exposing them to the malware awareness material. They were also encouraged to provide open-ended comments on the awareness program. This valuable feedback would inform the program’s continual refinement and improvement for future iterations. Figure 2 illustrates the procedure for the malware awareness program.  
  Fig. 2. The procedure of the awareness program  
  The results of the pre-test and post-test assessments of the malware awareness program provide valuable insights into the program’s effectiveness in enhancing knowledge of malware prevention strategies among higher learning institution students. The results show that the program has successfully improved students’ knowledge of various aspects of malware prevention. In addition, the improvements in the scores across the ten questions indicate that the program has effectively addressed misconceptions and enhanced knowledge in areas such as identifying security attacks, understanding malware-spreading mechanisms, and avoiding risky online behavior. The effectiveness of the malware awareness program can be measured by the significant improvements in the post-test scores compared to the pre-test scores. The results highlight the program’s success in achieving its objectives and providing students with the necessary knowledge and skills to protect themselves from malware attacks. The program’s success can also be attributed to its focus on delivery techniques, such as group discussions and presentations. These techniques have been proven effective in enhancing knowledge retention and promoting active learning [18]. Overall, the results of the pre-test and post-test assessments demonstrate the effectiveness of the malware awareness program in improving college students’ knowledge of malware prevention strategies. Again, the program’s success can be attributed to its engaging delivery techniques, which have enhanced knowledge retention and promoted active learning. In conclusion, the malware awareness program has enriched college students’ ability to recognize and combat malware threats. Furthermore, by addressing the students’ diverse levels of understanding, the program has successfully cultivated a more secure and informed digital environment for its participants.  
  5 Conclusion In conclusion, the malware awareness program has successfully improved college students’ understanding of malware prevention strategies. The significant improvements in the post-test scores compared to the pre-test scores highlight the program’s efficacy in addressing misconceptions and enhancing knowledge in various aspects of malware prevention. The results also emphasize the importance of engaging delivery techniques, such as group discussions, case studies, and interactive presentations, which have been proven effective in enhancing knowledge retention and promoting active learning. However, while the malware awareness program has achieved notable success, there is room for future improvements to ensure that students are well-equipped to protect themselves from the ever-evolving landscape of cyber threats. One area of potential future work could be the development of more advanced and adaptive learning modules that consider the students’ existing knowledge and tailor the content accordingly. It would provide a more personalized learning experience and could help improve students’ understanding of malware prevention strategies. Additionally, hands-on activities, such as simulated malware attacks or practical exercises in identifying and mitigating security vulnerabilities [19], could further enhance students’ skills and reinforce the knowledge gained from the program. It may also be beneficial to explore integrating the malware awareness program into the core curriculum of college courses, ensuring that all students have access to this essential knowledge  
  5 Conclusion In our study, a comparative evaluation was carried out to analyze and compare existing works in ITS, specifically to gauge how feedback was provided to students and the technique used in modeling the student model. Table 1 has depicted the feedback and hints and the techniques used in various ITSs to support various purposes of student learning. ITSs in the programming domain were created with the goal of assisting in the teaching of programming concepts and algorithms, such as using pseudocode and flowcharts, as well as providing proper guidance for students to write code correctly. The results of the comparative evaluation point to the use of Constraint-Based modeling, Model Tracing, Natural Language Processing, and Deep Learning as student model techniques. While delayed feedback is the most appropriate feedback for helping students learn to code, previous research on The LISP Tutor [5] indicates that instant feedback is also beneficial. However, the feedback message may distract the students, causing them to miss out on the experience of a real programmer. On the other hand, ITS in Computer Science education are primarily designed to assist students in learning programming concepts and algorithms such as pseudocode and flowcharts. Clustering techniques such as Bayesian networks are frequently recommended for these ITS. Many researchers have used this technique, which has been shown to help students by providing appropriate feedback. In ITS, both immediate and delayed feedback can be used to help students learn more effectively.  
  Analysis of the Effectiveness of Feedback Provision  
  exchange of ideas [17] and encourage the creation of innovative and imaginative solutions from diverse perspectives, resulting in a broad and varied range of ideas. Brainwriting potentially reduces the impact of status differentials, interpersonal conflicts, domination by a few members, pressure to conform to group norms and deviations from the main topic [14]. Additionally, it may eliminate production blocking, minimize social loafing, and encourage thoughtful processing of shared ideas [18]. Van Gundy [19] categorized Brainwriting into six types: Nominal Group Technique (NGT), Collective Notebook (NCB), Brainwriting Pool, Pin Cards, BattelleBildmappen-Brainwriting (BBB), and the Successive Integration of Problem Elements (SIL) method. According to van Gundy, the first four techniques are pure Brainwriting since they do not involve group discussion of written ideas during the idea-generation process. In contrast, the last two techniques (battelle-bildmappen-Brainwriting and the SIL method) are hybrids as they incorporate oral brainstorming and silent Brainwriting. Brainwriting is a robust approach to promoting idea generation and collaboration, which can result in more innovative and effective solutions.  
  3 Research Methodology As discussed earlier, this study aims to improve the design of Portal GREaT. The UX approach was used in this study to generate ideas for improving the web application. Therefore, this study conducted a simple e-voting session and a Brainwriting workshop to achieve the goal.  
  Azure. Therefore, the color of the portal will be improved by applying Azure. Apart from that, the study also held a workshop in which participants used the Brainwriting method to generate ideas for improving the web application. A UX research expert led the workshop, and 24 participants from the technical staff are currently working on our project. They had at least three years of experience in web application development. The following section will discuss the findings obtained from the workshop.  
  4 Discussion The goal of the workshop was to put the Brainwriting method into practice. As a result, the following procedures were followed: 1. Defining the Theories The majority of the participants are software developers. They have good programming skills but lack web design. Therefore, the workshop began with the instructor explaining the concept of web design and UX. Then, it continued with an explanation of some characteristics and principles of good design and the importance of UX. The instructor also explained the Brainwriting method and how it can be used for ideas generation. 2. Group Formation Following an introduction to the concepts and theories, the participants were divided into six (6) groups of four (4). 3. Explanation of Problem The project leader was asked to explain the problems that needed to be solved. The leader stated that the application needs to be improved. The existing application was created ad-hoc, and the requirements are constantly changing. As a result, the application’s goal is unclear, and the design is unstructured. When the group clearly understands the problem, they can begin the Brainwriting process. 4. Idea Generation Each team was given a piece of digital paper and asked to write down their ideas using Miro.com as a support tool for implementing the Brainwriting method. Each group is assigned a duration (5 min) to record their ideas on digital platforms. When the timer goes off, each team passes their sheet of paper to the group next to them. The following group reads the ideas and adds their own, building on what has been written previously. This process is repeated until each group has had an opportunity to contribute to each idea. Figure 2 depicts the outcome of this step. 5. Reviewing the Ideas Every group will have a collection of ideas refined and improved by multiple group members by the end of the Brainwriting session. When the Brainwriting session is finished, the group can go over the ideas and try to understand the suggestions made by each group. Table 1 shows the ideas generated by each group.  
  Unlocking the Potential of Enhancing User Experience in Portal GREaT  
  Fig. 2. Findings from the idea generation step  
  6. Prioritization of the Idea After everyone has grasped the ideas generated by each group, all participants are asked to evaluate and prioritize the ideas. The ideas are prioritized based on “Most Rational”, “Most Delightful”, “Darling”, and “Long Shot”, which reflect their feasibility, impact, and alignment with the problem statement. “Most Rational” refers to a decision or action based on logical and objective reasoning without being influenced by emotions or biases. It is the most sensible and reasonable option available. While “Most Delightful” refers to something extremely pleasing, enjoyable, or satisfying. It is the most pleasurable and delightful experience one can have. Then, “Darling” is an endearing term to describe someone or something cherished or beloved. It is often used to express affection or fondness towards a person, pet, or object. Finally, “Long shot” refers to a possibility or outcome that is unlikely or has a low probability of success. It is a risky option or an unlikely event worth attempting but may not be expected to succeed. Figure 3 depicts the outcome of the prioritization process. According to the outcome of this process, most participants agreed that the idea generated by Group 2 is feasible and necessary to implement. Group 2’s concept focuses on improving the design by focusing on various user categories. Users are divided into three groups: graduates, industry, and public users, with each group having different module or feature requirements. Furthermore, the banner size must be reduced. On the other hand, the idea from Group 6 is considered a “Long Shot” where the group proposed additional functions to assist users with disabilities. Adding this functionality is a good idea. However, it may necessitate extra time and budget to implement. In the future, the group can include this functionality, where the features can be identified by  
  Abstract. STEM education is vital in today’s learning curriculum. Incorporating Design Thinking into STEM projects such as smart farming, particularly in rural schools in Malaysia where access to technology and resources are scarce, allow teachers and students to experience an enriching teaching and learning experience. In this paper, we employ the Design Thinking approach to gauge the effectiveness of implementing STEM projects such as smart farming to the teachers and students in rural schools. A survey is carried out to measure the effectiveness of the approach and it is shown that the Design Thinking approach helped both teachers and students to develop innovative solutions to benefit the society. Students and teachers also demonstrate increased creativity, problem-solving skills, empathy, and collaborative learning when they solve STEM problems using the Design Thinking approach. Keywords: STEM Education · Internet of Things · Smart Farming  
  To address this shortcoming, the MCMC-UUM MakerSpace Laboratory in Universiti Utara Malaysia (UUM) have designed STEM-related projects to inculcate interests among teachers and students alike. Two schools namely Sekolah Kebangsaan Telaga Mas (Telaga Mas National Primary School) and Sekolah Kebangsaan Sungai Batu (Sungai Batu National Primary School) were chosen to be the pioneer in this project. A smart farming project was initiated in each school where both students and teachers were given the exposure on developing and implementing smart farming based on the Internet of Things (IoT) technology based on the Design Thinking [2, 3, 10] approach. The teachers and schoolchildren were assisted by student volunteers from UUM. In this paper, we document the experiences of seven teachers and 20 schoolchildren from both schools, as well as 12 UUM student volunteers who participated in this project. A survey based on the phases in Design Thinking was adapted to gauge the participants understanding on project implementation and reflection from each of them is recorded to gain better insights on the project. The experiences the teachers, schoolchildren and student volunteers undergo would set a pathway and best practices for school and small community development, as well as deliver actionable and sustainable solutions which benefit the community. The research questions and research objectives related to the STEM smart farming project aim to gauge the following: Research Questions 1: How effective is the Design Thinking approach in allowing participants to understand societal problems and formulating a smart farming solution? Research Question 2: How effective is the Design Thinking approach in nurturing creativity, innovation and problem solving among participants of smart farming project? Reflecting on the research questions above, the following research objectives are outlined: Research Objective 1: To investigate the awareness on Design Thinking approach among teachers and students when implementing the smart farming project. Research Objective 2: To investigate the effectiveness of Design Thinking approach in nurturing creativity, innovation and problem solving among teachers and students (participants) in smart farming project. This paper is organized as follows. In Sect. 2, this paper provides an overview of the related concepts of STEM education, IoT-based smart farming and the implementation of smart farming in both schools. In Sect. 3, this paper describes the sampling and instrument used in this study. The findings and results are then presented in Sect. 4, while Sect. 5 discusses pertinent observations from this study. Section 6 concludes the paper.  
  6 Conclusion STEM education plays an important role in both teaching and learning. It is vital that STEM education is expanded to not only in smart farming, but to include a diverse sector which will bring positive change to the community. A structured teaching approach such a Design Thinking presents opportunities for all to explore various problem-solving approach that can bring benefits to everyone. In this paper, we seek to measure the effectiveness of the Design Thinking approach in implementing a smart farming project. Our findings show that Design Thinking help improve learning experiences. This project, however, has its limitations such as a small number of participants and the projects conducted is is limited to smart farming projects only. In our future work, we aim to have a larger sample and involve various types of projects to cover the diverse communities that the MCMC-UUM MakerSpace Laboratory serves.  
  is described through the desire of a lecturer to assess himself through the emoji he sees on the application screen. When a lecturer has the highest degree of importance or rank / often appears in meeting the needs of completing a student’s final project, the representation of the central core will be present, and present peripheral elements related to shame [16]. 2.3 Shame Humans are taught to have shame since childhood. Some forms of shame such as shame to be disrespectful, and shame to do not comply with the norms adopted by a community. The meaning of the word shame shows that shame is closely related to ethics and morality. Shame as one of the basic emotions, continues to develop according to age levels and social changes that occur. For example, the meaning of shame in the younger generation is related to self-doubt, events that cause negative judgments, non-ideal physical appearance, violations of moral principles and inappropriate etiquette [18]. Shame triggers a person to modify his behavior so that it is easier to adapt to the environment. Shame arises when group performance is higher than personal performance [19]. Shame is a determining factor of social behavior [20]. In several studies in the field of psychology, it was found that shame arises when someone judges himself to feel unable to do something [21], feelings of helplessness and failure [22].  
  Fig. 1. Four steps of action research  
  mind when they heard the word’shame in guiding the final project’. The responses are then ranked by the participants according to their degree of importance. The data obtained by using the hierarchized evocation method can be analyzed in stages, (1) lexical analysis and categorization of the attributes that appear. At this stage, attributes that are semantically the same will be categorized in the same category and then their frequency is calculated, (2) statistical processing by calculating the percentage of the frequency of occurrence of the categorized attributes. The result of calculating the percentage of frequency is accompanied by a sequence of degrees of importance or ranking, which is used as a reference to determine the central core of social representation of “embarrassment in guiding the final project through the ELISTA application”. The responses that often appear and get the highest order in the degree of importance are the central core of social representation, while the responses that appear infrequently but get the highest order in the degree of importance and the responses that occur frequently but get low rankings are not counted as the central core of shame. 3.1 Pre-action There are two activities carried out in the pre-action. The first is to determine the type of response that will be counted as response time. This is important to identify because not all responses from lecturers are counted in response time. The second is to prepare an algorithm for calculating the lecturer’s response time. The pseudocode to calculate the average response time has input in the form of requesting guidance from students, response times from lecturers, other response times if there are responses from students. Lecturer responses are classified in the form of text and text accompanied by files. The time that will be calculated as response time is presented in the following table (Table 1).  
  Enhancing Supervisor Response Time  
  J. Marzal et al.  
  Meanwhile, the calculated baseline component includes the number of applications for supervising, the number of lecturers who have used the Elista application for supervising, the number of lecturers with a small response time of or equal to 2 days, between 2 days to 4 days, between 4 days to 6 days, between 6 to 8 days, and large from 8 days. 3.2 First Cycle This research aims to address the issue of extended response times among lecturers. To achieve this, the researchers first categorize the lecturer response times based on their speed of response, relying on past experience to determine the classification. Typically, the supervising lecturer provides guidance once a week. After processing the data on lecturer response times, specific actions were undertaken to tackle the problem.: 1. Create a feature to display the response time with the information submitted are: • If the average response time is small equal to 2 days, then the message that appears is Very Good in the green box, • If the average response time is more than 2 days and up to 4 days, the message that appears is Good in the blue box, • If the average response time is more than 4 days and up to 6 days then the message that appears is Normal in the yellow box, • If the average response time is more than 6 days and up to 8 days, the message that appears is the response time is Poor in the orange box, • If the average response time is more than 8 days then the message that appears is the response time is Very Poor in the red box, • And if you don’t have a response time, the message that appears is “you don’t have a response time” in the black box. 2. To strengthen the message about the response time zone, the service zone is equipped with text and emoji. Emojis can express emotions [24], and feelings easily [25]. 3. Disseminate the feature to all lecturers through various WhatsApp groups of lecturers and leaders. After the socialization is carried out, the response time reflection feature is applied within one month. The subjects in this study were all lecturers. At the end of cycle one, the researcher re-measured the response time and the number of lecturers who used Elista as a guidance tool. Figure 3 is an example of the response time display on a supervisor’s user account. 3.3 Second Cycle Based on the evaluation of the first cycle, the thing that will be done to increase response time is to display personal response times and peer response times on a supervisor’s account. It is hoped that with friends in one work unit seeing the response time, everyone will try to increase their respective response times. The information displayed is the name of the lecturer, the number of responses, the time, and the response zone. With this display, people may try to manipulate their behavior to avoid embarrassment by speeding up response times.  
  Enhancing Supervisor Response Time  
  Fig. 4. Percentage of users and response time per cycle.  
  From Fig. 4 above, it can be seen that there is an increase in the percentage of lecturers who have a response time and the percentage of supervisors who have a response time of a minimum normal category. Based on this graph, it can be temporarily concluded that displaying personal response times on a supervisor’s personal account that can be seen by the lecturer or colleagues in a study program can increase the response time. Supervisors will try to increase their response time so they do not feel embarrassed if they are labeled as slow lecturers in responding to the needs of student assignments. Meanwhile, the display of group response times will also encourage supervisors to increase their response times. This is based on group shame where the lecturer jointly avoids group shame. As a triangulation of the conclusions obtained above, an open questionnaire was distributed to the supervisors. This activity aims to find out the central core of shame in managing the final project. The response percentage is called high if it is greater than 10%, and the response ranking is called a high category if it is smaller than 2. These criteria are determined arbitrarily. Central core shame is the response of respondents who belong to the category of high percentage and high ranking. Based on the information in the following table, the central cores in the management of the final project are: 1) embarrassed if the personal response time is red, and 2) the work unit response time is red (Table 2).  
  Enhancing Supervisor Response Time  
  Table 2. Central core of shame in managing the final project High rank ( = 2)  
  final projects. By prioritizing response times and fostering effective communication, educational institutions can create an enriched and streamlined learning experience that benefits both lecturers and students alike. As we move forward, it is essential to embrace these findings and explore innovative approaches to further enhance academic practices and elevate the overall quality of education through platforms like ELISTA.  
  Abstract. School disengagement is one of the pressing topics for educational equity in many countries worldwide, which will lead to school dropout. Dropping out of school has adverse consequences, including negative effects on employment, lifetime earnings, and physical health. Several attempts have been made to solve engagement issues, for example, the development of an intelligent tutoring system (ITS), which is useful to know when a student has disengaged from a task and might benefit from a particular intervention. However, predicting disengagement on a trial-by-trial basis is a challenging problem, particularly in complex cognitive domains. This paper emphasizes the MyBuddy mobile application developed as a smart classifier to identify the level of school disengagement risk among at-risk students in secondary schools. The working engine of MyBuddy is translated from a computational model that comprises fourteen predictors of four main entities: student, family, school, and surroundings. This application employs advanced mathematical models to analyze the data and generate risk scores indicating the dropout likelihood. This empowers counselors to proactively intervene and provide targeted support to the students who require it the most. MyBuddy offers a centralized platform to access and analyze aggregated data from multiple schools. This functionality allows them to smartly identify patterns, trends, and systemic challenges contributing to dropout rates. With comprehensive datadriven insights, district and state officers can design effective interventions and allocate resources strategically to mitigate dropout risks. Utilizing MyBuddy to revolutionize dropout prevention and foster a more inclusive and equitable education system in Kedah, Malaysia, aligns with the fourth Sustainable Development Goal, which aims to provide quality education for everyone. Keywords: school disengagement · school dropout · school disengagement detection · smart classifier  
  N. ChePa et al.  
  3.1 Enhancing Student Engagement Mobile apps can significantly enhance student engagement by providing interactive and immersive learning experiences. These apps often incorporate gamification elements, multimedia content, and interactive quizzes, making learning more enjoyable and motivating for students. By offering personalized and adaptive learning experiences, apps can cater to individual learning styles, preferences, and paces, promoting active participation and deep understanding. Furthermore, mobile apps enable anytime, anywhere learning, allowing students to access educational resources and engage in learning activities beyond the confines of the traditional classroom. This flexibility promotes self-directed learning, encourages exploration, and fosters a sense of ownership over the learning process. 3.2 Improving Academic Performance Mobile apps offer various features and functionalities that can support and improve students’ academic performance. Educational apps can provide access to a wide range of subject-specific content, including textbooks, lecture notes, interactive simulations, and educational videos. By presenting information in engaging and accessible formats, apps can facilitate comprehension, reinforce learning, and encourage critical thinking. Moreover, mobile apps can facilitate communication and collaboration among students and between students and educators. Features like discussion forums, chat functionalities, and collaborative projects enable students to exchange ideas, seek clarifications, and engage in peer learning. This collaborative aspect fosters a sense of community and promotes deeper learning. Additionally, mobile apps can incorporate assessment and feedback mechanisms, allowing students to monitor their progress, receive immediate feedback, and identify areas for improvement. Adaptive learning apps can dynamically adjust the difficulty level and content based on student’s performance, ensuring targeted and personalized learning experiences. 3.3 Providing Support Services Mobile apps are crucial in providing access to support services and resources that enhance student well-being and success. These apps can include features such as virtual tutoring, homework help, academic planning tools, and career guidance resources. Such support services are particularly beneficial for students who may face barriers to accessing traditional support systems due to geographical or socioeconomic constraints. Furthermore, mobile apps can facilitate communication between students, educators, and support staff. Real-time messaging, online consultation, and remote access to counseling services enable students to seek guidance and support whenever needed, promoting their emotional well-being and overall academic success.  
  4 Design and Development of MyBuddy This section explains the design and development process of MyBuddy, a mobile application aimed at identifying at-risk secondary school students in Kedah, Malaysia, who are likely to drop out. The paper discusses the user-centric approach adopted in the design  
  Early Detection of School Disengagement Using MyBuddy Application  
  N. ChePa et al.  
  24 November 2022, 10 August 2022, and 6 October 2022, respectively. To begin with, low-fidelity wireframes and mockups were developed to visually represent the app’s overall structure, layout, and navigation. These initial prototypes served as a foundation for gathering feedback and insights from the end users. The prototypes were then shared with school counselors, district officers, and state officers, who were the primary users of the MyBuddy app. Through interviews, focus groups, and usability testing sessions, the users were encouraged to explore the prototypes and provide feedback on various aspects of the design, functionality, and user experience. The feedback obtained from the users was meticulously analyzed, considering their suggestions, concerns, and observations. The development team carefully considered the feedback and identified areas where improvements could be made to enhance the app’s usability, effectiveness, and user satisfaction. This analysis phase involved comparing the users’ expectations and preferences with the existing design, allowing a deeper understanding of the users’ needs and goals. Based on the insights gained from the feedback analysis, subsequent design iterations were created, incorporating the suggested improvements and addressing the identified issues. These iterations gradually refined the app’s design, ensuring it aligned more closely with the users’ requirements and expectations. The development team actively engaged with the users throughout the iterative process, fostering a collaborative and participatory environment. Regular feedback sessions and usability testing allowed for ongoing dialogue and knowledge exchange between the development team and the end users. This iterative feedback loop ensured that the design decisions were grounded in user insights and that the app evolved in response to user needs. Figure 1 illustrates selected interfaces of MyBuddy application. By embracing an iterative design process, MyBuddy benefited from continuous user input, enabling the development team to create a user-friendly interface and improve the overall user experience. The iterative approach allowed for a gradual refinement of the app’s design and functionality, ensuring that it effectively addressed the needs and constraints of school counselors, district officers, and state officers involved in dropout prevention efforts. 4.3 Development of MyBuddy Application MyBuddy prototype is developed based on the requirements garnered during requirement gathering sessions. Sketches and wireframes with a low level of detail were developed to illustrate the underlying architecture of the application as well as the user flows. Subsequently, a high-fidelity prototype was meticulously developed using Android Studio, allowing for a vivid visualization of the application’s interface and functionality. A database is created for this application to store students’ data that will be used to detect their risk of school dropout. This prototype will undergo extensive testing, including the incorporation of priceless user input and incremental modifications. Ultimately, it serves as the basis for the subsequent phase of large-scale development that will come after it. Figure 2 illustrates the main interface of MyBuddy application with fourteen predictors. Data of students need to be keyed in by School Counsellors. It involves 14 variables belonging to four main entities: students, family background, peers, and surrounding  
  Early Detection of School Disengagement Using MyBuddy Application  
  N. ChePa et al.  
  flag, medium risk is indicated with a yellow flag, while low risk will be indicated with a green flag, as shown in Fig. 3(a). District and State Education Officers can also view the whole report of student risk, as shown in Fig. 3(b). 4.4 Evaluation of MyBuddy Evaluation of MyBuddy application involves four steps: technical testing, usability testing, acceptance testing, and market testing. Technical testing was conducted on 25 July 2023 involving IT experts from different institutions on three dimensions: design and layout, interactions and simplicity, and functions and performance. The results of technical testing indicate a positive response by IT Experts. However, the experts give comments on improving the application in each dimension. Based on the results and technical testing feedback, the MyBuddy application’s improved version has been produced and will be used in other upcoming tests.  
  5 Challenges of MyBuddy Implementation Implementing MyBuddy may face challenges such as technological infrastructure limitations, data management and privacy concerns, user training and adoption difficulties, stakeholder engagement, integration with existing systems, cultural and contextual factors, resource allocation, scalability and sustainability, evaluation and continuous improvement, and stakeholder communication and buy-in. 5.1 Ethical Issue Integrating MyBuddy and similar applications in education raises important ethical considerations that must be carefully addressed. This section discusses three primary ethical concerns associated with using these applications: data privacy, bias in modeling, and potential stigmatization of students. By exploring these considerations, educators, policymakers, and developers can better understand the challenges and work towards ensuring such technology’s responsible and ethical use. One of the key ethical concerns when utilizing applications like MyBuddy is the protection of students’ data privacy. These applications collect and analyze sensitive information, such as attendance records, academic performance, and social indicators, to identify at-risk students. Implementing robust data privacy measures to safeguard this information and ensure compliance with applicable data protection regulations is crucial. Schools, developers, and policymakers must establish clear guidelines and protocols for data collection, storage, access, and sharing to maintain confidentiality and protect students’ personal information. Another ethical consideration is the potential for bias in the mathematical models used by applications like MyBuddy. The algorithms and models to predict dropout probabilities should be developed and tested carefully for fairness, accuracy, and inclusivity. It is essential to critically examine the training data used to develop these models to prevent the reinforcement or amplification of biases that may exist within the educational  
  Early Detection of School Disengagement Using MyBuddy Application  
  Abstract. The growth of e-commerce has brought about a growing concern regarding its impact on the environment. Activities such as excessive packaging, delivery, and returns have contributed to increased carbon emissions, resulting in a significant carbon footprint. To promote a sustainable e-commerce environment, a study is needed to assess the carbon footprint contribution of online businesses. This paper presents a framework for conducting a preliminary investigation into the carbon production and emissions of identified e-commerce organizations. The framework was formulated by analyzing the relevant literature from similar studies. It comprises a 3-phase research activity: Phase 1 involves identifying carbon footprint factors through a literature search; Phase 2 includes conducting a case study on an online business to construct a carbon consumption profile; and Phase 3 involves developing a measurement method to assess the carbon consumption of online businesses. The proposed framework can provide a preliminary understanding of e-commerce’s carbon footprint contribution and enable authorities to assess the level of carbon consumption and devise action plans to reduce its impact on the environment. However, challenges and implications are associated with implementing the framework, which is discussed in the paper. Keywords: Carbon Footprint · Preliminary Investigation Framework · E-commerce · Carbon Consumption Profile  
  S. S. Kamruddin et al.  
  the advancements in Internet technology, online payment security, and speedy delivery methods. The covid-19 pandemic also contributes to the demand for online shopping. The positive growth of e-Commerce has resulted in various benefits to the economy. However, there is a need to ensure that the sector players do not harm the environment due to the activities of searching, packaging, shipping, and returning items [4]. Research shows e-commerce players contribute to carbon production and greenhouse gas (GHG) emissions. The negative effects of e-commerce on the environment are increasing due to the problem of transporting individual shipments, using special packaging materials, and issues with product returns [5]. For instance, in terms of packaging, online retailers’ continuous use of cardboard, plastics, Styrofoam, and other packaging materials when shipping their products generates a continuous stream of waste. Besides excessive packaging waste pollution, the carbon emissions due to the transportation (delivery and return) activities associated with online purchasing also harm the environment. Studies on the environmental impact of e-commerce are diverse as researchers focus on different angles such as logistics [5–7], particularly the last mile delivery i.e. the process of shipping the products from delivery hubs directly to the customer’s door [8, 9]. Research also focuses on e-commerce consumer behavior and its environmental impact [10, 11]. Assessing the environmental impact of e-commerce is a non-trivial task due to the different factors and elements contributing to the problem. Furthermore, the emergence of new retail models such as omnichannel retail incorporating click-and-collect and ship-from-store functionalities, along with the introduction of new delivery services, has added to the complexity in understanding the e-commerce industry’s environmental impact [12]. Promoting sustainable carbon consumption practices among online businesses is the ultimate goal of the authorities. With long-term economic growth in mind, a comprehensive plan that consists of standards, taxes, subsidies, communications campaigns, and education needs to be devised. To come up with this plan, a preliminary investigation needs to be performed to understand the severity of the situation. This paper provides a framework that can be used to perform a preliminary investigation on the existing scenario of the e-commerce industry toward building a carbon footprint profile for it. The framework outlines the preliminary investigation’s objective, method, and output in three phases. Using this framework, a preliminary investigation of e-commerce carbon footprint contribution can be performed to build a model that can aid in promoting sustainable carbon consumption among e-commerce. The rest of the paper is organized as follows. Section 2 reviews relevant literature and discusses how carbon footprint has been defined and explored in previous research. Section 3 presents the proposed framework to perform a preliminary investigation of the current scenario of carbon footprint contribution among e-commerce. Section 4 discusses the challenges and impact of implementing the framework, and Sect. 5 concludes the paper.  
  E-commerce Carbon Footprint Contribution  
  For rail, air and sea transport:  
  where E is Emissions (kg CO2e), F is fuel consumption (liters) D is the distance traveled (km) and Ef is Emission factor (kg CO2e/liter for road or kg CO2e/km for rail, or kg CO2e/km/passenger for air and kg CO2e/km/tonne for sea), In each case, the emission factor is a value that reflects the amount of greenhouse gas emitted per unit of fuel or distance traveled. Regression Analysis. Shopping online has the potential to reduce the environmental impact compared to traditional brick-and-mortar retail, although the degree of impact depends on specific circumstances. However, due to the complexity of the factors involved, most studies that compare the carbon footprints of online and traditional retail provide a limited view. To provide a more comprehensive assessment, a study published  
  E-commerce Carbon Footprint Contribution  
  ti software. The secondary data will be subjected to content analysis. The participants will be determined by contacting the liaison officers or representatives at Lazada through LinkedIn and their company website. 3.3 Phase 3: Developing Carbon Contribution Measurement Method The third phase is to propose a measurement method and matrix suitable to measure carbon contribution for e-Commerce, logistics, and courier players based on best practices and industry standards across other sectors and/or jurisdictions. To achieve this objective, a literature search will be conducted on the best practice and industry standards in other countries or other sectors to devise the carbon contribution calculation. The output of this phase will be the measurement method for carbon contribution calculation for retailers, logistics, and courier players. The literature revealed various approaches for measuring carbon contribution, therefore, this phase will involve a thorough search of the literature to identify the most suitable approach. Once the approach is identified, the next step is to conduct an analytical approach on the identified carbon contributing factors and elements from Phase 1 and the carbon profile of the identified online business from Phase 2. Both this information will be analyzed to be incorporated into the construction of the carbon footprint measurement method.  
  4 Implementation Challenges and Impact The paper presents a framework for the preliminary investigation of the e-commerce carbon footprint contribution. This section discusses some challenges that may arise in implementing the proposed framework. We also discuss the implication of implementing the preliminary investigation using the proposed framework. 4.1 Implementation Challenges Conducting a preliminary investigation related to carbon footprint profiling of ecommerce comes with challenges such as data acquisition, ensuring the sufficiency of data to create the carbon footprint profile and the challenges of developing a model with insufficient data. One of the main challenges in conducting research related to carbon footprint profiling is gaining access to relevant organizations and their data. Acquiring data from these organizations can be difficult, especially if the organization is unwilling to cooperate or if the data is confidential. Also, it may be challenging to contact some organizations, which might make the research process more challenging. Researchers may need to build relationships with pertinent groups, explain the significance of the research and the advantages of participation, and reassure them that their data would be handled with privacy and security in mind to solve this difficulty. The unavailability of adequate data to build a model is another problem in carbon footprint profiling research. This is particularly true for small and medium-sized businesses, which could lack the tools or capability needed to gather and disclose information  
  S. S. Kamruddin et al.  
  on their carbon impact. Furthermore, it may be difficult to develop the model because of missing or faulty data effectively. We could need to leverage various data sources, including publicly available databases or secondary data, to address this problem, and we might need to create statistical models to estimate the missing data. Even with the best efforts, there may occasionally not be enough data to build a carbon footprint model. This could happen if the company hasn’t gathered data on particular parts of its operations or if the data is unreliable or erroneous. We may need to employ alternate strategies to study the potential effects of missing data in certain situations, such as creating scenario-based models or performing sensitivity analyses. Another strategy is to use proxy data, such as industry benchmarks or data from organizations with comparable purposes, to calculate carbon emissions. 4.2 The Impact of Implementation When the framework is implemented, the outcome of the preliminary investigation can provide the following implication; by identifying the contributing factors of the carbon footprint and constructing a carbon footprint profile for the e-commerce industry and its players, the preliminary investigation can provide valuable insights into the current practices and the severity of the existing carbon footprint produced by e-commerce companies. This information can help businesses make informed decisions about reducing their carbon emissions and developing more sustainable practices. For example, the researchers can identify which activities and processes have the highest carbon footprint and suggest ways to reduce emissions through changes in these areas. The development of a measurement/model/matrix can be used as a measuring scale to measure and evaluate the future carbon footprint contribution of the e-commerce industry. Furthermore, it may be used to create a standard and compare the carbon footprint of different companies and industries. Another benefit of the measurement/model/matrix is that it can be used to monitor carbon reduction progress and set achievable targets over time. This could motivate businesses to take action to lessen their carbon footprint and foster a culture of sustainability within the e-commerce sector.  
  5 Conclusion In conclusion, a carbon footprint profiling study can significantly benefit the e-commerce industry by providing a guideline to reduce carbon emissions and contribute to environmental sustainability. Establishing relationships with organizations, using different data sources, and devising alternative solutions can overcome the challenges faced in this research, such as gaining access to relevant organizations, dealing with incomplete data, and having difficulty adopting innovative modeling approaches. Identifying the contributing factors and constructing a carbon footprint profile can enable the e-commerce industry to gain insights into their operations’ carbon emissions, enabling them to make informed decisions on reducing their carbon footprint. Additionally, constructing a carbon footprint measurement/model/matrix can be viewed as an initiative to develop a standardized and transparent method of assessing carbon consumption, setting targets for reduction, and monitoring progress over time. This  
  Abstract. The significance of financial planning in individuals’ lives cannot be understated. People are transitioning from traditionally paper-based or tangible methods of tracking their finances to more digital-based approaches. However, recent studies indicate that the study on personal financial management systems is limited, and the traditional software did not meet the users’ needs. The current software lacks the ability to offer practical recommendations to users and direct them towards achieving their financial objectives. This study aims to design a personal financial management software that implements artificial intelligence techniques. A rule-based expert system technique was adopted to provide practical recommendations and support users in achieving their financial goals. Besides that, a prototype of the proposed system will be developed and evaluated by domain experts using a user perception survey. The results reveal that the respondents are satisfied with the design of the proposed system. The outcome of this study could provide practical recommendations that users can use to make informed financial decisions and accomplish their financial objectives. Keywords: Personal Financial Planning · Artificial Intelligence · Expert System  
  assets. It is an essential skill that needs to be learned and practised because it enables people to achieve their life goals. While managing personal finances is important, only some have the knowledge or capability to do this. Besides, even the most economically astute person can become confused or short-sighted. Munohsamy [2] explains the significance of personal financial management, stating that it allows for a higher standard of living, which leads to better health and less financial stress. Muske & Winter [3] also mention that long-term financial security is seen to be possible with good personal financial management. However, they discovered that only a few individuals applied the suggested personal money management strategies. Technology-related tools are believed to make personal financial management easier in daily life. People now, especially the younger generation, are expected to be able to adapt to technology in managing their finances. According to Hokin [4] spending without limits and goals is the biggest problem in personal finance. This issue may be solved when someone specifies their purpose with supporting tools that are able to manage their own personal finance. Therefore, when developing a personal financial management application for the users to support money management and decision making, it is critical to understand what they want to do so that these applications meet their needs and expectations rather than imagined ones. This research aims to (i) determine the requirements of the prototype for a rulebased personal financial management expert system, (ii) construct a prototype for a rule-based personal financial management expert system. (iii) evaluate the rule-based personal financial management expert system prototype.  
  2 Problem Statement Personal finance can help people better understand where money is spent and where it comes from. The user can draw conclusions based on this information and make better decisions for better financial capability. Moreover, it is important for an individual to understand that planning and budgeting can help avoid problems, such as overspending and budget gaps, especially for those with debt or loans. Next, the requirements for personal financial control keep increasing, and it is impossible for everyone to keep a lot of information in mind simultaneously. The main problem with personal finance is that people spend without knowing their limits and without having a plan. Therefore, many people start analysing their expenditures and budget. Unfortunately, there are some people who lack the time and information to deal with this. According to Vasyliuk & Basyuk [5], people sometimes only know the approximate value of their expenditure. However, the real statistics of the financial data are different from this figure. Vahidov & He [6] also highlight concerning trends in the personal finance domain that researchers have overlooked. Therefore, there is a need for a system that can help users keep track of and manage their financial data. However, the current study on the personal financial management system is still limited, especially in the requirement model for a personal financial management system. According to Lewis & Perry [7], the study on how users track their financial transaction every day is very limited, and little is known about how users keep track of their  
  B. C. C. Kit and N. F. A. Ghani  
  daily financial activities. Although some existing commercial software, such as Manilla, Quicken and FinanceWorks, currently supports personal financial management, they found out that most of the software was not used by users due to a mismatch with the needs, security, and inconvenience. They also show that the fulfilment of the users’ needs and requirements are the keys to successful personal financial management software. Moreover, the current commercial application requires in-app purchases and a high monthly subscription to function properly which is not affordable for most users [8]. In addition, Xie [9] found that most traditional software consists of meaningless operations and redundant functions. They suggest that personal financial management software users should focus on the data rather than spend energy and time to understand the complex user interface. This is supported by Kozhevnikov et al. [10], which mention that most of the current popular personal financial management applications have been created for a long time with outdated technology, inflexible user interfaces and a lack of important features. They also mention there is a need to identify the requirement or design for creating a next generation personal financial management information system. Apart from that, the current personal finance application lacks the capability to provide users with actionable advice [11]. As a result, the application cannot guide users towards achieving financial stability. Considering that most users make purchase decisions based on unconscious factors is crucial. ABased on the literature reviewed above, further research is still needed, particularly in designing an effective personal financial management system. To address this gap, this study proposes a new design for a personal financial management system that leverages AI capabilities to offer practical recommendations and support users in making informed decisions to achieve their financial goals.  
  3 Related Work According to Nafed [12], expert systems are classified as one of the most significant areas of artificial intelligence. It can be considered an artificial intelligence technique [13]. An expert system is an intelligent computer program that utilizes knowledge and inference techniques to address complex problems that typically require consultation with domain experts. In the same context, Jackson [14] defined an expert system as a software application that utilises knowledge and reasoning of a specialised subject matter to solve problems or provide advice. Giarratano & Riley [15] suggest that an expert system consists of two distinct components: an inference engine and a knowledge base. Moreover, Holsapple et al. [16] suggested the application of an expert system in financial management, including portfolio analysis, credit analysis, insurance, and security trading. In this study, a comparison and analysis will be conducted among three popular personal financial management applications, namely Money Manager, Dollarbird, and Goodbudget. Additionally, the important requirements of this application will be identified and elicited. Table 1 shows the results of the comparative analysis based on observations.  
  /  
  Based on the findings presented in Table 1, this study reveals that the selected popular personal finance management application could store and visualise the daily transaction of the users. However, these applications cannot revise the budget and provide actionable advice to users. As a result, this study reaches a similar conclusion to the study conducted by Althnian [11], which mentioned that these applications lack the ability to lead users to financial wellness. The common features of these applications such as adding, modifying, and visualising transaction will be gathered, analysed, and transformed into the requirements of the proposed system. In summary, expert systems can be used in a wide area of application, especially in financial management. The main reason is that it can be used to solve problems related to control, simulation, selection, design, prescription, diagnosis, planning, prediction, monitoring, and others [12]. The proposed system will focus on the area of personal financial planning due to the limited study in that domain. Personal finance management is an important domain researcher always overlook [6]. Apart from that, the problemsolving paradigm of the proposed expert system will be planning.  
  4 Methodology The research methodology used in a study is determined by the research objectives and the consideration by the researcher [17]. The mixed research approach, incorporating both qualitative and quantitative methods, will be employed in this study for data collection and analysis. The requirements will first be obtained through qualitative means such as observation and document analysis, followed by the creation of a prototype to demonstrate these requirements. The validity of the design will then be evaluated quantitatively. The study will adhere to the Design Science Research in Information Systems framework proposed by Vaishnavi & Kuechler [18], a widely adopted approach in various fields including computer science and engineering, as depicted in the accompanying Fig. 1.  
  Fig. 7. Record Income Page of PFMES  
  After Tim records his financial transactions, the system enables him to sort and visualise his financial data using various charts through the analysis module. As an example, Fig. 8 shows the pie chart of Tim’s expenditure categories in 2023. Figure 9 displays the visualisation of Tim’s expenditures using a bar chart for the year 2022. Additionally, the system will calculate the average expenditure for each transaction. Furthermore, the analysis module of the system will summarise Tim’s financial data and enable him to view his personal financial report, as illustrated in Fig. 10.  
  1 Introduction Rice is a staple food for many people worldwide, including Malaysians. Meeting domestic rice demand is difficult because of distracting variables such as rising population, changes in land use, soil quality, weather patterns, plantation diseases, and restricted access to innovation, technologies, and resources [2, 3]. A reliable system for forecasting future rice yield is required to achieve food security. However, because of the variability of the factors influencing rice output, developing a one-size-fits-all forecasting model is difficult. Moreover, the conventional practice often relies on historical data and expert recommendations and may not consider all factors influencing rice production in forecasting rice yields [17]. To overcome these challenges, machine learning approaches appear as recent alternatives to build prediction models for crop yields. It has a wider ability to capture complex relationships between various characteristics and may incorporate massive amounts of data, such as climate data. However, further study is needed in Malaysia on the utility of integrating weather-related information into machine learning models for rice crop prediction. In recent years, there has been a rise in interest in using machine learning approaches to create prediction models for crop output [2–4] and examine the influence of climate on agricultural productivity. Due to the continuous interaction of various variables impacting rice production, predicting rice yields using machine learning models can be challenging. Climate is one of the impacting factors in agriculture, including rice. A previous study has revealed that climatic conditions influence rice production in Malaysia [2, 3]. However, it is unclear how much climatic data can increase the accuracy of rice crop estimates in Malaysia. This study investigates the effectiveness of including climate data in predicting rice production in Malaysia using linear regression. Our hypothesis was that including climate data in the prediction model could increase the accuracy of rice yield estimation in Malaysia, as climate is crucial to rice production. This study aims to provide insights into the possible benefits of integrating climate data for rice production prediction in Malaysia by including it in a regression model. In relation to this, the yearly rice yield information, as well as season indicators for the main and secondary plantation seasons of 10 years from states in Malaysia, were employed in the modeling. In addition, climatic data as predictors such as wind speed, temperature, humidity, and rainfall were also included in the model. This study is organized as follows: a complete overview of related studies, methodology, findings, and a discussion of the research’s significance for agricultural practices and future research in Malaysia. The study is expected to contribute to the growing body of literature on the impact of climate change on agriculture in Malaysia, as well as give important insights for policymakers and farmers in this country.  
  2 Related Work Agriculture plays a crucial role in many countries, providing food and employment opportunities for millions of people. Accurate crop yield prediction is essential for farmers, policymakers, and other stakeholders because it can drive agricultural production,  
  M. F. Mohamad Mohsin et al.  
  distribution, and pricing decisions. Traditional approaches to yield prediction have relied on statistical models and expert knowledge. However, recent advances in machine learning and data science have led to the development of more accurate predictive models for crop yields [5]. Rice yield prediction models can be modeled based on three approaches that are mechanistic, statistical/machine learning, and deep learning-based [19]. Regression modeling is a popular technique under statistical machine learning models for developing predictive models in agriculture. Regression models aim to establish a relationship between a dependent variable (in this case, crop yield) and one or more independent variables (such as climate data, soil quality, and agricultural practices). Regression models can be simple or complex, depending on the number and type of independent variables used. The quality of the data and the correlation among the variables used to generate the model might have an impact on regression performance. [6]. There has been growing interest in using regression models to predict crop yields with climate data recently. Climate factors such as temperature, humidity, rainfall, and wind speed are essential predictors of crop yields in many regions [7]. By incorporating climate data into regression models, researchers have developed more accurate and reliable predictions of crop yields [2, 3]. [18] replicated factors influencing rice production by combining typical independent variables such as temperature, precipitation, sunlight hours, and relative humidity to develop a deep learning-based rice yield forecast model. Other rice yield prediction models based on deep learning can be seen in [19]. In Malaysia, machine learning in combination with climatic data has proved very useful for predicting rice yields. As a staple crop, thus forecasting rice harvests is critical for guaranteeing food security and economic stability [1]. In Malaysia, researchers discovered that adding climate data into regression models may greatly increase the accuracy of rice yield predictions [2, 3]. Because climate involves many types of predictors, an experiment using a different climate predictor may yield a different result. This study emphasizes the need of using climate variables when developing forecast models for rice yields in Malaysia. To summaries, regression modeling and the integration of climate information are important input for predicting crop yields in agriculture. Regression models may be used to create correlations between dependent and independent variables, and using climate data as an independent variable can enhance forecast accuracy. In the context of rice production prediction in Malaysia, adding climate data has shown to be a significant method for boosting forecast accuracy and guaranteeing food security in the country.  
  3 Methodology This study’s methodology section focuses on predicting rice yields in Malaysia using regression modelling techniques, with a particular emphasis on the role of climate data in this process. We employed a series of steps to achieve this, including data collection and analysis, data preparation, and regression modelling. Figure 1 provides a flowchart of these steps and their interconnectedness in the overall process.  
  Abstract. The gig economy has paved the way for sharing economy growth especially e-hailing applications, as more women turn to e-hailing applications for convenience in commuting and travelling. However, women are concerned about whether to trust or not to trust e-hailing applications due to the openness of their digital and spatial crowdsourcing nature. Additionally, the user’s locations are exposed to risks like stalking, identity theft, and physical safety, which can be extremely dangerous, especially for women. The paper begins by introducing the e-hailing industry and the gig economy. It then discusses women’s unique challenges in using e-hailing applications and their trust towards these platforms. This paper dives deep into the heart of the matter by employing a quantitative approach through the survey to investigate women’s trust in e-hailing apps using the UTAUT as the underpinning theory. The results show that e-hailing applications greatly influence women’s trust while puzzlingly having no significant impact on their trust towards e-hailing drivers. From eye-opening insights to practical recommendations, this study sheds light on the crucial role of trust in the sustainable growth of the e-hailing industry focusing on women. Keywords: Gig economy · E-hailing · Trust · Risk · UTAUT · women in sharing economy  
  Nowadays, e-hailing services are in great demand as they have many advantages from a consumer perspective [2]. E-hailing applications have provided women with greater mobility options, enabling them to navigate urban environments conveniently and safely. The e-hailing platforms provides transparent driver information and ease of requesting rides for women as alternatives to traditional taxis [3]. However, there are possibilities of risks involved such as physical safety like driver’s inappropriate behaviour [4], assaults and harassment in sharing rides services [5]. Thus, addressing women’s safety concerns is needed from both e-hailing companies and society. Companies must prioritize implementing strict policies and comprehensive training programs to foster a safe and respectful environment for passengers and drivers. Furthermore, data privacy concerns loom large in the digital era. E-hailing applications collect vast amounts of personal information from users, including location data, contact details, and transaction history. Women users, in particular, may be at risk of potential misuse or unauthorized access to their data [6]. The success of the E-hailing application is dependent upon the active participation of all customers, including women, who represent the target group selected for the current study. Likewise, customer trust has a major role in the success of applications, including E-hailing applications in particular, where trust is an attribute that develops over time due to customer interactions, and trust varies according to the application areas. For example, trust varies from the woman’s perspective [7]. In the online booking (E-hailing) application, privacy and security are two important factors for achieving trust. At the same time, there are negative factors affecting the success of E-hailing applications that can be identified by risks, including physical risks represented by sexual harassment, murder, kidnapping, and hate crimes, and the risks of hacking sensitive information of women by the platform itself, as it works through smart mobile phones and other devices that are often its security are weak and result in many risks [3, 8]. Most studies in this domain focus on the economic aspects and ignore the risks, despite various reported security incidents related to e-hailing applications like Uber, Grab, etc. [8]. Risks associated with e-hailing may influence women’s trust in e-hailing platforms. A useful theoretical framework for researching the adoption of technology, particularly e-hailing apps, is the Unified Theory of Acceptance and Use of Technology (UTAUT) by Venkatesh et al. [9] which help to provide a thorough understanding of users’ intention to embrace and use technology. It includes important constructs from multiple technology acceptance models. The relevance and applicability of UTAUT in predicting and explaining user acceptance of e-hailing services have been shown in studies such as in Liu et al. [10]. Thus, this article unpacks the relationship between women’s trust and the adoption of e-hailing applications through the lens of UTAUT theory.  
  2 Literature Review 2.1 E-Hailing The rise of the gig economy has shown significant adoption of digital technology such as e-hailing applications. E-hailing applications allow people to book rides services through mobile or internet-based applications at a specific time and at an appropriate price [2]. With the launch of an application-based electronic recall service, the public  
  RO2: To examine the influence of women’s trust in drivers on their intentions to use e-hailing applications. RO3: To investigate the moderating effect of risk on women’s trust in e-hailing applications and drivers on their intentions to use e-hailing applications. This study employed a quantitative approach using a survey for data collection. The study developed a questionnaire instrument that distinguished between trusting the e-hailing application and trusting the drivers. The survey consisted of 22 questions and collected demographic data from participants using four data structures. Responses were measured using a 7-point Likert scale, ranging from ‘strongly disagree’ (1) to ‘strongly agree’ (7). Table 1 provides an overview of the final miscatalogue, including combinations and relevant item codes. Table 1. Items of questionnaire Construct  
  Code  
  Risk moderates the relationship between trust in an Not Supported e-hailing driver and a woman’s intention to use an e-hailing application  
  5 Results and Discussion 5.1 The Influence of Women’s Trust in E-Hailing Applications on Their Intentions to Use the Applications The study indicates that women’s trust in e-hailing applications directly affects their intentions to use these applications. The structural model that was used in the study examined the relationships between different variables and found that trust in the applications was a significant predictor of women’s intentions to continue using them. In more detail, the study suggests that women who trust e-hailing applications are more likely to continue using them in the future. This trust may be related to factors such as the service’s reliability, the drivers’ safety, and the application’s ease of use. When women feel they can trust the application to provide a safe and reliable ride, they are more likely to use it again. It is important to note that trust is just a factor that influences women’s intentions to use e-hailing applications. There are other factors that were not covered in this study  
  K. A. Abdullah and M. Mahmod  
  such as pricing. The results of this study suggest that trust is an important factor in determining whether women will continue to use e-hailing applications over time. By building trust with their users, e-hailing applications can increase the likelihood that women will continue to use their services in the future. 5.2 The Influence of Women’s Trust in Drivers on Their Intentions to Use E-Hailing Applications This study showed women’s inclinations to use these apps are directly influenced by how much they trust the drivers of e-hailing services. The structural model employed in the research to analyse the correlations between various variables discovered that women’s intentions to continue using e-hailing services were significantly predicted by their confidence level in the drivers. Trust in drivers may be influenced by their skill level, dependability, and safety. Women are more likely to continue using the e-hailing service if they believe they have confidence in the driver to provide a secure and dependable journey. E-hailing apps may boost the possibility that women will continue using their services by establishing trust with their customers and ensuring that their drivers are capable, dependable, and safe in the future. 5.3 The Risks Affecting the Trust of Women Who Use E-Hailing Applications The effect of risk on the relationship between women’s trust in e-hailing applications and intent to use and women’s confidence in drivers of recall applications and intent to use was investigated. It was found that there is a significant effect of risk on the relationship between trust in e-hailing applications and women’s intention to use them. The study proved that the increase in risk affects the relationship between trust in e-hailing applications and the intention to use. Concerns related to information security, such as theft and unauthorised use of personally identifiable information, tracking, prosecution or harassment after information penetration, mitigate the intention to use e-hailing applications. This study is consistent with the results of previous studies, which proved that the privacy problem impacts the intention to use e-hailing applications [26–28]. Perplexingly, no risk influenced the relationship between confidence in e-hailing application drivers and women’s intention to use e-hailing applications. The current study did not provide any evidence regarding the risks women may face, such as kidnapping, rape, and harassment, that may affect the relationship between trust and intention to use. The study contributes to showing that trust in E-hailing applications affects women’s intentions to use E-hailing applications. The study hypotheses are supported by the use of risk as a moderator between the effect of trust in E-hailing applications and the intention to use E-hailing applications. Study findings indicate that trust in E-hailing drivers does not significantly impact customers’ intentions to use E-hailing applications. At first, study findings may seem surprising, but given that E-hailing applications connect strangers, it is likely that the application already considered most of the risks. Concerns related to information security such as theft and unauthorized use of personally identifiable information, tracking, prosecution, or harassment after information penetration mitigate the intention to use e-hailing applications. As a result, the study findings are consistent  
  The Application of UTAUT Theory to Determine Trust Among Women  
  Abstract. The housing department in Wisma Darul Aman plays an important role in planning, coordinating, and developing housing schemes for low-income Malaysians. However, the manual process of recording and storing applicant information on paper is time-consuming for the staff, and retrieving applicant information can be challenging. Thus, the main objective of this study is to design and develop the Housing Interview Management System (HIMS) for the housing department. The system manages applicant information, enabling storage, retrieval, modification, and deletion, while generating scores based on their responses, offering a more accurate and efficient solution for the Wisma Darul Aman housing department. To design and develop the system, Prototyping Software Development Methodology was used, which consists of identifying the requirements through interview sessions with housing department staff, designing the system, building the prototype using PHP and JavaScript, conducting user evaluation among housing department staff, refining prototype, and implementing and maintaining the system. The findings of the evaluation show that the system is useful and easy to use. Most respondents expressed high satisfaction with the system, indicating that the system worked as desired and effectively facilitated the housing interview process. The study highlights the significant potential of the HIMS to streamline the housing department’s interview process, contributing to improved affordable housing delivery for low-income Malaysians. Keywords: Interview Management System · Prototyping · Housing Department · Program Perumahan Rakyat (PPR) · Low income  
  A. Muniandy et al.  
  by reducing paper use and minimizing data loss. This system will also save time and energy for administrative staff, increase their satisfaction, and be more environmentally friendly. Moreover, the system is designed to be user-friendly and easy to maintain. It enables staff to add, update, and retrieve applicants’ information quickly and efficiently. The system can also generate applicant information, and scores obtained more clearly and accurately. Furthermore, the system can detect whether an applicant has previously applied for a PPR house, which is believed to improve the quality of hires significantly [9]. Overall, this system is expected to enhance the efficiency and effectiveness of housing programs, particularly in public housing authorities [8, 9]. 2.1 Tools for Housing Interview Management System Development The HIMS was developed using a combination of PHP, HTML, CSS, and JavaScript programming languages, while XAMPP was utilized as the database management system. PHP and related technologies are popular in web-based development [4] and can be easily deployed on web servers. Additionally, Miro was employed during the initial stages of the system design to provide a visual representation of the system’s architecture and design. The stand-alone application using PHP and XAMPP allows for efficient storage and retrieval of applicant information, minimizing data loss and reducing errors in the manual process. This system is expected to improve efficiency and accuracy in the housing interview process, leading to increased satisfaction for the staff.  
  3 Methodology of the Study The software development life cycle (SDLC) was selected as the appropriate model for this project, as it encompasses the stages of the software development process and ensures the efficient implementation of high-quality software in meeting the specified requirements. Additionally, prototyping methodology, a software development approach where a prototype is built, tested, and refined until satisfactory [10], was employed to develop the HIMS due to its simplicity and ease of understanding. Figure 1 shows the phases of the prototyping model.  
  Fig. 1. Phases of prototyping methodology [10]  
  3.1 Requirements To develop the HIMS, it was essential to understand the concept, the individuals involved in the process, and the requirements. The requirements-gathering process involved examining the interview form utilized by the Housing Department during the interview process. The form was analyzed to comprehend the entire interview process, including the calculations involved and how the assessments were evaluated. 3.2 Quick Design A simple framework design was created to give users a high-level system overview. The design was created using an online software called Miro and was developed based on the project scope and requirements, including the appearance and features of the system. The framework includes various components, including interview questions, to store applicant information. This would enable the staff to understand the flow, conveniently digitize all the interview details into the corresponding fields, and effortlessly maintain or refer to the stored information. 3.3 Build Prototype The prototype is developed based on the data collected from the quick design and is a scale-down working model of the system. The project mainly focuses on developing a stand-alone application for the working environment using Visual Studio Code (VSC) and Xampp for data storage. The system development process incorporated all the requirements collected during the initial stage. 3.4 User Evaluation After the system was created, it was converted into an actual application and tested in the real world. The staff evaluated the HIMS based on the requirements, functionality, and performance. They shared their comments and ideas for further enhancement or implementation during the next round of system redesign. The strengths and weaknesses of the HIMS were identified through user evaluation. 3.5 Refine Prototype The refined prototype is where the current system development will be revised for the next redesigning or rebuilding prototype. After collecting the ideas and comments from the staff, the HIMS has been reconsidered to achieve the user’s requirements and the project scope. This evaluation and refined prototype were developed in loop mode until the staff was satisfied with this system. 3.6 Implement and Maintain After the final system was developed based on the prototype, it went through testing before sending to production. As the HIMS met all the requirements and satisfied the staff after the re-evaluation process, it is now complete and ready to be used during the interview session.  
  Acknowledgement. The author wishes to thank the entire staff of the Housing Department, Wisma Darul Aman, for their advice and knowledge throughout completing this project.  
  2 Related Work It’s interesting to hear that consumer online shopping behavior is influenced by multiple factors such as attitude, psychology, product price, privacy, perceived benefit, and accessibility. Attitude towards online shopping has continued to be found to be an important predictor of online purchasing behavior. A recent study by [5] found that attitude towards online shopping significantly positively affected online purchase intention. A study by [6] indicated that how people feel about online shopping is related to how much they spend and how they plan to spend it. In the current situation where the pandemic has caused budget constraints, customers may be more willing to turn to cheaper products to save money. Additionally, one study from [7] suggested that transactional characteristics can be used to divide customers into different groups. By using this, we can know the behavior of each cluster. Three methods for grouping things were used: KMeans,  
  N. A. Mustakim et al.  
  KModes, and KMedoids. Psychological factors have also been found to be influential in online shopping behavior. According to a study by [8], several psychological elements can promote and affect consumers’ online shopping behavior during the COVID-19 pandemic. Product price continues to be an important factor in online shopping behavior, and recent research has explored how price sensitivity differs across product categories. Consumers may buy more if they think your price is cheaper than competitors’. However, the response may be disappointing if the price is substantially greater than planned. According to [9], product prices are divided into three dimensions: fair price, fixed price, and relative price. In contrast, price strategy can be categorized as odd pricing, bundle pricing, and discount pricing strategy. Several studies have shown that the price of a product is more important and relevant to consumer purchasing behavior [10]. Moreover, perceived risk plays a big role in how consumers buy things online. According to [11] the decision to buy something via the internet is heavily influenced by how secure and confidential consumers believe their information to be. A recent investigation by [12] found that financial and privacy risks are some of the risks online shoppers face. The accessibility of online shopping platforms is also an important factor in online shopping behavior. A recent study by [13] determined that accessibility is an important factor for the reputation of online shopping malls. However, [14] stated that accessibility factors do not much impact the development of customer attitudes. Customers in India prefer face-to-face contacts with insurance agents due to a lack of accurate product information on websites. Perceived benefit, trust and security also as stated by [15] has positive relationship with online purchase behavior. An empirical study [16] found that trust and perceived benefits determine consumer attitudes toward online shopping, and factor analysis and structural path model analysis were used to test the hypothesized relationships of the research model. This finding shows that the better the perceived benefit, the higher the likelihood of online buying behavior among consumers. To sum up, the identified factors that influence online purchasing behavior play a significant role in shaping consumers’ decisions and actions in the online marketplace.  
  panel of experts of four people with strong backgrounds in marketing, e-commerce, and information systems from each faculty. The process of collecting data is around 3 months, from Feb 2023 until Mei 2023. The survey, a questionnaire, was divided into three main components. Section A covers the consumer’s demographics, Section B focuses on measuring the factors that affect how people buy things online. This study looked at eleven factors: attitude, perceived risk, trust and security, psychological, hedonic motivation, promotion, product price, privacy, emotional, perceived benefit, and accessibility. In Section C, respondents were asked how they bought things online. Additionally, we utilized modified scales from past research to develop scales for measuring various constructs. Table 1 illustrates the references from which the measurement scale has been adapted and modified as per the study. Sections B and C were measured using a five-point Likert scale (from strongly disagree to strongly agree).  
  4 Results and Discussion 4.1 Respondent Background The background characteristics of the respondents that have been highlighted include their gender, age, level of education, ethnicity, annual income, employment status, state, how often they use online shopping sites, how much they spend on average, and what kind of products they buy. A summary of the respondents’ profile is presented in Table 1. Table 1. Respondent’s Profile Demographic Information Gender Age  
  5 Conclusion This paper has presented the important variables that influence Malaysian consumers’ online purchase behavior. The grouping of factors has been identified as the major determinants of online purchasing behavior. When people shop online, these variables significantly influence their choices and actions. To improve the online shopping experience for Malaysian consumers, businesses must recognize and address these factors to create effective marketing strategies, optimize pricing strategies, address privacy concerns, highlight the advantages of their products, and improve accessibility. Businesses may more effectively meet the wants and preferences of Malaysian customers by considering these influencing elements, ultimately boosting their online sales. According to the findings, accessibility, attitude, perceived risk, psychological effects, product pricing, privacy, and perceived advantage all impact online purchasing behavior. Acknowledgement. The researchers would like to thank UiTM and MARA for their support of academic research and funding possibilities to share research findings locally.  
  H. O. Abdullahi and M. Mahmud  
  development [1, 2]. In addition, it has ensured increased productivity, reduced poverty, and created employment opportunities for the rural population [1, 3, 4]. Besides, agricultural expansion in sub-Saharan Africa is hampered by agribusiness firms’ lack of new advanced technological capabilities, management and staff incompetence, and infrastructure and resources [3, 5, 6]. Other challenges include poor implementations of ICT services, which leads to the resistance of agribusiness partitioners in agricultural business industries. This indicates that companies do not prioritize the strategic implementation of ICT in their operations. On the other hand, ICT has played a crucial role in the progress and improvement of countries, especially when it is used effectively. The successful implementation of ICT in the agriculture industry in developed countries has led to significant advancements in the efficiency and productivity of the agriculture value chain. However, the adoption of ICT in the agriculture industry in sub-Saharan Africa was slow, and major transformations still need to occur [7]. Several studies have discovered that using ICT in agribusiness can enhance productivity, reduce costs, and improve the livelihoods of farmers and agribusiness practitioners. It also contributes to the overall operations of agribusiness activities and the value chain process. Nevertheless, adopting ICT services is not always straightforward, and various factors can influence an individual’s intentions to use these services [8]. A few agricultural businesses in Somalia have started utilizing technology to stay competitive and access global markets [9]. However, they face significant challenges, such as a lack of management and staff expertise, inadequate infrastructure and resources, and the absence of innovative technologies. These obstacles may impede the adoption and effectively using new technologies, lower productivity, and hamper overall progress. Hence, this study analyzes the relationship between PU and PEOU on intention to use ICT services and how these factors influence ICT services adoption among agribusiness practitioners in Somalia. In addition, this research strengthens the literature on ICT service adoption regarding agribusiness. Furthermore, it provides insights into factors that may enhance the adoption and use of ICT services in Somalia’s agribusiness sector. This study employs TAM theory to explain ICT services’ adoption by agribusiness practitioners in Somalia. Here, the results inform strategies and policies that can improve the adoption and use of ICT services in the agribusiness sector, improve productivity, and promote overall progress in Somalia. The remaining sections of the paper will follow a specific structure. First, there will be an overview of the literature review. Subsequently, the research framework and hypothesis development will be discussed. The research setting will then be introduced, followed by an examination of the methodology applied in this study. Consequently, the data analysis will be presented, and the study’s findings will be explained. Finally, the paper will conclude by summarizing the primary research points.  
  2 Literature Review 2.1 ICT in Agriculture Literature on using ICTs in agriculture suggests that ICTs can enhance efficiency, productivity, and competitiveness in the agricultural sector. For example, ICTs Services have been employed in agriculture with precision farming. This method utilizes drones,  
  sensors, and Global Positioning System (GPS) technology to enhance crop yields and decrease expenses [10]. According to Ayim et al. [7] study, various obstacles impede the adoption of ICT in African agribusiness, such as inadequate policies, limited expertise, and insufficient technological infrastructure. Another study indicated that personal ICT gadgets such as phones, radios, and televisions could improve climate-smart agriculture adoption by providing farmers with timely information and forecasts. This access to information can significantly enhance farmers’ welfare and facilitate the uptake of other agricultural innovations, such as biological control. Furthermore, this study emphasizes the importance of providing farmers with ICT tools to enhance their decision-making abilities and promote sustainable agriculture. Therefore, policymakers and agricultural practitioners should prioritize adopting ICT tools to improve farm productivity and sustainability [11]. Additionally, it is found that ICT use in small agribusinesses in Nigeria’s native communities. The researchers have established that social factors are crucial in these communities and that a balance between designing ICT solutions and addressing social elements is necessary to increase the acceptance of ICT advancements, such as Internet access, computers, and online portals [12] Lokeswari [13] has determined that adopting ICT services among agricultural staff is influenced by various factors such as infrastructure, education, content availability, affordability, and PU. Despite awareness of the benefits of ICT, the lack of infrastructure and technical knowledge is a significant barrier to adoption. Hence, efforts should be made to improve ICT infrastructure, provide relevant training and education, and develop affordable and accessible ICT solutions to increase adoption and use among rural farmers [13]. 2.2 ICT Services in Agribusiness Several factors, including access to technology, education level, business size, and existing technology usage influence the adoption of ICT in small-scale agribusiness enterprises in Somalia. Additionally, limited knowledge and skills and inadequate infrastructure were identified as significant barriers to adopting ICT. In addition, government policies and regulations may have a substantial role in enhancing ICT adoption and supporting the growth of small-scale agribusiness enterprises in Somalia [9]. Nevertheless, other research has shown that ICT can drive sustainable agribusiness innovation [14]. For example, precision agriculture technologies, such as precision planting and fertilization, can improve crop yields while reducing the use of resources and minimizing the environmental impact [15]. Similarly, using ICT in supply chain management can improve the traceability and transparency of food products, leading to more efficient and sustainable production and distribution processes [16]. Furthermore, food sustainability transitions are enhanced using information and communication technologies (ICTs), improving resource productivity, reducing inefficiencies, lowering management costs, and improving chain coordination across agro-food value chains [16]. Conversely, ICT services refer to the many services offered through information and communication technology. (ICT). It covers various topics, such as telecommunications, software, hardware, the internet, cloud computing, IT support, e-commerce, and many more [17]. Additionally, these ICT services have played an essential role in facilitating
10. ICONIP_0 conference:
Biao Luo (Editor)  , Long Cheng (Editor)  , Zheng-Guang Wu (Editor)    
 Publish Date:  November 27th, 2023   
 Publisher:   
 Springer   
 Add to wishlist     
  You must have JavaScript enabled to use this form.   
  Available Formats  Paperback (11/27/2023)  Paperback (11/15/2023)  Digital - Audio (Libro.FM)     
 Usually Ships in 1 to 5 Days   
  The ICONIP conference aims to provide a leading international forum for researchers, scientists, and industry professionals who are working in neuroscience, neural networks, deep learning, and related fields to share their new ideas, progress, and achievements.  
 Other Books in Series  
 Upcoming Event  
 Dec  05    
 Interrogative Design Symposium

output:1. ICNC_2 information:
2. ICNC_3 information:
3. ICNP_0 information:
4. ICNP_2 information:
5. ICNP_3 information:
6. ICOCI_0 information:
7. ICOCI_1 information:
8. ICOCI_2 information:
9. ICOCI_3 information:
10. ICONIP_0 information:
