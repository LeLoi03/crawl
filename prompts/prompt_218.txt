input:
1. NOSSDAV_0 conference:
Network and Operating System Support for Digital Audio and Video (NOSSDAV)  
 NOSSDAV is a uniquely interactive and discussion-oriented workshop attended by top researchers, students, developers, and practitioners from academia and industry to discuss new ideas and future directions in multimedia applications, networking, operating systems, and other related areas of computing. Over the years, the scope of NOSSDAV has broadened to include networked games, sensor networks, multimedia interfaces and peer-to-peer networking.  
 NOSSDAV takes place annually and is co-located with ACM MMSys  . NOSSDAV is guided by the ACM MMSys Steering Committee.  
 Number | Year | Dates | Venue | Chair(s) 
  Last updated on Apr. 28, 2024 by acbegen.
2. NOSSDAV_1 conference:
Organization 
  Call 
  Submission Process 
  Program 
  Attending 
 WELCOME TO ACM NOSSDAV 2023   
  Metaverse, Web3 Multimedia, NFT 
 Submission Process  
  Prospective authors are invited to submit an electronic version of full papers, in PDF format, up to six (6) printed pages plus one (1) page solely for references in length (double column ACM style format  ) at the workshop website. Authors must prepare their papers in a way that preserves the anonymity of the authors.  Please do NOT include the author names under the title. The workshop proceedings will be published by ACM Digital Library.  
 Submission deadline: 10th March, 2023 
  Acceptance notification: 31st March, 2023 
  Camera-ready deadline: 17th April, 2023 
  Speaker: Prof. Jiangchuan Liu (Simon Fraser University) 
  Abstract: Cyberspace has long had an ambitious goal --- connecting the world, understanding the world, and interacting with the world, both physically and virtually, for human beings and machines, anytime and anywhere. This remained a dream a decade ago. With the unprecedented development in the Information and Communication Technology (ICT) sector in the past decade, however, it is now solid and reachable to a great extent, if not all. In this talk, we will discuss our recent works on immersive media over the Internet and advanced mobile networks, from synergized neuro and traditional compression, to personalized distribution, and to multi-modal interaction. Besides human beings as consumers, we will further explore the new world when machines act as consumers, with a focus on mobile video analytics. We will also examine their applications in extreme environments as well as the opportunities and challenges from the latest advances in communication and computing infrastructures, including streaming over low earth orbit (LEO) satellite constellations, batteryless spatial acoustic sensing, and cross-region analytics. 
  Paper Session 1: Video Analytics and Media Representations   
 RepCaM: Re-parameterization Content-aware Modulation for Neural Video Delivery | Rongyu Zhang, Lixuan Du, Jiaming Liu, Congcong Song, Fangxin Wang, Xiaoqi Li, Ming Lu, Yandong Guo, Shanghang Zhang 
  DMGC:Deep Triangle Mesh Geometry Compression via Connectivity Prediction | Xudong Zhao, Xinyao Zeng, Linyao Gao, Yiling Xu, Yanfeng Wang 
  A Low Cost Cross-Platform Video/Image Process Framework Empowers Heterogeneous Edge Application | Danyang Song, Cong Zhang, Yifei Zhu, Jiangchuan Liu 
  Paper Session 2: Immersive Media and Emerging Networks   
 Will Dynamic Foveation Boost the Gaming Experience in Cloud VR? | Jia-Wei Fang, Kuan-You Lee, Teemu Kämäräinen, Matti Siekkinen, Cheng-Hsin Hsu 
  Implementing Partial Atlas Selector for Viewport-dependent MPEG Immersive Video Streaming | Soonbin Lee, Jong-Beom Jenong,Eun-Seok Ryu 
  Realtime Multimedia Services over Starlink: A Reality Check | Haoyuan Zhao, Hao Fang, Feng Wang, Jiangchuan Liu 
  Paper Session 3: Adaptive Video Streaming   
 Cross that boundary: Investigating the feasibility of cross-layer information sharing for enhancing ABR decision logic over QUIC | Joris Herbots, Arno Verstraete, Maarten Wijnants, Peter Quax, Wim Lamotte 
  Improving ABR Performance for Short Video Streaming Using Multi-Agent Reinforcement Learning with Expert Guidance | Yueheng Li, Qianyuan Zheng, Zicheng Zhang, Hao Chen, Zhan Ma 
 Attending  
  NOSSDAV'23 will be co-located with MMSys'23. Venue information will be posted later in the year.  
  Registration is to be completed on MMSys'23 Web site  for both the authors and attendees.
3. NPC_1 conference:
TENSOR 
  PhD symposium 
  Camera Ready | Conference 
  Workshops 
  Patrons 
  What to Visit 
 IFIP NETWORKING 2023   
 June 12-15, 2023   
 Barcelona, Spain   
 IFIP Networking 2023   
 The International Federation for Information Processing (IFIP) Networking 2023 Conference (NETWORKING 2023) will be held in Barcelona, Spain exclusively in-person.  
  This is the 22nd edition of the series, sponsored by the IFIP Technical Committee on Communication Systems (TC6). High quality papers will be recommended for fast track publications in selected journals.  
  The main objective of Networking 2023 is to bring together academic and industrial experts of the networking community to discuss the most recent advances in networking, to highlight key issues, identify trends, and develop a vision of the future Internet from a design, deployment and operation standpoints. Networking 2023 technical sessions will be structured around the following yet non limitative areas:  
  Network Architectures, Applications and Services 
 Best Paper Award   
 Event   
 Important dates   
 Full paper submission: | January 30 | February 13, 2023 (extended) 
  Notification of acceptance: | March 31 | April 14, 2023 
  Camera-ready version: | May 2 | May 6, 2023 
  Conference Date: | June 12-15, 2023 
  TENSOR 
  PhD symposium 
  Camera Ready | Conference 
  Workshops 
  Patrons
4. NPC_2 conference:
CFP 
  Organization 
  Submission | Paper submission 
  NPC Publication Model 
  Registration 
  Keynotes 
  Past Conference 
  NPC 2022 Accepted Submission 
  Camera Ready 
 Welcome to NPC 2022     
 The 19 th   Annual IFIP International Conference on Network and Parallel Computing (NPC 2022) will be held in Jinan, China on  September  24-25, 2022.    
 Important dates     
 Paper submission:    June 30, 2022   
 Author notification:    August 5, 2022   
  Extended to August 15, 2022   
 Camera-Ready:    August 20, 2022   
 Extended to August 25, 2022   
 Registration:   August 20, 2022    
 Conference date:    September 24-25, 2022   
 Postponed to December 14-15, 2022   
 (Due to the COVID-19 issue)   
 Sponsored By     
 Important Dates 
  Paper submission:  June 30, 2022   
 (Due to large amounts of extension requests)   
 Author notification:  August 5, 2022   
 Extended to August 15, 2022   
 Camera-Ready:  August 20, 2022   
 Extended to August 25, 2022   
 Registration:  August 20, 2022  
 Conference date:  September 24-25, 2022   
 Postponed to December 14-15, 2022   
 (Due to the COVID-19 issue) 
 Contact Info 
  If you have any questions or queries on NPC 2022, please contact us by email  
  Email address:  
  NPC-2022@outlook.com 
 Important Dates 
  Paper submission:  June 30, 2022   
 (Due to large amounts of extension requests)   
 Author notification:  August 5, 2022   
 Extended to August 15, 2022   
 Camera-Ready:  August 20, 2022   
 Extended to August 25, 2022   
 Registration:  August 20, 2022  
 Conference date:  September 24-25, 2022   
 Postponed to December 14-15, 2022   
 (Due to the COVID-19 issue) 
 Contact Info 
  If you have any questions or queries on NPC 2022, please contact us by email  
  Email address:  
  NPC-2022@outlook.com 
 Jilin University
5. NPC_3 conference:
中文  |  English  |  Español  |  日本語     
 Conference Partner  » Conferences  » NPC    
  Conference Information   
 NPC 2024: International Conference on Network and Parallel Computing  
 Submission Date: | 2024-08-15 Extended 
 Notification Date: | 2024-10-01 
 Conference Date: | 2024-12-07 
 Location: | Haikou, China 
  Call For Papers   
 The IFIP International Conference on Network and Parallel Computing (NPC) is a prestigious annual gathering that serves as a global platform for researchers, academics, and industry professionals to explore and exchange cutting-edge ideas, research findings, and innovative solutions in the fields of network, distributed and parallel computing. Topics of interest include, but are not limited to: Networked, distributed and parallel computing applications and algorithms • Networked, distributed and parallel computing issues and opportunities with Artificial intelligence applications. • Networked, distributed and parallel algorithms for computational and data-enabled scientific, engineering, biological, and medical applications. • Networked, distributed and parallel algorithms for accelerators, neuromorphic architectures, and other emerging architectures. Networked, distributed and parallel computing architectures and systems • Computing and network convergence technologies and systems, computing power networks, computing utility • Emerging architectures and systems at all scales, from embedded, device, edge to cloud. • Novel distributed and parallel architectures and systems for AI model training and inference, domain-specific accelerators for AI • High-performance computing, systems for enabling parallelism at extreme scale • Power-efficient and green computing systems • Non-traditional Computing Technology • Neuromorphic architectures and cognitive computing accelerators • Heterogeneous multicore architectures and accelerators • In-Memory and near-data computing. • Network and interconnect architectures. • Storage systems with novel architectures. Networked, distributed and parallel computing software environments and tools • Programming models and compilation for existing and emerging platforms. • Dataflow programming models, frameworks, languages and environments for data-enabled platforms. • Virtualization of machines, networks, and storage. • I/O, file systems, and data management. • Resource management, scheduling, and load balancing.  Last updated by Kang Yucheng  in 2024-07-30   
  Acceptance Ratio   
 Year | Submitted | Accepted | Accepted(%) 
  Related Conferences   
 CCF | CORE | QUALIS | Short | Full Name | Submission | Notification | Conference 
 NLCAI | International Conference on Natural Language Computing and AI | 2023-03-04 | 2023-03-11 | 2023-03-18 
 CDICS | International Conference on Data, Information and Computing Science | 2024-11-05 | 2024-11-20 | 2024-12-06 
 AISC' | International Conference on Artificial Intelligence, Soft Computing | 2023-07-08 | 2023-07-22 | 2023-07-22 
 Short | Full Name | Submission | Conference 
 NLCAI | International Conference on Natural Language Computing and AI | 2023-03-04 | 2023-03-18 
 CDICS | International Conference on Data, Information and Computing Science | 2024-11-05 | 2024-12-06 
 AISC' | International Conference on Artificial Intelligence, Soft Computing | 2023-07-08 | 2023-07-22
6. NSDI_1 conference:
Questions 
 NSDI '23 Technical Sessions  
 Papers and Proceedings  
 The full Proceedings published by USENIX for the symposium are available for download below. Individual papers can also be downloaded from their respective presentation pages. Copyright to the individual works is retained by the author[s].  
 Proceedings Front Matter   
  Proceedings Cover  | Title Page and List of Organizers  | Message from the Program Co-Chairs  | Table of Contents   
   NSDI '23 Errata Slip #1 (PDF)   
 Attendee Files   
 (Registered attendees: Sign in  to your USENIX account to download these files.)  
  NSDI '23 Attendee List (PDF)    
  Expanded 
 Monday, April 17, 2023   
 Continental Breakfast  
 Available Media   
 Intra-host networking was considered robust in the RDMA (Remote Direct Memory Access) network and received little attention. However, as the RNIC (RDMA NIC) line rate increases rapidly to multi-hundred gigabits, the intra-host network becomes a potential performance bottleneck for network applications. Intra-host network bottlenecks may result in degraded intra-host bandwidth and increased intra-host latency, which can severely impact network performance. However, when intra-host bottlenecks occur, they can hardly be noticed due to the lack of a monitoring system. Furthermore, existing bottleneck diagnosis mechanisms fail to diagnose intra-host bottlenecks efficiently. In this paper, we analyze the symptom of intra-host bottlenecks based on our longterm troubleshooting experience and propose Hostping, the first bottleneck monitoring and diagnosis system dedicated to intra-host networks. The core idea of Hostping is conducting loopback tests between RNICs and endpoints within the host to measure intra-host latency and bandwidth. Hostping not only discovers intra-host bottlenecks we already knew but also reveals six bottlenecks we did not notice before.  
 Understanding RDMA Microarchitecture Resources for Performance Isolation   
 Available Media   
 Recent years have witnessed the wide adoption of RDMA in the cloud to accelerate first-party workloads and achieve cost savings by freeing up CPU cycles. Now cloud providers are working towards supporting RDMA in general-purpose guest VMs to benefit third-party workloads. To this end, cloud providers must provide strong performance isolation so that the RDMA workloads of one tenant do not adversely impact the RDMA performance of another tenant. Despite many efforts on network performance isolation in the public cloud, we find that RDMA brings unique challenges due to its complex NIC microarchitecture resources (e.g., the NIC cache).  
 In this paper, we aim to systematically understand the impact of RNIC microarchitecture resources on performance isolation. We present a model that represents how RDMA operations use RNIC resources. Using this model, we develop a test suite to evaluate RDMA performance isolation solutions. Our test suite can break all existing solutions in various scenarios. Our results are acknowledged and reproduced by one of the largest RDMA NIC vendors. Finally, based on the test results, we summarize new insights on designing future RDMA performance isolation solutions.  
 Empowering Azure Storage with RDMA   
 Available Media   
 Containers are widely used for resource management in datacenters. A common practice to support deep learning (DL) training in container clouds is to statically bind GPUs to containers in entirety. Due to the diverse resource demands of DL jobs in production, a significant number of GPUs are underutilized. As a result, GPU clusters have low GPU utilization, which leads to a long job completion time because of queueing.  
 We present TGS (Transparent GPU Sharing), a system that provides transparent GPU sharing to DL training in container clouds. In stark contrast to recent application-layer solutions for GPU sharing, TGS operates at the OS layer beneath containers. Transparency allows users to use any software to develop models and run jobs in their containers. TGS leverages adaptive rate control and transparent unified memory to simultaneously achieve high GPU utilization and performance isolation. It ensures that production jobs are not greatly affected by opportunistic jobs on shared GPUs. We have built TGS and integrated it with Docker and Kubernetes. Experiments show that (i) TGS has little impact on the throughput of production jobs; (ii) TGS provides similar throughput for opportunistic jobs as the state-of-the-art application-layer solution AntMan, and improves their throughput by up to 15× compared to the existing OS-layer solution MPS.  
 Available Media   
 Modern state-of-the-art deep learning (DL) applications tend to scale out to a large number of parallel GPUs. Unfortunately, we observe that the collective communication overhead across GPUs is often the key limiting factor of performance for distributed DL. It under-utilizes the networking bandwidth by frequent transfers of small data chunks, which also incurs a substantial I/O overhead on GPU that interferes with computation on GPU. The root cause lies in the inefficiency of CPU-based communication event handling as well as the inability to control the GPU's internal DMA engine with GPU threads.  
 To address the problem, we propose a GPU-driven code execution system that leverages a GPU-controlled hardware DMA engine for I/O offloading. Our custom DMA engine pipelines multiple DMA requests to support efficient small data transfer while it eliminates the I/O overhead on GPU cores. Unlike existing GPU DMA engines initiated only by CPU, we let GPU threads to directly control DMA operations, which leads to a highly efficient system where GPUs drive their own execution flow and handle communication events autonomously without CPU intervention. Our prototype DMA engine achieves a line-rate from a message size as small as 8KB (3.9x better throughput) with only 4.3us of communication latency (9.1x faster) while it incurs little interference with computation on GPU, achieving 1.8x higher all-reduce throughput in a real training workload.  
 Available Media   
 Graph neural networks (GNNs) have extended the success of deep neural networks (DNNs) to non-Euclidean graph data, achieving ground-breaking performance on various tasks such as node classification and graph property prediction. Nonetheless, existing systems are inefficient to train large graphs with billions of nodes and edges with GPUs. The main bottlenecks are the process of preparing data for GPUs – subgraph sampling and feature retrieving. This paper proposes BGL, a distributed GNN training system designed to address the bottlenecks with a few key ideas. First, we propose a dynamic cache engine to minimize feature retrieving traffic. By co-designing caching policy and the order of sampling, we find a sweet spot of low overhead and a high cache hit ratio. Second, we improve the graph partition algorithm to reduce cross-partition communication during subgraph sampling. Finally, careful resource isolation reduces contention between different data preprocessing stages. Extensive experiments on various GNN models and large graph datasets show that BGL significantly outperforms existing GNN training systems by 1.9x on average.  
 Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training   
 Remote Procedure Call (RPC) is a widely used abstraction for cloud computing. The programmer specifies type information for each remote procedure, and a compiler generates stub code linked into each application to marshal and unmarshal arguments into message buffers. Increasingly, however, application and service operations teams need a high degree of visibility and control over the flow of RPCs between services, leading many installations to use sidecars or service mesh proxies for manageability and policy flexibility. These sidecars typically involve inspection and modification of RPC data that the stub compiler had just carefully assembled, adding needless overhead. Further, upgrading diverse application RPC stubs to use advanced hardware capabilities such as RDMA or DPDK is a long and involved process, and often incompatible with sidecar policy control.  
 In this paper, we propose, implement, and evaluate a novel approach, where RPC marshalling and policy enforcement are done as a system service rather than as a library linked into each application. Applications specify type information to the RPC system as before, while the RPC service executes policy engines and arbitrates resource use, and then marshals data customized to the underlying network hardware capabilities. Our system, mRPC, also supports live upgrades so that both policy and marshalling code can be updated transparently to application code. Compared with using a sidecar, mRPC speeds up a standard microservice benchmark, DeathStarBench, by up to 2.5× while having a higher level of policy flexibility and availability.  
 Canvas: Isolated and Adaptive Swapping for Multi-Applications on Remote Memory   
 Available Media   
 Conventional host networking features various traffic shaping layers (e.g., buffers, schedulers, and pacers) with complex interactions and wide implications for performance metrics. These interactions can lead to large bursts at various time scales. Understanding the nature of traffic bursts is important for optimal resource provisioning, congestion control, buffer sizing, and traffic prediction but is challenging due to the complexity and feature velocity in host networking.  
 We develop Valinor, a traffic measurement framework that consists of eBPF hooks and measurement modules in a programmable network. Valinor offers visibility into traffic burstiness over a wide span of timescales (nanosecond- to secondscale) at multiple vantage points. We deploy Valinor to analyze the burstiness of various classes of congestion control algorithms, qdiscs, Linux process scheduling, NIC packet scheduling, and hardware offloading. Our analysis counters the assumption that burstiness is primarily a function of the application layer and preserved by protocol stacks, and highlights the pronounced role of lower layers in the formation and suppression of bursts. We also show the limitations of canonical burst countermeasures (e.g., TCP pacing and qdisc scheduling) due to the intervening nature of segmentation offloading and fixed-function NIC scheduling. Finally, we demonstrate that, far from a universal invariant, burstiness varies significantly across host stacks. Our findings underscore the need for a measurement framework such as Valinor for regular burst analysis.  
 Poseidon: Efficient, Robust, and Practical Datacenter CC via Deployable INT   
 Today's distributed tracing frameworks are ill-equipped to troubleshoot rare edge-case requests. The crux of the problem is a trade-off between specificity and overhead. On the one hand, frameworks can indiscriminately select requests to trace when they enter the system (head sampling), but this is unlikely to capture a relevant edge-case trace because the framework cannot know which requests will be problematic until after-the-fact. On the other hand, frameworks can trace everything and later keep only the interesting edge-case traces (tail sampling), but this has high overheads on the traced application and enormous data ingestion costs.  
 In this paper we circumvent this trade-off for any edge-case with symptoms that can be programmatically detected, such as high tail latency, errors, and bottlenecked queues. We propose a lightweight and always-on distributed tracing system, Hindsight, which implements a retroactive sampling abstraction: instead of eagerly ingesting and processing traces, Hindsight lazily retrieves trace data only after symptoms of a problem are detected. Hindsight is analogous to a car dash-cam that, upon detecting a sudden jolt in momentum, persists the last hour of footage. Developers using Hindsight receive the exact edge-case traces they desire without undue overhead or dependence on luck. Our evaluation shows that Hindsight scales to millions of requests per second, adds nanosecondlevel overhead to generate trace data, handles GB/s of data per node, transparently integrates with existing distributed tracing systems, and successfully persists full, detailed traces in real-world use cases when edge-case problems are detected.  
 DiSh: Dynamic Shell-Script Distribution   
 Available Media   
 Shell scripting remains prevalent for automation and data-processing tasks, partly due to its dynamic features—e.g., expansion, substitution—and language agnosticism—i.e., the ability to combine third-party commands implemented in any programming language. Unfortunately, these characteristics hinder automated shell-script distribution, often necessary for dealing with large datasets that do not fit on a single computer. This paper introduces DiSh, a system that distributes the execution of dynamic shell scripts operating on distributed filesystems. DiSh is designed as a shim that applies program analyses and transformations to leverage distributed computing, while delegating all execution to the underlying shell available on each computing node. As a result, DiSh does not require modifications to shell scripts and maintains compatibility with existing shells and legacy functionality. We evaluate DiSh against several options available to users today: (i) Bash, a single-node shell-interpreter baseline, (ii) PaSh, a state-of-the-art automated-parallelization system, and (iii) Hadoop Streaming, a MapReduce system that supports language-agnostic third-party components. Combined, our results demonstrate that DiSh offers significant performance gains, requires no developer effort, and handles arbitrary dynamic behaviors pervasive in real-world shell scripts.  
 Waverunner: An Elegant Approach to Hardware Acceleration of State Machine Replication   
 Available Media   
 State machine replication (SMR) is a core mechanism for building highly available and consistent systems. In this paper, we propose Waverunner, a new approach to accelerate SMR using FPGA-based SmartNICs. Our approach does not implement the entire SMR system in hardware; instead, it is a hybrid software/hardware system. We make the observation that, despite the complexity of SMR, the most common routine—the data replication—is actually simple. The complex parts (leader election, failure recovery, etc.) are rarely used in modern datacenters where failures are only occasional. These complex routines are not performance critical; their software implementations are fast enough and do not need acceleration. Therefore, our system uses FPGA assistance to accelerate data replication, and leaves the rest to the traditional software implementation of SMR.  
 Atsutse Kludze and Yasaman Ghasempour, Princeton University   
  Awarded Best Paper!    
 Available Media   
 Available Media   
 To comply with the increasing number of government regulations about data placement and processing, and to protect themselves against major cloud outages, many users want the ability to easily migrate their workloads between clouds. In this paper we propose doing so not by imposing uniform and comprehensive standards, but by creating a fine-grained two-sided market via an intercloud broker. These brokers will allow users to view the cloud ecosystem not just as a collection of individual and largely incompatible clouds but as a more integrated Sky of Computing. We describe the design and implementation of an intercloud broker, named SkyPilot, evaluate its benefits, and report on its real-world usage.  
 Unlocking unallocated cloud capacity for long, uninterruptible workloads   
 Available Media   
 DNN models across many domains continue to grow in size, resulting in high resource requirements for effective training, and unpalatable (and often unaffordable) costs for organizations and research labs across scales. This paper aims to significantly reduce training costs with effective use of preemptible instances, i.e., those that can be obtained at a much cheaper price while idle, but may be preempted whenever requested by priority users. Doing so, however, requires new forms of resiliency and efficiency to cope with the possibility of frequent preemptions – a failure model that is drastically different from the occasional failures in normal cluster settings that existing checkpointing techniques target.  
 We present Bamboo, a distributed system that tackles these challenges by introducing redundant computations into the training pipeline, i.e., whereby one node performs computations over not only its own layers but also over some layers in its neighbor. Our key insight is that training large models often requires pipeline parallelism where "pipeline bubbles" naturally exist. Bamboo carefully fills redundant computations into these bubbles, providing resilience at a low cost. Across a variety of widely used DNN models, Bamboo outperforms traditional checkpointing by 3.7× in training throughput, and reduces costs by 2.4× compared to a setting where on-demand instances are used.  
 NSDI '23 Poster Session and Reception  
 Sponsored by Amazon   
 Palm Garden  
 Check out the cool new ideas and the latest preliminary research on display at the Poster Session and Reception. Enjoy dinner, drinks, and the chance to connect with other attendees, authors, and symposium organizers. View the list of accepted posters  .  
 Tuesday, April 18, 2023   
 Continental Breakfast  
 Available Media   
 Configuration of production datacenters is challenging due to their scale (many switches), complexity (specific policy requirements), and dynamism (need for many configuration changes). This paper introduces Aura, a production-level synthesis system for datacenter routing policies. It consists of a high-level language, called RPL, that expresses the desired behavior and a compiler that automatically generates switch configurations. Unlike existing approaches, which generate full network configuration for a static policy, Aura is built to support frequent policy and network changes. It generates and deploys multiple parallel policy collections, in a way that supports smooth transitions between them without disrupting live production traffic. Aura has been deployed for over two years in Meta datacenters and has greatly improved our management efficiency. We also share our operational requirements and experiences, which can potentially inspire future research.  
 Formal Methods for Network Performance Analysis   
 Available Media   
 In this paper, we consider how to provide fast estimates of flow-level tail latency performance for very large scale data center networks. Network tail latency is often a crucial metric for cloud application performance that can be affected by a wide variety of factors, including network load, inter-rack traffic skew, traffic burstiness, flow size distributions, oversubscription, and topology asymmetry. Network simulators such as ns-3 and OMNeT++ can provide accurate answers, but are very hard to parallelize, taking hours or days to answer what if questions for a single configuration at even moderate scale. Recent work with MimicNet has shown how to use machine learning to improve simulation performance, but at a cost of including a long training step per configuration, and with assumptions about workload and topology uniformity that typically do not hold in practice.  
 We address this gap by developing a set of techniques to provide fast performance estimates for large scale networks with general traffic matrices and topologies. A key step is to decompose the problem into a large number of parallel independent single-link simulations; we carefully combine these link-level simulations to produce accurate estimates of end-to-end flow level performance distributions for the entire network. LikeMimicNet, we exploit symmetry where possible to gain additional speedups, but without relying on machine learning, so there is no training delay. On a large-scale network where ns-3 takes 11 to 27 hours to simulate five seconds of network behavior, our techniques runin one to two minutes with accuracy within 9% for tail flow completion times.  
 Available Media   
 Modern datacenter applications are concurrent, so they require synchronization to control access to shared data. Requests can contend for different combinations of locks, depending on application and request state. In this paper, we show that locks, especially blocking synchronization, can squander throughput and harm tail latency, even when the CPU is underutilized. Moreover, the presence of a large number of contention points, and the unpredictability in knowing which locks a request will require, make it difficult to prevent contention through overload control using traditional signals such as queueing delay and CPU utilization.  
 The breakthroughs in deep learning enable unstructured data to be represented as high-dimensional feature vectors for serving a wide range of applications. Processing vector queries (i.e., finding the nearest neighbor vectors for an input vector) for large unstructured datasets (with billions of items) is challenging, especially for applications with strict service level objectives (SLOs). Existing solutions trade query accuracy for latency, but without any guarantees, causing SLO violations.  
 This paper presents Auncel, a vector query engine for large unstructured datasets that provides bounded query errors and bounded query latencies. The core idea of Auncel is to exploit local geometric properties of individual query vectors to build a precise error-latency profile (ELP) for each query. This profile enables Auncel to sample the right amount of data to process a given query while satisfying its error or latency requirements. Auncel is a distributed solution that can scale out with multiple workers. We evaluate Auncel with a variety of benchmarking datasets. The experimental results show that Auncel outperforms state-of-the-art approximate solutions by up to 10× on query latency with the same error bound (≤ 10%). In particular, Auncel only takes 25 ms to process a vector query on the DEEP1B dataset that contains one billion items with four c5.metal EC2 instances.  
 Arya: Arbitrary Graph Pattern Mining with Decomposition-based Sampling   
 Available Media   
 Cross-silo federated learning (FL) adopts various cryptographic operations to preserve data privacy, which introduces significant performance overhead. In this paper, we identify nine widely-used cryptographic operations and design an efficient hardware architecture to accelerate them. However, directly offloading them on hardware statically leads to (1) inadequate hardware acceleration due to the limited resources allocated to each operation; (2) insufficient resource utilization, since different operations are used at different times. To address these challenges, we propose FLASH, a high-performance hardware acceleration architecture for cross-silo FL systems. At its heart, FLASH extracts two basic operators—modular exponentiation and multiplication— behind the nine cryptographic operations and implements them as highly-performant engines to achieve adequate acceleration. Furthermore, it leverages a dataflow scheduling scheme to dynamically compose different cryptographic operations based on these basic engines to obtain sufficient resource utilization. We have implemented a fully-functional FLASH prototype with Xilinx VU13P FPGA and integrated it with FATE, the most widely-adopted cross-silo FL framework. Experimental results show that, for the nine cryptographic operations, FLASH achieves up to 14.0× and 3.4× acceleration over CPU and GPU, translating to up to 6.8× and 2.0× speedup for realistic FL applications, respectively. We finally evaluate the FLASH design as an ASIC, and it achieves 23.6× performance improvement upon the FPGA prototype.  
 Large-scale cloud providers rely on cluster managers for container allocation and load balancing (e.g., Kubernetes), VM provisioning (e.g., Protean), and other management tasks. These cluster managers use algorithms or heuristics whose behavior depends upon multiple configuration parameters. Currently, operators manually set these parameters using a combination of domain knowledge and limited testing. In very large-scale and dynamic environments, these manually-set parameters may lead to sub-optimal cluster states, adversely affecting important metrics such as latency and throughput.  
 In this paper we describe SelfTune, a framework that automatically tunes such parameters in deployment. SelfTune piggybacks on the iterative nature of cluster managers which, through multiple iterations, drives a cluster to a desired state. Using a simple interface, developers integrate SelfTune into the cluster manager code, which then uses a principled reinforcement learning algorithm to tune important parameters over time. We have deployed SelfTune on tens of thousands of machines that run a large-scale background task scheduler at Microsoft. SelfTune has improved throughput by as much as 20% in this deployment by continuously tuning a key configuration parameter that determines the number of jobs concurrently accessing CPU and disk on every machine. We also evaluate SelfTune with two Azure FaaS workloads, the Kubernetes Vertical Pod Autoscaler, and the DeathStar microservice benchmark. In all cases, SelfTune significantly improves cluster performance.  
 CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation   
 Abdullah Alomar, Pouya Hamadanian, Arash Nasr-Esfahany, Anish Agarwal, Mohammad Alizadeh, and Devavrat Shah, MIT   
  Awarded Best Paper!    
 Available Media   
 Available Media   
 LoRa is one of the most widely used LPWAN communication techniques operating in the unlicensed sub-GHz ISM bands. Its long range however also results in increased interference from other LoRa and non-LoRa networks, undermining network throughput due to packet collisions. This has motivated extensive research in the area of collision resolution techniques for concurrent LoRa transmissions and continues to be a topic of interest. In this paper, we verify the implementation and efficacy of four of the most recent works on LoRa packet collisions, in addition to standard LoRa. We implement OpenLoRa, an open-source, unified platform to evaluate these works and is extensible for future researchers to compare against existing works. We implement each of the four techniques in Python as well as separate the demodulator and decoder to provide benchmarks for future demodulators that can be plugged into the framework for fair and easy comparison against existing works. Our evaluation indicates that existing contention resolution techniques fall short in their throughput performance, especially due to poor packet detection in low and ultra-low SNR regimes.  
 VeCare: Statistical Acoustic Sensing for Automotive In-Cabin Monitoring   
 Available Media   
 Advances in wireless technologies have transformed wireless networks from a pure communication medium to a pervasive sensing platform, enabling many sensorless and contactless applications. After years of effort, wireless sensing approaches centering around conventional signal processing are approaching their limits, and meanwhile, deep learning-based methods become increasingly popular and have seen remarkable progress. In this paper, we explore an unseen opportunity to push the limit of wireless sensing by jointly employing learning-based spectrogram generation and spectrogram learning. To this end, we present SLNet, a new deep wireless sensing architecture with spectrogram analysis and deep learning co-design. SLNet employs neural networks to generate super-resolution spectrogram, which overcomes the limitation of the time-frequency uncertainty. It then utilizes a novel polarized convolutional network that modulates the phase of the spectrograms for learning both local and global features. Experiments with four applications, i.e., gesture recognition, human identification, fall detection, and breathing estimation, show that SLNet achieves the highest accuracy with the smallest model and lowest computation among the state-of-the-art models. We believe the techniques in SLNet can be widely applied to fields beyond WiFi sensing.  
 Wednesday, April 19, 2023   
 Continental Breakfast  
 Available Media   
 The promise of in-network computing continues to be unrealized in realistic deployments (e.g., clouds and ISPs) as serving concurrent stateful applications on a programmable switch is challenging today due to limited switch's on-chip resources. In this paper, we argue that an on-rack switch resource augmentation architecture that augments a programmable switch with other programmable network hardware, such as smart NICs, on the same rack can be a pragmatic and incrementally scalable solution. To realize this vision, we design and implement ExoPlane, an operating system for on-rack switch resource augmentation to support multiple concurrent applications. In designing ExoPlane, we propose a practical runtime operating model and state abstraction to address challenges in managing application states correctly across multiple devices with minimal performance and resource overheads. Our evaluation with various P4 applications shows that ExoPlane can provide applications with low latency, scalable throughput, and fast failover while achieving these with small resource overheads and no or little modifications on applications.  
 Sketchovsky: Enabling Ensembles of Sketches on Programmable Switches   
 Futuristic integrated space and terrestrial networks (ISTN) not only hold new opportunities for pervasive, low-latency Internet services, but also face new challenges caused by satellite dynamics on a global scale. It should be useful for researchers to run various experiments to systematically explore new problems in ISTNs. However, existing experimentation methods either attain realism but lack flexibility (e.g. live satellites), or achieve flexibility but lack realism (e.g. ISTN simulators).  
 This paper presents StarryNet, a novel experimentation framework that enables researchers to conveniently build credible and flexible experimental network environments (ENE) mimicking satellite dynamics and network behaviors of large-scale ISTNs. StarryNet simultaneously achieves constellation-consistency, networked system realism and flexibility, by adopting a real-data-driven, lightweight-emulation-aided approach to build a digital twin of physical ISTNs in the terrestrial virtual environment. Driven by public and real constellation-relevant information, we show StarryNet's acceptable fidelity and demonstrate its flexibility to support various ISTN experiments, such as evaluating different inter-networking mechanisms for space-ground integration, and assessing the network resilience of futuristic ISTNs.  
 POLYCORN: Data-driven Cross-layer Multipath Networking for High-speed Railway through Composable Schedulerlets   
 Available Media   
 Acoustic sensing is increasingly popular owing to widely available devices that support them. Yet the sensing resolution and range are still limited due to limited bandwidth and sharp decay in the signal at inaudible frequencies. Inspired by recent development in acoustic metasurfaces, in this paper, we first perform an in-depth study of acoustic metasurface (AMS) and compare it with the phased array speaker. Our results show that AMS is attractive as it achieves a significant SNR increase while maintaining a compact size. A major limitation of existing AMS is its static configuration. Since our target may be at any possible location, it is important to support scanning in different directions. We develop a novel acoustic system that leverages a metasurface and a small number of speakers. We jointly optimize the configuration of metasurface and transmission signals from the speakers to achieve low-cost dynamic steering. Using a prototype implementation and extensive evaluation, we demonstrate its effectiveness in improving SNR, acoustic sensing accuracy, and acoustic communication reliability over a wide range of scenarios.  
 Serverless applications are typically composed of function workflows in which multiple short-lived functions are triggered to exchange data in response to events or state changes. Current serverless platforms coordinate and trigger functions by following high-level invocation dependencies but are oblivious to the underlying data exchanges between functions. This design is neither efficient nor easy to use in orchestrating complex workflows – developers often have to manage complex function interactions by themselves, with customized implementation and unsatisfactory performance.  
 In this paper, we argue that function orchestration should follow a data-centric approach. In our design, the platform provides a data bucket abstraction to hold the intermediate data generated by functions. Developers can use a rich set of data trigger primitives to control when and how the output of each function should be passed to the next functions in a workflow. By making data consumption explicit and allowing it to trigger functions and drive the workflow, complex function interactions can be easily and efﬁciently supported. We present Pheromone – a scalable, low-latency serverless platform following this data-centric design. Compared to well-established commercial and open-source platforms, Pheromone cuts the latencies of function interactions and data exchanges by orders of magnitude, scales to large workflows, and enables easy implementation of complex applications.  
 Doing More with Less: Orchestrating Serverless Applications without an Orchestrator   
 DOTE: Rethinking (Predictive) WAN Traffic Engineering   
 Yarin Perry, Hebrew University of Jerusalem;  Felipe Vieira Frujeri, Microsoft Research;  Chaim Hoch, Hebrew University of Jerusalem;  Srikanth Kandula and Ishai Menache, Microsoft Research;  Michael Schapira, Hebrew University of Jerusalem;  Aviv Tamar, Technion   
  Awarded Best Paper!    
 Available Media   
 Available Media   
 Billions of people remain without Internet access due to availability or affordability of service. In this paper, we present Magma, an open and flexible system for building low-cost wireless access networks. Magma aims to connect users where operator economics are difficult due to issues such as low population density or income levels while preserving features expected in cellular networks such as authentication and billing policies. To achieve this, and in contrast to traditional cellular networks, Magma adopts an approach that extensively leverages Internet design patterns, terminating access network-specific protocols at the edge and abstracting the access network from the core architecture. This decision allows Magma to refactor the wireless core using SDN (software-defined networking) principles and leverage other techniques from modern distributed systems. In doing so, Magma lowers cost and operational complexity for network operators while achieving resilience, scalability, and rich policy support.  
 Modern applications have been emerging towards a cloud-based programming model where applications depend on cloud services for various functionalities. Such “cloud native” practice greatly simplifies application deployment and realizes cloud benefits (e.g., availability). Meanwhile, it imposes emerging reliability challenges for addressing fault models of the opaque cloud and less predictable Internet connections.  
 In this paper, we discuss these reliability challenges. We develop a taxonomy of bugs that render cloud-backed applications vulnerable to common transient faults. We show that (mis)handling transient error(s) of even one REST call interaction can adversely affect application correctness.
7. NSDI_2 conference:
Questions 
 NSDI '23 Fall Accepted Papers  
 NSDI '23 offers authors the choice of two submission deadlines. The list of accepted papers from the fall deadline is available below. The full program will be available soon.  
 Fall Accepted Papers  
 Available Media   
 Containers are widely used for resource management in datacenters. A common practice to support deep learning (DL) training in container clouds is to statically bind GPUs to containers in entirety. Due to the diverse resource demands of DL jobs in production, a significant number of GPUs are underutilized. As a result, GPU clusters have low GPU utilization, which leads to a long job completion time because of queueing.  
 We present TGS (Transparent GPU Sharing), a system that provides transparent GPU sharing to DL training in container clouds. In stark contrast to recent application-layer solutions for GPU sharing, TGS operates at the OS layer beneath containers. Transparency allows users to use any software to develop models and run jobs in their containers. TGS leverages adaptive rate control and transparent unified memory to simultaneously achieve high GPU utilization and performance isolation. It ensures that production jobs are not greatly affected by opportunistic jobs on shared GPUs. We have built TGS and integrated it with Docker and Kubernetes. Experiments show that (i) TGS has little impact on the throughput of production jobs; (ii) TGS provides similar throughput for opportunistic jobs as the state-of-the-art application-layer solution AntMan, and improves their throughput by up to 15× compared to the existing OS-layer solution MPS.  
 Available Media   
 Cross-silo federated learning (FL) adopts various cryptographic operations to preserve data privacy, which introduces significant performance overhead. In this paper, we identify nine widely-used cryptographic operations and design an efficient hardware architecture to accelerate them. However, directly offloading them on hardware statically leads to (1) inadequate hardware acceleration due to the limited resources allocated to each operation; (2) insufficient resource utilization, since different operations are used at different times. To address these challenges, we propose FLASH, a high-performance hardware acceleration architecture for cross-silo FL systems. At its heart, FLASH extracts two basic operators—modular exponentiation and multiplication— behind the nine cryptographic operations and implements them as highly-performant engines to achieve adequate acceleration. Furthermore, it leverages a dataflow scheduling scheme to dynamically compose different cryptographic operations based on these basic engines to obtain sufficient resource utilization. We have implemented a fully-functional FLASH prototype with Xilinx VU13P FPGA and integrated it with FATE, the most widely-adopted cross-silo FL framework. Experimental results show that, for the nine cryptographic operations, FLASH achieves up to 14.0× and 3.4× acceleration over CPU and GPU, translating to up to 6.8× and 2.0× speedup for realistic FL applications, respectively. We finally evaluate the FLASH design as an ASIC, and it achieves 23.6× performance improvement upon the FPGA prototype.  
 SlimWiFi: Ultra-Low-Power IoT Radio Architecture Enabled by Asymmetric Communication   
 DOTE: Rethinking (Predictive) WAN Traffic Engineering   
 Yarin Perry, Hebrew University of Jerusalem;  Felipe Vieira Frujeri, Microsoft Research;  Chaim Hoch, Hebrew University of Jerusalem;  Srikanth Kandula and Ishai Menache, Microsoft Research;  Michael Schapira, Hebrew University of Jerusalem;  Aviv Tamar, Technion   
  Awarded Best Paper!    
 Award:   
 Best Paper   
 Available Media   
 Remote Procedure Call (RPC) is a widely used abstraction for cloud computing. The programmer specifies type information for each remote procedure, and a compiler generates stub code linked into each application to marshal and unmarshal arguments into message buffers. Increasingly, however, application and service operations teams need a high degree of visibility and control over the flow of RPCs between services, leading many installations to use sidecars or service mesh proxies for manageability and policy flexibility. These sidecars typically involve inspection and modification of RPC data that the stub compiler had just carefully assembled, adding needless overhead. Further, upgrading diverse application RPC stubs to use advanced hardware capabilities such as RDMA or DPDK is a long and involved process, and often incompatible with sidecar policy control.  
 In this paper, we propose, implement, and evaluate a novel approach, where RPC marshalling and policy enforcement are done as a system service rather than as a library linked into each application. Applications specify type information to the RPC system as before, while the RPC service executes policy engines and arbitrates resource use, and then marshals data customized to the underlying network hardware capabilities. Our system, mRPC, also supports live upgrades so that both policy and marshalling code can be updated transparently to application code. Compared with using a sidecar, mRPC speeds up a standard microservice benchmark, DeathStarBench, by up to 2.5× while having a higher level of policy flexibility and availability.  
 Sketchovsky: Enabling Ensembles of Sketches on Programmable Switches   
 Modern applications have been emerging towards a cloud-based programming model where applications depend on cloud services for various functionalities. Such “cloud native” practice greatly simplifies application deployment and realizes cloud benefits (e.g., availability). Meanwhile, it imposes emerging reliability challenges for addressing fault models of the opaque cloud and less predictable Internet connections.  
 In this paper, we discuss these reliability challenges. We develop a taxonomy of bugs that render cloud-backed applications vulnerable to common transient faults. We show that (mis)handling transient error(s) of even one REST call interaction can adversely affect application correctness.  
 Available Media   
 Modern state-of-the-art deep learning (DL) applications tend to scale out to a large number of parallel GPUs. Unfortunately, we observe that the collective communication overhead across GPUs is often the key limiting factor of performance for distributed DL. It under-utilizes the networking bandwidth by frequent transfers of small data chunks, which also incurs a substantial I/O overhead on GPU that interferes with computation on GPU. The root cause lies in the inefficiency of CPU-based communication event handling as well as the inability to control the GPU's internal DMA engine with GPU threads.  
 To address the problem, we propose a GPU-driven code execution system that leverages a GPU-controlled hardware DMA engine for I/O offloading. Our custom DMA engine pipelines multiple DMA requests to support efficient small data transfer while it eliminates the I/O overhead on GPU cores. Unlike existing GPU DMA engines initiated only by CPU, we let GPU threads to directly control DMA operations, which leads to a highly efficient system where GPUs drive their own execution flow and handle communication events autonomously without CPU intervention. Our prototype DMA engine achieves a line-rate from a message size as small as 8KB (3.9x better throughput) with only 4.3us of communication latency (9.1x faster) while it incurs little interference with computation on GPU, achieving 1.8x higher all-reduce throughput in a real training workload.  
 Available Media   
 Conventional host networking features various traffic shaping layers (e.g., buffers, schedulers, and pacers) with complex interactions and wide implications for performance metrics. These interactions can lead to large bursts at various time scales. Understanding the nature of traffic bursts is important for optimal resource provisioning, congestion control, buffer sizing, and traffic prediction but is challenging due to the complexity and feature velocity in host networking.  
 We develop Valinor, a traffic measurement framework that consists of eBPF hooks and measurement modules in a programmable network. Valinor offers visibility into traffic burstiness over a wide span of timescales (nanosecond- to secondscale) at multiple vantage points. We deploy Valinor to analyze the burstiness of various classes of congestion control algorithms, qdiscs, Linux process scheduling, NIC packet scheduling, and hardware offloading. Our analysis counters the assumption that burstiness is primarily a function of the application layer and preserved by protocol stacks, and highlights the pronounced role of lower layers in the formation and suppression of bursts. We also show the limitations of canonical burst countermeasures (e.g., TCP pacing and qdisc scheduling) due to the intervening nature of segmentation offloading and fixed-function NIC scheduling. Finally, we demonstrate that, far from a universal invariant, burstiness varies significantly across host stacks. Our findings underscore the need for a measurement framework such as Valinor for regular burst analysis.  
 SPEEDEX: A Scalable, Parallelizable, and Economically Efficient Decentralized EXchange   
 Available Media   
 The promise of in-network computing continues to be unrealized in realistic deployments (e.g., clouds and ISPs) as serving concurrent stateful applications on a programmable switch is challenging today due to limited switch's on-chip resources. In this paper, we argue that an on-rack switch resource augmentation architecture that augments a programmable switch with other programmable network hardware, such as smart NICs, on the same rack can be a pragmatic and incrementally scalable solution. To realize this vision, we design and implement ExoPlane, an operating system for on-rack switch resource augmentation to support multiple concurrent applications. In designing ExoPlane, we propose a practical runtime operating model and state abstraction to address challenges in managing application states correctly across multiple devices with minimal performance and resource overheads. Our evaluation with various P4 applications shows that ExoPlane can provide applications with low latency, scalable throughput, and fast failover while achieving these with small resource overheads and no or little modifications on applications.  
 Acoustic Sensing and Communication Using Metasurface   
 Available Media   
 Acoustic sensing is increasingly popular owing to widely available devices that support them. Yet the sensing resolution and range are still limited due to limited bandwidth and sharp decay in the signal at inaudible frequencies. Inspired by recent development in acoustic metasurfaces, in this paper, we first perform an in-depth study of acoustic metasurface (AMS) and compare it with the phased array speaker. Our results show that AMS is attractive as it achieves a significant SNR increase while maintaining a compact size. A major limitation of existing AMS is its static configuration. Since our target may be at any possible location, it is important to support scanning in different directions. We develop a novel acoustic system that leverages a metasurface and a small number of speakers. We jointly optimize the configuration of metasurface and transmission signals from the speakers to achieve low-cost dynamic steering. Using a prototype implementation and extensive evaluation, we demonstrate its effectiveness in improving SNR, acoustic sensing accuracy, and acoustic communication reliability over a wide range of scenarios.  
 Waverunner: An Elegant Approach to Hardware Acceleration of State Machine Replication   
 Available Media   
 State machine replication (SMR) is a core mechanism for building highly available and consistent systems. In this paper, we propose Waverunner, a new approach to accelerate SMR using FPGA-based SmartNICs. Our approach does not implement the entire SMR system in hardware; instead, it is a hybrid software/hardware system. We make the observation that, despite the complexity of SMR, the most common routine—the data replication—is actually simple. The complex parts (leader election, failure recovery, etc.) are rarely used in modern datacenters where failures are only occasional. These complex routines are not performance critical; their software implementations are fast enough and do not need acceleration. Therefore, our system uses FPGA assistance to accelerate data replication, and leaves the rest to the traditional software implementation of SMR.  
 Available Media   
 Advances in wireless technologies have transformed wireless networks from a pure communication medium to a pervasive sensing platform, enabling many sensorless and contactless applications. After years of effort, wireless sensing approaches centering around conventional signal processing are approaching their limits, and meanwhile, deep learning-based methods become increasingly popular and have seen remarkable progress. In this paper, we explore an unseen opportunity to push the limit of wireless sensing by jointly employing learning-based spectrogram generation and spectrogram learning. To this end, we present SLNet, a new deep wireless sensing architecture with spectrogram analysis and deep learning co-design. SLNet employs neural networks to generate super-resolution spectrogram, which overcomes the limitation of the time-frequency uncertainty. It then utilizes a novel polarized convolutional network that modulates the phase of the spectrograms for learning both local and global features. Experiments with four applications, i.e., gesture recognition, human identification, fall detection, and breathing estimation, show that SLNet achieves the highest accuracy with the smallest model and lowest computation among the state-of-the-art models. We believe the techniques in SLNet can be widely applied to fields beyond WiFi sensing.  
 Tambur: Efficient loss recovery for videoconferencing via streaming codes   
 Available Media   
 Configuration of production datacenters is challenging due to their scale (many switches), complexity (specific policy requirements), and dynamism (need for many configuration changes). This paper introduces Aura, a production-level synthesis system for datacenter routing policies. It consists of a high-level language, called RPL, that expresses the desired behavior and a compiler that automatically generates switch configurations. Unlike existing approaches, which generate full network configuration for a static policy, Aura is built to support frequent policy and network changes. It generates and deploys multiple parallel policy collections, in a way that supports smooth transitions between them without disrupting live production traffic. Aura has been deployed for over two years in Meta datacenters and has greatly improved our management efficiency. We also share our operational requirements and experiences, which can potentially inspire future research.  
 Electrode: Accelerating Distributed Protocols with eBPF   
 CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation   
 Abdullah Alomar, Pouya Hamadanian, Arash Nasr-Esfahany, Anish Agarwal, Mohammad Alizadeh, and Devavrat Shah, MIT   
  Awarded Best Paper!    
 Award:   
 Best Paper   
 Available Media   
 Available Media   
 Shell scripting remains prevalent for automation and data-processing tasks, partly due to its dynamic features—e.g., expansion, substitution—and language agnosticism—i.e., the ability to combine third-party commands implemented in any programming language. Unfortunately, these characteristics hinder automated shell-script distribution, often necessary for dealing with large datasets that do not fit on a single computer. This paper introduces DiSh, a system that distributes the execution of dynamic shell scripts operating on distributed filesystems. DiSh is designed as a shim that applies program analyses and transformations to leverage distributed computing, while delegating all execution to the underlying shell available on each computing node. As a result, DiSh does not require modifications to shell scripts and maintains compatibility with existing shells and legacy functionality. We evaluate DiSh against several options available to users today: (i) Bash, a single-node shell-interpreter baseline, (ii) PaSh, a state-of-the-art automated-parallelization system, and (iii) Hadoop Streaming, a MapReduce system that supports language-agnostic third-party components. Combined, our results demonstrate that DiSh offers significant performance gains, requires no developer effort, and handles arbitrary dynamic behaviors pervasive in real-world shell scripts.  
 Invisinets: Removing Networking from Cloud Networks   
 Available Media   
 To comply with the increasing number of government regulations about data placement and processing, and to protect themselves against major cloud outages, many users want the ability to easily migrate their workloads between clouds. In this paper we propose doing so not by imposing uniform and comprehensive standards, but by creating a fine-grained two-sided market via an intercloud broker. These brokers will allow users to view the cloud ecosystem not just as a collection of individual and largely incompatible clouds but as a more integrated Sky of Computing. We describe the design and implementation of an intercloud broker, named SkyPilot, evaluate its benefits, and report on its real-world usage.  
 mmWall: A Steerable, Transflective Metamaterial Surface for NextG mmWave Networks
8. NSDI_3 conference:
Questions 
 NSDI '23 Sponsor and Exhibitor Information  
 Important Dates and Deadlines  
 Hotel discount deadline: | Monday, March 27, 2023 
  Early bird registration deadline: | Monday, March 27, 2023 
  Monitor orders due  : | Tuesday, April 14, 2023 
  Shipping window for tabletop items: | Wednesday, April 12 - Friday, April 14 
  Deadline to send your shipment’s tracking numbers: | Wednesday, April 12 (if applicable) 
  LeadCapture orders due: | Monday, April 10, 2023 
  *Note:  Refer to the NSDI '23 program  for complete details and breaks schedule.  
 Exhibit Space Details  
 NSDI ‘23 takes place at the Boston Marriott Long Wharf  . All tables will be located in the Grand Ballroom Foyer, adjacent to the sessions. You are welcome to bring a pop-up banner, table cloth, and giveaways, but please be aware that you will need to transport these in and out on your own, or ship directly to the venue per instructions below. Popups should not exceed 6’ w x 8’h.   
 Six (6)-foot draped table 
  Two (2) chairs 
 USENIX provides free WiFi access to all our attendees and exhibitors. All tables are equipped with access to the conference WiFI network and electricity. If you need a hardwired ethernet line, please let us know. Please bring your own cables as needed for your devices.  
 Monitor Rental  
 If you would like a monitor at your table, you may order one through Show Imaging (SI) by Monday, April 3, 2023.  
 Monitor Order Form   
 Lead Retrieval  
 USENIX can secure small items and boxes for you overnight, please coordinate with us on site.  
 Shipping  
 Send all boxes to arrive between Wednesday, April 12 and Friday, April 14. Label each box with the below information, and fill in your company name where indicated.  
 Casey Henderson  
  NSDI '23, April 17–19  
  Box ___ of ___:  
  Company: __________________  
 Please send us your tracking information  and box contents by April 12, and have sponsorship@usenix.org  cc’d on your tracking information so we can best locate items at the hotel.  
 Ship out:  If you need to ship any items out after the conference, please bring prepaid and pre-printed shipping labels.  
 Exhibitor-only Registration
9. NSPW_0 conference:
Skip to main content    New Security Paradigms Workshop    
  NSPW 2023: Call for Papers   
 Segovia, Spain   
  September, 2023   
  Format: PDF file (ACM SIG formatting) via Easychair   
 Notification of acceptance: June 30  July 14, 2023  
  Pre-proceedings deadline: August 4, 2023  
  Invitations sent: July 21, 2023  
  Workshop:  September 18 - 21, 2023  
  Final version: October 27, 2023  
 The New Security Paradigms Workshop (NSPW) seeks embryonic, disruptive, and unconventional ideas on information and cybersecurity that benefit from early, in-depth, and constructive feedback. Submissions typically address current limitations of information security, directly challenge long-held beliefs or the very foundations of security, or discuss problems from an entirely novel angle, leading to new solutions. We welcome papers both from computer science and other disciplines that study adversarial relationships and other aspects of security, as well as from practice. The workshop is invitation-only; all accepted papers receive a 1 hour plenary time slot for presentation and  
  discussion. In order to maximize diversity of perspectives, we particularly encourage submissions from new NSPW authors, from Ph.D. students, and from non-obvious disciplines and institutions.  
 In addition to regular submissions presenting new security paradigms, NSPW also has an optional theme each year to encourage submissions in specific areas of interest or importance. The theme for NSPW 2023 is Inclusive Security  .  
 Technology is changing the world we live in and being able to use and/or co-exist with technology safely and securely is becoming essential for individuals, organizations, and communities. The inclusive security theme is appropriate for submissions looking at, for example, how access to all aspects of security (including education, engagement, skills, occupations, tools, and more) can be expanded as well as how systems can be designed to facilitate the safety and security of all those that use and interact with them.  
 Relevant topics for this theme include:  
  The effectiveness of equality, diversity and inclusion initiatives for cyber security professions 
  Other related topics and interpretations of inclusive security are welcome.  
 NSPW 2023 is scheduled to be held in the Parador de Segovia, Spain. As in the past, this choice of venue is designed to facilitate interactions between the invited attendees throughout the workshop.  
 Submission Instructions  
 NSPW accepts three categories of submissions:  
 Regular Submissions present a new approach (paradigm) to a security problem or critique existing approaches. While regular submissions may present research results (mathematical or experimental), unlike papers submitted to most computer security venues, these results should not be the focus of the submission; instead, the change in approach should be the focus. 
  Theme Submissions are focused on “Inclusive Security”, and should explain the connection with the theme in the justification statement (see below). They follow the format of a regular submission. 
  Panel Proposals describe a debatable topic of interest to the security community that merits significant discussion. Proposals should describe the major perspectives on the chosen topic. They should also present the background of the panelists, explaining how they are the right people to discuss the chosen topic at NSPW. 
  Submissions must be made in PDF format, 6-15 pages, ACM "sigconf" formatting, through EasyChair. Submissions should blind author identity where possible. The LaTeX document option anonymous=true provides a minimum level of protection; however, authors should also avoid referencing their own work in the first person or other obvious de-anonymization in the submission.  
 NSPW Submissions must include both a cover page with authors' names, affiliation, and participation statement and a justification statement before or after the abstract.  
 To support double-blind reviewing, the cover page should not be part of the PDF submission, but will be submitted separately on EasyChair. 
  The participation statement (on the cover page) must specify which author(s) will attend upon acceptance/invitation, that all authors will engage in good faith with the feedback given in the review and revision periods, and that all authors will abide by the NSPW code of conduct. 
  The justification statement (included in the PDF submission) briefly explains why the submission is appropriate for NSPW and the chosen submission category. The justification statement will not appear in the final publication. 
  Papers not including both statements risk rejection without review.  
 Organizers and PC members are allowed to submit, but will not be involved in the evaluation of their own papers. All submissions are treated as confidential as a matter of policy. NSPW does not accept previously published or concurrently submitted papers.  
 Given the focus on the development and exploration of new ideas, the process for paper submission, revision, presentation, and publication at NSPW is different from many other security conferences. Accepted papers are shepherded and revised before the workshop; this revised version is then presented and discussed in an hour-long session. After the workshop, the final version is produced, incorporating the discussions and feedback. Acceptance to the workshop is conditional on engagement with this process.  
 Attendance  
  and complying with the code of conduct. The workshop is preceded by an evening reception allowing attendees to meet each other beforehand.  
 NSPW is making arrangements for (some) child care during the event. Any potential attendee who would like to take advantage of this or has questions, please email general-chairs@nspw.org  now. We would like to get a general understanding of interest and capacity to assist with planning for the event.  
 Program Committee Co-chairs:   
  Tristan Caufield, University College London, t.caulfield@ucl.ac.uk   
 Show — Main menu  Hide — Main menu  Home 
  NSPW 2024 | Call for papers 
  Important Dates 
  Program 
  Accepted papers 
  Organizers 
  Past Workshops | NSPW 2023 | Program 
  NSPW 2022
10. NSPW_1 conference:
Introduction   
 NSPW 2024   will be held on September 16-19, 2024 in Bedford, PA USA. Submissions were due April 28, 2024.  
 The New Security Paradigms Workshop (NSPW) is an annual, small invitation-only workshop for researchers in information security and related disciplines. NSPW's focus is on work that challenges the dominant approaches and perspectives in computer security. In the past, such challenges have taken the form of critiques of existing practice as well as novel, sometimes controversial, and often immature approaches to defending computer systems. By providing a forum for important security research that isn't suitable for mainstream security venues, NSPW aims to foster paradigm shifts  in information security.  
 NSPW is an ACSA  conference. The proceedings of NSPW are published by the ACM  . Full proceedings are available  .  
 Main menu  
 Show — Main menu  Hide — Main menu  Home 
  NSPW 2024 | Call for papers 
  Important Dates 
  Program

output:1. NOSSDAV_0 information:
2. NOSSDAV_1 information:
3. NPC_1 information:
4. NPC_2 information:
5. NPC_3 information:
6. NSDI_1 information:
7. NSDI_2 information:
8. NSDI_3 information:
9. NSPW_0 information:
10. NSPW_1 information:
