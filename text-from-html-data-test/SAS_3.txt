Anmelden 
  Registrierung 
  Deutsch  English 
  Español 
  Português 
  Français 

     Dom 
  Najlepsze kategorie | CAREER & MONEY 
  PERSONAL GROWTH 
  POLITICS & CURRENT AFFAIRS 
  SCIENCE & TECH 
  HEALTH & FITNESS 
  LIFESTYLE 
  ENTERTAINMENT 
  BIOGRAPHIES & HISTORY 
  FICTION 
  Najlepsze historie 
  Najlepsze historie 
  Dodaj historię 
  Moje historie 

 Home 
  Static Analysis. 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings 9783031442445, 9783031442452 

 Static Analysis. 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings 9783031442445, 9783031442452   
   
  181    103    
  English   Pages 580   Year 2023    
  Report DMCA / Copyright    
  DOWNLOAD FILE   
   
 Polecaj historie   

 Automated Technology for Verification and Analysis: 21st International Symposium, ATVA 2023, Singapore, October 24–27, 2023, Proceedings [2] 303145331X, 9783031453311  
 This book constitutes the refereed proceedings of the 21st International Symposium on Automated Technology for Verificat  
  130    121    Read more   

 Static Analysis: 26th International Symposium, SAS 2019, Porto, Portugal, October 8–11, 2019, Proceedings [1st ed. 2019] 978-3-030-32303-5, 978-3-030-32304-2  
 This book constitutes the refereed proceedings of the 26th International Symposium on Static Analysis, SAS 2019, held in  
  869    69    20MB    Read more   

 Static Analysis: 27th International Symposium, SAS 2020, Virtual Event, November 18–20, 2020, Proceedings [1st ed. 2020] 3030654737, 9783030654733  
 This book constitutes the refereed proceedings of the 27th International Symposium on Static Analysis, SAS 2020, held in  
  656    97    35MB    Read more   

 Static Analysis: 27th International Symposium, SAS 2020, Virtual Event, November 18–20, 2020, Proceedings [1st ed. 2020] 3030654737, 9783030654733  
 This book constitutes the refereed proceedings of the 27th International Symposium on Static Analysis, SAS 2020, held in  
  495    31    14MB    Read more   

 #388, October/2023 EDGE  
  
  160    120    51MB    Read more   

 Bioinformatics Research and Applications: 19th International Symposium, ISBRA 2023, Wrocław, Poland, October 9–12, 2023, Proceedings (Lecture Notes in Bioinformatics) 9819970733, 9789819970735  
 This book constitutes the refereed proceedings of the 19th International Symposium on Bioinformatics Research and Applic  
  145    79    Read more   

 Algorithms and Data Structures. 18th International Symposium, WADS 2023 Montreal, QC, Canada, July 31 – August 2, 2023 Proceedings 9783031389054, 9783031389061  
  
  307    37    16MB    Read more   

 Reachability Problems : 17th International Conference, RP 2023, Nice, France, October 11–13, 2023, Proceedings [14235, 1 ed.] 9783031452857, 9783031452864  
 This book constitutes the refereed proceedings of the 17th International Conference on Reachability Problems, RP 2023, h  
  156    84    7MB    Read more   

 NASA Formal Methods: 15th International Symposium, NFM 2023, Houston, TX, USA, May 16–18, 2023, Proceedings 3031331699, 9783031331695  
 This book constitutes the proceedings of the 15th International Symposium on NASA Formal Methods, NFM 2023, held in Hous  
  472    35    19MB    Read more   

 Model Checking Software: 29th International Symposium, SPIN 2023, Paris, France, April 26–27, 2023, Proceedings 9783031321573, 9783031321566, 303132157X  
 This book constitutes the refereed proceedings of the 29th International Symposium on Model Checking Software, SPIN 2023  
  342    37    18MB    Read more   

 Author / Uploaded 
  Manuel V. Hermenegildo 
  José F. Morales 

 Table of contents :  
  Preface  
  Organization  
  Goal-Directed Abstract Interpretation and Event-Driven Frameworks (Abstract of Invited Talk)  
  Contents  
  Invited Talks  
  Verifying Infinitely Many Programs at Once  
  1 Introduction  
  2 Proving Unrealizability of Synthesis Problems  
  3 Unrealizability Logic  
  4 Conclusions  
  References  
  Abstract Interpretation in Industry – Experience and Lessons Learned  
  1 Introduction  
  2 Sound Worst-Case Execution Time Analysis  
  2.1 Our View of and Our Solution to the WCET-Analysis Problem  
  2.2 The Development of Our WCET-Analysis Technique  
  2.3 Improvements  
  2.4 Tool Qualification  
  2.5 Impact in Industry and Academia  
  2.6 Application to Non-Timing-Predictable Architectures  
  2.7 Spin-Off: Worst-Case Stack Usage Analysis  
  3 Sound Runtime Error Analysis  
  3.1 The Origins  
  3.2 Further Development  
  4 The User Perspective  
  5 The Role of Safety Norms  
  6 Conclusion  
  References  
  Building Trust and Safety in Artificial Intelligence with Abstract Interpretation  
  1 Introduction  
  2 Certification for Testing Model Safety  
  3 Certification for Training Safe DNNs  
  4 Certification for Interpreting DNNs  
  References  
  Regular Papers  
  Modular Optimization-Based Roundoff Error Analysis of Floating-Point Programs  
  1 Introduction  
  2 Background  
  3 Modular Roundoff Error Analysis  
  3.1 Roundoff Error Abstraction  
  3.2 Propagation Error Abstraction  
  3.3 Instantiation of Error  
  3.4 Handling Nested Procedures  
  4 Implementation  
  5 Evaluation  
  5.1 Experimental Setup  
  5.2 RQ1: Accuracy-Performance Trade-Off  
  5.3 RQ2: Comparison with the State of the Art  
  6 Related Work  
  7 Conclusion  
  A Appendix  
  References  
  Unconstrained Variable Oracles for Faster Numeric Static Analyses  
  1 Introduction  
  2 Detecting Likely Unconstrained Variables  
  2.1 A Dataflow Analysis for LU Variables  
  2.2 The Program Transformation Step  
  3 Implementation and Experimental Evaluation  
  3.1 The Impact of the Havoc Transformation  
  3.2 Precision and Efficiency of the Target Analyses  
  4 Related Work  
  5 Conclusion  
  References  
  Symbolic Transformation of Expressions in Modular Arithmetic  
  1 Introduction  
  2 Preliminary Results on Modular Integer Arithmetic  
  3 Syntax and Semantics of the Language  
  3.1 Syntax of the Language  
  3.2 Concrete Semantics of the Language  
  4 Soundness Requirements of Expression Rewriting  
  5 Abstract Representation of Expressions  
  5.1 Abstract Syntax of Expression  
  6 Generic Abstraction  
  6.1 Primitives over Abstract Expressions  
  6.2 Translation from Classical to Abstract Expressions  
  7 Instantiation of the Generic Framework  
  7.1 Intervalization  
  7.2 Simplification of Abstract Expressions  
  7.3 Linear Interpolation  
  Implementation Presentation  
  9 Conclusion  
  References  
  A Formal Framework to Measure the Incompleteness of Abstract Interpretations  
  1 Introduction  
  2 Background  
  3 Distances on Orderings  
  4 Deriving Pre-metrics from Domains  
  5 Partial Forward/Backward-Completeness Properties  
  6 Characterizing Local Forward/Backward-Completeness  
  7 Related Work  
  8 Conclusion  
  References  
  BREWasm: A General Static Binary Rewriting Framework for WebAssembly  
  1 Introduction  
  2 Background  
  2.1 WebAssembly Binary  
  2.2 Binary Rewriting  
  3 BREWasm  
  3.1 Overview  
  3.2 Challenges  
  4 Approach  
  4.1 Wasm Parser and Wasm Encoder  
  4.2 Section Rewriter  
  4.3 Semantics Rewriter  
  5 Implementation and Evaluation  
  5.1 Implementation  
  5.2 Research Questions and Experimental Setup  
  5.3 RQ1: Efficiency  
  5.4 RQ2: Correctness and Effectiveness  
  5.5 RQ3: Practicability  
  6 Related Work  
  7 Limitations and Discussion  
  8 Conclusion  
  References  
  Quantum Constant Propagation  
  1 Introduction  
  2 Preliminaries  
  3 Methodology  
  3.1 Union-Table  
  3.2 Representation of a Quantum State  
  3.3 Restricted Simulation  
  3.4 Control Reduction  
  4 Correctness of Control Reduction  
  5 Running Time Analysis  
  6 Evaluation  
  6.1 Experiments  
  6.2 Results  
  7 Related Work  
  8 Conclusions  
  References  
  Error Invariants for Fault Localization via Abstract Interpretation  
  1 Introduction  
  2 Motivating Examples  
  3 The Language and Its Semantics  
  4 Inferring Abstract Error Invariants  
  4.1 Abstract Analysis  
  4.2 Iterative Abstract Analysis  
  4.3 BDD Abstract Domain Functor  
  5 Evaluation  
  6 Related Work  
  7 Conclusion  
  References  
  Generalized Program Sketching by Abstract Interpretation and Logical Abduction  
  1 Introduction  
  2 Motivating Examples  
  3 Language and Semantics  
  3.1 Syntax  
  3.2 Concrete Semantics and Analyses  
  3.3 Abstract Semantics and Analyses  
  3.4 Abduction  
  4 Synthesis Algorithm  
  5 Evaluation  
  6 Related Work  
  7 Conclusion  
  References  
  Mutual Refinements of Context-Free Language Reachability  
  1 Introduction  
  2 Motivating Example  
  3 Preliminary  
  3.1 L-Reachability  
  3.2 CFL-Reachability  
  4 Mutual Refinement  
  4.1 Overview  
  4.2 Contributing Edges  
  4.3 Tracing Algorithm  
  4.4 Mutual Refinement Algorithm  
  5 Experiments  
  5.1 Experimental Setup  
  5.2 RQ1: Precision Improvement  
  5.3 RQ2: Performance Overhead  
  5.4 RQ3: Combination with the LZR Algorithm  
  6 Discussion  
  6.1 Generality of Mutual Refinement  
  6.2 Different Grammars for the Same CFL  
  6.3 Order of Mutual Refinement  
  6.4 Cost of Mutual Refinement  
  6.5 Generalization to the Single-Pair Case  
  6.6 Generalization to Other Algorithms  
  7 Related Work  
  8 Conclusion  
  References  
  ADCL: Acceleration Driven Clause Learning for Constrained Horn Clauses  
  1 Introduction  
  2 Preliminaries  
  3 Acceleration Driven Clause Learning  
  3.1 Syntactic Implicants and Redundancy  
  3.2 The ADCL Calculus  
  3.3 Properties of ADCL  
  4 Implementing ADCL  
  5 Related Work and Experiments  
  References  
  How Fitting is Your Abstract Domain?  
  1 Introduction  
  2 Background  
  2.1 Abstract Interpretation  
  2.2 Regular Commands  
  3 From Completeness to Adequacy  
  3.1 Abstract Domain Completeness and its Limits  
  3.2 Abstract Domain Adequacy  
  3.3 Adequacy for `3́9`42`"̇613A``45`47`"603AL Programs  
  4 Adjusting Abstract Domains  
  5 Abstract Domain Adequacy Logic  
  6 Conclusions  
  References  
  A Product of Shape and Sequence Abstractions  
  1 Introduction  
  2 Overview  
  3 Abstract Domain for Sequences  
  3.1 Sequences Abstraction  
  3.2 Abstract Operations  
  3.3 Lattice Operations  
  4 Combination of Sequence Abstraction and Shape Analysis  
  4.1 Language and Semantics  
  4.2 Combined Memory and Sequence Abstraction  
  4.3 Computation of Abstract Post-conditions  
  4.4 Computation of Lattice Operations  
  4.5 Static Analysis of a Simple Language  
  5 Shape and Sequence Predicates for Non-linear Structures  
  6 Implementation and Evaluation  
  7 Related Works  
  8 Conclusion and Future Works  
  References  
  Error Localization for Sequential Effect Systems  
  1 Introduction  
  2 Background  
  2.1 Implementing Effect Systems  
  3 Local Errors for Sequential Effect Systems  
  3.1 Residuals  
  3.2 Residual Examples  
  3.3 Atomicity  
  3.4 Reentrancy  
  3.5 Connecting Residuals to Type Checking  
  3.6 Limitations  
  4 Algorithms  
  5 Implementation  
  6 Evaluation  
  6.1 Reproducing Atomicity Errors  
  6.2 Reentrancy  
  7 Related Work  
  8 Conclusions  
  References  
  Scaling up Roundoff Analysis of Functional Data Structure Programs  
  1 Introduction  
  2 State-of-the-Art in Rounding Error Analysis  
  3 DSL for List-Like Data Structures  
  3.1 Benchmark Set  
  3.2 A Functional DSL  
  3.3 DSL Functions  
  4 Data-Structure Guided Analysis  
  4.1 DS-Based Concrete Domain  
  4.2 DS-Based Abstract Domain  
  4.3 DS Analysis  
  4.4 Optimized Evaluation of fold  
  5 Implementation  
  6 Experimental Evaluation  
  6.1 State-of-the-Art Tools  
  6.2 RQ1: Comparison to State-of-the-Art Tools  
  6.3 RQ2: DS-Based Abstraction Accuracy/Performance Tradeoff  
  7 Related Work  
  8 Conclusion  
  A Appendix  
  References  
  Reverse Template Processing Using Abstract Interpretation  
  1 Introduction  
  2 Motivation and Example  
  3 Background: The rtl Template Language  
  4 Semantics for Reversing Template  
  4.1 Parsing as Natural Deduction  
  4.2 A Backward Denotational Semantics  
  5 Sound Reversal of Templates  
  5.1 Abstract Databases  
  5.2 Abstract Environments  
  5.3 Abstract Semantics  
  6 Precision and Exactness  
  6.1 Optimal Precision  
  6.2 Inherent Imprecisions  
  6.3 Imprecision Coming from the Abstraction  
  7 Reduction for Constraint Propagation  
  8 Implementations and Evaluation  
  8.1 Extensions for a Full-Featured Template Language  
  8.2 Arbitrary Expressions  
  8.3 Parsing  
  8.4 Handling Cyclic Recursive Calls  
  8.5 Evaluation  
  9 Related Work  
  10 Conclusion  
  A Appendix  
  A.1 Template Viewed as a Context-Free Grammar  
  A.2 Definition of Footprint  
  References  
  Domain Precision in Galois Connection-Less Abstract Interpretation  
  1 Introduction  
  2 Background  
  2.1 Abstract Interpretation  
  2.2 The Reference Language  
  3 Weak Abstract Interpretation Framework  
  4 Characterizing Weak Completeness  
  4.1 Completeness is Abstract Non-Interference  
  4.2 Making Abstract Domains Weak Complete  
  5 A Deductive System for Completeness  
  6 Statically Verifying (Weak) Completeness  
  6.1 Hypercompleteness: Completeness as a Hyperproperty  
  6.2 Static Analysis for Completeness  
  7 Conclusions  
  A Selected Proofs  
  References  
  Lifting On-Demand Analysis to Higher-Order Languages  
  1 Introduction  
  2 Overview of Our Approach  
  2.1 Precision  
  2.2 Termination and Soundness  
  3 On-Demand Call Graph Soundness  
  3.1 Program Semantics  
  3.2 Queries  
  3.3 Call Graphs  
  3.4 Answering Call Graph Queries  
  3.5 On-Demand Call Graph Construction as a Transition System  
  3.6 Discussion  
  4 Evaluation  
  5 Related Work  
  6 Conclusions  
  A Single-Threaded Performance Results  
  References  
  Octagons Revisited  
  1 Introduction  
  2 Relational Domains  
  3 Weakly Relational Domains  
  4 2-Projectivity  
  5 Incremental Normalization  
  6 Abstract Transformers for Linear Assignments  
  7 Linear Programming with Octagon Constraints  
  8 Abstract Assignments for Octagons  
  9 Related Work  
  10 Conclusion and Future Work  
  A 2-Projectivity for Extensions of Octagons  
  References  
  Polynomial Analysis of Modular Arithmetic  
  1 Introduction  
  2 Modular Polynomial Abstract Domain  
  2.1 Modular Arithmetic  
  2.2 Polynomials  
  2.3 Closure  
  2.4 MPAD  
  2.5 Null Polynomials  
  3 Gröbner Bases  
  3.1 Rank and Divisibility in Zm  
  3.2 Monomial Orderings  
  3.3 Reduction  
  3.4 Gröbner Bases  
  3.5 Buchberger's Algorithm  
  4 Calculating Variable Elimination and Join  
  4.1 Variable Elimination  
  4.2 Join  
  5 Calculating Closure and Meet  
  5.1 Covering  
  5.2 Closure  
  5.3 Meet  
  6 Forwards Analysis of Polynomial Programs  
  6.1 Polynomial Programs  
  6.2 Concrete Semantics  
  6.3 Abstract Semantics  
  6.4 Correctness and Precision  
  6.5 Illustrative Example  
  6.6 Implementation  
  7 Related Work  
  8 Conclusions  
  References  
  Boosting Multi-neuron Convex Relaxation for Neural Network Verification  
  1 Introduction  
  2 Preliminaries  
  2.1 Neural Network Verification  
  2.2 Multi-neuron Convex Relaxation  
  3 Volume Approximation Based Grouping  
  3.1 Overview of Our Approach  
  3.2 Volume Approximation  
  3.3 Detailed Algorithm  
  3.4 Generalization  
  4 Experiments  
  4.1 Experiment Configurations  
  4.2 Comparison of Verification Precision and Runtime  
  4.3 Comparison with Other State-of-the-Art Verifiers  
  4.4 Comparison on Sigmoid and Tanh Neural Networks  
  5 Related Work  
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  LNCS 14284  
   
  ARCoSS  
   
  Manuel V. Hermenegildo José F. Morales (Eds.)  
   
  Static Analysis 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings  
   
  Lecture Notes in Computer Science  
   
  14284  
   
  Founding Editors Gerhard Goos Juris Hartmanis  
   
  Editorial Board Members Elisa Bertino, USA Wen Gao, China  
   
  Bernhard Steffen , Germany Moti Yung , USA  
   
  Advanced Research in Computing and Software Science Subline of Lecture Notes in Computer Science Subline Series Editors Giorgio Ausiello, University of Rome ‘La Sapienza’, Italy Vladimiro Sassone, University of Southampton, UK  
   
  Subline Advisory Board Susanne Albers, TU Munich, Germany Benjamin C. Pierce, University of Pennsylvania, USA Bernhard Steffen , University of Dortmund, Germany Deng Xiaotie, Peking University, Beijing, China Jeannette M. Wing, Microsoft Research, Redmond, WA, USA  
   
  More information about this series at https://link.springer.com/bookseries/558  
   
  Manuel V. Hermenegildo José F. Morales Editors  
   
  •  
   
  Static Analysis 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings  
   
  123  
   
  Editors Manuel V. Hermenegildo U. Politécnica de Madrid (UPM) and IMDEA Software Institute Pozuelo de Alarcón, Madrid, Spain  
   
  José F. Morales U. Politécnica de Madrid (UPM) and IMDEA Software Institute Pozuelo de Alarcon, Spain  
   
  ISSN 0302-9743 ISSN 1611-3349 (electronic) Lecture Notes in Computer Science ISBN 978-3-031-44244-5 ISBN 978-3-031-44245-2 (eBook) https://doi.org/10.1007/978-3-031-44245-2 © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 Chapters “Quantum Constant Propagation” and “Scaling up Roundoff Analysis of Functional Data Structure Programs” are licensed under the terms of the Creative Commons Attribution 4.0 International License (http:// creativecommons.org/licenses/by/4.0/). For further details see license information in the chapters. This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations. This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland Paper in this product is recyclable.  
   
  Preface  
   
  This volume contains the proceedings of the 30th edition of the International Static Analysis Symposium, SAS 2023, held on October 22–24, 2023, in Cascais, Portugal. The conference was a co-located event of SPLASH, the ACM SIGPLAN conference on Systems, Programming, Languages, and Applications: Software for Humanity. Static analysis is widely recognized as a fundamental tool for program veriﬁcation, bug detection, compiler optimization, program understanding, and software maintenance. The series of Static Analysis Symposia serves as the primary venue for the presentation of theoretical, practical, and applied advances in the area. Previous symposia were held in Auckland, Chicago, Porto, Freiburg, New York, Edinburgh, Saint-Malo, Munich, Seattle, Deauville, Venice, Perpignan, Los Angeles, Valencia, Kongens Lyngby, Seoul, London, Verona, San Diego, Madrid, Paris, Santa Barbara, Venice, Pisa, Paris, Aachen, Glasgow, and Namur. SAS 2023 called for papers on topics including, but not limited to, abstract interpretation, automated deduction, data ﬂow analysis, debugging techniques, deductive methods, emerging applications, model checking, data science, program optimizations and transformations, program synthesis, program veriﬁcation, machine learning and veriﬁcation, security analysis, tool environments and architectures, theoretical frameworks, type checking, and distributed or networked systems. Besides the regular papers, authors were encouraged to submit short submissions in the NEAT category to discuss experiences with static analysis tools, industrial reports, and case studies, along with tool papers, brief announcements of work in progress, well-motivated discussions of new questions or new areas, etc. Authors were encouraged to submit artifacts accompanying their papers to strengthen evaluations and the reproducibility of results. The conference employed a double-blind reviewing process with an author response period, supported on EasyChair. This year, SAS had 40 full submitted papers (38 regular and two NEAT).The Program Committee used a two-round review process, where each remaining submission received at least three ﬁrst-round reviews, and most four reviews, which the authors could then respond to. In addition to the PC members, 23 external reviewers were also involved in the process. The author response period was followed by a Program Committee discussion where consensus was reached on the papers to be accepted, after a thorough assessment of the relevance and the quality of the work. Overall, 20 papers were accepted for publication (19 regular and one NEAT) and appear in this volume. The submitted papers were authored by researchers around the world: China, USA, France, Germany, Italy, Sweden, India, Macedonia, Taiwan, UK, Israel, Cuba, Denmark, Switzerland, the Netherlands, Czechia, Japan, Mexico, and Canada. We view the artifacts as being equally important for the success and development of static analysis as the written papers. It is important for researchers to be able to independently reproduce experiments, which is greatly facilitated by having the original artifacts available. Marc Chevalier, the artifact committee chair, set up the artifact  
   
  vi  
   
  Preface  
   
  committee. In line with SAS 2022, the authors could submit either Docker or Virtual Machine images as artifacts. A public archival repository for the artifacts is available on Zenodo, hosted at https://zenodo.org/communities/sas-2023. The artifacts have badges awarded at three levels: Validated (correct functionality), Extensible (with source code), and Available (on the Zenodo repository). The artwork for the badges is by Arpita Biswas (Harvard University) and Suvam Mukherjee (Microsoft). SAS 2023 had 12 valid artifact submissions. The review process for the artifacts was similar to that for the papers. Each artifact was evaluated by three members of the artifact evaluation committee, and 11 out of 12 valid artifacts were accepted, at different levels. In addition to the contributed papers, SAS 2023 also featured four invited talks by distinguished researchers: Gagandeep Singh (University of Illinois at Urbana-Champaign, VMware Research, USA), Bor-Yuh Evan Chang (U. of Colorado at Boulder, USA), Loris D’Antoni (U. of Wiscounsin at Madison, USA), and Daniel Kästner (AbsInt Gmbh, Germany). The Program Committee also selected the recipient of the Radhia Cousot Young Researcher Best Paper Award, given to a paper with a signiﬁcant contribution from a student. This award was instituted in memory of Radhia Cousot, for her fundamental contributions to static analysis and having been one of the main promoters and organizers of the SAS series of conferences. The SAS program would not have been possible without the efforts of many people. We thank them all. The members of the Program Committee, the artifact evaluation committee, and the external reviewers worked tirelessly to select a strong program, offering constructive and helpful feedback to the authors in their reviews. We would also like to thank the organizing committee of SPLASH 2023, chaired by Vasco T. Vasconcelos (LASIGE, University of Lisbon, Portugal) for all their efforts to make the conference a success, and the 2022 chairs Caterina Urban and Gagandeep Singh and the whole SAS Steering Committee for their help in passing the torch. We also thank our sponsors, Google, ENS Foundation, Meta, AbsInt, Springer, and the IMDEA Software Institute for their generous support of the conference. Finally, we thank Springer for publishing these proceedings. August 2023  
   
  Manuel V. Hermenegildo José F. Morales  
   
  Organization  
   
  Program Committee Chairs Manuel Hermenegildo (Co-chair) Jose F. Morales (Co-chair)  
   
  Universidad Politécnica de Madrid and IMDEA Software Institute, Spain Universidad Politécnica de Madrid and IMDEA Software Institute, Spain  
   
  Steering Committee Gagandeep Singh Caterina Urban Bor-Yuh Evan Chang Patrick Cousot Cezara Dragoi Kedar Namjoshi David Pichardie Andreas Podelski  
   
  VMware Research and UIUC, USA Inria and ENS|PSL, France University of Colorado Boulder, USA New York University, USA Inria and ENS|PSL and Informal Systems, France Nokia Bell Labs, USA Meta, France University of Freiburg, Germany  
   
  Program Committee Gogul Balakrishnan Liqian Chen Yu-Fang Chen Patrick Cousot Michael Emmi Pietro Ferrara Roberto Giacobazzi Roberta Gori Francesco Logozzo Isabella Mastroeni Antoine Miné Kedar Namjoshi Jorge A. Navas Martin Rinard Daniel Schoepe Helmut Seidl Mihaela Sighireanu Gagandeep Singh  
   
  Google, USA National University of Defense Technology, China Academia Sinica, Taiwan New York University, USA Amazon Web Services, USA Ca’ Foscari University of Venice, Italy University of Airzona, USA Università di Pisa, Italy Facebook, USA Università di Verona, Italy LIP6, UPMC - Sorbonne Université, France Nokia Bell Labs, USA Certora Inc., USA Massachusetts Institute of Technology, USA Amazon, UK Technical University of Munich, Germany ENS Paris-Saclay, France University of Illinois Urbana-Champaign (UIUC) and VMware Research, USA  
   
  viii  
   
  Organization  
   
  Fu Song Yulei Sui Laura Titolo Jingling Xue Xin Zhang  
   
  ShanghaiTech University, China University of New South Wales, Australia National Institute of Aerospace, NASA LaRC, USA University of New South Wales, Australia Peking University, China  
   
  Artifact Evaluation Committee Chair Marc Chevalier (Chair)  
   
  Snyk, Switzerland  
   
  Artifact Evaluation Committee Vincenzo Arceri Dorra Ben Khalifa Jérôme Boillot Marco Campion Yifan Chen Xiao Cheng Kai Jia Daniel Jurjo Jonathan Laurent Denis Mazzucato Facundo Molina Luca Negrini Vivek Notani Luca Olivieri Francesco Parolini Louis Rustenholz Ryan Vrecenar  
   
  University of Parma, Italy Université de Perpignan Via Domitia, France École Normale Supérieure and INRIA, France Inria & École Normale Supérieure, Université PSL, France Peking University, China University of Technology Sydney, Australia Massachusetts Institute of Technology, USA IMDEA Software Institute and U. Politécnica de Madrid, Spain Carnegie Mellon University, USA Inria and École Normale Supérieure, France IMDEA Software Institute, Spain Corvallis SRL, Ca’ Foscari University of Venice, Italy University of Verona, Italy Ca’ Foscari University of Venice, Italy Sorbonne Université, France IMDEA Software Institute and U. Politécnica de Madrid, Spain Sandia National Laboratories, USA  
   
  Publicity Chair Louis Rustenholz  
   
  IMDEA Software Institute and U. Politécnica de Madrid, Spain  
   
  Additional Reviewers Arceri, Vincenzo Ascari, Flavio Assolini, Nicola Bruni, Roberto Cheng, Xiao  
   
  Dalla Preda, Mila Demangeon, Romain Dolcetti, Greta Feliu Gabaldon, Marco Antonio Izycheva, Anastasiia  
   
  Organization  
   
  Kan, Shuangxiang Lei, Yuxiang Liu, Jiaxiang Lo, Fang-Yi Petter, Michael Ren, Jiawei Stucki, Sandro  
   
  Tsai, Wei-Lun Wang, Jiawei Xu, Feng Yan, Zhenyu Zaffanella, Enea Zhang, Min  
   
  ix  
   
  Goal-Directed Abstract Interpretation and Event-Driven Frameworks (Abstract of Invited Talk) Bor-Yuh Evan Chang1,2 1  
   
  University of Colorado Boulder, USA [email protected]  2  
   
  Amazon, USA  
   
  Abstract. Static analysis is typically about computing a global over-approximation of a program’s behavior from its source code. But what if most of the program code is missing or unknown to the analyzer? What if even where the program starts is unknown? This fundamentally thorny situation arises when attempting to analyze interactive applications (apps) developed against modern, event-driven software frameworks. Rich event-driven software frameworks enable software engineers to create complex applications on sophisticated computing platforms (e.g., smartphones with a broad range of sensors and rich interactivity) with relatively little code by simply implementing callbacks to respond to events. But developing apps against them is also notoriously difﬁcult. To create apps that behave as expected, developers must follow the complex and opaque asynchronous programming protocols imposed by the framework. So what makes static analysis of apps hard is essentially what makes programming them hard: the speciﬁcation of the programming protocol is unclear and the possible control ﬂow between callbacks is largely unknown. While the typical workaround to perform static analysis with an unknown framework implementation is to either assume it to be arbitrary or attempt to eagerly specify all possible callback control ﬂow, this solution can be too pessimistic to prove properties of interest or too burdensome and tricky to get right. In this talk, I argue for a rethinking of how to analyze app code in the context of an unknown framework implementation. In particular, I present some beneﬁts from taking a goal-directed or backward-from-error formulation to prove just the assertions of interest and from designing semantics, program logics, speciﬁcation logics, and abstract domains to reason about the app-framework boundary in a ﬁrst-class manner. What follows are hopefully lines of I would like to especially thank the following for making signiﬁcant contributions to the research described in this talk: Ph.D. students Shawn Meier, Benno Stein, and Sam Blackshear; postdoc Sergio Mover; and collaborators Manu Sridharan and Gowtham Kaki.The University of Colorado Programming Languages and Veriﬁcation (CUPLV) Group has offered the essential community with insightful discussions to conduct this work. This research was supported in part by NSF awards CCF-1055066, CCF-1619282, CCF-2008369 and DARPA award FA8750-14-2-0263. Bor-Yuh Evan Chang holds concurrent appointments at the University of Colorado Boulder and as an Amazon Scholar. This talk describes work performed at the University of Colorado Boulder and is not associated with Amazon.  
   
  xii  
   
  B.-Y. Evan Chang work that make analyzing modern interactive applications more targeted, more compositional, and ultimately more trustworthy. Keywords: Goal-directed veriﬁcation  Backwards abstract interpretation  Event-driven framework modeling  
   
  Contents  
   
  Invited Talks Verifying Infinitely Many Programs at Once . . . . . . . . . . . . . . . . . . . . . . . . Loris D’Antoni  
   
  3  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned . . . . . . . Daniel Kästner, Reinhard Wilhelm, and Christian Ferdinand  
   
  10  
   
  Building Trust and Safety in Artificial Intelligence with Abstract Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gagandeep Singh  
   
  28  
   
  Regular Papers Modular Optimization-Based Roundoff Error Analysis of Floating-Point Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Rosa Abbasi and Eva Darulova  
   
  41  
   
  Unconstrained Variable Oracles for Faster Numeric Static Analyses . . . . . . . . Vincenzo Arceri, Greta Dolcetti, and Enea Zaffanella  
   
  65  
   
  Symbolic Transformation of Expressions in Modular Arithmetic . . . . . . . . . . . Jérôme Boillot and Jérôme Feret  
   
  84  
   
  A Formal Framework to Measure the Incompleteness of Abstract Interpretations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 Marco Campion, Caterina Urban, Mila Dalla Preda, and Roberto Giacobazzi BREWasm: A General Static Binary Rewriting Framework for WebAssembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 Shangtong Cao, Ningyu He, Yao Guo, and Haoyu Wang Quantum Constant Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 Yanbin Chen and Yannick Stade Error Invariants for Fault Localization via Abstract Interpretation . . . . . . . . . . 190 Aleksandar S. Dimovski  
   
  xiv  
   
  Contents  
   
  Generalized Program Sketching by Abstract Interpretation and Logical Abduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 Aleksandar S. Dimovski Mutual Refinements of Context-Free Language Reachability . . . . . . . . . . . . . 231 Shuo Ding and Qirun Zhang ADCL: Acceleration Driven Clause Learning for Constrained Horn Clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 Florian Frohn and Jürgen Giesl How Fitting is Your Abstract Domain? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286 Roberto Giacobazzi, Isabella Mastroeni, and Elia Perantoni A Product of Shape and Sequence Abstractions . . . . . . . . . . . . . . . . . . . . . . 310 Josselin Giet, Félix Ridoux, and Xavier Rival Error Localization for Sequential Effect Systems . . . . . . . . . . . . . . . . . . . . . . 343 Colin S. Gordon and Chaewon Yun Scaling up Roundoff Analysis of Functional Data Structure Programs . . . . . . . 371 Anastasia Isychev and Eva Darulova Reverse Template Processing Using Abstract Interpretation . . . . . . . . . . . . . . 403 Matthieu Lemerre Domain Precision in Galois Connection-Less Abstract Interpretation . . . . . . . . 434 Isabella Mastroeni and Michele Pasqua Lifting On-Demand Analysis to Higher-Order Languages. . . . . . . . . . . . . . . . 460 Daniel Schoepe, David Seekatz, Ilina Stoilkovska, Sandro Stucki, Daniel Tattersall, Pauline Bolignano, Franco Raimondi, and Bor-Yuh Evan Chang Octagons Revisited: Elegant Proofs and Simplified Algorithms . . . . . . . . . . . . 485 Michael Schwarz and Helmut Seidl Polynomial Analysis of Modular Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . 508 Thomas Seed, Chris Coppins, Andy King, and Neil Evans Boosting Multi-neuron Convex Relaxation for Neural Network Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 Xuezhou Tang, Ye Zheng, and Jiaxiang Liu Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565  
   
  Invited Talks  
   
  Verifying Infinitely Many Programs at Once Loris D’Antoni(B) University of Wisconsin, Madison, WI 53706-1685, USA [email protected]   
   
  Abstract. In traditional program verification, the goal is to automatically prove whether a program meets a given property. However, in some cases one might need to prove that a (potentially infinite) set of programs meets a given property. For example, to establish that no program in a set of possible programs (i.e., a search space) is a valid solution to a synthesis problem specification, e.g., a property ϕ, one needs to verify that all programs in the search space are incorrect, e.g., satisfy the property ¬ϕ. The need to verify multiple programs at once also arises in other domains such as reasoning about partially specified code (e.g., in the presence of library functions) and self-modifying code. This paper discusses our recent work in designing systems for verifying properties of infinitely many programs at once.  
   
  1  
   
  Introduction  
   
  In traditional program veriﬁcation, the goal is to automatically prove whether a program meets a given property. However, in some cases we might need to prove that a potentially infinite set of programs meets a given property. For example, consider the problem of establishing that a program-synthesis problem is unrealizable (i.e., has no solution in a given search space of programs) [4,5,9]. To establish unrealizability, i.e., that no program in a set of possible programs (i.e., a search space) is a valid solution to a synthesis problem speciﬁcation, e.g., a property ϕ, one needs to verify that all programs in the search space are incorrect, e.g., satisfy the property ¬ϕ. Example 1 (Proving Unrealizability). Consider the synthesis problem sy first where the goal is to synthesize a function f that takes as input a state (x, y), and returns a state where y = 10. Assume, however, that the search space of possible programs in sy first is deﬁned using the following grammar Gfirst : Start → y := E E → x | E + 1 Work done in collaboration with Qinheping Hu, Jinwoo Kim, and Thomas W. Reps. Supported by NSF under grants CCF-{1750965, 1918211, 2023222}; by a Facebook Research Faculty Fellowship, by a Microsoft Research Faculty Fellowship. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors, and do not necessarily reflect the views of the sponsoring entities. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  M. V. Hermenegildo and J. F. Morales (Eds.): SAS 2023, LNCS 14284, pp. 3–9, 2023. https://doi.org/10.1007/978-3-031-44245-2_1  
   
  4  
   
  L. D’Antoni  
   
  Clearly y := 10 ∈ L(Start); moreover, all programs in L(Start) are incorrect on at least one input. For example, on the input x = 15 every program in the grammar sets y to a value greater than 15. Consequently, sy first is unrealizable. While it is trivial for a human to establish that sy first is indeed unrealizable, it is deﬁnitely not trivial to automatically verify that all the inﬁnitely many programs accepted by the grammar Gfirst are incorrect on at least one input. Another setting where one may want to prove a property about multiple programs is when verifying partial programs—i.e., programs where some components are unknown [10]. Example 2 (Symbolically Executing Partial Programs). Consider the following program foo that outputs the diﬀerence between the number of elements of an array for which applying a function f results in a positive number and the number of elements for which the result of applying f is a negative number. 1 2 3 4 5 6 7 8  
   
  def foo (f , array ): count = 0 for i in range ( len ( array )): if f ( array [ i ]) > 0: count += 1 else : count -= 1 return count  
   
  Now assume that we know this program will always receive a function f drawn from the following grammar: F → λx.G G → abs(H) | -abs(H) H → x | H+H | H*H | H-H | abs(H) We might be interested in symbolically executing the program foo to identify feasible paths and understand if the program can be pruned or whether perhaps it is equivalent to a simpler program. One can do so using an uninterpreted function to model the behavior of f, but this approach would be imprecise because the grammar under consideration is such that all the inﬁnitely many programs in L(F ) either always return a positive number (i.e., f is of the form λx.abs(H)) or always return a negative number (i.e., f is of the form λx.-abs(H)). Using an uninterpreted function one would detect that the path that follows the line numbers [1, 2, 3, 4, 5, 3, 6, 7]—i.e., the path that reaches line 5 in the ﬁrst iteration of the loop and line 7 in the second iteration—is feasible, which is a false positive since the witness for f for this path is a function that does not have a counterpart in L(F ). We would like to devise a veriﬁcation technique that can symbolically execute foo and avoid this source of imprecision.  
   
  Verifying Infinitely Many Programs at Once  
   
  5  
   
  The examples we showed illustrate how veriﬁcation sometimes requires one to reason about more than one program at once. In fact, our examples require one to reason about infinitely many programs at once! Our work introduced automated techniques to prove properties of inﬁnite sets of programs. First, we designed sound but incomplete automated veriﬁcation techniques specialized in proving unrealizability of synthesis problems [4,5,9] (Sect. 2). Second, we designed unrealizability logic [8] a sound and relatively complete Hoare-style logical framework for expressing proofs that inﬁnitely many programs satisfy a pre-post condition pair (Sect. 3). We conclude this extended abstract with some reﬂections of the current limitations and directions of our work.  
   
  2  
   
  Proving Unrealizability of Synthesis Problems  
   
  Program synthesis refers to the task of discovering a program, within a given search space, that satisﬁes a behavioral speciﬁcation (e.g., a logical formula, or a set of input-output examples) [1,2,11]. While tools are becoming better at synthesizing programs, one property that remains diﬃcult to reason about is the unrealizability of a synthesis problem, i.e., the non-existence of a solution that satisﬁes the behavioral speciﬁcation within the search space of possible programs. Unrealizability has many applications; for example, one can show that a certain synthesized solution is optimal with respect to some metric by proving that a better solution to the synthesis problem does not exist—i.e., by proving that the synthesis problem where the search space contains only programs of lower cost is unrealizable [6]. In our work, we built tools that can establish unrealizability for several types of synthesis problems. Our tools Nay [5] and Nope [4] can prove unrealizability for syntax-guided synthesis (SyGuS) problems where the input grammar only contains expressions, whereas our tool MESSY [9] can prove unrealizability for semantics-guided synthesis (SemGuS) problems. Nay The key insight behind Nay is that one can, given a synthesis problem, build a nondeterministic recursive program that always satisﬁes an assertion if and only if the given problem is unrealizable. For example, for the problem in Example 1, one would build a program like the following to check that no program in the search space of the synthesis problem, when given an input that sets x to 15, can produce an output where y is set to 10. 1 2 3 4 5 6 7 8 9  
   
  def genE (x , y ): if nondet (): # nondeterministic guard return x # simulates E -> x return genE (x , y ) + 1 # simulates E -> E +1 def genStart (x , y ): return (x , genE (x , y )) # simulates Start -> y := E def main (): genStart (15 , nondet ()) # if we set x to 15 assert y != 10 # y is never 10  
   
  6  
   
  L. D’Antoni  
   
  nope The key observation of Nope is that for problems involving only expressions, unrealizability can be checked by determining what set of values η(i) a certain set of programs can produce for a given input i, and making sure that the output value we would like our program to produce for input i does not lie in that set η(i). For example, for the problem in Example 1, one can deﬁne the following equation that computes the possible values ηStart (15) of x and y, when the input value of x is 15. ηStart (15) = {(vx , vy ) | vy ∈ ηE (15)} ηS (15) = {v | v = 15 ∨ (v  ∈ ηE (15) ∧ v = v  + 1)} Nope can solve this type of equations for a limited set of SyGuS problems using ﬁxed-point algorithms and can then check if ηStart (15) contains a pair where the second element is 10 (in this case it does not). Nope can automatically prove unrealizability for many problems involving linear integer (LIA) arithmetic and is in fact sound and complete for conditional linear integer arithmetic (CLIA) when the speciﬁcation is given as a set of examples—i.e., Nope provides a decision procedure for this fragment of SyGuS (Theorem 6.2 in [4]). Some of the techniques presented in Nope have been extended to design specialized unrealizabilitychecking algorithms for problems involving bit-vector arithmetic [7]. MESSY SemGuS is a general framework for specifying synthesis problems, which also allows one to deﬁne synthesis problems involving, for example, imperative constructs. In SemGuS, one can specify a synthesis problem by providing a grammar of programs and constrained Horn clauses (CHCs) that describe the semantics of programs in the grammar. For the problem in Example 1, the following CHC can capture the semantics of the assignment y := e using two relations: (i ) The relation SemStart (p, (x, y), (x , y  )) holds when evaluating the program p on state (x, y) results in the state (x , y  ), and (ii ) The relation SemE (e, (x, y), v) holds when evaluating expression p on state (x, y) results in the value v. SemE (e, (x, y), v) x = x y  = v y := E SemStart (y := e, (x, y), (x , y  )  
   
  (1)  
   
  Once a problem is modeled with CHCs (i.e., we have semantic rules for all the possible constructs in the language), proving unrealizability can be phrased as a proof search problem. In particular, if we add the following CHC to the set of CHCs deﬁning the semantics, the relation Solution(p) captures all programs that on input x = 15 set the value of y to 10. SemStart (p, (x, y), (x , y  ) x = 15 Solution(p)  
   
  y  = 10  
   
  y := E  
   
  (2)  
   
  MESSY then uses a CHC solver to ﬁnd whether there exists a program p such that Solution(p) is provable using the given set of CHCs. If the answer is no, the problem is unrealizable. MESSY is currently the only automated tool that can (sometimes) prove unrealizability for problems involving imperative programs and could, for example, prove that no imperative program that only uses bitwise and and bitwise or can implement a bitwise xor.  
   
  Verifying Infinitely Many Programs at Once  
   
  3  
   
  7  
   
  Unrealizability Logic  
   
  The works we discussed in Sect. 3 provide automatic techniques to establish that a problem is unrealizable; however, these techniques are all closed-box, meaning that they conceal the reasoning behind why a synthesis problem is unrealizable. In particular, these techniques typically do not produce a proof artifact that can be independently checked. Our most recent work presents unrealizability logic [8], a Hoare-style proof system for reasoning about the unrealizability of synthesis problems (In this section, we include some excerpts from [8].). In addition to the main goal of reasoning about unrealizability, unrealizability logic is designed with the following goals in mind: – to be a general logic, capable of dealing with various synthesis problems; – to be amenable to machine reasoning, as to enable both automatic proof checking and to open future opportunities for automation; – to provide insight into why certain synthesis problems are unrealizable through the process of completing a proof tree. Via unrealizability logic, one is able to (i ) reason about unrealizability in a principled, explicit fashion, and (ii ) produce concrete proofs about unrealizability. To prove whether the problem in Example 1 is unrealizble, one would use unrealizability logic to derive the following triple, which states that if one starts in a state where x = 15, executing any program in the set L(Start) will result in a state where y is diﬀerent than 10: {|x = 15|} L(Start) {|y = 10|} Unrealizability logic shares much of the intuition behind Hoare logic and its extension toward recursive programs. However, these concepts appearing in Hoare logic alone are insuﬃcient to model unrealizability, which motivated us to develop the new ideas that form the basis of unrealizability logic. Hoare logic is based on triples that overapproximate the set of states that can be reached by a program s; i.e., the Hoare triple {P } s {Q} asserts that Q is an overapproximation of all states that may be reached by executing s, starting from a state in P . The intuition in Hoare logic is that one will often attempt to prove a triple like {P } s {¬X} for a set of bad states X, which ensures that execution of s cannot reach X. Unrealizability logic operates on the same overapproximation principle, but diﬀers in two main ways from standard Hoare logic. The diﬀerences are motivated by how synthesis problems are typically deﬁned, using two components: (i ) a search space S (i.e., a set of programs), and (ii ) a (possibly inﬁnite) set of related input-output pairs {(i1 , o1 ), (i2 , o2 ), · · ·}.  
   
  8  
   
  L. D’Antoni  
   
  To reason about sets of programs, in unrealizability logic, the central element (i.e., the program s) is changed to a set of programs S. The unrealizability-logic triple {|P |} S {|Q|} thus asserts that Q is an overapproximation of all states that are reachable by executing any possible combination of a pre-state p ∈ P and a program s ∈ S. The second diﬀerence concerns input-output pairs: in unrealizability logic, we wish to place the input states in the precondition, and overapproximate the set of states reachable from the input states (through a set of programs) as the postcondition. Unfortunately, the input-output pairs of a synthesis problem cannot be tracked using standard pre- and postconditions; nor can they be tracked using auxiliary variables, because of a complication arising from the fact that unrealizability logic must reason about a set of programs—i.e., we want our possible output states to be the results of executing the same program on the given input (for all possible programs) and prevent output states where diﬀerent inputs are processed by diﬀerent programs in the search space. To keep the input-output relations in check, the predicates of unrealizability logic talk instead about (potentially inﬁnite) vector-states, which are sets of states in which each individual state is associated with a unique index—e.g., variable x of the state with index i is referred to as xi . We defer the reader to the original unrealizability logic paper for these details [8]. The proof system for unrealizability logic has sound underpinnings, and provides a way to build proofs of unrealizability similar to the way Hoare logic [3] provides a way to build proofs that a given program cannot reach a set of bad states. Furtheremore the systems is relatively complete in the same sense as Hoare logic is.  
   
  4  
   
  Conclusions  
   
  This paper outlines recent advances in reasoning about inﬁnite sets of programs at once. We presented techniques for proving unrealizability of synthesis problems that draw inspiration from traditional program veriﬁcation. However, such techniques did not provide ways to produce proof artifact and to address this limitation, we discussed unrealizability logic, the ﬁrst proof system for overapproximating the execution of an inﬁnite set of programs. This logic is also the ﬁrst approach that allows one to prove unrealizability for synthesis problems that require inﬁnitely many inputs to be proved unrealizable. The name “unrealizability logic” is perhaps misleading as the logic allows one to reason about many properties beyond unrealizability. The fact that unrealizability logic is both sound and relatively complete means that this proof system can prove (given powerful enough assertion languages) any property of a given set of programs expressed as a grammar. For example, the problem given in Example 2 of identifying whether a symbolic execution path is infeasible can be phrased as proving whether an unrealizability triple holds.  
   
  Verifying Infinitely Many Programs at Once  
   
  9  
   
  It is thus natural to conclude with two open questions: (i ) What applications besides unrealizability can beneﬁt from unrealizability logic as a proof system? (ii ) Can unrealizability logic be automated in the same successful way Hoare logic has been automated for traditional program veriﬁcation?  
   
  References 1. Feser, J.K., Chaudhuri, S., Dillig, I.: Synthesizing data structure transformations from input-output examples. ACM SIGPLAN Not. 50(6), 229–239 (2015) 2. Gulwani, S.: Automating string processing in spreadsheets using input-output examples. ACM SIGPLAN Not. 46(1), 317–330 (2011) 3. Hoare, C.A.R.: An axiomatic basis for computer programming. Commun. ACM 12(10), 576–580 (1969) 4. Hu, Q., Breck, J., Cyphert, J., D’Antoni, L., Reps, T.: Proving unrealizability for syntax-guided synthesis. In: Dillig, I., Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11561, pp. 335–352. Springer, Cham (2019). https://doi.org/10.1007/978-3-03025540-4_18 5. Hu, Q., Cyphert, J., D’Antoni, L., Reps, T.: Exact and approximate methods for proving unrealizability of syntax-guided synthesis problems. In: Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 1128–1142 (2020) 6. Hu, Q., D’Antoni, L.: Syntax-guided synthesis with quantitative syntactic objectives. In: Chockler, H., Weissenbacher, G. (eds.) CAV 2018. LNCS, vol. 10981, pp. 386–403. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-96145-3_21 7. Kamp, M., Philippsen, M.: Approximate bit dependency analysis to identify program synthesis problems as infeasible. In: Henglein, F., Shoham, S., Vizel, Y. (eds.) VMCAI 2021. LNCS, vol. 12597, pp. 353–375. Springer, Cham (2021). https://doi. org/10.1007/978-3-030-67067-2_16 8. Kim, J., D’Antoni, L., Reps, T.: Unrealizability logic. Proc. ACM Program. Lang. 7(POPL), 659–688 (2023). https://doi.org/10.1145/3571216 9. Kim, J., Hu, Q., D’Antoni, L., Reps, T.: Semantics-guided synthesis. Proc. ACM Programm. Lang. 5(POPL), 1–32 (2021) 10. Mechtaev, S., Griggio, A., Cimatti, A., Roychoudhury, A.: Symbolic execution with existential second-order constraints. In: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 389–399 (2018) 11. Phothilimthana, P.M., et al.: Swizzle inventor: data movement synthesis for GPU kernels. In: Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 65–78 (2019)  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned Daniel K¨ astner1(B) , Reinhard Wilhelm2 , and Christian Ferdinand1 1  
   
  2  
   
  AbsInt GmbH, Science Park 1, 66123 Saarbr¨ ucken, Germany [email protected]  Saarland University, Stuhlsatzenhausweg 69, 66123 Saarbr¨ ucken, Germany  
   
  Abstract. In this article we will give an overview of the development and commercialization of two industry-strength Abstract Interpretationbased static analyzers, aiT WCET Analyzer and Astr´ee. We focus on development steps, adaptations to meet industry requirements and discuss criteria for a successful transfer of formal veriﬁcation methods to industrial usage. Keywords: abstract interpretation · WCET analysis analysis · functional safety · cybersecurity  
   
  1  
   
  · runtime error  
   
  Introduction  
   
  Abstract interpretation is a formal method for sound semantics-based static program analysis [8]. It supports formal correctness proofs: it can be proved that an analysis will terminate and that it is sound in the sense that it computes an overapproximation of the concrete program semantics. Abstract interpretation-based static analyzers provide full control and data coverage and allow conclusions to be drawn that are valid for all program runs with all inputs. As of today, abstract interpretation-based static analyzers are most widely used to determine non-functional software quality properties [22,23]. On the one hand that includes source code properties, such as compliance to coding guidelines, compliance to software architectural requirements, as well as absence of runtime errors and data races [34]. On the other hand also low-level code properties are covered, such as absence of stack overﬂows and violation of timing constraints [24,25]. Violations of non-functional software quality requirements often either directly represent safety hazards and cybersecurity vulnerabilities in safety- or security-relevant code, or they can indirectly trigger them. Corresponding veriﬁcation obligations can be found in all current safety and security norms, such as DO-178C [48], IEC-61508 [15], ISO-26262 [17], and EN-50128 [6]. Many formal veriﬁcation tools, including abstract interpretation-based static analyzers, originate from academic research projects. However, the transition from academia into industry is far from straightforward. In this article we will c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  M. V. Hermenegildo and J. F. Morales (Eds.): SAS 2023, LNCS 14284, pp. 10–27, 2023. https://doi.org/10.1007/978-3-031-44245-2_2  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  11  
   
  give an overview of our experience in development and commercialization of two industry-strength sound analyzers, aiT WCET analyzer and Astr´ee. We will discuss the lessons learned, and present recommendations to improve dissemination and acceptance in industrial practice.  
   
  2  
   
  Sound Worst-Case Execution Time Analysis  
   
  Time-critical embedded systems have deadlines derived from the physical environment. They need assurance that their execution time does not exceed these deadlines. Essential input to a response-time analysis are the safe upper bounds of all execution times of tasks to be executed on the same execution platform. These are commonly called Worst-case Execution times, WCET. The WCETanalysis problem had a solution for architectures with constant execution times for instructions, so-called Timing Schemata [54]. These described how WCETs could be computed by structural induction over programs. However, in the 1990s industry started using microprocessors employing performance-enhancing architectural components and features such as caches, pipelines, and speculation. These made methods based on timing schemata obsolete. The execution-time of an instruction now depended on the execution state in which the instruction were executed. The variability of execution times grew with several architectural parameters, e.g. the cache-miss penalty and the costs for pipeline stalls and for control-ﬂow mis-predictions. 2.1  
   
  Our View of and Our Solution to the WCET-Analysis Problem  
   
  We developed the following view of the WCET-analysis problem for architectures with state-dependent execution times: Any architectural eﬀect that lets an instruction execute longer than its fastest execution time is a Timing Accident. Some of such timing accidents are cache misses, pipeline stalls, bus-access conﬂicts, and branch mis-predictions. Each such timing accident has to be paid for, in terms of execution-time cycles, by an associated Timing Penalty. The size of a timing penalty can be constant, but may also depend on the execution state. We consider the property that an instruction in the program will not cause a particular timing accident as a safety property. The occurrence of a timing accident thus violates a corresponding safety property. The essence of our WCET-analysis method then consists in the attempt to verify for each instruction in the program as many safety properties as possible, namely that some of the potential timing accidents will never happen. The proof of such safety properties reduces the worst-case execution-time bound for the instruction by the penalties for the excluded timing accidents. This so-called Microarchitectural Analysis, embedded within a complex tool architecture, is the central innovation that made our WCET analysis work and scale. We use Abstract Interpretation to compute certain invariants at each program point, namely an upper approximation of the set of execution states that are possible when execution reaches this program point and then derive safety properties, that certain timing accidents will not happen, from these invariants.  
   
  12  
   
  2.2  
   
  D. K¨ astner et al.  
   
  The Development of Our WCET-Analysis Technique  
   
  We started with a classifying cache analysis [1,12], an analysis that attempts to classify memory accesses in programs as either always hitting or always missing the caches, i.e. instruction and data caches. Our Must analysis, used to identify cache hits, computes an under-approximation of the set of cache states that may occur when execution reaches a program point. Our May analysis determines an over-approximation of this set of cache states. Both can be represented by compact, eﬃciently updatable abstract cache states. At the start of the development, the caches we, and everybody else, considered used LRU replacement. This made our life easy, but application to real-life processors diﬃcult since the hardware logic for implementing LRU replacement is expensive, and therefore LRU replacement is rarely used in real-life processors. Involved in the European project Daedalus with Airbus we were confronted with two processors using very diﬀerent cache-replacement strategies. The ﬁrst processor, ﬂying the Airbus A340 plane, was a Motorola Coldﬁre processor which used a cheap emulation of a random-replacement cache. The second projected to ﬂy the A380 plane was a Motorola PowerPC 755. It used a Pseudo-LRU replacement strategy. We noticed that our cache analysis for the Coldﬁre processor could only track the last loads into the cache, and that our cache analysis for the PowerPC 755 could only track 4 out of the 8 ways in each cache set. This inspired us to very fruitful research about Timing Predictability [60] and in particular to the ﬁrst formal notion of timing predictability, namely that for caches [50]. Next Stephan Thesing developed our pipeline analysis [39]. Unfortunately, pipelines in real-life processors do not admit compact abstract pipeline states. Therefore, expensive powerset domains are used. The pipeline analysis turned out to be the most expensive part of the WCET analysis. A basic block could easily generate a million pipeline states and correspondingly many transitions for analysis. There was a tempting idea to follow only local worst-case transitions and ignore all others. However, real-life processors exhibit Timing Anomalies [51]. These entail that a local non-worst-case may contribute to a global worst case. In the Daedalus project, Airbus also asked for a modeling of their system controller. So far, all WCET research had concentrated on processors. However, a system controller contributes heavily to overall system timing and therefore needs an accurate model and precise analysis [59]. The Micro-architectural analysis was applied to basic blocks, i.e. maximally long straight-line code sequences that can only be entered at the beginning and only be left at the end. The control ﬂow, which had been extracted from the binary executable [57], was translated into an Integer Linear Program (ILP) [58]. The solution of this ILP presented a longest path through the program and the associated execution time. This approach, termed Implicit Path Enumeration Technique (IPET), had been adopted from [40]. At EMSOFT 2001 we presented our breakthrough paper [11]. In summary, a generic tool architecture has emerged which consists of the following stages:  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  13  
   
  Decoding: The instruction decoder identiﬁes the machine instructions and reconstructs the call and control-ﬂow graph. Value analysis: Value analysis aims at statically determining enclosing intervals for the contents of the registers and memory cells at each program point and for each execution context. The results of the value analysis are used to predict the addresses of data accesses, the targets of computed calls and branches, and to ﬁnd infeasible paths. Micro-architectural analysis: The execution of a program is statically simulated by feeding instruction sequences from the control-ﬂow graph to a microarchitectural timing model which is centered around the cache and pipeline architecture. It computes the system state changes induced by the instruction sequence at cycle granularity and keeps track of the elapsing clock cycles. Path analysis: Based on the results of the combined cache/pipeline analysis the worst-case path of the analyzed code is computed with respect to the execution timing. The execution time of the computed worst-case path is the worst-case execution time for the task. We had shown that our sound WCET-analysis method not only solved the singlecore WCET-analysis problem, but was even more accurate than the unsound, measurement-based method Airbus had previously used. This meant that their worst-case execution times they had presented in certiﬁcation had been reliable. Consequently we collaborated with Airbus to satisfy their needs for a sound, industrially viable WCET analysis. 2.3  
   
  Improvements  
   
  Although the results of our our analysis were already quite accurate, overestimating the ever observed worst-case execution times by roughly 25%, Airbus wanted more accurate results. Also the integration into industrial development processes needed consideration and some eﬀort. Increasing Precision. Programs are known to spend most of their time in (recursive) procedures and in loops. The IPET approach using worst-case execution times of basic blocks as input was theoretically pleasing, but lost too much accuracy at the border between basic block and between loop iterations. Controlled loop unrolling increased the accuracy by the necessary extent. However, until today we confuse the competition by using the IPET approach in our explanations. Often, the software developers knew what they were doing, i.e., they knew properties of their software that inﬂuenced execution time, but which were not explicit in the software. Our tool oﬀered to be instructed by adding annotations to the software. Some annotations were even absolutely necessary, like loop and recursion bounds if those could not be automatically derived by our Value Analysis, essentially an interval analysis [9], modiﬁed to work on binary programs. We will later see that annotations could be automatically inserted if the WCETanalysis tool had been integrated with a model-based design tool.  
   
  14  
   
  D. K¨ astner et al.  
   
  Integration with Model-Based Design and Schedulability Tools. Much of the safety-critical embedded software is developed using Model-based Design (MBD) tools. These automatically generate code from models speciﬁed by the software developer. When our WCET tool aiT is integrated with such a MBD tool, model information can be automatically inserted as annotations. Also approximate timing information can be provided on the model level to the developer by back annotation during the development process. The determined WCETs are typically input into a schedulability analysis. Consequently, aiT has been integrated with several such tools. 2.4  
   
  Tool Qualiﬁcation  
   
  Whenever the output of a tool is either part of a safety-critical system to be certiﬁed or the tool output is used to eliminate or reduce any development or veriﬁcation eﬀort for such a system, that tool needs to qualiﬁed [22]. Safety norms like DO-178C and ISO 26262 impose binding regulations for tool qualiﬁcation; they mandate to demonstrate that the tool works correctly in the operational context of its users and/or that the tool is developed in accordance to a safety standard. To address this, a Qualiﬁcation Support Kit has been developed, which consists of several parts. The Tool Operational Requirements (TOR) document lists the tool functions and technical features which are stated as low-level requirements to the tool behavior under normal operating conditions. Additionally, the TOR describes the tool operational context and conditions in which the tool computes valid results. A second document (Tool Operational Veriﬁcation and Validation Cases and Procedures, TOVVCP) deﬁnes a set of test cases demonstrating the correct functioning of all speciﬁed requirements from the TOR. Test case deﬁnitions include the overall test setup as well as a detailed structural and functional description of each test case. The test part contains an extensible set of test cases with a scripting system to automatically execute them and generate reports about the results. These tests also include model validation tests, in fact, a signiﬁcant part of the development eﬀort for aiT is to validate the abstract hardware model; [25] gives an overview. In addition, the QSK provides a set of documents that give details about the AbsInt tool development and veriﬁcation processes and demonstrate their suitability for safety-critical software. 2.5  
   
  Impact in Industry and Academia  
   
  A painful insight was that hardly any two WCET customers of AbsInt used the same hardware conﬁguration in his systems. The costs for an instantiation of our WCET-analysis technology for a new processor can take quite an eﬀort, making the resulting tool by necessity quite expensive. Still, aiT has been successfully employed in industry and is available for a variety of microprocessors ranging from simple processors like ARM7 to complex superscalar processors with timing anomalies and domino eﬀects like Freescale MPC755, or MPC7448,  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  15  
   
  and multi-core processors like Inﬁneon AURIX TC27x. Our development of a sound method that actually solved a real problem of real industry was considered a major success story for the often disputed formal-methods domain. AbsInt became the favorite partner for the industrialization of academic prototypes. First, Patrick Cousot and his team oﬀered their prototype of Astr´ee, which in cooperation with some of the developers has been largely extended by AbsInt – more about this in Sect. 3. Then, we entered a cooperation with Xavier Leroy on the result of his much acclaimed research project, CompCert, the ﬁrst formally veriﬁed optimizing C compiler [29,30]. The CompCert front-end and back-end compilation passes, and their compositions, are all formally proved to be free of miscompilation errors. The property that is formally veriﬁed, using machineassisted mathematical proofs, is semantic preservation between the input code and output code of every pass. Hence, the executable code CompCert produces is proved to behave exactly as speciﬁed by the formal semantics of the source C program. Both Astr´ee and CompCert are now available as AbsInt products. 2.6  
   
  Application to Non-Timing-Predictable Architectures  
   
  Multi-core processors with shared resources pose a severe problem for sound and precise WCET analysis. To interconnect the several cores, buses, meshes, crossbars, and also dynamically routed communication structures are used. In that case, the interference delays due to conﬂicting, simultaneous accesses to shared resources (e.g. main memory) can cause signiﬁcant imprecision. Multicore processors which can be conﬁgured in a timing-predictable way to avoid or bound inter-core interferences are amenable to static WCET analysis [27,63,64]. Examples are the Inﬁneon AURIX TC275 [16], or the Freescale MPC 5777. The Freescale P4080 [13] is one example of a multi-core platform where the interference delays have a huge impact on the memory access latencies and cannot be satisfactorily predicted by purely static techniques. In addition, no public documentation of the interconnect is available. Nowotsch et al. [46] measured maximal write latencies of 39 cycles when only one core was active, and maximal write latencies of 1007 cycles when all eight cores were running. This is more than 25 times longer than the observed single-core worst case. Like measuring task execution on one core with interference generators running on all other cores, statically computed WCET bounds will signiﬁcantly overestimate the timing delays of the system in the intended ﬁnal conﬁguration. In some cases, robust partitioning [64] can be achieved with approaches approaches like [53] or [46]. For systems which do not implement such rigorous software architectures or where the information needed to develop a static timing model is not available, hybrid WCET approaches are the only solution. For hybrid WCET analysis, the same generic tool architecture as described in Sect. 2.2 can be used, as done in the tool TimeWeaver [37]. It performs Abstract Interpretation-based context-sensitive path and value analysis analysis, but replaces the Microarchitectural Analysis stage by non-intrusive real-time instruction-level tracing to provide worst-case execution time estimates. The trace information covers interference eﬀects, e.g., by accesses to shared resources  
   
  16  
   
  D. K¨ astner et al.  
   
  from diﬀerent cores, without being distorted by probe eﬀects since no instrumentation code is needed. The computed estimates are upper bounds with respect to the given input traces, i.e., TimeWeaver derives an overall upper timing bound from the execution time observed in the given traces. This approach is compliant to the recommendations of CAST-32a and AMC 20–193 [7,10]. 2.7  
   
  Spin-Oﬀ: Worst-Case Stack Usage Analysis  
   
  In embedded systems, the run-time stack (often just called ”the stack”) typically is the only dynamically allocated memory area. It is used during program execution to keep track of the currently active procedures and facilitate the evaluation of expressions. Each active procedure is represented by an activation record, also called stack frame or procedure frame, which holds all the state information needed for execution. Precisely determining the maximum stack usage before deploying the system is important for economical reasons and for system safety. Overestimating the maximum stack usage means wasting memory resources. Underestimation leads to stack overﬂows: memory cells from the stacks of diﬀerent tasks or other memory areas are overwritten. This can cause crashes due to memory protection violations and can trigger arbitrary erroneous program behavior, if return addresses or other parts of the execution state are modiﬁed. In consequence stack overﬂows are typically hard to diagnose and hard to reproduce, but they are a potential cause of catastrophic failure. The accidents caused by the unintended acceleration of the 2005 Toyota Camry illustrate the potential consequences of stack overﬂows: the expert witness’ report commissioned by the Oklahoma court in 2013 identiﬁes a stack overﬂow as probable failure cause [3,61]. The generic tool architecture of Sect. 2.2 can be easily adapted to perform an analysis of the worst-case stack usage, by exchanging the Microarchitectural analysis step with a dedicated value analysis for the stack pointer register(s) [24]. In 2001, the resulting tool, called StackAnalyzer, was released, which was the ﬁrst commercial tool to safely prove the absence of stack overﬂows in safetycritical systems, and since then has been widely adopted in industry.  
   
  3  
   
  Sound Runtime Error Analysis  
   
  The purpose of the Astr´ee analyzer is to detect source-level runtime errors due to undeﬁned or unspeciﬁed behaviors of C programs. Examples are faulty pointer manipulations, numerical errors such as arithmetic overﬂows and division by zero, data races, and synchronization errors in concurrent software. Such errors can cause software crashes, invalidate separation mechanisms in mixed-criticality software, and are a frequent cause of errors in concurrent and multi-core applications. At the same time, these defects also constitute security vulnerabilities, and have been at the root of a multitude of cybersecurity attacks, in particular buﬀer overﬂows, dangling pointers, or race conditions [31].  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  3.1  
   
  17  
   
  The Origins  
   
  Astr´ee stands for Analyseur statique de logiciels temps-r´eel embarqu´es (real-time embedded software static analyzer). The development of Astr´ee started from ´ scratch in Nov. 2001 at the Laboratoire d’Informatique of the Ecole Normale ´ Sup´erieure (LIENS), initially supported by the ASTREE project, the Centre ´ National de la Recherche Scientiﬁque, the Ecole Normale Sup´erieure and, since September 2007, by INRIA (Paris-Rocquencourt). First industrial applications of Astr´ee appeared two years after starting the project. Astr´ee has achieved the following unprecedented results on the static analysis of synchronous, time-triggered, real-time, safety critical, embedded software written or automatically generated in the C programming language: – In Nov. 2003, Astr´ee was able to prove completely automatically the absence of any RTE in the primary ﬂight control software of the Airbus A340 ﬂy-bywire system. – From Jan. 2004 on, Astr´ee was extended to analyze the electric ﬂight control codes then in development and test for the A380 series. – In April 2008, Astr´ee was able to prove completely automatically the absence of any RTE in a C version of the automatic docking software of the Jules Vernes Automated Transfer Vehicle (ATV) enabling ESA to transport payloads to the International Space Station [4]. In Dec. 2009, AbsInt started the commercialization of Astr´ee in cooperation with LIENS, in particular Patrick Cousot, J´erˆome Feret, Laurent Mauborgne, Antoine Min´e, and Xavier Rival. 3.2  
   
  Further Development  
   
  From a technical perspective, the ensuing development activities can be grouped into several categories: Usability. The original version of Astr´ee was a command-line tool, however, to facilitate commercial use, a graphical user interface was developed. The purpose is not merely to make the tool more intuitive to use, but – even more importantly – to help users understand the results. Astr´ee targets corner cases of the C semantics which requires a good understanding of the language, and it shows defects due to behavior unexpected by the programmer. To facilitate understanding the unexpected behavior, we have developed a large set of graphical and interactive exploration views. To give some examples, all parents in the call stack, relevant loop iterations or conditional statements that lead to the alarm can be accessed by mouse click, tool tips show the values of values, the call graph can be interactively explored, etc. [28]. In all of this, there is one crucial requirement: all views and graphs have to be eﬃciently computable and suitable for large-scale software consisting of millions of lines of code [20]. Further usability enhancements were the integration of a preprocessor into Astr´ee (the original version read preprocessed C code), automated preprocessor  
   
  18  
   
  D. K¨ astner et al.  
   
  conﬁguration based on JSON compilation ﬁles, Windows support, and the ability to classify and comment ﬁndings from the GUI. Apart from easy usability, an important requirement of contemporary development processes is the ability to integrate a tool in a CD/CI (continuous development/continuous integration) platform. To support this, Astr´ee can be started from the command line with full functionality, the conﬁguration is given as an XML ﬁle which can be automatically created, results can be exported in machinereadable formats (xml, csv, html) that support post-processing. Furthermore, there is a large number of plugins and tool couplings which have been developed, e.g., to model-based development tools like Matlab/Simulink/TargetLink [26,38], as well as CI tools and IDEs such as Jenkins, Eclipse, and Keil µVision. Formal Requirements. The primary use-case of Astr´ee is to ﬁnd defects in safety-critical or security-relevant software, hence the same tool qualiﬁcation requirements apply as described in Sect. 2.3. So, the development of a Qualiﬁcation Support Kit for Astr´ee was a mandatory; its structure is similar to the aiT QSK as described above. Another constraint is that in certain safety processes, no code modiﬁcations are allowed which cannot be traced to functional software requirements. Also, in the case of model-based software development, where the code is automatically generated, it is infeasible to modify the source code to interact with a static analyzer. Astr´ee provides numerous analysis directives that allow users to interact with the tool, e.g., to pass certain preconditions such as constraints on input value ranges or volatile variable ranges to the analyzer. Alarms can be classiﬁed (e.g., as true defect or false alarms) via source code comments or analysis directives. Finally Astr´ee’s domains have been speciﬁcally developed to support ﬁne-grained precision tuning to eliminate false alarms. One example is the trace partitioning domain, a generic framework that allows the partitioning of traces based on the history of the control ﬂow [52]. By inserting analysis directives into the code, users can inﬂuence the partitioning strategy of the analyzer for limited parts of the code. To also support use cases where code modiﬁcations are infeasible, a formal language AAL has been developed [36] which provides a robust way to locate analyzer directives in the abstract syntax tree without modifying the source code. It is also possible to automatically generate such annotations from the build environment or an interface speciﬁcation. New Capabilities Interleaving Semantics and Integration Analysis. While the ﬁrst versions of Astr´ee targeted sequential code, most of today’s industry applications are multithreaded. In such software systems, it is highly desirable to be able to do runtime error analysis at the integration veriﬁcation stage, i.e., to analyze the entire software stack in order to capture the interactions between all components of the  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  19  
   
  system, determine their eﬀect on data and control ﬂow and detect runtime errors triggered by them. To support this, Antoine Min´e has developed a low-level concurrent semantics [42] which provides a scalable sound abstraction covering all possible thread interleavings. The interleaving semantics enables Astr´ee, in addition to the classes of runtime errors found in sequential programs, to report data races and lock/unlock problems, i.e., inconsistent synchronization. The set of shared variables does not need to be speciﬁed by the user: Astr´ee assumes that every global variable can be shared, and discovers which ones are eﬀectively shared, and on which ones there is a data race. To implement its interleaving semantics, Astr´ee provides primitives which expose OS functionality to the analyzer, such as mutex un-/locks, interrupt dis-/enabling, thread creation, etc. Since Astr´ee is aware of all locks held for every program point in each concurrent thread, Astr´ee can also report all potential deadlocks. Astr´ee also supports several stages of concurrent execution so that initialization tasks can be separated from periodic/acyclic tasks. Each thread can be associated to one or several concurrent execution stages. Using the Astr´ee concurrency primitives, abstract OS libraries have been developed, which currently support the OSEK/AUTOSAR and ARINC 653 norms [2,43]. A particularity of OSEK/AUTOSAR is that system resources, including tasks, mutexes and spin-locks, are not created dynamically at program startup; instead they are hardcoded in the system: a speciﬁc tool reads a conﬁguration ﬁle in OIL (OSEK Implementation Language) or ARXML (AutosaR XML) format describing these resources and generates a specialized version of the system to be linked against the application. A dedicated ARXML converter has been developed for Astr´ee which automatically generates the appropriate data structures and access functions for the Astr´ee analysis, and enables a fully automatic integration analysis of AUTOSAR projects [20]. Code Guideline Checking. Coding guidelines aim at improving code quality and can be considered a prerequisite for developing safety- or security-relevant software. In particular, obeying coding guidelines is strongly recommended by all current safety standards. Their purpose is to reduce the risk of programming errors by enforcing low complexity, enforcing usage of a language subset, using well-trusted design principles, etc. According to ISO 26262, the language subset to be enforced should exclude, e.g., ambiguously deﬁned language constructs, language constructs that could result in unhandled runtime errors, and language constructs known to be error-prone. Since the Astr´ee architecture is well suited for sound and precise code guideline checking, over the years, the analyzer has been extended to support all major coding guidelines, such as MISRA C/C++ [41,44,45], SEI CERT C/C++ [55], CWE [56], etc. Cybersecurity Vulnerability Scanning. Many security attacks can be traced back to behaviors undeﬁned or unspeciﬁed according to the C semantics. By applying sound static runtime error analyzers, a high degree of security can be achieved for  
   
  20  
   
  D. K¨ astner et al.  
   
  safety-critical software since the absence of such defects can be proven. In addition, security hyperproperties require additional analyses to be performed which, by nature, have a high complexity. To support this, Astr´ee has been extended by a generic abstract domain for taint analysis that can be freely instantiated by the users [33]. It augments Astr´ee’s process-interleaving interprocedural code analysis by carrying and computing taint information at the byte level. Any number of taint hues can be tracked by Astr´ee, and their combinations will be soundly abstracted. Tainted input is speciﬁed through directives attached to program locations. Such directives can precisely describe which variables, and which part of those variables is to be tainted, with the given taint hues, each time this program location is reached. Any assignment is interpreted as propagating the join of all taint hues from its right-hand side to the targets of its left-hand side. In addition, speciﬁc directives may be introduced to explicitly modify the taint hues of some variable parts. This is particularly useful to model cleansing function eﬀects or to emulate changes of security levels in the code. The result of the analysis with tainting can be explored in the Astr´ee GUI, or explicitly dumped using dedicated directives. Finally, the taint sink directives may be used to declare that some parts of some variables must be considered as taint sinks for a given set of taint hues. When a tainted value is assigned to a taint sink, then Astr´ee will emit a dedicated alarm, and remove the sinked hues, so that only the ﬁrst occurrence has to be examined to ﬁx potential issues with the security data ﬂow. The main intended use of taint analysis in Astr´ee is to expose potential vulnerabilities with respect to security policies or resilience mechanisms. Thanks to the intrinsic soundness of the approach, no tainting can be forgotten, and that without any bound on the number of iterations of loops, size of data or length of the call stack. Based on its taint analysis, Astr´ee provides an automatic detection of Spectre-PHT vulnerabilities [32]. Data and Control Flow. All current safety norms require determining the data and control ﬂow in the source code and making sure that it is compliant to the intended control and data ﬂow as deﬁned in the software architecture. To meet this requirement, Astr´ee has been extended by a data and control ﬂow analysis module, which tracks accesses to global, static, and local variables. The soundness of the analysis ensures that all potential targets of data and function pointers are discovered. Data and control ﬂow reports show the number of read and write accesses for every global, static, and out-of-frame local variable, lists the location of each access and shows the function from which the access is made. All variables are classiﬁed as being thread-local, eﬀectively shared between diﬀerent threads, or subject to a data race. To further support integration veriﬁcation, a recent extension of Astr´ee provides a generic concept for specifying software components, enabling the analyzer to lift the data and control ﬂow analysis to report data and control ﬂow interactions between software components. This is complemented by an automatic taint analysis that eﬃciently tracks the ﬂow of values between components, and automatically reports undesired data ﬂow and undesired control dependencies.  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  21  
   
  The combination of augmented data and control analysis and the taint analysis for software components provides a sound interference analysis [35]. C++. To respond to the increasing interest in C++ even in the domain of safetycritical software, since 2020 Astr´ee also provides a dedicated analysis mode for C++ and mixed C/C++. It uses the same analysis technology as Astr´ee’s semantic C code analysis and has similar capabilities. At the same time it is also subject to the same restrictions. The analyzer is designed to meet the characteristics of safety-critical embedded software. Typical properties of such software include a static execution model that uses a ﬁxed number of threads, no or limited usage of dynamic memory allocation and dynamic data structures. Astr´ee provides an abstract standard template library, that models the behavior of STL containers in an abstract way suitable for analysis with Astr´ee. Astr´ee does not attempt to analyze the control ﬂow of exceptions; it only reports if an exception could be raised. Precision and Eﬃciency. Constant development eﬀort is required to work at precision and scalability of the analyzer. Over the years, various additional abstract domains have been developed to avoid false alarms on common embedded software elements. Examples are domains for ﬁnite integer sets, gauges [21,62], domains for precise analysis of interpolation functions, ﬁnite state machines, etc. Astr´ee’s state machine domain heuristically detects state variables and disambiguates them by state partitioning in the relevant program scope [14]. In consequence the analyzer becomes aware of the exact transitions of the state machine and the false alarms due to control ﬂow over-approximation can be avoided. Over the past years, the size of embedded software has grown signiﬁcantly; typical automotive AUTOSAR projects span 5–50 million lines of (preprocessed) code. One prerequisite to enable an eﬃcient analysis of such large-scale projects is an eﬃcient strategy to heuristically control the contextsensitivity of the analyzer and distinguish critical call chains where full ﬂow- and context-sensitivity is needed from less critical ones where building a summary context is enough [20].  
   
  4  
   
  The User Perspective  
   
  Whereas from an academic perspective, software veriﬁcation can be fun and is a topic of great merit, this is not necessarily a view shared by every software developer working in the ﬁeld. In fact, the ISO 26262 norm puts en emphasis on the need to embrace functional safety in the company organization and establish a safety culture [18]. Veriﬁcation activities should not be – as they often are – perceived as a burden that drains on development cost, delays delivery and does not provide an added value to the end product. Introducing new veriﬁcation steps should not be perceived as admitting a mistake. The capability of defect prevention, the eﬃciency in defect detection, and the degree of automation is crucial for user acceptance.  
   
  22  
   
  D. K¨ astner et al.  
   
  Advanced program analysis requires signiﬁcant technical insights, including knowledge about the programming language semantics, microprocessor design, and system conﬁguration. Without the necessary understanding, program analysis tools are hard to use. On the other hand, it is necessary for tools to expose the information users need to understand the results as intuitively as possible. Finally, users expect tools to solve real problems, e.g., the worst-case execution time on a particular microcontroller in the conﬁguration given, or the occurrence of runtime errors in the tasks as they are deployed in the real system. When providing partial solutions to a problem, it is necessary to explain how to use them to help dealing with the full problem.  
   
  5  
   
  The Role of Safety Norms  
   
  Functional safety and security are aspects of dependability, in addition to reliability and availability. Functional safety is usually deﬁned as the absence of unreasonable risk to life and property caused by malfunctioning behavior of the system. Correspondingly, cybersecurity can be deﬁned as absence of unreasonable risk caused by malicious misusage of the system. Functional safety norms aim at formalizing the minimal obligations for developers of safety-critical systems to make sure that unreasonable safety risks are avoided. In addition, advances in system development and veriﬁcation since the publication date of a given norm have to be taken into account. In other words, safety norms deﬁne the minimal requirements to develop safety-relevant software with due diligence. Safety standards typically are domain-speciﬁc; examples DO-178B/DO-178C [47,48] (aerospace), ISO 26262 [17] (automotive), CENELEC EN 50128/EN 50657 [5,6] (railway), IEC 61508 [15] (general electrical and/or electronic systems), IEC 62304 (medical products), etc. The DO-178C [48] has been published with supplements focusing on technical advances since release of the predecessor norm DO-178B, in particular the DO-333 (Formal Methods Supplement to DO-178C and DO-278A) [49], that addresses the use of formal methods to complement or replace dynamic testing. It distinguishes three categories of formal analyses: deductive methods such as theorem proving, model checking, and abstract interpretation. The computation of worst-case execution time bounds and the maximal stack usage are listed as reference applications of abstract interpretation. However, the standard does not mandate the use of formal methods. Table 7 and Table 10 of ISO 26262 Part 6 [19] give a list of recommended methods for veriﬁcation of software unit design and implementation, and integration veriﬁcation, respectively. They contain separate entries for formal veriﬁcation, control ﬂow analysis, data ﬂow analysis, static code analysis, and static analysis by abstract interpretation. Static analysis in general is highly recommended for all criticality levels (ASILs), Abstract Interpretation is recommended for all ASILs. The current versions of EN 50128 and IEC 62304 lack an explicit reference to Abstract Interpretation. Since for industrial system development, functional safety norms are deﬁning what is considered to be (minimal) state of the art, the availability of mature  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  23  
   
  development and veriﬁcation techniques should be reﬂected in them. To create the necessary awareness, an exchange between software and safety communities is essential.  
   
  6  
   
  Conclusion  
   
  The focus of this article is to describe the application of Abstract Interpretation to two diﬀerent real-life problems: to compute sound worst-case execution time bounds, and to perform sound runtime error analysis for C/C++ programs. We have summarized the development history of aiT WCET Analyzer and Astr´ee, discussed design choices, and illustrated the exigencies imposed by commercial users and industrial processes. We also addressed derived research and applications to other topics, in particular hybrid WCET analysis and worstcase stack usage analysis. In summary, the tools discussed in this article provide a formal methods-based ecosystem for verifying resource usage in embedded software projects. The three main causes of software-induced memory corruption in safety-critical systems are runtime errors, stack overﬂows, and miscompilation. The absence of runtime errors and stack overﬂows can be proven by abstract interpretation-based static analyzers. With the formally proven compiler CompCert, miscompilation can be ruled out, hence all main sources of softwareinduced memory corruption are addressed. Industrial application of mathematically rigorous veriﬁcation methods strongly depends on their representation in industrial safety norms; the corresponding methods and tools have to become better known to the safety community and their advantages compared to legacy methods better explained. Acknowledgment. Many people contributed to aiT and Astr´ee and their success. We want to thank them all.  
   
  References 1. Alt, M., Ferdinand, C., Martin, F., Wilhelm, R.: Cache behavior prediction by abstract interpretation. In: Cousot, R., Schmidt, D.A. (eds.) SAS 1996. LNCS, vol. 1145, pp. 52–66. Springer, Heidelberg (1996). https://doi.org/10.1007/3-54061739-6 33 2. AUTOSAR: AUTOSAR (AUTomotive Open System ARchitecture). http://www. autosar.org 3. Barr, M.: Bookout v. Toyota, 2005 Camry software Analysis by Michael Barr (2013). http://www.safetyresearch.net/Library/BarrSlides FINAL SCRUBBED. pdf 4. Bouissou, O., et al.: Space software validation using abstract interpretation. In: Proceedings of the 13thData Systems in Aerospace (DASIA 2009) (2009) 5. BS EN 50657: Railway applications - Rolling stock applications - Software on Board Rolling Stock (2017) 6. CENELEC EN 50128: Railway Applications - Communication, Signalling and Processing Systems - Software for Railway Control and Protection Systems (2011)  
   
  24  
   
  D. K¨ astner et al.  
   
  7. Certiﬁcation Authorities Software Team (CAST): Position Paper CAST-32A Multi-core Processors (2016) 8. Cousot, P., Cousot, R.: Abstract interpretation: a uniﬁed lattice model for static analysis of programs by construction or approximation of ﬁxpoints. In: Proceedings of the POPL’77, pp. 238–252. ACM Press (1977). http://www.di.ens.fr/∼cousot/ COUSOTpapers/POPL77.shtml. Accessed Sep 2017 9. Cousot, P., Cousot, R.: Static determination of dynamic properties of generalized type unions. In: Wortman, D.B. (ed.) Proceedings of an ACM Conference on Language Design for Reliable Software (LDRS), Raleigh, North Carolina, USA, 28-30 March 1977, pp. 77–94. ACM (1977). https://doi.org/10.1145/800022.808314 10. EASA: AMC-20 - amendment 23 - AMC 20–193 use of multi-core processors (2022) 11. Ferdinand, C., et al.: Reliable and precise WCET determination for a real-life processor. In: Henzinger, T.A., Kirsch, C.M. (eds.) EMSOFT 2001. LNCS, vol. 2211, pp. 469–485. Springer, Heidelberg (2001). https://doi.org/10.1007/3-54045449-7 32 12. Ferdinand, C., Wilhelm, R.: Eﬃcient and precise cache behavior prediction for real-time systems. Real-Time Syst. 17(2–3), 131–181 (1999) 13. Freescale Inc.: QorIQTM P4080 Communications Processor Product Brief (2008). rev. 1 14. Giet, J., Mauborgne, L., K¨ astner, D., Ferdinand, C.: Towards zero alarms in sound static analysis of ﬁnite state machines. In: Romanovsky, A., Troubitsyna, E., Bitsch, F. (eds.) SAFECOMP 2019. LNCS, vol. 11698, pp. 3–18. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-26601-1 1 15. IEC 61508: Functional safety of electrical/electronic/programmable electronic safety-related systems (2010) 16. Inﬁneon Technologies AG: AURIXTM TC27x D-Step User’s Manual (2014) 17. ISO 26262: Road vehicles - Functional safety (2018) 18. ISO 26262: Road vehicles - Functional safety - Part 2: Management of functional safety (2018) 19. ISO 26262: Road vehicles - Functional safety - Part 6: Product development at the software level (2018) 20. Kaestner, D., Wilhelm, S., Mallon, C., Schank, S., Ferdinand, C., Mauborgne, L.: Automatic sound static analysis for integration veriﬁcation of AUTOSAR software. In: WCX SAE World Congress Experience. SAE International (2023). https://doi. org/10.4271/2023-01-0591 21. Karos, T.: The Gauge Domain in Astr´ee. Master’s thesis, Saarland University (2015) 22. K¨ astner, D.: Applying abstract interpretation to demonstrate functional safety. In: Boulanger, J.L. (ed.) Formal Methods Applied to Industrial Complex Systems. ISTE/Wiley, London, UK (2014) 23. K¨ astner, D., Ferdinand, C.: Eﬃcient veriﬁcation of non-functional safety properties by abstract interpretation: timing, stack consumption, and absence of runtime errors. In: Proceedings of the 29th International System Safety Conference ISSC2011. Las Vegas (2011) 24. K¨ astner, D., Ferdinand, C.: Proving the absence of stack overﬂows. In: Bondavalli, A., Di Giandomenico, F. (eds.) SAFECOMP 2014. LNCS, vol. 8666, pp. 202–213. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10506-2 14 25. K¨ astner, D., Pister, M., Gebhard, G., Schlickling, M., Ferdinand, C.: Conﬁdence in timing. In: SAFECOMP 2013 Workshop: Next Generation of System Assurance Approaches for Safety-Critical Systems (SASSUR) (2013)  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  25  
   
  26. K¨ astner, D., et al.: Model-driven code generation and analysis. In: SAE World Congress 2014. SAE International (2014). https://doi.org/10.4271/2014-01-0217 27. K¨ astner, D., et al.: Meeting real-time requirements with multi-core processors. SAFECOMP 2012 Workshop: Next Generation of System Assurance Approaches for Safety-Critical Systems (SASSUR) (2012) 28. K¨ astner, D., et al.: Astr´ee: proving the absence of runtime errors. In: Embedded Real Time Software and Systems Congress ERTS 2 (2010) 29. K¨ astner, D., et al.: CompCert: practical experience on integrating and qualifying a formally veriﬁed optimizing compiler. In: ERTS2 2018 - Embedded Real Time Software and Systems. 3AF, SEE, SIE, Toulouse, France (2018). https://hal.inria. fr/hal-01643290, archived in the HAL-INRIA open archive, https://hal.inria.fr/ hal-01643290/ﬁle/ERTS 2018 paper 59.pdf 30. K¨ astner, D., Leroy, X., Blazy, S., Schommer, B., Schmidt, M., Ferdinand, C.: Closing the gap - the formally veriﬁed optimizing compiler CompCert. In: SSS 2017: Developments in System Safety Engineering: Proceedings of the Twenty-ﬁfth Safety-critical Systems Symposium, pp. 163–180. CreateSpace (2017) 31. K¨ astner, D., Mauborgne, L., Ferdinand, C.: Detecting safety- and security-relevant programming defects by sound static analysis. In: Falk, R., Chan, J.C.B.S. (eds.) The Second International Conference on Cyber-Technologies and Cyber-Systems (CYBER 2017). IARIA Conferences, vol. 2, pp. 26–31. IARIA XPS Press (2017) 32. K¨ astner, D., Mauborgne, L., Ferdinand, C.: Detecting spectre vulnerabilities by sound static analysis. In: Anne Coull, R.F., Chan, S. (ed.) The Fourth International Conference on Cyber-Technologies and Cyber-Systems (CYBER 2019). IARIA Conferences, vol. 4, pp. 29–37. IARIA XPS Press (2019). http://www. thinkmind.org/download.php?articleid=cyber 2019 3 10 80050 33. K¨ astner, D., Mauborgne, L., Grafe, N., Ferdinand, C.: Advanced sound static analysis to detect safety- and security-relevant programming defects. In: Falk, R., Steve Chan, J.C.B. (eds.) 8th International Journal on Advances in Security. vol. 1 & 2, pp. 149–159. IARIA (2018), https://www.iariajournals.org/security/ 34. K¨ astner, D., Mauborgne, L., Wilhelm, S., Ferdinand, C.: high-precision sound analysis to ﬁnd safety and cybersecurity defects. In: 10th European Congress on Embedded Real Time Software and Systems (ERTS 2020). Toulouse, France (2020). https://hal.archives-ouvertes.fr/hal-02479217 35. K¨ astner, D., Mauborgne, L., Wilhelm, S., Mallon, C., Ferdinand, C.: Static data and control coupling analysis. In: 11th Embedded Real Time Systems European Congress (ERTS2022). Toulouse, France (2022). https://hal.archives-ouvertes.fr/ hal-03694546 36. K¨ astner, D., Pohland, J.: Program analysis on evolving software. In: Roy, M. (ed.) CARS 2015 - Critical Automotive applications: Robustness & Safety. Paris, France (2015). https://hal.archives-ouvertes.fr/hal-01192985 37. K¨ astner, D., H¨ umbert, C., Gebhard, G., Pister, M., Wegener, S., Ferdinand, C.: Taming Timing - Combining Static Analysis With Non-intrusive Tracing to Compute WCET Bounds on Multicore Processors. Embedded World Congress (2021) 38. K¨ astner, D., Salvi, S., Bienm¨ uller, T., Ferdinand, C.: Exploiting synergies between static analysis and model-based testing (2015). https://doi.org/10.1109/EDCC. 2015.20 39. Langenbach, M., Thesing, S., Heckmann, R.: Pipeline modeling for timing analysis. In: Hermenegildo, M.V., Puebla, G. (eds.) SAS 2002. LNCS, vol. 2477, pp. 294–309. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45789-5 22  
   
  26  
   
  D. K¨ astner et al.  
   
  40. Li, Y.T.S., Malik, S.: Performance analysis of embedded software using implicit path enumeration. In: Proceedings of the 32nd ACM/IEEE Design Automation Conference, pp. 456–461 (1995) 41. Limited, M.: MISRA C++:2008 Guidelines for the use of the C++ language in critical systems (2008) 42. Min´e, A.: Static analysis of run-time errors in embedded real-time parallel C programs. Logic. Meth. Comput. Sci. (LMCS) 8(26), 63 (2012) 43. Min´e, A., Delmas, D.: Towards an industrial use of sound static analysis for the veriﬁcation of concurrent embedded avionics software. In: Proceedings of the 15th International Conference on Embedded Software (EMSOFT 2015), pp. 65–74. IEEE CS Press (2015) 44. MISRA (Motor Industry Software Reliability Association) Working Group: MISRA-C:2012 Guidelines for the use of the C Language in Critical Systems. MISRA Limited (2013) 45. MISRA (Motor Industry Software Reliability Association) Working Group: MISRA-C:2023 Guidelines for the use of the C Language in Critical Systems. MISRA Limited (2023) 46. Nowotsch, J., Paulitsch, M., B¨ uhler, D., Theiling, H., Wegener, S., Schmidt, M.: Multi-core interference-sensitive wcet analysis leveraging runtime resource capacity enforcement. In: ECRTS 2014: Proceedings of the 26th Euromicro Conference on Real-Time Systems (2014) 47. Radio Technical Commission for Aeronautics: RTCA DO-178B. Software Considerations in Airborne Systems and Equipment Certiﬁcation (1992) 48. Radio Technical Commission for Aeronautics: RTCA DO-178C. Software Considerations in Airborne Systems and Equipment Certiﬁcation (2011) 49. Radio Technical Commission for Aeronautics: RTCA DO-333. Formal Methods Supplement to DO-178C and DO-278A (2011) 50. Reineke, J., Grund, D., Berg, C., Wilhelm, R.: Timing predictability of cache replacement policies. Real-Time Syst. 37(2), 99–122 (2007) 51. Reineke, J., et al.: A deﬁnition and classiﬁcation of timing anomalies. In: Mueller, F. (ed.) International Workshop on Worst-Case Execution Time Analysis (WCET) (2006) 52. Rival, X., Mauborgne, L.: The trace partitioning abstract domain. ACM Trans. Program. Lang. Syst. 29(5), 26 (2007). https://doi.org/10.1145/1275497.1275501 53. Schranzhofer, A., Chen, J.J., Thiele, L.: Timing predictability on multi-processor systems with shared resources. In: Workshop on Reconciling Performance with Predictability (RePP), 2010 (2009) 54. Shaw, A.C.: Reasoning about time in higher-level language software. IEEE Trans. Softw. Eng. 15(7), 875–889 (1989). https://doi.org/10.1109/32.29487 55. Software Engineering Institute SEI - CERT Division: SEI CERT C Coding Standard - Rules for Developing Safe, Reliable, and Secure Systems. Carnegie Mellon University (2016) 56. The MITRE Corporation: CWE – Common Weakness Enumeration. https://cwe. mitre.org. Accessed Sep 2017 57. Theiling, H.: Extracting safe and precise control ﬂow from binaries. In: Proceedings of the 7th Conference on Real-Time Computing Systems and Applications. Cheju Island, South Korea (2000) 58. Theiling, H.: ILP-based interprocedural path analysis. In: Sangiovanni-Vincentelli, A., Sifakis, J. (eds.) EMSOFT 2002. LNCS, vol. 2491, pp. 349–363. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45828-X 26  
   
  Abstract Interpretation in Industry – Experience and Lessons Learned  
   
  27  
   
  59. Thesing, S.: Modeling a system controller for timing analysis. In: Min, S.L., Yi, W. (eds.) Proceedings of the 6th ACM & IEEE International conference on Embedded software, EMSOFT 2006, 22-25 October 2006, Seoul, Korea, pp. 292–300. ACM (2006). https://doi.org/10.1145/1176887.1176929 60. Thiele, L., Wilhelm, R.: Design for timing predictability. Real-Time Syst. 28(2–3), 157–177 (2004). https://doi.org/10.1023/B:TIME.0000045316.66276.6e 61. Transcript of Morning Trial Proceedings had on the 14th day of October, 2013 Before the Honorable Patricia G. Parrish, District Judge, Case No. CJ-2008-7969 (2013). http://www.safetyresearch.net/Library/Bookout v Toyota Barr REDACTED.pdf 62. Venet, A.: The gauge domain: scalable analysis of linear inequality invariants (2012). https://doi.org/10.1007/978-3-642-31424-7 15 63. Wegener, S.: Towards multicore WCET analysis. In: Reineke, J. (ed.) 17th International Workshop on Worst-Case Execution Time Analysis (WCET 2017). OpenAccess Series in Informatics (OASIcs), vol. 57, pp. 1–12. Schloss Dagstuhl–LeibnizZentrum fuer Informatik, Dagstuhl, Germany (2017). https://doi.org/10.4230/ OASIcs.WCET.2017.7, http://drops.dagstuhl.de/opus/volltexte/2017/7311 64. Wilhelm, R., Reineke, J., Wegener, S.: Keeping up with real time. In: Durak, U., Becker, J., Hartmann, S., Voros, N.S. (eds.) Advances in Aeronautical Informatics, pp. 121–133. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-75058-3 9  
   
  Building Trust and Safety in Artificial Intelligence with Abstract Interpretation Gagandeep Singh1,2(B) 1  
   
  1  
   
  University of Illinois at Urbana-Champaign (UIUC), Champaign, USA [email protected]  2 VMware Research, Palo Alto, USA  
   
  Introduction  
   
  Deep neural networks (DNNs) are currently the dominant technology in artiﬁcial intelligence (AI) and have shown impressive performance in diverse applications including autonomous driving [9], medical diagnosis [2], and text generation [10]. However, their black-box construction [46] and vulnerability against environmental and adversarial noise [30,57] have raised concerns about their safety, when deployed in the real world. Standard training [28] optimizes the model’s accuracy but does not take into account desirable safety properties such as robustness [48], fairness [18], and monotonicity [49]. The standard practice of testing and interpreting DNN behavior on a ﬁnite set of unseen test inputs cannot guarantee safe and trustworthy DNN behavior on new inputs seen during deployment [59,66]. This is because the DNN can misbehave if the inputs observed during deployment deviate even slightly from those in the test set [20,23,36]. To address these limitations, there is growing work on checking the safety of DNN models [3,5,11,17,25,27,31,42–45,50–53,58,61,62,65,68,70] and interpreting their behavior [7], on an inﬁnite set of unseen inputs using formal certiﬁcation. Testing and interpreting with formal methods provide a more reliable metric for measuring a model’s safety than standard methods [12]. Formal methods can also be used during training [6,22,38,40,69,72,74] to guide the model to satisfy desirable safety and trustworthy properties. DNN Certification Problem. The certiﬁcation problem consists of two main components: (i) a trained DNN f , (ii) a property speciﬁcation in the form of a tuple (φ, ψ) containing symbolic formulas φ and ψ. Here the formula φ is a precondition specifying the set of inputs on which, the DNN should not misbehave. The formula ψ is a postcondition that determines constraints that the DNN outputs f (φ) [26] or its gradients f  (φ) [33,34] corresponding to the inputs in φ should satisfy, for its behaviors to be considered safe and trustworthy. A DNN certiﬁer tries to check whether f (φ) ⊆ ψ (or f  (φ) ⊆ ψ) holds. Both φ, ψ are typically speciﬁed as disjunctions of convex polyhedra. The property speciﬁcations are domain dependent and usually designed by DNN developers. Local vs Global Properties. The precondition φ for local properties deﬁnes a local neighborhood around a sample input from the test set. For example, c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  M. V. Hermenegildo and J. F. Morales (Eds.): SAS 2023, LNCS 14284, pp. 28–38, 2023. https://doi.org/10.1007/978-3-031-44245-2_3  
   
  Building Trust and Safety in Artiﬁcial Intelligence  
   
  29  
   
  Fig. 1. Neural network certiﬁcation with abstract interpretation involves computing an abstract element (in blue) containing the true network output f (φ) (in white) at each layer using the corresponding abstract transformer. (Color ﬁgure online)  
   
  given a test image correctly classiﬁed as a car by a DNN, the popular local robustness property speciﬁes that all images generated by rotating the original image within ±5 degrees are also classiﬁed as a car [5]. In contrast, φ for global properties does not depend upon a test input. For domains where the input features have semantic meaning, e.g., air traﬃc collision avoidance systems [26] or security classiﬁers [13], global properties can be speciﬁed by deﬁning a valid range for the input features expected in a real-world deployment. Certifying global properties yields stronger safety guarantees, however, they are diﬃcult to formulate for popular domains, such as vision and NLP, where the individual features processed by the DNN have no semantic meaning. While certifying local properties is not ideal, the local certiﬁcation results enable testing the safety of the model on an inﬁnite set of unseen inputs, not possible with standard methods.  
   
  2  
   
  Certification for Testing Model Safety  
   
  DNN certiﬁcation can be seen as an instance of program veriﬁcation (DNNs can be written as programs) making it undecidable. State-of-the-art certiﬁers are therefore incomplete in general. These certiﬁers can be formulated using the elegant framework of abstract interpretation [15]. While abstract interpretationbased certiﬁers can certify both local and global properties, for the remainder of this paper, we focus on the certiﬁcation of local properties as they are more common in real-world applications. Figure 1 shows the high-level idea behind DNN certiﬁcation with abstract interpretation. Here, the certiﬁer is parameterized by the choice of an abstract domain. The certiﬁer ﬁrst computes an abstract element α(φ) ⊇ φ that includes the input region φ. Next, the analyzer symbolically propagates α(φ) through the diﬀerent layers of the network. At each layer, the analyzer computes an abstract element (in blue) overapproximating the exact layer output (in white) corresponding to φ. The element is computed by applying an abstract transformer that approximates the eﬀect of the operations (e.g., ReLU, aﬃne) applied at the layer. Propagation through all the layers yields an abstract element g(α(φ)) ⊇ f (φ) at the output layer. Next, the certiﬁer checks  
   
  30  
   
  G. Singh  
   
  Fig. 2. Development pipeline for building fast, accurate, and trustworthy DNNs. Certiﬁcation is used for testing model trustworthiness (green diamond). (Color ﬁgure online)  
   
  if g(α(φ)) ⊆ ψ holds for the bigger region g(α(φ)). If the answer is yes, then f (φ) ⊆ ψ also holds for the smaller region f (φ). Because of the overapproximation, it can be the case that g(α(φ)) ⊆ ψ does not hold while f (φ) ⊆ ψ holds. To reduce the amount of overapproximation, reﬁnements [41,47,53,62,63,68,71] can be applied. To obtain an eﬀective certiﬁer, it is essential to design an abstract domain and corresponding abstract transformers such that g(α(φ)) is as close as possible to the true output f (φ) while g can also be computed in a reasonable amount of time for practical networks. The classical domains, such as Polyhedra [16,55] and Octagons [37,54], used for analyzing programs are not well suited for DNN certiﬁcation. This is because the DNNs have a diﬀerent structure compared to traditional programs. For example, DNNs have a large number of non-linear assignments but typically do not have inﬁnite loops. For eﬃcient certiﬁcation, new abstract domains and transformers tailored for DNN certiﬁcation have been developed. Examples include DeepPoly [52], DeepZ [51], Star sets [58], and DeepJ [33]. These custom solutions can scale to realistic DNNs with upto a million neurons [39], or more than 100 layers [68], certifying diverse safety properties in diﬀerent real-world applications including autonomous driving [72], job-scheduling [68], data center management [12], and ﬁnancial modeling [32]. Incremental Certification. By leveraging formal certiﬁcation to check DNN safety and trust, the development pipeline shown in Fig. 2 can be employed [61] to obtain fast, accurate, and trustworthy DNNs. First, a DNN is trained to maximize its test accuracy. Next, a domain expert designs a set of safety speciﬁcations (e.g., robustness, fairness) deﬁning the expected network behavior in diﬀerent real-world scenarios. If the model satisﬁes the desired speciﬁcations, then the DNN is considered ﬁt for deployment. Otherwise, it is iteratively repaired (e.g., by ﬁne-tuning [1] or LP-solving [56]) till we obtain a fast, accurate, and trustworthy DNN. We note that repair is preferred over retraining as it is cheaper.  
   
  Building Trust and Safety in Artiﬁcial Intelligence  
   
  31  
   
  However, if a repair is not possible, then the DNN is retrained from scratch. After deployment, the DNN is monitored to check for distribution shifts, generating inputs not covered by the speciﬁcations. If a distribution shift is detected, then new speciﬁcations are designed, and the model is repaired or retrained. Domain experts usually design a large number of local properties (around 10-100K). Therefore, the certiﬁer needs to be run several thousand times on the same DNN. Further, as shown in Fig. 2, the model repair is applied, before or after deployment, in case the DNN does not satisfy the desired speciﬁcations. The certiﬁer is needed again to check the safety of the repaired model. Existing certiﬁers do not scale in such a deployment setting: they can precisely certify individual speciﬁcations in a few seconds or minutes, however, the certiﬁcation of a large and diverse set of speciﬁcations on a single DNN can take multiple days to years or the certiﬁer can run out of memory. Given multiple DNNs are generated due to repair or retraining, it makes using existing certiﬁers for safe and trustworthy development infeasible. The ineﬃciency is because the certiﬁer needs to be run from scratch for every new pair of speciﬁcations and DNNs. A straightforward approach to overcome this limitation is to run the certiﬁer on several machines. However, such an approach is not sustainable due to its huge environmental cost [8,67]. Further, in many cases, large computational resources are not available. For example, to preserve privacy, reduce latency, and increase battery lifetime, DNNs are increasingly employed on edge devices with limited computational power [14,64]. Therefore, for sustainable, democratic, and trustworthy DNN development, it is essential to develop new general approaches for incremental certiﬁcation to improve the certiﬁer scalability, when certifying multiple speciﬁcations and networks. The main challenge in developing incremental certiﬁers is determining information that (i) can be reused across multiple speciﬁcations and DNNs to improve scalability, and (ii) is eﬃcient to compute and store. Recent works [19,61] have developed general mechanisms to enable incremental certiﬁcation by reusing proofs across multiple speciﬁcations and DNNs. These methods can be plugged into state-of-the-art certiﬁers based on abstract interpretation [51,52] to improve their scalability inside the development pipeline of Fig. 2. [19] introduced the concept of proof sharing across multiple speciﬁcations on the same DNN. Proof sharing is based on the key insight that it is possible to construct a small number of abstract elements as proof templates at an intermediate DNN layer, that capture the intermediate proofs of a large number of speciﬁcations. To certify a new speciﬁcation, we run the certiﬁer partially till the layer at which the templates are available. If the intermediate proof is subsumed by an existing template, then the speciﬁcation is proved without running the certiﬁer till the end, saving time and memory. The work of [61] introduced the concept of proof transfer across similar networks obtained after incremental changes to an original network (e.g., after ﬁne-tuning [1]). The key insight behind this concept is that it is possible to eﬃciently transfer the proof templates generated on the original network to multiple similar networks, such that the transformed templates capture the proofs of a large number of speciﬁcations on similar networks. The transferred templates  
   
  32  
   
  G. Singh  
   
  Fig. 3. Certiﬁed training involves computing the point z ∈ g(α(φ)) where the robust loss if maximum. The resulting loss is backpropagated through the certiﬁer code to update the model parameters.  
   
  can improve certiﬁer precision and scalability when certifying multiple speciﬁcations on similar networks. [60] considers incremental certiﬁcation for certiﬁers combining abstract interpretation with branch and bound (BaB) [11] and uses the trace of BaB as proof templates to improve certiﬁcation speed across multiple similar DNNs.  
   
  3  
   
  Certification for Training Safe DNNs  
   
  DNNs trained to only maximize accuracy with standard training [28] are often unsafe [36]. Next, we describe how certiﬁers can be leveraged during training to obtain safe DNNs. While the description here applies to diﬀerent safety properties, we focus on robustness as it is the most common property for safe training considered in the literature. Robust training involves deﬁning a robust loss function LR for each point x ∈ φ with the property that LR at x is ≤ 0 iﬀ in the DNN output z = f (x), the score zc for the correct class c is higher than all other classes zi , i.e., zc > zi . The DNN is robust iﬀ LR ≤ 0 for all x ∈ φ. The DNN parameters can be updated during training to minimize the maximum value of LR . This min-max formulation makes robust training a harder optimization problem than standard training. Computing the worst-case robust loss exactly requires computing f (φ) which is infeasible. Therefore an approximation of LR is computed in practice. Adversarial training methods [36] compute a lower bound on the worst-case robust loss by heuristically computing a point x ∈ φ at which the robust loss is high. x is then added to the training dataset. On the other hand, certiﬁed training [6,21,38,69,72,74] methods compute an upper bound on the worst-case robust loss using abstract interpretation-based DNN certiﬁers. Figure 3 shows the high-level idea behind certiﬁed training which leverages the output g(α(φ)) computed by the DNN certiﬁer. Here one computes z ∈ g(α(φ)) where the robust loss is maximum and then updates the model with respect to the resulting loss value. State-of-the-art certiﬁed training methods employ diﬀerentiable certiﬁers [38,51], which makes the computation of the worst-case robust loss diﬀerentiable. As a result, the parameter updates are performed by diﬀerentiating through the certiﬁer code directly. Since certiﬁed training computes an upper bound on the worst-case robust loss when this loss is ≤ 0, the actual loss is also ≤ 0. This is not the case with  
   
  Building Trust and Safety in Artiﬁcial Intelligence  
   
  33  
   
  the lower bound computed by adversarial training. As a result, DNNs trained with certiﬁed training achieve higher robustness guarantees than those trained with adversarial training [38]. They are also easier to certify than those trained with adversarial and standard training. Even imprecise abstract domains such as intervals give precise certiﬁcation results for DNNs trained with certiﬁed training. The work of [4] theoretically shows the existence of two DNNs f, f  such that (i) they have the same accuracy, and (ii) interval analysis achieves the same certiﬁcation results on f  as a more precise certiﬁer on f . Training with only the robust loss deteriorates model accuracy, therefore in practice, robust loss is combined with standard accuracy loss during training using custom mechanisms [21]. While one would expect that training with precise certiﬁers yields more accurate and robust DNNs than imprecise ones, as they reduce the approximation error in computing the robust loss, in practice, the highly imprecise interval domain performs the best for certiﬁed training. This is because the optimization problem for training becomes harder with more complex abstract domains [24]. Most certiﬁed training methods target robustness with respect to norm-based changes to pixel intensities in images. Even with all the progress in this direction, DNNs trained with state-of-the-art certiﬁed training methods [6,40,74] suﬀer signiﬁcant loss of accuracy on popular datasets such as CIFAR10 [29]. There have been conﬂicting hypotheses in the literature about whether accuracy conﬂicts with norm-based robustness [59] or not [73]. The work of [72] is the ﬁrst to build a certiﬁed training method for challenging geometric robustness by developing a fast geometric certiﬁer that can be eﬃciently parallelized on GPUs. Interestingly, the work shows that it is possible to achieve both high accuracy and robustness on the autonomous driving dataset [9]. Therefore, in certain practical scenarios, both high accuracy and safety may be achievable.  
   
  4  
   
  Certification for Interpreting DNNs  
   
  Abstract interpretation-based DNN certiﬁers [51,52,70] generate highdimensional abstract elements at diﬀerent layers capturing complex relationships between neurons and DNN inputs to prove DNN safety. However, the individual neurons and inputs in the DNN do not have any semantic meaning, unlike the variables in programs, therefore it is not clear whether the safety proofs are based on any meaningful features learned by the DNN. If the DNN is proven to be safe but the proof is based on meaningless features not aligned with human intuition, then the DNN behavior cannot be considered trustworthy. While there has been a lot of work on interpreting black-box DNNs, standard methods [46,66] can only explain the DNN behavior on individual inputs and cannot interpret the complex invariants encoded by the abstract elements capturing DNN behavior on an inﬁnite set of inputs. The main challenge in interpreting DNN proofs is in mapping the complex abstract elements to human understandable interpretations. The work of [7] is the ﬁrst to develop a method for interpreting robustness proofs computed by DNN certiﬁers. The method can interpret proofs computed  
   
  34  
   
  G. Singh  
   
  by diﬀerent certiﬁers. It builds upon the novel concept of proof features that are computed by projecting the high-dimensional abstract elements onto individual neurons. The proof features can be analyzed independently by generating the corresponding interpretations. Since certain proof features can be more important for the proof than others, a priority function over the proof features that signify the importance of each individual proof feature in the complete proof is deﬁned. The method extracts a set of proof features by retaining only the more important parts of the proof that preserve the property. A comparison of proof interpretations for DNNs trained with standard and robust training methods [6,36,74] on the popular MNIST [35] and CIFAR10 datasets [29] shows that the proof features corresponding to the standard networks rely on meaningless input features while the proofs of adversarially trained DNNs [36] ﬁlter out some of these spurious features. In contrast, the networks trained with certiﬁable training [74] produce proofs that do not rely on any spurious features but they also miss out on some meaningful features. Proofs for training methods that combine both empirical and certiﬁed robustness [6] not only preserve meaningful features but also selectively ﬁlter out spurious ones. These observations are empirically shown to be not contingent on any speciﬁc DNN certiﬁer. These insights suggest that DNNs can satisfy safety properties but their behavior can still be untrustworthy.  
   
  References 1. Agrawal, P., Girshick, R., Malik, J.: Analyzing the performance of multilayer neural networks for object recognition. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8695, pp. 329–344. Springer, Cham (2014). https:// doi.org/10.1007/978-3-319-10584-0 22 2. Amato, F., L´ opez, A., Pe˜ na-M´endez, E.M., Vaˇ nhara, P., Hampl, A., Havel, J.: Artiﬁcial neural networks in medical diagnosis. J. Appl. Biomed. 11(2) (2013) 3. Anderson, G., Pailoor, S., Dillig, I., Chaudhuri, S.: Optimization and abstraction: a synergistic approach for analyzing neural network robustness. In: Proceedings of the Programming Language Design and Implementation (PLDI), pp. 731–744 (2019) 4. Baader, M., Mirman, M., Vechev, M.: Universal approximation with certiﬁed networks. In: International Conference on Learning Representations (2020) 5. Balunovic, M., Baader, M., Singh, G., Gehr, T., Vechev, M.: Certifying geometric robustness of neural networks. In: Advances in Neural Information Processing Systems, vol. 32. Curran Associates, Inc. (2019) 6. Balunovic, M., Vechev, M.T.: Adversarial training and provable defenses: bridging the gap. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, 26–30 April 2020. OpenReview.net (2020) 7. Banerjee, D., Singh, A., Singh, G.: Interpreting robustness proofs of deep neural networks (2023) 8. Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of stochastic parrots: can language models be too big? In: Elish, M.C., Isaac, W., Zemel, R.S. (eds.) FAccT ’21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event/Toronto, Canada, 3–10 March 2021, pp. 610–623. ACM (2021)  
   
  Building Trust and Safety in Artiﬁcial Intelligence  
   
  35  
   
  9. Bojarski, M., et al.: End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 (2016) 10. Brown, T.B., et al.: Language models are few-shot learners. In: Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) (2020) 11. Bunel, R., Lu, J., Turkaslan, I., Kohli, P., Torr, P., Mudigonda, P.: Branch and bound for piecewise linear neural network veriﬁcation. J. Mach. Learn. Res. 21(2020) (2020) 12. Chakravarthy, A., Narodytska, N., Rathis, A., Vilcu, M., Sharif, M., Singh, G.: Property-driven evaluation of rl-controllers in self-driving datacenters. In: Workshop on Challenges in Deploying and Monitoring Machine Learning Systems (DMML) (2022) 13. Chen, Y., Wang, S., Qin, Y., Liao, X., Jana, S., Wagner, D.A.: Learning security classiﬁers with veriﬁed global robustness properties. In: Proceedings of the Conference on Computer and Communications Security (CCS), pp. 477–494. ACM (2021) 14. Chugh, U., et al.: An automated approach to accelerate DNNs on edge devices. In: ISCAS, pp. 1–5. IEEE (2021) 15. Cousot, P., Cousot, R.: Abstract interpretation: a uniﬁed lattice model for static analysis of programs by construction or approximation of ﬁxpoints. In: Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, Los Angeles, California, USA, January 1977, pp. 238–252. ACM (1977) 16. Cousot, P., Halbwachs, N.: Automatic discovery of linear restraints among variables of a program. In: Conference Record of the Fifth Annual ACM Symposium on Principles of Programming Languages, Tucson, Arizona, USA, January 1978, pp. 84–96. ACM Press (1978) 17. Dathathri, S., et al.: Enabling certiﬁcation of veriﬁcation-agnostic networks via memory-eﬃcient semideﬁnite programming. In: Advances in Neural Information Processing Systems, vol. 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 6–12 December 2020, virtual (2020) 18. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.S.: Fairness through awareness. In: Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, 8–10 January 2012, pp. 214–226. ACM (2012) 19. Fischer, M., Sprecher, C., Dimitrov, D.I., Singh, G., Vechev, M.T.: Shared certiﬁcates for neural network veriﬁcation. In: Shoham, S., Vizel, Y. (eds.) Computer Aided Veriﬁcation. CAV 2022. LNCS, vol. 13371, pp. 127–148. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-13185-1 7 20. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014) 21. Gowal, S., et al.: On the eﬀectiveness of interval bound propagation for training veriﬁably robust models. CoRR abs/1810.12715 (2018) 22. Gowal, S., et al.: Scalable veriﬁed training for provably robust image classiﬁcation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 4842–4851 (2019) 23. Heo, J., Joo, S., Moon, T.: Fooling neural network interpretations via adversarial model manipulation. In: Advances in Neural Information Processing Systems (NeurIPS), pp. 2921–2932 (2019) 24. Jovanovic, N., Balunovic, M., Baader, M., Vechev, M.T.: On the paradox of certiﬁed training. Trans. Mach. Learn. Res. 2022 (2022) 25. Kabaha, A., Drachsler-Cohen, D.: Boosting robustness veriﬁcation of semantic feature neighborhoods. In: Singh, G., Urban, C. (eds.) Static Analysis. SAS 2022.  
   
  36  
   
  26.  
   
  27. 28.  
   
  29. 30. 31.  
   
  32. 33. 34.  
   
  35. 36. 37. 38.  
   
  39.  
   
  40.  
   
  41.  
   
  42.  
   
  43.  
   
  G. Singh LNCS, vol. 13790, pp. 299–324. Springer, Cham (2022). https://doi.org/10.1007/ 978-3-031-22308-2 14 Katz, G., Barrett, C., Dill, D., Julian, K., Kochenderfer, M.: Reluplex: an eﬃcient SMT solver for verifying deep neural networks. In: Proceedings of the 29th International Conference on Computer Aided Veriﬁcation (CAV), pp. 97–117 (2017) Katz, G., et al.: The Marabou framework for veriﬁcation and analysis of deep neural networks, pp. 443–452, July 2019 Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, 7–9 May 2015, Conference Track Proceedings (2015) Krizhevsky, A.: Learning multiple layers of features from tiny images (2009) Kurakin, A., Goodfellow, I.J., Bengio, S.: Adversarial examples in the physical world. In: ICLR (Workshop). OpenReview.net (2017) Lan, J., Zheng, Y., Lomuscio, A.: Tight neural network veriﬁcation via semideﬁnite relaxations and linear reformulations. In: Thirty-Sixth AAAI Conference on Artiﬁcial Intelligence, AAAI 2022, pp. 7272–7280. AAAI Press (2022) Laurel, J., Qian, S.B., Singh, G., Misailovic, S.: Synthesizing precise static analyzers for automatic diﬀerentiation. Proc. ACM Program. Lang. (OOPSLA2) (2023) Laurel, J., Yang, R., Singh, G., Misailovic, S.: A dual number abstraction for static analysis of Clarke Jacobians. Proc. ACM Program. Lang. 6(POPL), 1–30 (2022) Laurel, J., Yang, R., Ugare, S., Nagel, R., Singh, G., Misailovic, S.: A general construction for abstract interpretation of higher-order automatic diﬀerentiation. Proc. ACM Program. Lang. 6(OOPSLA2), 1007–1035 (2022) LeCun, Y., et al.: Handwritten digit recognition with a back-propagation network. In: NIPS, pp. 396–404 (1989) Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017) Min´e, A.: The octagon abstract domain. High. Order Symb. Comput. 19(1), 31–100 (2006) Mirman, M., Gehr, T., Vechev, M.: Diﬀerentiable abstract interpretation for provably robust neural networks. In: Proceedings of the International Conference on Machine Learning (ICML), pp. 3578–3586 (2018) M¨ uller, C., Serre, F., Singh, G., P¨ uschel, M., Vechev, M.T.: Scaling polyhedral neural network veriﬁcation on GPUs. In: Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, 5–9 April 2021. mlsys.org (2021) M¨ uller, M.N., Eckert, F., Fischer, M., Vechev, M.T.: Certiﬁed training: small boxes are all you need. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, 1–5 May 2023. OpenReview.net (2023) M¨ uller, M.N., Makarchuk, G., Singh, G., P¨ uschel, M., Vechev, M.: Precise multi-neuron abstractions for neural network certiﬁcation. arXiv preprint arXiv:2103.03638 (2021) Munakata, S., Urban, C., Yokoyama, H., Yamamoto, K., Munakata, K.: Verifying attention robustness of deep neural networks against semantic perturbations. In: Rozier, K.Y., Chaudhuri, S. (eds.) NASA Formal Methods. NFM 2023. LNCS, vol. 13903, pp. 37–61. Springer, Cham (2023). https://doi.org/10.1007/978-3-03133170-1 3 Palma, A.D., Behl, H.S., Bunel, R., Torr, P.H.S., Kumar, M.P.: Scaling the convex barrier with active sets. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, 3–7 May 2021. OpenReview.net (2021)  
   
  Building Trust and Safety in Artiﬁcial Intelligence  
   
  37  
   
  44. Paulsen, B., Wang, J., Wang, C.: Reludiﬀ: diﬀerential veriﬁcation of deep neural networks. In: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 714–726. ICSE ’20 (2020) 45. Ranzato, F., Urban, C., Zanella, M.: Fairness-aware training of decision trees by abstract interpretation. In: CIKM ’21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, 1–5 November 2021, pp. 1508–1517. ACM (2021) 46. Ribeiro, M.T., Singh, S., Guestrin, C.: Why should I trust you?: explaining the predictions of any classiﬁer. In: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135–1144. ACM (2016) 47. Ryou, W., Chen, J., Balunovic, M., Singh, G., Dan, A., Vechev, M.: Scalable polyhedral veriﬁcation of recurrent neural networks. In: Silva, A., Leino, K.R.M. (eds.) CAV 2021. LNCS, vol. 12759, pp. 225–248. Springer, Cham (2021). https://doi. org/10.1007/978-3-030-81685-8 10 48. Shaﬁque, M., et al.: Robust machine learning systems: challenges, current trends, perspectives, and the road ahead. IEEE Des. Test 37(2), 30–57 (2020) 49. Sill, J.: Monotonic networks. In: Advances in Neural Information Processing Systems, vol. 10, [NIPS Conference, Denver, Colorado, USA, 1997], pp. 661–667. The MIT Press (1997) 50. Singh, G., Ganvir, R., P¨ uschel, M., Vechev, M.: Beyond the single neuron convex barrier for neural network certiﬁcation. Adv. Neural Inf. Process. Syst. (2019) 51. Singh, G., Gehr, T., Mirman, M., P¨ uschel, M., Vechev, M.: Fast and eﬀective robustness certiﬁcation. Adv. Neural Inf. Process. Syst. 31 (2018) 52. Singh, G., Gehr, T., P¨ uschel, M., Vechev, M.: An abstract domain for certifying neural networks. Proc. ACM Program. Lang. 3(POPL) (2019) 53. Singh, G., Gehr, T., P¨ uschel, M., Vechev, M.: Robustness certiﬁcation with reﬁnement. In: International Conference on Learning Representations (2019) 54. Singh, G., P¨ uschel, M., Vechev, M.T.: Making numerical program analysis fast. In: Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation, Portland, OR, USA, 15–17 June 2015, pp. 303–313. ACM (2015) 55. Singh, G., P¨ uschel, M., Vechev, M.T.: Fast polyhedra abstract domain. In: Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL 2017, Paris, France, 18–20 January 2017, pp. 46–59. ACM (2017) 56. Sotoudeh, M., Thakur, A.V.: Provable repair of deep neural networks. In: PLDI ’21: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Virtual Event, Canada, 20–25 June 2021, pp. 588–603. ACM (2021) 57. Szegedy, C., et al.: Intriguing properties of neural networks. In: ICLR (Poster) (2014) 58. Tran, H.-D., et al.: Star-based reachability analysis of deep neural networks. In: ter Beek, M.H., McIver, A., Oliveira, J.N. (eds.) FM 2019. LNCS, vol. 11800, pp. 670–686. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-30942-8 39 59. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., Madry, A.: Robustness may be at odds with accuracy. In: Proceedings of the International Conference on Learning Representations, ICLR. OpenReview.net (2019) 60. Ugare, S., Banerjee, D., Misailovic, S., Singh, G.: Incremental veriﬁcation of neural networks. Proc. ACM Program. Lang. 7(PLDI) (2023) 61. Ugare, S., Singh, G., Misailovic, S.: Proof transfer for fast certiﬁcation of multiple approximate neural networks. Proc. ACM Program. Lang. 6(OOPSLA1), 1–29 (2022)  
   
  38  
   
  G. Singh  
   
  62. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Eﬃcient formal safety analysis of neural networks. Adv. Neural Inf. Process. Syst. (2018) 63. Wang, S., et al.: Beta-crown: eﬃcient bound propagation with per-neuron split constraints for complete and incomplete neural network veriﬁcation. arXiv preprint arXiv:2103.06624 (2021) 64. Wang, X., Hersche, M., T¨ omekce, B., Kaya, B., Magno, M., Benini, L.: An accurate EEGNet-based motor-imagery brain-computer interface for low-power edge computing. In: IEEE International Symposium on Medical Measurements and Applications, (MeMeA), pp. 1–6. IEEE (2020) 65. Wong, E., Kolter, J.Z.: Provable defenses against adversarial examples via the convex outer adversarial polytope. In: Proceedings of the International Conference on Machine Learning, ICML. Proceedings of Machine Learning Research, vol. 80, pp. 5283–5292. PMLR (2018) 66. Wong, E., Santurkar, S., Madry, A.: Leveraging sparse linear layers for debuggable deep networks. In: Proceedings of the 38th International Conference on Machine Learning, ICML. Proceedings of Machine Learning Research, vol. 139, pp. 11205– 11216. PMLR (2021) 67. Wu, C., et al.: Sustainable AI: environmental implications, challenges and opportunities. In: MLSys. mlsys.org (2022) 68. Wu, H., Barrett, C., Sharif, M., Narodytska, N., Singh, G.: Scalable veriﬁcation of GNN-based job schedulers. Proc. ACM Program. Lang. 6(OOPSLA2) (2022) 69. Xu, K., et al.: Automatic perturbation analysis for scalable certiﬁed robustness and beyond. In: Proceedings of the Neural Information Processing Systems (NeurIPS), pp. 1129–1141 (2020) 70. Xu, K., et al.: Fast and complete: Enabling complete neural network veriﬁcation with rapid and massively parallel incomplete veriﬁers. In: International Conference on Learning Representations (2021) 71. Yang, P., et al.: Improving neural network veriﬁcation through spurious region guided reﬁnement. In: Groote, J.F., Larsen, K.G. (eds.) Tools and Algorithms for the Construction and Analysis of Systems. TACAS 2021. LNCS, vol. 12651, pp. 389–408. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-72016-2 21 72. Yang, R., Laurel, J., Misailovic, S., Singh, G.: Provable defense against geometric transformations. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, 1–5 May 2023. OpenReview.net (2023) 73. Yang, Y., Rashtchian, C., Zhang, H., Salakhutdinov, R., Chaudhuri, K.: A closer look at accuracy vs. robustness. In: Advances in Neural Information Processing Systems, vol. 33: Annual Conference on Neural Information Processing Systems (NeurIPS) (2020) 74. Zhang, H., et al.: Towards stable and eﬃcient training of veriﬁably robust neural networks. In: Proceedings of the International Conference on Learning Representations (ICLR) (2020)  
   
  Regular Papers  
   
  Modular Optimization-Based Roundoﬀ Error Analysis of Floating-Point Programs Rosa Abbasi1 1  
   
  and Eva Darulova2(B)  
   
  MPI-SWS, Kaiserslautern and Saarbrücken, Germany [email protected]  2  
   
  Uppsala University, Uppsala, Sweden [email protected]   
   
  Abstract. Modular static program analyses improve over global wholeprogram analyses in terms of scalability at a tradeoﬀ with analysis accuracy. This tradeoﬀ has to-date not been explored in the context of sound ﬂoating-point roundoﬀ error analyses; available analyses computing guaranteed absolute error bounds eﬀectively consider only monolithic straight-line code. This paper extends the roundoﬀ error analysis based on symbolic Taylor error expressions to non-recursive procedural ﬂoating-point programs. Our analysis achieves modularity and at the same time reasonable accuracy by automatically computing abstract procedure summaries that are a function of the input parameters. We show how to eﬀectively use ﬁrst-order Taylor approximations to compute precise procedure summaries, and how to integrate these to obtain endto-end roundoﬀ error bounds. Our evaluation shows that compared to an inlining of procedure calls, our modular analysis is signiﬁcantly faster, while nonetheless mostly computing relatively tight error bounds. Keywords: modular veriﬁcation error · Taylor approximation  
   
  1  
   
  · ﬂoating-point arithmetic · roundoﬀ  
   
  Introduction  
   
  One of the main challenges of automated static program analysis is to strike a suitable trade-oﬀ between analysis accuracy and performance [6]. This trade-oﬀ is inevitable, as a certain amount of abstraction and thus over-approximation is necessary to make an analysis feasible for unbounded (or very large) input domains. There are typically diﬀerent ways to introduce abstractions; for instance by considering abstract domains that are more or less accurate [6,17,22], c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  M. V. Hermenegildo and J. F. Morales (Eds.): SAS 2023, LNCS 14284, pp. 41–64, 2023. https://doi.org/10.1007/978-3-031-44245-2_4  
   
  42  
   
  R. Abbasi and E. Darulova  
   
  or in the context of procedural code, by abstracting procedure calls by summaries or speciﬁcations to obtain a modular analysis [8]. A modular analysis allows each procedure to be analyzed independently once, regardless of how often it is being called in an application, rather than being reanalyzed in possibly only slightly diﬀerent contexts at every call site. This saves analysis time and thus increases the scalability of the analysis at the expense of some loss of accuracy: the procedure summaries need to abstract over diﬀerent calling contexts. This paper presents a modular roundoﬀ error analysis for non-recursive procedural ﬂoating-point programs without conditional branches. Our approach extends the roundoﬀ error analysis ﬁrst introduced in the FPTaylor tool [27] that is based on symbolic Taylor expressions and global optimization and that has been shown to produce tight error bounds for straight-line arithmetic expressions. Our analysis ﬁrst computes, for each procedure separately, error speciﬁcations that provide an abstraction of the function’s behavior as a function of the input parameters. In a second step, our analysis instantiates the error speciﬁcations at the call sites to compute an overall roundoﬀ error bound for each procedure. The main challenge is to achieve a practically useful tradeoﬀ between analysis accuracy and performance. A naive, albeit simple, approach would simply compute the worst-case roundoﬀ error for each procedure as a constant, and would use this constant as the error at each call site. This approach is, however, particularly suboptimal for a roundoﬀ error analysis, because roundoﬀ errors depend on the magnitude of arguments. For reasonable analysis accuracy, it is thus crucial that the error speciﬁcations are parametric in the procedure’s input parameters. At the same time, the error speciﬁcations need to introduce some abstraction as we otherwise end up re-analyzing each procedure at each call site. We achieve this balance by computing error speciﬁcations that soundly overapproximate roundoﬀ errors using ﬁrst-order Taylor approximations separately for propagation of input and roundoﬀ errors. By keeping ﬁrst-order terms of both approximations unevaluated, we obtain parametric procedure summaries, and by eagerly evaluating higher-order terms we achieve abstraction that has a relatively small impact on accuracy. Available sound ﬂoating-point roundoﬀ error analyses have largely focused on abstractions for the (global) analysis of straight-line code and require function calls to be inlined manually [10,18,24,27] and are thus non-modular. The tool PRECiSA [26,28] analyzes function calls compositionally, however, does not apply abstraction when doing so. The analysis can thus be considered modular (in principle), but the computed symbolic function summaries can be very large and negatively aﬀect the eﬃciency of the analysis. Goubault et al. [19] present a modular roundoﬀ error analysis based on the zonotopic abstract domain that does apply abstraction at function calls. However, the implementation is not available and the roundoﬀ error analyses based on zonotopes have been shown to be less accurate than the alternative approach based on symbolic Taylor expressions.  
   
  Modular Floating-Point Error Analysis  
   
  43  
   
  Like most existing roundoﬀ error analyses, our analysis computes absolute roundoﬀ errors for programs without loops or recursive procedure calls and without conditional branches; these remain an open, but orthogonal, challenge for ﬂoating-point roundoﬀ error analysis [11,28]. The optimization-based approach that we extend in this paper has been used for the computation of relative error bounds [21] as well, however, relative errors are fundamentally undeﬁned for input domains that include zeros and are thus less widely applicable. We implement our analysis in a tool called Hugo and evaluate it on two case studies that are inspired by existing ﬂoating-point benchmarks [2]. Our evaluation shows that compared to an approach based on procedure inlining and an analysis by state of the art roundoﬀ analysis tools, our modular analysis provides an interesting tradeoﬀ: it is signiﬁcantly faster, while computing comparable error bounds that are often within the same order of magnitude and thus, in our opinion, practically useful. Contributions. To summarize, this paper presents the following contributions: – a sound modular roundoﬀ error analysis for non-recursive procedural code without conditionals that combines modularity and abstraction when analyzing function calls; – a prototype implementation of our analysis is available open-source at https://doi.org/10.5281/zenodo.8175459; – an empirical evaluation of the accuracy-performance tradeoﬀ of our analysis.  
   
  2  
   
  Background  
   
  In this section, we provide necessary background on ﬂoating-point arithmetic and roundoﬀ error analysis, focusing on the symbolic Taylor expression-based roundoﬀ error analysis that has been implemented in several tools. We extend this analysis in Sect. 3 to support procedure calls. Throughout, we use bold symbols to represent vectors. Floating-Point Arithmetic. The IEEE754 standard [1] formalizes ﬂoating-point numbers and the operations over them. A ﬂoating-point number is deﬁned as a triple (sng, sig, exp) indicating its sign, signiﬁcant, and exponent, with the numerical value being (−1)sng ×sig ×2exp . The standard introduces four general binary formats (16, 32, 64 and 128 bits) varying on the sizes of sig and exp. We assume 64 bit double precision throughout this paper, but our approach generalizes to other formats as well. The standard introduces several rounding operators that return the ﬂoatingpoint number that is closest to the input real number where the closeness is deﬁned by the speciﬁc rounding operator. The most common rounding operator is rounding to nearest (ties to even), which we assume in this paper. The distance between the real value and the ﬂoating-point representation is called the roundoﬀ error. Computing this diﬀerence exactly is practically infeasible for all but very short computations. Instead, we and most other roundoﬀ error  
   
  44  
   
  R. Abbasi and E. Darulova  
   
  analysis tools assume the following rounding model that holds for the rounding to nearest mode: rnd(op) = op(1 + e) + d  
   
  where |e| ≤ , |d| ≤ δ  
   
  (1)  
   
  where op is an arithmetic operation (or an input value or constant),  bounds the relative error and δ bounds the absolute error. For the standard arithmetic operations +, −, ∗, /, the IEEE754 standard speciﬁes for double precision  = 2−53 and δ = 2−1075 , where the latter captures the roundoﬀ error of subnormal ﬂoating-point numbers, i.e. numbers very close to zero. For library function calls to common mathematical functions, e.g. sin, exp, etc., the library speciﬁcation typically speciﬁes the corresponding error(s); in this paper we assume 2 ∗  (most libraries provide this bound or better, but our analysis is parametric). Sound Roundoﬀ Error Analysis. The goal of a roundoﬀ error analysis is to compute the worst-case absolute error: max |f (x) − f˜(˜ x)|  
   
  x,˜ x∈I  
   
  (2)  
   
  where f (x) denotes an idealized (purely) numerical program, where x is a possibly multivariate input, and f˜(˜ x) represents the function corresponding to the ﬂoating-point implementation, which has the same syntax tree but with opera˜ , is a tions interpreted in ﬂoating-point arithmetic. Note that the input to f˜, x rounded version of the real-valued input since that may not be exactly representable in ﬁnite precision and may need to be rounded. We want to maximize the above equation for a set of meaningful inputs I that depends on a particular application. Bounding roundoﬀ errors for unbounded input ranges is not practically useful as the error bounds are then in general unbounded. In this paper, we consider programs that consist of several procedures and the goal is to compute an error bound for each of them. The procedure bodies consists of arithmetic expressions, mathematical library function calls, (immutable) variable declarations and (possibly) calls to procedures deﬁned within the program. To estimate the rounding error for such programs with existing roundoﬀ error analyses, the procedure calls need to be eﬀectively inlined—either manually by a user before an analysis tool is run, or automatically by the tool [26]. For larger programs, especially with more procedure calls, this can result in very large (symbolic) expressions and thus long analysis times. This approach is also fundamentally not suitable for integration into modular veriﬁcation frameworks, such as KeY [3] or Frama-C [23]. For our modular analysis, the procedure calls do not need to be inlined. Instead, for each procedure of the program, our analysis ﬁrst computes an error speciﬁcation that is a function of the input parameters and that abstracts some of the error computation. Our analysis instantiates these error speciﬁcations at the call sites to compute an overall roundoﬀ error bound (it also checks that the preconditions of the called procedures are respected).  
   
  Modular Floating-Point Error Analysis  
   
  45  
   
  Symbolic Taylor Expression-Based Roundoﬀ Analysis. The approach to roundoﬀ error analysis (for straight-line code) that we extend in Sect. 3 was ﬁrst proposed in the tool FPTaylor [27]. This approach abstracts the ﬂoating-point function f˜(˜ x) using the rounding model from Eq. 1 into a real-valued function fˆ(x, e, d) to compute a bound on the roundoﬀ error: max |f (x) − fˆ(x, e, d))| x∈I  
   
  However, while now entirely real-valued, this expression is in general too complex for (continuous, real-valued) optimization tools to handle. To reduce complexity, FPTaylor applies a Taylor expansion: f (x) = f (a) +  
   
  k k   ∂f ∂2f (a)(xi − ai ) + 1/2 (p)(xi − ai )(xj − aj ) (3) ∂xi ∂xi ∂xj i=1 i,j=1  
   
  that allows to approximate an arbitrary suﬃciently smooth function by a polynomial expression around some point a. p is a point which depends on x and a and k is the number of input parameters of f . Taylor series deﬁne inﬁnite expansions, however, in practice these are terminated after some ﬁnite number of terms, and a remainder term soundly bounds (over-estimates) the skipped higher-order terms. In Eq. 3 the last term is the remainder. Applying a ﬁrst-order Taylor approximation to the abstracted ﬂoating-point function fˆ(x, e, d) around the point (x, 0, 0) we get: fˆ(x, e, d) = fˆ(x, 0, 0) + R2 (x, e, d) = 1/2  
   
  2k  i,j=1  
   
  k k   ∂ fˆ ∂ fˆ (x, 0, 0)(ei − 0) + (x, 0, 0)(di − 0) + R2 (x, e, d) ∂e ∂d i i i=1 i=1  
   
  ∂ 2 fˆ (x, p)yi yj ∂yi ∂yj  
   
  (4) where y1 , . . . y2k range over e1 , . . . , ek , d1 , . . . , dk respectively. Since fˆ(x, 0, 0) = f (x), one can approximate |fˆ(x, e, d) − f (x)| by: k k   ∂ fˆ ∂ fˆ (x, 0, 0)ei + (x, 0, 0)di + R2 (x, e, d)| ∂ei ∂di i=1 i=1 (FPTaylor Error) To compute a concrete roundoﬀ error bound, the above expression is maximized over a given input domain I using rigorous global optimization techniques such as interval arithmetic [25] or branch-and-bound [27]. The above model can be straight-forwardly extended to capture input errors on (particular) variables by increasing the bound on the corresponding error variables ei and/or di . Similarly, library functions for mathematical functions such as sin, cos, exp, ..., are supported by setting the bound on their corresponding error variables according to the speciﬁcation. Note that since the derivatives of the standard mathematical library functions are well-deﬁned, the partial derivatives in the equations can be immediately computed.  
   
  |fˆ(x, e, d) − f (x)| = |  
   
  46  
   
  3  
   
  R. Abbasi and E. Darulova  
   
  Modular Roundoﬀ Error Analysis  
   
  In principle, one can apply FPTaylor’s approach (Equation FPTaylor Error) directly to programs with procedure calls by inlining them to obtain a single arithmetic expression. This approach, however, results in potentially many reevaluations of the same or very similar expressions. In this section, we extend FPTaylor’s approach to a modular analysis by considering procedure calls explicitly. At a high-level, our modular error computation is composed of two stages: 1. The abstraction stage computes an error speciﬁcation for each procedure of the input program (Sect. 3.1 and Sect. 3.2); 2. The instantiation stage instantiates the pre-computed error speciﬁcations for each procedure at their call-sites with their appropriate contexts. Note that each procedure is processed only once in each of these stages, regardless of how often it is called in other procedures. The main challenge is to compute the error speciﬁcations such that they, on one hand, abstract enough over the individual arithmetic operations to provide a beneﬁt for the analysis in terms of performance, and on the other hand do not lose too much accuracy during this abstraction to still provide meaningful results. A naive way to achieve modularity is to compute, for each procedure, a roundoﬀ error bound as a constant value, and use that in the analysis of the procedure calls. This simple approach is, however, not enough, since in order to analyze a calling procedure, we do not only need to know which new error it contributes, but we also need to bound its eﬀect on already existing errors, i.e. how it propagates them. The situation is even further complicated in the presence of nested procedure calls. Alternatively, one can attempt to pre-compute only the derivatives from Equation FPTaylor Error and leave all evaluation to the call sites. This approach then eﬀectively amounts to caching of the derivative computations, and does not aﬀect the analysis accuracy, but its performance beneﬁt will be modest as much of the computation eﬀort will still be repeated. Our approach rests on two observations from the above discussion. We ﬁrst split the error of a procedure into the propagation of input errors and roundoﬀ errors due to arithmetic operations, following [11]: x) − f˜(˜ x)| |f (x) − f˜(˜ x)| = |f (x) − f (˜ x) + f (˜ x) − f˜(˜ x)| ≤ |f (x) − f (˜ x)| + |f (˜       propagation error  
   
  round-oﬀ error  
   
  and compute error speciﬁcations for each of these errors separately. This allows us to handle the propagation issue from which the naive approach suﬀers. We employ suitable, though diﬀerent, Taylor approximations for each of these parts. Secondly, we pre-evaluate, at the abstraction stage already, part of the resulting Taylor approximations, assuming the context, resp. input speciﬁcation of  
   
  Modular Floating-Point Error Analysis  
   
  47  
   
  each procedure. This results in some accuracy loss when the procedure is called in a context that only requires a narrower range, but saves analysis time. Naturally, our pre-computed error speciﬁcations are only sound if they are called from contexts that satisfy the assumed input speciﬁcations. Our implementation checks that this is indeed the case. Running Example. We use the following simple example for explaining and illustrating our technique: g(x) = x2  
   
  where  
   
  x ∈ [0.0, 100.0]  
   
  f (y, z) = g(y) + g(z) where  
   
  y ∈ [10.0, 20.0], z ∈ [20.0, 80.0] (Running Example)  
   
  Here, g is being called twice with arguments with diﬀerent input speciﬁcations, but which are both within the allowed range of g of [0, 100]. We will consider nested procedure calls in Sect. 3.4. Notation. We use f , g and h to denote procedures, x, y, z, w and t for input parameters and also as input arguments if a procedure contains procedure calls, and a, b and c for input arguments. Bold symbols are used to represent vectors. Each error speciﬁcation of a procedure f consists of a roundoﬀ error function denoted by βf and the propagation error function, denoted by γf . We use βf (a) and γf (a) to denote the evaluation of roundoﬀ and propagation error speciﬁcations for a procedure f with a as the vector of input arguments. We denote the initial errors of input parameters by u, the relative error of a rounding operator by e, and the absolute error by d. The maximum values for the relative and absolute errors are represented by  and δ respectively. We assume  to denote the maximum error  for our default precision, i.e. double precision. We will use ∂g ˆ ∂g ˆ to denote ∂e (x, 0, 0) for readability reasons. the notation ∂e  i i x,0,0  
   
  3.1  
   
  Roundoﬀ Error Abstraction  
   
  In this section, we extend FPTaylor’s approach with a rounding model for procedure calls and show how it can be used to compute roundoﬀ error speciﬁcations. Since input errors are handled by the propagation error speciﬁcation (Sect. 3.2), we assume here that procedure inputs have no errors. One of the main challenges of such an extension is that contrary to how the library function calls are handled (see Sect. 2), there is no given derivative and ﬁxed upper-bound on the rounding error for arbitrary procedure calls. If g is a procedure with input arguments a at the call site, and βg the corresponding roundoﬀ error speciﬁcation of g, then we extend the IEEE754 rounding model to procedure calls by: g˜(a) = g(a) + βg (a)  
   
  48  
   
  R. Abbasi and E. Darulova  
   
  That is, we abstract the rounding error by an absolute error, whose magnitude is determined by the error speciﬁcation of f that is a function of the input arguments. With this, we can proceed to extend the (FPTaylor Error) with procedure calls. Suppose we have a procedure f (x) that contains the procedure calls g1 (a1 ), . . . gl (al ) to procedures g1 , . . . gl , where a1 , . . . , al are the input arguments, and β g (a) = (βg1 (a1 ), . . . , βgl (al )) is the vector of corresponding roundoﬀ error speciﬁcations. Then the roundoﬀ error speciﬁcation βf for the procedure f (x) is given by: βf = fˆ(x, e, d, βg (a)) − f (x)   k k     ∂ fˆ  ∂ fˆ  =  ei +  ∂ei  ∂di  i=1  
   
  x,0  
   
  i=1  
   
  where R2 (x, e, d, β g (a)) = 1/2  
   
  di +  
   
  x,0  
   
  2k+l  i,j=1  
   
  l  i=1  
   
    ∂ 2 fˆ   ∂yi ∂yj   
   
    ∂ fˆ   ∂βgi (ai )   
   
  βgi (ai ) + R2 (x, e, d, β g (a)),  
   
  x,0  
   
  yi yj  
   
  x,p  
   
  (Roundoﬀ Speciﬁcation) where y1 , . . . y2k deﬁne e1 , . . . , ek , d1 , . . . , dk as before, and y2k+1 , . . . , y2k+l correspond to βg1 (a1 ), . . . , βgl (al ) respectively. Note that to derive the roundoﬀ error speciﬁcation for f , the concrete roundoﬀ speciﬁcations for gi are not required, i.e. we treat βg as a symbolic variable in the same way as ei and di . They are only instantiated at the evaluation phase, at which point all βg s are available. Correctness. Note that if we were to inline all βg roundoﬀ speciﬁcations in βf above (potentially recursively), we would reach the same roundoﬀ error formula as given by (FPTaylor Error) for a program where all procedure calls are inlined. Depending on the nesting, one needs higher-order terms of the Taylor expansion to achieve such equivalence. Running Example. To see this, lets consider our (Running Example). In order to compute the roundoﬀ speciﬁcations for procedures g and f , we ﬁrst compute the real-valued abstractions of the ﬂoating-point procedures of g and f , (i.e., gˆ(x, e1 , d1 ) and fˆ(y, z, e2 , βg (y), βg (z)) respectively) by applying the ﬂoatingpoint rounding model and the rounding model for procedure calls, on the ﬂoating-point functions g˜ and f˜: gˆ(x, e1 , d1 ) = x2 (1 + e1 ) + d1 fˆ(y, z, e2 , βg (y), βg (z)) = (g(y) + βg (y) + g(z) + βg (z))(1 + e2 ) The next step is to compute the roundoﬀ error speciﬁcations βg and βf . Since g does not contain any procedure calls, then βg follows the (FPTaylor Error) formula directly:  
   
  Modular Floating-Point Error Analysis  
   
  49  
   
    ∂ˆ g  ∂ˆ g  βg = e1 + d1 ∂e1 x,0,0 ∂d1 x,0,0 Next, we compute the roundoﬀ speciﬁcation for f :  
   
   ∂ fˆ  βf =  ∂e2   
   
  y,z,0  
   
   ∂ fˆ  e2 +  ∂βg (y)   
   
  y,z,0  
   
   ∂ fˆ  βg (y) +  ∂βg (z)   
   
  βg (z) + R2 (y, z, e2 , βg (y), βg (z)), y,z,0  
   
  (5) where,  
   
   ∂ 2 fˆ  R2 (y, z, e2 , βg (y), βg (z)) = 1/2( βg (y)e2 +  ∂βg (y)∂e2  y,z,e2 ,βg (y),βg (z)   2ˆ ∂ f  βg (z)e2 +  ∂βg (z)∂e2  y,z,e2 ,βg (y),βg (z)   2ˆ ∂ f  e2 βg (y) +  ∂e2 ∂βg (y)  y,z,e2 ,βg (y),βg (z)  ∂ 2 fˆ  e2 βg (z)).  ∂e2 ∂βg (z)  y,z,e2 ,βg (y),βg (z)  
   
  If we replace the βg functions in Eq. 5 by their respective Taylor expansions we reach the following:   ∂ fˆ  βf = R2 (y, z, e2 , βg (y), βg (z)) +  ∂e2   
   
  e2  
   
  y,z,0  
   
    ∂ fˆ  +  ∂βg (y)   
   
    ∂ˆ g (y)  ∂ˆ g (y)  ( e1 + d1 ) ∂e1 y,0,0 ∂d1 y,0,0 y,z,0     
   
    ∂ fˆ  +  ∂βg (z)   
   
  (6)  
   
  βg (y)  
   
    ∂ˆ g (z)  ∂ˆ g (z)  e + d1 ) 1 ∂e1 z,0,0 ∂d1 z,0,0 y,z,0    (  
   
  βg (z)  
   
  Based on the rounding model for procedure calls, we can deduce that ∂ fˆ ∂g ˆ(a) .  
   
  ∂ fˆ ∂βg (a)  
   
  =  
   
  ˆ ˆ If we replace ∂β∂gf(y) and ∂β∂gf(z) in Eq. 6 and also apply the chain rule ∂g ˆ(y) ∂ fˆ ∂ fˆ ∂g ˆ(y) × ∂e1 = ∂e1 ), we reach a formula that is equal to applying the  
   
  (e.g., (FPTaylor Error) on  
   
  fˆin = (y 2 (1 + e1 ) + d1 + z 2 (1 + e2 ) + d2 )(1 + e3 ), which is the abstraction of the ﬂoating-point inlined version of f , i.e. f˜in (y, z) = y 2 + z 2 . For simplicity, here we did not expand on the remainder. However, the reasoning is similar.  
   
  50  
   
  R. Abbasi and E. Darulova  
   
  Partial Evaluation. Besides abstraction that happens in the presence of nested function calls due to considering only ﬁrst-order Taylor expansions and no higher-order terms, we abstract further by evaluating those error terms in (Roundoﬀ Speciﬁcation) that tend to be small already at the abstraction phase. Speciﬁcally, we evaluate: – the ﬁrst-order derivatives w.r.t. absolute errors for subnormals, i.e. di s, – the remainder terms that do not contain any β terms themselves. For this evaluation, we use the input speciﬁcation of the procedure call. By doing so, we skip the re-instantiation of these small term at the call sites and overapproximate the (small) error of these terms. Since these terms are mostly of higher-order (especially the remainder terms) over-approximating them improves the analysis performance while having small impact on the analysis accuracy. 3.2  
   
  Propagation Error Abstraction  
   
  The goal is to compute the propagation error speciﬁcation γf for a procedure f as a function of the input parameters, while achieving a reasonably tight error bound. We over-approximate the propagation error, i.e., max |f (x) − f (˜ x)| by following the approach proposed in [11] while extending it to also support procedure calls. We ﬁrst explain how the propagation error speciﬁcation is computed, when there are no procedure calls (or they are inlined) and then we explain our extension to support procedure calls. Suppose ui , . . . uk are the initial errors of the input variables x1 , . . . , xk , i.e. ˜ = x + u. Similarly to the roundoﬀ speciﬁcation, we apply the Taylor expansion x to f (˜ x), but this time we take the derivatives w.r.t. the input variables: f (˜ x) − f (x) =  
   
  k k   ∂f ∂2f ui + 1/2 ui uj ∂xi ∂xi ∂xj i=1 i,j=1  
   
  (7)  
   
  Now consider the case where f (x) contains procedure calls of g1 (a1 ), . . . gl (al ), where a1 , . . . , al are the input arguments and γ(a) = (γg1 (a1 ), . . . , γgl (al )) is the vector of corresponding propagation error speciﬁcations. We compute the propagation error speciﬁcation for a procedure f as follows: γf =  
   
  k l   ∂f ∂f γgi (ai ) + R2 (x, u, γ(a)) ui + ∂x ∂g i i (ai ) i=1 i=1  
   
  (Propagation Speciﬁcation) where, R2 (x, u, γ(a)) = 1/2(  
   
  k,l k   ∂ 2 f (x) ∂ 2 f (x) ui uj + ui γgj (aj ) + ∂xi ∂xj ∂xi ∂gj i,j=1 i,j=1 l,k l   ∂ 2 f (x) ∂ 2 f (x) γgi (ai )γgj (aj ) + γg (ai )uj ) ∂gi ∂gj ∂gi ∂xj i i,j=1 i,j=1  
   
  Modular Floating-Point Error Analysis  
   
  51  
   
  That is, we compute and add the propagation error of the called procedures by computing the derivatives of the calling procedure w.r.t the called procedures and multiplying such terms by their respective γ function, which is the propagation error of the called procedure. The remainder terms w.r.t called procedures are computed similarly. Correctness. Just as with the roundoﬀ speciﬁcations, if we were to replace the γgi s by their corresponding formulas in γf , we would reach the same propagation error speciﬁcation as if we had computed it with Eq. 7 for a program with all procedures inlined. Again, higher-order Taylor expansion terms may be needed for an equivalence. Running Example. To see this, lets consider our (Running Example). Suppose ux , uy , and uz are the initial errors for procedures g and f respectively. The propagation speciﬁcations for g and f are computed as follows: ∂g ∂2g 2 ux + 1/2( u ) = 2xux + u2x ∂x ∂x x ∂f ∂f γg (y) + γg (z) = γg (y) + γg (z) = 2xuy + 2yuz + u2y + u2z γf = ∂g(y) ∂g(z) γg =  
   
  Note that replacing the γg functions with their equivalent Taylor expansion ∂f ∂f in γf and applying the chain rule (e.g., ∂g(y) × ∂g(y) ∂y = ∂y ), would result in the Taylor expansion of the inlined version of f (˜ x). Partial Evaluation. While computing the propagation speciﬁcation γ, we evaluate the small error terms of the error speciﬁcation and add them as constant error terms to the error speciﬁcation. These small terms are the remainder terms that do not contain any γ terms themselves. Doing so, we skip the re-evaluation of these small terms at the call sites and therefore, speed-up the analysis. 3.3  
   
  Instantiation of Error  
   
  In the second step of our analysis, we instantiate the propagation and roundoﬀ error speciﬁcations of each procedure of the program using its input intervals. In other words, for each procedure, we compute upper bounds for the β and γ error speciﬁcations. For the instantiation of the error speciﬁcations, one can use diﬀerent approaches to maximize the error expressions. In Hugo, one can chose between interval arithmetic and a branch-and-bound optimization. The instantiation of an error speciﬁcation for a procedure is conducted in a recursive fashion. In order to compute an upper bound on the error for a procedure, we instantiate the error terms of the corresponding error speciﬁcation using interval analysis. While instantiating the error, we may come across β or γ functions corresponding to the called procedures. In such cases, we fetch the  
   
  52  
   
  R. Abbasi and E. Darulova  
   
  error speciﬁcation of these called procedures and instantiate them using the input intervals of the calling procedure. Note that in the ﬁrst stage of the analysis and while computing the error speciﬁcations, we over-approximated the error by pre-evaluating the smaller terms there and adding them as constants to the error speciﬁcations. As a result, in this stage and before instantiating an error speciﬁcation of a called procedure, we check that the input intervals of input parameters of the called procedure— for which the error speciﬁcation function is computed—enclose the intervals of input arguments at the call site. This precondition check can also be applied post analysis. For the (Running Example), instantiating the roundoﬀ error speciﬁcation of g results in the following evaluated β functions. βg =  max |x2 | + δ, βf =  max |g(y) + g(z)| + (1 + 2) max |βg (y) + βg (z)| 3.4  
   
  Handling Nested Procedures  
   
  We now explain how our analysis extends beyond the simple case discussed so far, and in particular how it supports the case when a procedure argument is an arithmetic expression or another procedure call. In such a case, one needs to take into account the roundoﬀ and propagation error of such input arguments. We treat both cases uniformly by observing that arithmetic expression arguments can be refactored into separate procedures, so that we only need to consider nested procedure calls. We compute the roundoﬀ and propagation error speciﬁcation of the nested procedure call in a similar fashion as before. Though, while computing the β and γ speciﬁcations with nested procedure calls we incorporate their respective β and γ functions in the solution. That is, we take the β function of a nested procedure into account while we create a rounding abstraction for a procedure call. For example, for the procedure call f (g(a)), the rounding model is: f˜(g(a)) = f (g(a) + βg (a)) + βf (g(a) + βg (a)) On the other hand, while computing the propagation error speciﬁcation of a procedure call such as f (g(a)), instead of multiplying the computed derivatives by their respected initial error, they get multiplied by the respective propagation error speciﬁcation, i.e. γg (a). Example. We illustrate how we handle nested procedure calls with a slightly more involved example: g(x) = x2 , where x ∈ [0.0, 500.0] h(y, z) = y + z, where y ∈ [10.0, 20.0], z ∈ [10.0, 20.0] f (w, t) = g(h(w, t))  
   
  where  
   
  w ∈ [12.0, 15.0], t ∈ [12.0, 15.0]  
   
  (8)  
   
  Modular Floating-Point Error Analysis  
   
  53  
   
  The roundoﬀ error speciﬁcation for g is computed as before for our (Running Example) and since h does not contain any procedure calls, βh is computed straight-forwardly as before. The abstraction of the ﬂoating-point procedure of f is as follows: f (w, t, βg , βh ) = g(χ(w, t)) + βg (χ(w, t)) where, χ(w, t) = h(w, t) + βh (w, t) Next we compute βf : βf =  
   
  ∂ fˆ ∂ fˆ ∂ 2 fˆ βh (w, t) + βg (χ(w, t)) + 1/2 βh2 (w, t) = ∂βh (w, t) ∂βg (χ(w, t)) ∂βh (w, t) ∂g(χ(w, t)) βh (w, t) + βg (χ(w, t)) + βh2 (w, t) ∂(h(w, t)βh (w, t) + βh (w, t))  
   
  If βf is instantiated then we obtain: βf = max |3(w + t)2 + 32 (w + t)2 + 3 (w + t)2 | If we compute the roundoﬀ speciﬁcation for the inlined version of f i.e., (w + t)2 , using the Taylor expansion, however up to the third-order derivative terms, we reach the same error speciﬁcation as in βf above. To compute the propagation error, consider ux as the initial error for g, uy and uz as initial errors for h and uw and ut as initial errors in f . The propagation error speciﬁcation for g is as computed before for (Running Example) and is equal to 2xux + u2x . The propagation error speciﬁcations of h and f are as follows: ∂h ∂h uy + uz = uy + uz ∂y ∂z ∂f γg (h(w, t)) = γg (h(w, t)) = 2h(w, t)γh (w, t) + γh2 (w, t) γf = ∂g(h(w, t))  
   
  γh =  
   
  Therefore, γf = 2(w + t)(uw + ut ) + (uw + ut )2 The inlined version of f has the same propagation error speciﬁcation.  
   
  4  
   
  Implementation  
   
  We have implemented our proposed modular error analysis technique in a prototype tool that we call Hugo in the Scala programming language. We did  
   
  54  
   
  R. Abbasi and E. Darulova  
   
  not ﬁnd it feasible to extend an existing implementation of the symbolic Taylor expression-based approach in FPTaylor [27] (or another tool) to support procedure calls. We thus opted to re-implement the straight-line code analysis inside the Daisy analysis framework [10] which supports function calls at least in the frontend. We implement our modular approach on top of it and call it Hugo in our evaluation. Our implementation does not include all of the performance or accuracy optimizations that FPTaylor includes. Speciﬁcally, it is not our goal to beat existing optimized tools in terms of result accuracy. Rather, our aim is to evaluate the feasibility of a modular roundoﬀ error analysis. We expect that most, if not all, of FPTaylor’s optimizations (e.g. detecting constants that can be exactly represented in binary and thus do not incur any roundoﬀ error) to be equally beneﬁcial to Hugo. Nevertheless, our evaluation suggests that our re-implementation is reasonable. Hugo takes as input a (single) input ﬁle that includes all of the procedures. Integrating Hugo into a larger veriﬁcation framework such as KeY [3] or FramaC [23] is out of scope of this paper. In Hugo, we use intervals with arbitrary-precision outer bounds (with outwards rounding) using the GNU MPFR library [15] to represent all computed values, ensuring a sound as well as an eﬃcient implementation. Hugo supports three diﬀerent procedures to bound the ﬁrst-order error terms in equations Roundoﬀ Speciﬁcation and Propagation Speciﬁcation: standard interval arithmetic, our own implementation of the branch-and-bound algorithm or Gelpia [4], the branch-and-bound solver that FPTaylor uses. However, we have had diﬃculties to obtain reliable (timely) results from Gelpia. Higher-order terms are evaluated using interval arithmetic.  
   
  5  
   
  Evaluation  
   
  We evaluate our modular roundoﬀ error analysis focusing on the following research questions: RQ1: What is the trade-oﬀ between performance and accuracy of our modular approach? RQ2: How does the modular approach compare to the state-of-the-art? 5.1  
   
  Experimental Setup  
   
  We evaluate Hugo on two case studies, complex and matrix, that reﬂect a setting where we expect a modular analysis to be beneﬁcial. Each case study consists of a number of procedures; some of these would appear as library functions that are (repeatedly) called by the other procedures. Each procedure consists of arithmetic computations and potentially procedure calls, and has a precondition describing bounds on the permitted input arguments. Our two case studies are inspired by existing ﬂoating-point benchmarks used for verifying the absence of ﬂoating-point runtime errors in the KeY veriﬁcation  
   
  Modular Floating-Point Error Analysis  
   
  55  
   
  Table 1. Case study statistics benchmark  
   
  # top level # procedure # arith. ops. # arith. ops. procedures calls inlined  
   
  matrix matrixXL matrixXS  
   
  5 6 4  
   
  complex 15 complexXL 16 complexXS 13  
   
  15 33 6  
   
  26 44 17  
   
  371 911 101  
   
  152 181 136  
   
  98 127 72  
   
  699 1107 464  
   
  framework [2]. We adapted the originally object-oriented ﬂoating-point Java programs to be purely procedural. We also added additional procedures and procedure calls to reﬂect a more realistic setting with more code reuse where a modular analysis would be expected to be beneﬁcial. Note that the standard ﬂoating-point benchmark set FPBench [9] is not suitable for our evaluation as it consists of only individual procedures. matrix. The matrix case study contains library procedures on 3 × 3 matrices, namely for computing the matrix’ determinant and for using this determinant to solve a system of three linear equations with three variables, using Cramer’s Rule. Finally, we deﬁne a procedure (solveEquationsVector) that solves three systems of equations and computes the average of the returned values, representative of application code that uses the results of the systems of equations. See Listing 1.2 in the Appendix for the (partial) matrix code.  
   
  The complex case study contains library procedures on complex numbers such as division, reciprocal and radius, as well as procedures that use complex numbers for computing properties of RL circuits. For example, the radius procedure uses Pythagoras’ theorem to compute the distance to the origin of a point represented by a complex number in the complex plane. The computeRadiusVector demonstrates how the radius library procedure may be called to compute the radius of a vector of complex numbers. The approxEnergy procedure approximates the energy consumption of an RL circuit in 5 time steps. Listing 1.1 shows partial code of our complex case study. The procedure _add is a helper procedure that implements an arithmetic expression (and not just a single variable) that is used as argument of a called procedure; see Sect. 3.4 for how our method modularly incorporates the roundoﬀ and propagation errors resulting from such an expression. For now this refactoring is done manually, but this process can be straight-forwardly automated. Table 1 gives an overview of the complexity of our case studies in terms of the number of procedures and procedure calls, as well as the number of arithmetic operations in both the inlined and the procedural (original) versions of complex.  
   
  56  
   
  R. Abbasi and E. Darulova  
   
  Listing 1.1. 2  
   
  complex  
   
  case study  
   
  object complex { def _add(rm1: Real): Real = {...} def divideRe(re1: Real, im1: Real, re2: Real, im2: Real): Real = {...}  
   
  4  
   
  def divideIm(re1: Real, im1: Real, re2: Real, im2: Real): Real = {...}  
   
  6  
   
  def reciprocalIm(re1: Real, im1: Real): Real = { ... }  
   
  def reciprocalRe(re1: Real, im1: Real): Real = {...} def impedanceIm(frequency5: Real, inductance: Real): Real = {...} 8  
   
  def instantVoltage(maxVoltage: Real, frequency4: Real, time: Real): Real = {...} def computeCurrentRe(maxVoltage: Real, frequency3: Real, inductance: Real, resistance: Real): Real = {...}  
   
  10  
   
  def computeCurrentIm(maxVoltage: Real, frequency2: Real, inductance: Real, resistance: Real): Real = {...}  
   
  12  
   
  def radius(re: Real, im: Real): Real = {...} 14  
   
  def computeInstantCurrent(frequency1: Real, time: Real, maxVoltage: Real, inductance: Real, resistance: Real): Real = {...}  
   
  16  
   
  def approxEnergy(frequency: Real, maxVoltage: Real, inductance: Real, resistance: Real): Real = {  
   
  18  
   
  require(((frequency >= 1.0) && (frequency = 1.0) && (maxVoltage = 0.001) && (inductance = 1.0) && (resistance = 1) && (re = 1) && (im 'a -> 'a t -> 'a t  
   
  swap i j ut int -> int -> 'a t -> 'a t  
   
  swaps two elements with each other. It alters the permutation that serves as a view of the content.  
   
  card i ut  
   
  returns the number of elements in the entry pointed to by  
   
  i.  
   
  int -> 'a t -> int  
   
  same i j ut int -> int -> 'a t -> bool  
   
  returns true if and only if the union-table.  
   
  i  
   
  and  
   
  j  
   
  point to the same entry in  
   
  QuantumState that is a hash table with bit combinations as keys and complex  
   
  numbers as values inspired by the data structure used in [7]. The bit combinations correspond to basis states, e. g., if there are stored three qubits in a group in the union-table, the binary number 11bin corresponds to the quantum state |011 where the ﬁrst of the three qubits is in the state |0 and the other two in |1. The values denote the amplitudes for each state. For this to work correctly, the length of keys must not be limited by the number of bits used for an integer, e. g., 32 or 64 bits; instead, we use arbitrary large integers as keys for the hash table. The indices of qubits in each group in the union-table are ordered; this way, one gets a mapping from the global index of a qubit to its position within the state. Figure 3 shows the representation of the state √ (|10000 − |10101 + |11000 − |11101) /2 ⊗ (1 + i)/ 2 |0 using a union-table and bitwise representation of the quantum states. Merging Entanglement Groups. The correct ordering of indices becomes, in particular, tricky when two entanglement groups are merged and, consequently, also their quantum states must be merged. For this purpose, the union func-  
   
  170  
   
  Y. Chen and Y. Stade  
   
  0: 1: 2: 3: 4: 5:  
   
  |100 |00 |0 →  
   
  √1 ; |110 2 √1 ; |11 2  
   
  √1 2 √1 2  
   
  1+i √ 2  
   
  Fig. 3. The representation of a quantum system with six qubits using the union-table data structure (left), where the quantum state of each entanglement group is a hash table with the basis states as keys and their complex amplitudes as values (right). The qubits in each state are indexed from left to right. The √ represented quantum state is (|10000 − |10101 + |11000 − |11101) /2 ⊗ (1 + i)/ 2 |0. Using the usual tensorproduct notation, it is not obvious that qubits {0, 1, 3} and {2, 4} are separable.  
   
  tion requires a combine function to combine the two entries, i. e., in our case the two quantum states. The union-table merges the sequences of two groups in one merge step known from the merge-sort algorithm. The combine function receives the order in which the elements from the two former groups were merged, in order to apply the same merging behavior to the quantum states. When the quantum state for n qubits contains k many basis states, the combine function requires O(k · n) steps. Application of Gates. Here, we only describe the application of a single-qubit gate to a state; the approach generalizes to gates operating on multiple qubits. Let U be the matrix representation of some gate that is to be applied on qubit i in a given quantum state where U = (uij )i,j∈{0,1} . We iterate over the keys in the quantum state: For keys with the i-th bit equal to 0, we map the old keyvalue pair (|Ψ , α) to the two new pairs (|Ψ , α · u11 ) and (|Ψ  , α · u21 ), where |Ψ  emerges from |Ψ by ﬂipping the i-th bit; for keys with the i-th bit equal to 1, we map the old key-value (|Φ , β) pair to the two new pairs (|Φ  , β · u12 ) and (|Φ , β · u22 ), where, again, |Φ  emerges from |Φ by ﬂipping the i-th bit. Generated pairs with the same key (basis state) are merged by adding their values (amplitudes). For a matrix of dimension 2d and k states in the quantum state of n qubits, this procedure takes O(2d · k · n) steps where d is the number of aﬀected qubits. 3.3  
   
  Restricted Simulation  
   
  Restrict Complexity. The state in Fig. 2 contains only two basis states as opposed to 23 = 8 possible ones for which a complex number needs to be stored each. Exploiting this fact, the key features of the restricted simulation are (i) to keep track of the quantum state as separable entanglement groups of qubits, where qubits are included in the same entanglement group if and only if they are entangled, and  
   
  Quantum Constant Propagation  
   
  171  
   
  (ii) to limit the number of basis states representing the quantum state of an entanglement group by a chosen constant. Note that the number of basis states allowed in the quantum state of one entanglement group corresponds to the number of amplitudes required to be stored; all other amplitudes are assumed to be zero. Reaching Maximum Complexity. The careful reader may ask how to proceed when the maximum number of allowed basis states is reached. We set the state of the entanglement group of which the limit is exceeded to , meaning that we no longer track any information about this group of qubits. By doing so, we can continue simulating the remaining entanglement groups until they may also end up in the . For this, we utilize a ﬂat lattice that consists of either an element representing a concrete quantum state of an entanglement group, , or ⊥ (not used) satisfying the partial order in Fig. 4. The following deﬁnitions establish the relation between the concrete quantum states and their abstract description.  
   
  Fig. 4. Lattice for the abstract description of quantum states.  
   
  Deﬁnition 1 (Abstract state). The abstract state s is an abstract description of a concrete quantum state if and only if s =  or s = |ψ where |ψ is a concrete quantum state consisting of at most nmax many non-zero amplitudes. Deﬁnition 2 (Abstract description relation). Let Δ denote the description relation between quantum states and their abstract description. Furthermore, let |ψ be a quantum state and s be an abstract description. The quantum state |ψ is described by s, formally |ψ Δ s, if and only if s =  or s = |ψ. Consequently, the entry in the union-table is not the quantum state itself but an element of the ﬂat lattice, an abstract description of the quantum state. Deﬁnition 3 deﬁnes a concretization operator for abstract states. Deﬁnition 3 (Concretization operator). Let γ be the concretization operator, and s be an abstract description. Then γ s = {|ψ | |ψ Δ s}. Next, we deﬁne the abstract eﬀect for gates acting on quantum states.  
   
  172  
   
  Y. Chen and Y. Stade  
   
  Deﬁnition 4 (Abstract gate). Let [[U ]] denote the abstract eﬀect of the quantum gate U . For an abstract description s, the abstract eﬀect of U is:  s = |ψ U |ψ  [[U ]] s = if s=  Theorem 1 justiﬁes the above-deﬁned abstract denotation of quantum states. It follows directly from Deﬁnition 1, Deﬁnition 2, and Deﬁnition 4. Theorem 1 (Correctness of abstract denotation). For any quantum state |ψ and abstract description s satisfying |ψ Δ s, U |ψ Δ [[U ]] s holds. Operating with Separated States. In the beginning, every qubit constitutes its own entanglement group. Single-qubit gates can be applied without further ado by modifying the corresponding amplitudes accordingly; the procedure behind is matrix multiplication which can be implemented in constant time given the constant size of matrices. The case where multi-qubit gates are applied is split into two subcases. When the multi-qubit gate is applied only to qubits within one entanglement group, the same argument for applying single-qubit gates still holds. Applying a multi-qubit gate across several entanglement groups will most likely entangle those; hence, we need to merge the aﬀected groups into one. Applying Multi-qubit Gates. In the case of an uncontrolled multi-qubit gate, such as an echoed cross-resonance (ecr) gate, we ﬁrst merge all aﬀected entanglement groups into one. If one of those groups is already in the  state, we must set the entire newly formed entanglement group to . Otherwise, we can apply the merging strategy of the involved quantum states as described in Sect. 3.2. Afterward, matrix multiplication is performed to reﬂect the expected transformation of the state. A special case is the swap gate: we leave the entanglement groups as is and keep track of the eﬀect of the swap gate in the permutation embedded in the quantum state structure. Before we apply a controlled gate, we perform control reduction—the central part of the optimization—which we outline in the next section, to remove superﬂuous controls. 3.4  
   
  Control Reduction  
   
  Classically Determined Qubits. The central task of quantum constant propagation is to remove superﬂuous controls from controlled gates. First, we identify and remove all classically determined qubits, i. e. those that are either in |0 or |1. If we ﬁnd a qubit always in |0, the controlled gate can be removed. If we ﬁnd qubits always in |1, those controls can be removed since they are always satisﬁed. Satisﬁable Combination. By ﬁltering out classically determined qubits as described above, a set of qubits may remain in some superposition. Even then, for the target operation to be applied, there must be a basis-state where each of the controls is satisﬁed, i. e., each is in |1. If no such combination exists, the gate can be removed entirely.  
   
  Quantum Constant Propagation  
   
  173  
   
  Implied Qubits. When a combination with all controls in the |1 state was found, there can still be some superﬂuous controls among the remaining qubits. Consider the situation in Fig. 5. Here, the upper two qubits are both in |1 state when the third qubit is as well; hence, the third qubit implies the ﬁrst and second one. The semantics of the controlled gate remains unchanged when we remove the two upper controls. To generalize this idea, we consider every group of entangled qubits separately since there can not be any implications among diﬀerent entanglement groups. Within each entanglement group, we look for implications, i. e., whether one qubit being in |1 state implies that other qubits are also in the |1 state. Those implied qubits can be removed from the list of controls.  
   
  |0 |0 |0 |0 1 2  
   
  H H  
   
  ≡  
   
  |0 |0 |0 |0  
   
  H H  
   
  (|0000 + |0100 + |1000 + |1110 )  
   
  Fig. 5. For the rightmost gate, the third qubit implies the ﬁrst and second qubit, hence the ﬁrst and second control qubits can be removed from it.  
   
  Further Optimization Potential. In some cases, there might be an equivalence relation between two qubits; here, either one or the other qubit can be removed. This choice is made arbitrarily right now; by considering the circuit to the left or right of the gate, more optimization potential could be exploited. Moreover, the information of more than one qubit might be needed to imply another qubit. Here, we limit ourselves to the described approach because of two reasons: First, in currently common circuits [21] multi-controlled gates with more than two controls rarely occur, and for two controls, our approach ﬁnds all possible implications; second, to ﬁnd the minimal set of controlling qubits is a computationally expensive task that is to the best of our knowledge exponential in the number of controls. Handle the Abstract State . If some of the entanglement groups covered by the controls are in , the optimization techniques can be applied nevertheless. Within groups that are in  no classically determined qubits or implications between qubits can be identiﬁed; however, this is still possible in all other groups. To check whether a satisﬁable combination exists across all entanglement groups, we assume one within each group that is . This is a safe choice: It does not lead to any unsound optimizations since there could be a satisﬁable combination in such groups.  
   
  174  
   
  Y. Chen and Y. Stade  
   
  Application of Controlled Gates. Before applying a controlled gate, we assume that all superﬂuous controls are already removed according to the approach described in Sect. 3.4. Like the application of an uncontrolled multi-qubit gate explained in Sect. 3.2, all involved entanglement groups must be merged. Then, all states that satisfy all remaining controls after the control reduction are ﬁltered. To those, the gate is applied to the target qubits via matrix multiplication, whereas the amplitudes of all other states remain unchanged. However, if one of the controls belongs to an entanglement group in , the resulting state cannot be determined, and we set the merged entanglement group to .  
   
  Fig. 6. Quantum constant propagation removes the control from the gate (3) and the gates (6), (10), and (12) entirely. For an explanation, see Example 1  
   
  Example 1. We will demonstrate the eﬀect of our optimization on an example taken from [26]. Vandersypen et al. perform Shor’s algorithm on 7 qubits to factor the number 15. In this process, they design the circuit from Fig. 6. From the gate labeled with (3), the optimization will remove the control because the state of the controlling qubit is known to be |1 at this point. Gate (6) will be removed entirely because the controlling qubit is known to be in the |0 state. Also, gates (10) and (12) are removed since their controlling qubit will be in the |0 state. These optimizations seem to be trivial. However, the diﬃcult part when automating this process is to scale it to larger and larger circuits without sacriﬁcing eﬃcient running time. Here, we provide the right tool for that with .... our proposed restricted simulation.2  
   
  4  
   
  Correctness of Control Reduction  
   
  In this section, we complement the intuitive justiﬁcation for the correctness of the optimization with a rigorous proof. We ﬁrst establish the required deﬁnitions to characterize the concrete semantics of controlled operations. Similar reasoning 2  
   
  Note that our optimization will not remove redundant gates such as the two Hadamard gates on top; we leave this step for other optimization tools since we already have enough to perform this task suﬃciently well.  
   
  Quantum Constant Propagation  
   
  175  
   
  about the correctness is contained in [13]; we see our style as more comprehensible since it argues only over the superﬂuousness of one qubit at a time but is still suﬃcient to show the correctness of the optimization. n  
   
  n  
   
  Deﬁnition 5 (Controlled gate). Let U ∈ C2 ×2 for n ∈ N be a unitary matrix of a gate. Let C m (U ) denote the matrix representing the m-controlled version of this gate (the application of gate U is controlled on m qubits). Example 2. Consider the X-gate. The corresponding matrix is given by X = ( 01 10 ) . The doubly-controlled version C 2 (X) (the Toﬀoli-gate), amounts to ⎛1 ⎜ ⎝  
   
  1  
   
  ⎞ 1  
   
  1  
   
  1  
   
  1  
   
  ⎟ 8×8 ⎠∈C . 01 10  
   
  .... m+n  
   
  Deﬁnition 6 (Superﬂuousness of controls). Given a state |Ψ ∈ C2 n n and a unitary U ∈ C2 ×2 . Let I ∈ C2×2 denote the identity matrix. The ﬁrst one of m controls is superﬂuous with respect to |Ψ if C m (U ) |Ψ = I ⊗ C m−1 (U ) |Ψ .  
   
  (1)  
   
  For the following, we assume without loss of generality that the ﬁrst m qubits are the controlling ones for a gate applied to the following n qubits. Theorem 2 (Superﬂuousness of controls). With the notation from Deﬁni2m −1 2n −1 tion 6 and |Ψ = i=0 j=0 λi,j |i ⊗ |j, the condition from Deﬁnition 6 is equivalent to ⎞  
   
  ⎛  
   
  λi,0  
   
  ⎜ .. ⎟  
   
  ⎝ . ⎠  
   
  λi,2n −1 m−1 i=2  
   
  −1  
   
  being an eigenvector of U for the eigenvalue 1 or the 0 vector.3 Proof. When we write out the left-hand side of Eq. (1) in Deﬁnition 6 using the deﬁnition of |Ψ, we get the following equation:4  
   
  n −1 n m n 2 −1 2 2 −2 2 −1  
   
  m  
   
  uj,k λi,k |i |j  
   
  + λi,j |i |j (2) C (U ) |Ψ =  
   
  m j=0 i=0 j=0 k=0 i=2 −1  
   
  3 4  
   
  We index elements in matrices starting with 0 as opposed to the mathematical convention with 1 such that the λ corresponding to the basis state |0 has index 0. For simplicity we omitted the ⊗ sign to multiply the states.  
   
  176  
   
  Y. Chen and Y. Stade  
   
  We do the same with the right-hand side of Eq. (1), which results in: n −1  
   
  n −1 2 2  
   
  C m (U ) |Ψ = uj,k λi,k |i |j  
   
  m k=0 i∈{2m−1 −1,2m −1} j=0  
   
  i=2 −1  
   
  +  
   
  m 2 −1  
   
  n 2 −1  
   
  (3)  
   
  λi,j |i |j  
   
  i=0 j=0 i∈ / {2m−1 −1,2m −1}  
   
  Such that Eq. (1) in Deﬁnition 6 is satisﬁed, both, Eq. (2) and (3) must be equal, which gives us:  
   
  n −1  
   
  n n  
   
  2 −1 2 2 −1  
   
  ! uj,k λi,k |i |j = λi,j |i |j  
   
  m−1 j=0 j=0 k=0 i=2  
   
  −1  
   
  By performing a summand-wise comparison, this reduces to:  
   
  n 2 −1  

  uj,k λi,k = λi, j  
   
  ∀j ∈ {0, . . . , 2n − 1}  
   
  k=0  
   
  i=2m−1 −1   
   
  This is equivalent to (λi,0 , . . . , λi,2n −1 ) with i = 2m−1 − 1 being an eigenvector to the eigenvalue 1 or being the zero-vector, concluding the proof.  
   
  From Theorem 2 we can derive a corollary that brings this result in a closer relationship with our optimization using the following deﬁnition. Deﬁnition 7 (Implied control). Using the notation from Deﬁnition 5, we say the ﬁrst control is implied by the other controls if λi,j = 0 for i = 2m−1 − 1 and all j ∈ {0, . . . , 2n − 1}. If one interprets the basis states as equal-length bitstrings representing variable assignments of m + n truth variables, then this condition intuitively states that the implication x1 ∧ · · · ∧ xm−1 =⇒ x0 holds. Corollary 1 (Suﬃcient condition for a control to be superﬂuous). If the ﬁrst control is implied by the other controls, it is superﬂuous. The following main theorem shows that each of the three possible modiﬁcations, as described in Sect. 3.4, does not change the semantics of the circuit. Theorem 3. Quantum constant propagation does not change the semantics relative to the initial conﬁguration with all qubits in the |0 state. Proof. Without loss of generality, we can assume that the optimization pass detects the ﬁrst one of m controlling qubits as superﬂuous. Depending on the state of the ﬁrst qubit, the optimization continues in three diﬀerent ways.  
   
  Quantum Constant Propagation  
   
  177  
   
  (i) The ﬁrst qubit is in |0: Here, diﬀerent from the other two cases, not just the controlling qubit is removed from the controlled gate, rather than the entire gate is removed. Thus, we need to show C m (U ) |Ψ = |Ψ . Since all λi,j = 0 where i = 2m − 1 the sum on the right of Eq. (2) reduces to 0 and the claim follows. (ii) The ﬁrst qubit is in |1: Then the amplitude of all basis state with the ﬁrst qubit in |0 are equal to 0, i. e., λi,j = 0 where i = 2m−1 − 1 and for all j ∈ {1, . . . , 2n − 1}. Consequently, the condition in Theorem 2 is satisﬁed, and the ﬁrst control qubit can safely be removed. (iii) Otherwise: This case can only occur if the optimization found another qubit j among the controlling ones such that the ﬁrst qubit is only in |1 if the j-th qubit is also in |1. Hence, the suﬃcient condition from Corollary 1 is satisﬁed and here the ﬁrst control qubit can be safely removed. Altogether, this proves the correctness of the QCP optimization pass.  

  We continue with an analysis to show that QCP runs in polynomial time.  
   
  5  
   
  Running Time Analysis  
   
  Variable Deﬁnition. For the rest of this section, let m be the number of gates in the input circuit and n the number of qubits. Furthermore, let k be the maximum number of controls attached to any gate in the circuit. Each entanglement group is limited in the number of basis states by the custom constant nmax . The achieved asymptotic running time of our QCP is then established by the following lemmas and the main theorem of this section. Lemma 1. Control reduction runs in O(k 2 · n) time. Proof. As described in Sect. 3.4, the control reduction procedure consists of three steps. First, scanning for classically determined qubits takes O(n · k) time since the state of all controlling qubits needs to be determined and the entanglement group contains at most nmax basis states, which is constant. The factor of n comes from retrieving the position and later the state of a speciﬁc qubit within the entanglement group which comprises O(n) qubits, see also Table 1. Second, the check for a combination where every controlling qubit is in |1, requires splitting the controlling qubits into groups according to their entanglement groups and then checking within each such group whether a combination of all controlling qubits in |1 exists. There can be O(k) groups containing each O(k) qubits in the worst case. For each such group, a basis state among the at most nmax basis states where all contained controlling qubits are in |1, needs to be found. This requires retrieving the position and then the state of the individual controlling qubits, which takes O(n) for each of those. Together, this step runs in O(k 2 · n).  
   
  178  
   
  Y. Chen and Y. Stade  
   
  For the third step of ﬁnding implications between qubits, we need to consider every pair of qubits in each group already calculated for the previous step. For each pair, we need to retrieve the position and state of the corresponding qubits again, which takes O(n) times. Since there are O(k 2 ) pairs to consider, this gives us a running time of O(k 2 · n) for this step.  
   
  Combined, the running time of the entire control reduction is O(k 2 · n). To perform the control reduction, the current quantum state needs to be tracked. The running time required for that is given by the next lemma. Lemma 2. The application of one gate requires O(n) time. Proof. For multi-qubit (un- and controlled) gates, ﬁrst the aﬀected entanglement groups need to be merged. With the results mentioned in Sect. 3, this requires O(n) time considering that nmax is constant. For uncontrolled gates, there are only single-qubit and two-qubit gates available in current quantum programming tools, hence, we consider the size of the unitary that deﬁnes the transformation of those as constant. We ﬁrst check whether the number of basis states would exceed nmax after the application of the gate; this can be done in O(n) by iterating over the basis states in the entanglement group and counting the states relevant for the matrix multiplication. For the application of the associated unitary, one must iterate over the states in the entanglement group and add for each the corresponding states with their modiﬁed amplitudes as described in Sect. 3.2. Since the number of states in an entanglement group is bound by nmax and the unitary is constant in size, this requires O(n) time. Checking the state of a speciﬁc qubit in a basis state within the entanglement group comprising O(n) qubits requires O(n) time. For the controlled case, the procedure is slightly more complicated, since the unitary transformation shall only be applied to basis states where all controlling qubits are satisﬁed. This can be done by ﬁltering out the right states and then applying the same procedure as above. Hence, since there are at most nmax states, this does not change the overall running time. Consequently, the whole application of one gate can be performed in O(n) time.  
   
  Theorem 4. QCP runs in O(m · k 2 · n). Proof. Lemma 1 and Lemma 2 show together, that processing one gate takes O(k 2 · n + n) = O(k 2 · n) time. With m the number of gates present in the input circuit, this gives us the claimed result.  
   
  In particular, this shows that the entire QCP runs in polynomial time which we consider important for an eﬃcient optimization. This is due to the restriction of the number of states in each entanglement group since this number could otherwise grow exponentially in the number of qubits, i. e., would be in O(2n ).  
   
  Quantum Constant Propagation  
   
  6  
   
  179  
   
  Evaluation  
   
  The QCP, we propose, only applies control reduction and gate cancellation because of unsatisﬁable controls. This may facilitate the elimination of duplicate gates or rotation folding afterward— optimizations which we leave for existing tools capable of this task. In more detail, with the evaluation presented here, we pursue three objectives: (i) Measure the eﬀectiveness of QCP in terms of its ability to facilitate widely used quantum circuit optimizers. (ii) Show that QCP extends existing optimizations that also use the idea of constant propagation, namely the Relaxed Peephole Optimization (RPO) [16]. (iii) Demonstrate the eﬃciency (polynomial running time) of QCP even when processing circuits of large scale. In the following, we describe the experiments performed to validate our objectives, and afterward, we show and interpret their results. The corresponding artifact [4] provides the means to reproduce the results reported here. 6.1  
   
  Experiments  
   
  The Benchmark Suite. To provide realistic performance numbers for our optimization, we evaluate it on the comprehensible benchmark suite MQTBench [21]. This benchmark contains circuit representations of 28 algorithms at diﬀerent abstraction levels; most are scalable in the number of qubits ranging from 2 to 129 qubits. We use the set of circuits at the target-independent level compiled with Qiskit using optimization level 1. This results in a total number of 1761 circuits of varying sizes. Representation of Numeric Parameters. Due to considerations of practicability and to avoid dealing with symbolic representations of numeric parameters of gates, we convert the parameters to ﬂoats and introduce a threshold5 of ε = 10−8 ; numbers that diﬀer by less than this threshold are treated as equal, especially numbers less than ε are treated equal to zero. Consequently, some gates in the input circuits reduce to the identity gate; we remove those from the benchmark circuit in a preprocessing step. Test Settings. For purpose (i), we evaluate the inﬂuence of QCP with diﬀerent values for nmax on optimization passes provided by three well-established and widely accepted circuit optimizers—PyZX [14], Qiskit [20], and T|ket [24]. For that, we let those passes run on all benchmark circuits without QCP to create results for a baseline; these numbers are compared with those resulting from ﬁrst processing the circuits with QCP for diﬀerent nmax values and then applying 5  
   
  Based on personal discussion with Johannes Zeiher from the Max Planck Institute for Quantum Optics, gate parameters can be realized with a precision of π · 10−3 .  
   
  180  
   
  Y. Chen and Y. Stade  
   
  Fig. 7. This shows how many circuits remained unchanged, changed, or failed due to timeout (of one minute) or another error when ﬁrst applying QCP with nmax = 1024 and then the corresponding optimization tool.  
   
  those passes. For purpose (ii), we compare the results of the optimization composed by RPO and Qiskit with those when placing QCP before or after RPO into this pipeline. The above comparisons are both conducted for two metrics, namely gate count and control count. For purpose (iii), we record the running times of QCP alone on each input circuit. All experiments are executed on a server running Ubuntu 18.04.6 LTS with two Intel® Xeon® Platinum 8260 CPU @ 2.40 GHz processors oﬀering in total 48 physical cores. Pre-processing to Fit Circuit Optimizer. Each circuit optimizer supports only a speciﬁc gate set. Therefore, certain pre-processing is required to adapt the circuits to the circuit optimizer. This pre-processing includes parameter formatting, gate substitution, and gate decomposition. The latter modiﬁcation leads to a larger gate count than the input circuit. However, this larger gate count will already be included in our baseline for each circuit optimizer and hence, will not lead to a deterioration of the gate count through the optimization. 6.2  
   
  Results  
   
  Statistics of the Benchmark Suite. As mentioned in the previous section, we evaluate the QCP on 1761 circuits using between 2 and 129 qubits. The smallest circuits comprise only two gates, whereas the largest circuit contains almost 4.9 million gates. However, except for 16 circuits, the majority contain less than 50 thousand gates. The entire benchmark comprises 23.3 million gates and 22.5 million controls, of which approximately 17 thousand belong to a doubly controlled X-gate and the rest to single-controlled gates. The preprocessing of the circuits to make them suitable for the diﬀerent circuit optimizers must be considered a best-eﬀort approach. Consequently, some circuits still could not be parsed by the corresponding circuit optimizer. Figure 7 shows exemplarily how many of the 1761 circuits failed either due to a timeout of one minute or another error, remained unchanged regarding their gate count, or changed when ﬁrst applying QCP for nmax = 1024 and then the corresponding optimizer. Improvement of Standard Optimizers. Figure 8 shows a summary of the ﬁrst experiment; the plots show how many more gates and controls, respectively, could be removed in total over the entire benchmark utilizing QCP than just using the corresponding optimizer alone. The plots for Qiskit and T|ket show  
   
  Quantum Constant Propagation  
   
  181  
   
  Fig. 8. This plot depicts the aggregated number of gate count (top) and control count (bottom) reduction relative to the baseline, respectively, when applying QCP with diﬀerent values for nmax (x-axis) and then the corresponding optimizer. Note that a y-value greater than 0 corresponds to an improvement over the baseline of only performing the corresponding optimization alone (i. e., PyZX, Qiskit, or T|ket).  
   
  that the reduction of gates and controls increases gradually with the value of nmax . Note that the absolute numbers for PyZX are smaller since PyZX fails on a lot more circuits compared to the other two optimization tools. In any case, it is evident from the plot that QCP improves the result of each optimizer. Distribution of Relative Improvement. To show the impact of QCP in more detail, we calculate the relative gate reduction for each circuit by dividing the absolute gate reduction by the total gate count before optimization; analogously, we calculate the relative control reduction for every gate. Only for those circuits that fall into the category changed in Fig. 7, we plot the respective distribution of the relative gate and control count reduction. Figure 9 shows the histograms when applying QCP with nmax = 1024 before each circuit optimizer. In those plots, the width of each bin amounts to 0.02. We only plot these plots for nmax = 1024 because they look almost identical for other values of nmax . These plots show that the impact of QCP is small on the majority of circuits. However, some circuits beneﬁt considerably, especially when applying the optimizer T|ket afterward, which looks for patterns to replace with fewer gates; apparently, QCP modiﬁes the circuit such that more of those patterns occur in the circuit. Interaction with RPO. RPO [16] propagates the initial state through the circuit as long as the single qubits are in a pure state, see also Sect. 7. To achieve this type of state propagation in our framework, a value for nmax of two suﬃces. Still, QCP with nmax = 2 can track more information as RPO since also two basis states can suﬃce to express multiple qubits that are in a superposition of two basis states. Figure 10 and Fig. 11 depict the mutual inﬂuence of RPO and QCP. For values 1 and 2 for the parameter nmax , QCP does deteriorate the results of RPO when applied before RPO. This is because RPO also implements some circuit pattern matching together with circuit synthesis; when QCP destroys  
   
  182  
   
  Y. Chen and Y. Stade  
   
  Fig. 9. The relative reduction of gates (top) and controls (bottom) of the circuits that appear in the category changed in the plot from Fig. 7.  
   
  Fig. 10. Those two plots show the reduction of gates and controls, respectively, when applying QCP with diﬀerent values for nmax (x-axis) after RPO and ﬁnally Qiskit.  
   
  such a pattern, this optimization can not be applied at this position anymore. However, for larger values for nmax , those plots show that QCP ﬁnds additional optimization potential and is therefore not subsumed by RPO. When looking at Fig. 10, one can see that RPO even beneﬁts QCP: In this setting, approximately 10 times more gates can be removed compared to only using QCP with Qiskit afterward. These remarkable results are mainly due to two circuit families, namely qpeexact and qpeinexact, where RPO removes some controlled gates with their technique in the ﬁrst place and facilitates that QCP can remove even more controlled gates. Analysis of QCP Alone. QCP only fails on six circuits, of which one is a timeout, and ﬁve produce an error because of an unsupported gate. QCP needs the most time on the grover and qwalk circuits; on all other circuits, it ﬁnishes processing after at most 3.6 s. In general, the running time of QCP is high if it must track high entanglement for many gates. Accordingly, Fig. 12 shows the running time of QCP on the circuits that belong to the family of Quantum Fourier Transform.  
   
  Quantum Constant Propagation  
   
  183  
   
  Fig. 11. Those two plots show the reduction of gates and controls, respectively, when applying RPO after QCP with diﬀerent values for nmax (x-axis) and ﬁnally Qiskit.  
   
  Those produce maximum entanglement among the qubits where all possible basis states are represented at the end of the circuit. The plot displays the running time against the number of qubits. Note that the number of gates, and therefore the size of the circuit, grows quadratically with the number of qubits. A full simulation of those circuits would result in exponential running time. The plots indicate that QCP circumvents the exponential running time by limiting the number of basis states to express the state of an entanglement group by nmax . Explanation of Outliers. The plot in Fig. 12 shows outliers, especially for larger values for nmax . Those outliers indicate an exponential running time cut-oﬀ at a speciﬁc qubit count depending on the value of nmax . Considering the circuits reveals that due to the generation pattern of those circuits, the chunk of gates executed on the maximal possible entanglement gradually increases in size until the qubit count where the running time drops again. For example, the maximum outlier in the plot for nmax = 4096 is reached for 112 qubits. In this circuit, 271 gates are executed on the maximum entanglement comprising 4096 basis states without increasing the entanglement before the gate that increases the entanglement above the limit is processed. In the circuit for one more qubit, i. e., 113 qubits, just 13 gates are executed on the largest possible entanglement. This is due to the order in which the gates in the input ﬁle are arranged. In summary, those practical running time measurements underpin our theoretical statements from Sect. 5 since the exponential growth would continue unrestrained otherwise. The results provided indicate diﬀerences from existing optimizations. In the next section, we compare our proposed optimization with those and other optimization techniques on a broader basis.  
   
  7  
   
  Related Work  
   
  Other (Peephole) Optimizations. Existing optimization tools [2,20,24] mostly look for known patterns consisting of several gates that can be reduced to a smaller number of gates with the same eﬀect. A special case of those optimizations is gate cancellation that removes redundant gates: Many of the common gates are hermitian, i. e., they are self-inverse; when they appear twice directly after each other, both can be dropped without inﬂuencing the semantics of the program. When we applied the optimization tools mentioned at the beginning  
   
  184  
   
  Y. Chen and Y. Stade  
   
  Fig. 12. This plot shows the running time of QCP for diﬀerent values of nmax against the number of qubits (x-axis). The outliers occur due to the structure in which the circuits are generated; more details can be found in the text.  
   
  of this paragraph on the circuit shown in Fig. 1, none of those could reduce the circuit to the equivalent empty circuit. Bitwise Simulation. As already mentioned in Sect. 3.2, the idea to use a hash table to store the quantum state goes back to a simulator developed by Da Rosa et al. [7]. They use a hash table in the same way as we described in Sect. 3.2 with the basis states as keys and their associated amplitudes as values. However, our approach improves upon theirs by keeping qubits separated as long as they are not entangled following the idea in [3] and, hence, be able to store some quantum states even more eﬃciently. In contrast, Da Rosa et al. use one single hash table for the entire quantum state. Since they want to simulate the circuit and not optimize it as we aim for, they do not change to  if the computed quantum state becomes too complex. Consequently, their simulation runs still in exponential time even though it is not exponential in the number of qubits but rather in the degree of entanglement [7]. Initial-State Optimizations. Circuit optimization tools developed by Liu et al. [16] and Jang et al. [13] both take advantage of the initial state. Liu et al. leverage the information on the single-qubit state which could be eﬃciently determined at compile time [16]. They implement state automata to keep track of the single-qubit information on each pure state for circuit simpliﬁcations. Singlequbit information is lost though when a multi-qubit gate is applied except for a few special cases since a pure state could then turn into a mixed state. To tackle  
   
  Quantum Constant Propagation  
   
  185  
   
  this issue, users are allowed to insert annotations from which some single-qubit information can be recovered. Our approach, however, avoids treating qubits as independent of each other and tries to trace the global state of the quantum system, enabling us not to lose all the information on qubits even after applying a multi-qubit gate on them. The circuit optimizer proposed by Jang et al. aims to remove redundant control signals from controlled gates based on the state information [13]. Instead of classical simulation, they repeatedly perform quantum measurements at truncation points to determine state information. Besides, in order to consider the noise of quantum computers, they set thresholds depending on gate errors and the number of gates and drop observations that are below the thresholds. Although their approach is lower in computational cost compared to classical simulation, the fact that quantum measurements are needed disallows their tool to run at the compile time only, since shipping circuits to the quantum runtime is necessary for performing measurements. Additionally, in their scheme, it is assumed that the controlled gate in the circuit is either a Toﬀoli gate or a singly-controlled unitary operation denoted to avoid computations growing exponentially, therefore gate decompositions are needed to guarantee that the assumption holds. In contrast, our approach runs statically at compile time and no prior assumption or pre-processing is required for the success of the analysis. In addition, Markov et al. [17] and Vandersypen et al. [26] optimize their circuits manually using arguments based on initial-state information. Quantum Abstract Interpretation. Another point of view within the static analysis of quantum programs was established by Yu and Palsberg [28]. They introduce a way of abstract interpretation for quantum programs to verify assumptions on the ﬁnal state reached after the execution of the program, hence their focus is not on the optimization of the circuit but rather to verify its correctness. Interestingly, their approach to focus on a particular set of qubits mimics our separation of entanglement groups, or to put it the other way around, our separation can be seen as one instantiation of their abstract domain just that we allow to alter the groups during simulation of the circuit instead of keeping them ﬁxed over the entire computation as they do. Consequently, our approach dynamically adapts to the current circuit whereas Yu and Palsberg need to ﬁx the set of qubits to focus on statically for their quantum abstract interpretation. Classical Constant Propagation. When designing our optimization we were inspired by constant propagation known from classical compiler optimizations for interprocedural programs such as C/C++ programs [22]. However, our QCP diﬀers fundamentally from classical constant propagation: In our case, we just need to pass the information along a linear list of instructions (the gates); the problem here is the sheer mass of information that needs to be tracked. In the classical case, the challenge is to deal with structural program elements such as loops and conditional branches that prevent linearly passing information about values. Here, a constraint system consisting of equations over an abstract domain is derived from the program which then needs to be solved.  
   
  186  
   
  8  
   
  Y. Chen and Y. Stade  
   
  Conclusions  
   
  Summary. In our work, we take the idea of utilizing the most common execution condition of quantum circuits where the initial states of all qubits are in |0 and propose our optimization, QCP, which simulates circuits in a restricted but computationally eﬃcient way and has demonstrated its power in one of the circuit optimization tasks, namely control reduction. In addition, QCP works in harmony with quantum computers: QCP runs in polynomial time and hence can be executed eﬃciently on classical computers, the output of QCP, optimized circuits, which cannot be eﬃciently simulated on classical computers, are submitted to quantum computers for execution. That is, we let the classical computer do all where it is good at and leave only the rest for the quantum computer. The success of QCP not only proves the value that resides within initial state information but also contributes to the research on quantum circuit optimization based on methods of static analysis running on classical computers. It is already clear that quantum circuits are expected to grow larger and larger, where building blocks containing multi-controlled gates will be heavily used. For example, OpenQASM 3.0, a highly accepted Quantum assembly language for circuit description, allows users to write arbitrarily many controls for gates [6]. Therefore, it is likely that our QCP will play to its strengths even more in the future. Future Work. It is worthwhile to consider other abstract domains, e. g., the one used by Yu and Palsberg [28] that keep partial information about the state and still maintain the eﬃciency we desire. Additionally, it could be useful for QCP to consider an abstract state of meta-superposition which stores possible states after the measurement in a probability distribution. The use of metasuperposition would allow QCP to simulate circuits with intermediate measurements, i. e., measurements that happen not at the end of the circuit. We also plan to incorporate and evaluate the idea of the threshold from [13], so that QCP will be able to discard basis states that are not signiﬁcant to the simulation and will be indistinguishable from noise on a real quantum computer. Besides, currently QCP is not able to detect when qubits become separable again after they were entangled. Implementing such detection facilitates keeping more state information and thus performs better optimizations. Another direction is to increase the capability of the control reduction itself: For this, we want to generalize the ideas proposed in [16] that use only pure state information of single qubits, to our setting. This includes replacing fully simulated parts of the circuit by means of circuit synthesis methods, such as KAK decomposition [25]. It is possible that performing QCP causes a loss of opportunities for other optimizations. So, one might also be interested to study how to determine the optimal order to perform diﬀerent optimization passes. Acknowledgements. We thank our reviewers for their tremendous eﬀorts and their helpful suggestions. This has greatly improved the quality of this article. We are grateful to our supervisor Helmut Seidl for many fruitful discussions and his support at all times.  
   
  Quantum Constant Propagation  
   
  187  
   
  Johannes Zeiher from Max Planck Institute of Quantum Optics has also provided us with precious advice. This work has been supported in part by the Bavarian state government through the project Munich Quantum Valley with funds from the Hightech Agenda Bayern Plus.  
   
  References 1. Aaronson, S., Chen, L.: Complexity-Theoretic Foundations of Quantum Supremacy Experiments (2016). https://doi.org/10.48550/ARXIV.1612.05903 2. Amy, M., Gheorghiu, V.: Staq - a full-stack quantum processing toolkit. Quantum Sci. Technol. 5(3), 034016 (2020). https://doi.org/10.1088/2058-9565/ab9359 3. Bauer-Marquart, F., Leue, S., Schilling, C.: symQV: automated symbolic veriﬁcation of quantum programs. In: Chechik, M., Katoen, J.P., Leucker, M. (eds.) FM 2023. LNCS, vol. 14000, pp. 181–198. Springer, Cham (2023). https://doi.org/10. 1007/978-3-031-27481-7 12 4. Chen, Y., Stade, Y.: Artifact for Quantum Constant Propagation (2023). https:// doi.org/10.5281/zenodo.8033829 5. Chow, J., Dial, O., Gambetta, J.: IBM Quantum breaks the 100-qubit processor barrier (2021). https://research.ibm.com/blog/127-qubit-quantum-processoreagle 6. Cross, A.W., et al.: OpenQASM 3: a broader and deeper quantum assembly language. ACM Trans. Quantum Comput. 3(3), 1–50 (2022). https://doi.org/10.1145/ 3505636 7. Da Rosa, E.C.R., De Santiago, R.: Ket quantum programming. J. Emerg. Technol. Comput. Syst. 18(1), 1–25 (2022). https://doi.org/10.1145/3474224 8. Farhi, E., Goldstone, J., Gutmann, S., Zhou, L.: The quantum approximate optimization algorithm and the Sherrington-Kirkpatrick model at inﬁnite size. Quantum 6, 759 (2022). https://doi.org/10.22331/q-2022-07-07-759 9. Feynman, R.P.: Simulating physics with computers. Int. J. Theor. Phys. 21(6), 467–488 (1982). https://doi.org/10.1007/BF02650179 10. Grover, L.K.: A fast quantum mechanical algorithm for database search. In: Proceedings Twenty-Eighth Annual ACM Symposium Theory Computing, STOC 1996, Philadelphia, Pennsylvania, USA, pp. 212–219. ACM Press (1996). https:// doi.org/10.1145/237814.237866 11. Haferkamp, J., Hangleiter, D., Bouland, A., Feﬀerman, B., Eisert, J., BermejoVega, J.: Closing gaps of a quantum advantage with short-time Hamiltonian dynamics. Phys. Rev. Lett. 125(25), 250501 (2020). https://doi.org/10.1103/ PhysRevLett.125.250501 12. Hidary, J.D.: Quantum Computing: An Applied Approach. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-83274-2 13. Jang, W., et al.: Initial-state dependent optimization of controlled gate operations with quantum computer. Quantum 6, 798 (2022). https://doi.org/10.22331/ q-2022-09-08-798 14. Kissinger, A., van de Wetering, J.: PyZX: large scale automated diagrammatic reasoning. Electron. Proc. Theor. Comput. Sci. 318, 229–241 (2020). https://doi. org/10.4204/EPTCS.318.14 15. Knill, E.: Quantum computing with very noisy devices. Nature 434(7029), 39–44 (2005). https://doi.org/10.1038/nature03350  
   
  188  
   
  Y. Chen and Y. Stade  
   
  16. Liu, J., Bello, L., Zhou, H.: Relaxed peephole optimization: a novel compiler optimization for quantum circuits. In: 2021 IEEE/ACM International Symposium on Code Generation and Optimization, CGO, Seoul, Korea (South), pp. 301–314. IEEE (2021). https://doi.org/10.1109/CGO51591.2021.9370310 17. Markov, I.L., Saeedi, M.: Constant-Optimized Quantum Circuits for Modular Multiplication and Exponentiation (2015) 18. Nielsen, M.A., Chuang, I.L.: Quantum Computation and Quantum Information: 10th Anniversary Edition, 1st edn. Cambridge University Press (2012). https:// doi.org/10.1017/CBO9780511976667 19. Peruzzo, A., et al.: A variational eigenvalue solver on a photonic quantum processor. Nat. Commun. 5(1), 4213 (2014). https://doi.org/10.1038/ncomms5213 20. Qiskit contributors: Qiskit: an open-source framework for quantum computing (2023). https://doi.org/10.5281/zenodo.2573505 21. Quetschlich, N., Burgholzer, L., Wille, R.: MQT Bench: Benchmarking Software and Design Automation Tools for Quantum Computing (2022). https://doi.org/ 10.48550/arXiv.2204.13719 22. Seidl, H., Wilhelm, R., Hack, S.: Compiler Design: Analysis and Transformation. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-17548-0 23. Shor, P.: Algorithms for quantum computation: discrete logarithms and factoring. In: Proceedings 35th Annual Symposium on Foundations of Computer Science, Santa Fe, NM, USA, pp. 124–134. IEEE Computer Society Press (1994). https:// doi.org/10.1109/SFCS.1994.365700 24. Sivarajah, S., Dilkes, S., Cowtan, A., Simmons, W., Edgington, A., Duncan, R.: T$|$ket$\rangle$: a retargetable compiler for NISQ devices. Quantum Sci. Technol. 6(1), 014003 (2021). https://doi.org/10.1088/2058-9565/ab8e92 25. Tucci, R.R.: An Introduction to Cartan’s KAK Decomposition for QC Programmers (2005) 26. Vandersypen, L.M.K., Steﬀen, M., Breyta, G., Yannoni, C.S., Sherwood, M.H., Chuang, I.L.: Experimental realization of Shor’s quantum factoring algorithm using nuclear magnetic resonance. Nature 414(6866), 883–887 (2001). https://doi.org/ 10.1038/414883a 27. Wu, X.C., Davis, M.G., Chong, F.T., Iancu, C.: QGo: Scalable Quantum Circuit Optimization Using Automated Synthesis (2020). https://doi.org/10.48550/ ARXIV.2012.09835 28. Yu, N., Palsberg, J.: Quantum abstract interpretation. In: Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pp. 542–558. ACM, Virtual Canada (2021). https://doi.org/10. 1145/3453483.3454061  
   
  Quantum Constant Propagation  
   
  189  
   
  Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapter’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.  
   
  Error Invariants for Fault Localization via Abstract Interpretation Aleksandar S. Dimovski(B) Mother Teresa University, st. Mirche Acev nr. 4, 1000 Skopje, North Macedonia [email protected]  https://aleksdimovski.github.io/ Abstract. Fault localization aims to automatically identify the cause of an error in a program by localizing the error to a relatively small part of the program. In this paper, we present a novel technique for automated fault localization via error invariants inferred by abstract interpretation. An error invariant for a location in an error program over-approximates the reachable states at the given location that may produce the error, if the execution of the program is continued from that location. Error invariants can be used for statement-wise semantic slicing of error programs and for obtaining concise error explanations. We use an iterative reﬁnement sequence of backward-forward static analyses by abstract interpretation to compute error invariants, which are designed to explain why an error program violates a particular assertion. We demonstrate the eﬀectiveness of our approach to localize errors in realistic C programs. Keywords: Fault localization · Error invariants interpretation · Statement-wise semantic slicing  
   
  1  
   
  · Abstract  
   
  Introduction  
   
  Static program analyzers [6,8,22,27,36] are today often applied to ﬁnd errors in real-world programs. They usually return an error report, which shows how an assertion can be violated. However, the programmers still need to process the error report, in order to isolate the cause of an error to a manageable number of statements and variables that are relevant for the error. Using this information, they can subsequently repair the given program either manually or automatically by running specialized program repair tools [31,33]. In this paper, we present a novel technique for automated fault localization, which automatically generates concise error explanations in the form of c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  M. V. Hermenegildo and J. F. Morales (Eds.): SAS 2023, LNCS 14284, pp. 190–211, 2023. https://doi.org/10.1007/978-3-031-44245-2_10  
   
  Error Invariants for Fault Localization via Abstract Interpretation  
   
  191  
   
  statements relevant for a given error that describe the essence of why the error occurred. In particular, we describe a fault localization technique based on socalled error invariants inferred via abstract interpretation. An error invariant for a given location in a program captures states that may produce the error, that is, there may be executions of the program continued from that location violating a given assertion. We observe that the same error invariants that hold for consecutive locations characterize statements in the program that are irrelevant for the error. A statement that is enclosed by the same error invariant does not change the nature of the error. Hence, error invariants can be used to ﬁnd only relevant statements and information about reachable states that helps to explain the cause of an error. They also identify the relevant variables whose values should be tracked when executing the program. The obtained relevant statements constitute the so-called statement-wise semantic slicing of the error program, which can be changed (repaired) to make the entire program correct. Abstract interpretation [7,29] is a general theory for approximating the semantics of programs. It has been successfully applied to deriving computable and approximated static analysis that infer dynamic properties of programs, due to its soundness guarantee (all conﬁrmative answers are indeed correct) and scalability (with a good trade-oﬀ between precision and cost). In this paper, we focus on applying abstract interpretation to automate fault localization via inferring error invariants. More speciﬁcally, we use a combination of backward and forward reﬁning analyses based on abstract interpretation to infer error invariants from an error program. Each next iteration of backward-forward analyses produces more reﬁned error invariants than the previous iteration. Finally, the error invariants found in the last iteration are used to compute a slice of the error program that contains only relevant statements for the error. The backward (over-approximating) numerical analysis is used for computing the necessary preconditions of violating the target assertion, thus reducing the input space that needs further exploration. Error invariants are constructed by going backwards step-by-step starting at the property violation, i.e. by propagating the negated assertion backwards. The negated assertion represents an error state space. When there is a precision loss caused by merging the branches of an if statement, we collect in a set of predicates the branching condition of that conditional. Subsequently, the forward (over-approximating) numerical analysis of a program with reduced abstract sub-input is employed to reﬁne error invariants in all locations, thus also reﬁning (reducing) the error state space. Based on the inferred error invariants, we can ﬁnd the relevant statements and relevant variables for the assertion violation. Initially, in the ﬁrst iteration, both analyses are performed on a base abstract domain (e.g., intervals, octagons, polyhedra). In the subsequent iterations, we use the set of predicates generated by the previous backward analysis to design a binary decision diagram (BDD) abstract domain functor, which can express disjunctive properties with respect to the given set of predicates. A decision node in a BDD abstract element stores a predicate, and each leaf node stores an abstract value from a base abstract domain under speciﬁc evaluation results of predicates. When the obtained set of predicates as  
   
  192  
   
  A. S. Dimovski  
   
  well as the (abstract) error state sub-space stay the same over two iterations, the iterative process stops and reports the inferred error invariants. Otherwise, the reﬁnement process continues by performing backward-forward analyses on a BDD abstract domain based on the reﬁned set of predicates as well as on a reduced error state sub-space. The BDD abstract domain and the reduced error state sub-space enable our analyses in the subsequent iterations to focus on smaller state sub-spaces, i.e. partitionings of the total state space, in each of which the program may involve fewer disjunctive or non-linear behaviours and thus the analyses may produce more precise results. We have implemented our abstract interpretation-based approach for fault localization of C programs in a prototype tool. It takes as input a C program with an assertion, and returns a set of statements whose replacement can eliminate the error. The tool uses the numerical abstract domains (e.g., intervals, octagons, polyhedra) from the APRON library [25], and the BDD abstract domains from the BDDAPRON library [24]. BDDAPRON uses any abstract domain from the APRON library for the leaf nodes. The tool also calls the Z3 SMT solver [30] to compute the error invariants from the information inferred via abstract interpretation-based analyses. We discuss a set of C programs from the literature, SV-COMP and TCAS suites that demonstrate the usefulness of our tool. In summary, this work makes the following contributions: (1) We deﬁne error invariants as abstract representations of the reason why the program may go wrong if it is continued from that location; (2) We propose an iterative abstract interpretation-based analyses to compute error invariants of a program. They are used to identify statements and program variables that are relevant for the fault in the program; (3) We implemented the approach in a prototype tool, which uses domains from APRON and BDDAPRON libraries as well as Z3 SMT solver; (4) We evaluate our approach for fault localization on a set of C benchmarks.  
   
  2  
   
  Motivating Examples  
   
  We demonstrate our technique for fault localization using the illustrative examples in Figs. 1, 2, and 3. The ﬁrst example, program1, in Fig. 1 shows a program code that violates the assertion (x > y) for all values of the parameter x, since y = x + 1 holds at the end of the program. A static analysis of this program will establish the assertion violation. However, the static analysis returns a full list of invariants in all locations of the program, including details that are irrelevant for the speciﬁc error. Similarly, other veriﬁcation tools will also report many irrelevant details for the error (e.g. full execution paths). Our technique works as follows. We begin with the ﬁrst iteration of the backward-forward analyses. The backward analysis deﬁned over the Polyhedra 4 By propagating it domain starts with the negated assertion (x ≤ y) at loc. . 3 2 and  backwards, it infers the preconditions: (x ≤ y) at loc. , (x ≤ z) at loc. , 1 1 and at loc. . The subsequent forward analysis starts with invariant  at loc. , 2 3 (z = x+1∧y = x+1) at loc. , and then infers invariants: (z = x+1) at loc. ,  
   
  Error Invariants for Fault Localization via Abstract Interpretation void main(int input){ input ≤ 41 1 int x := 1;   
   
  input ≤ 41 2 int y := input − 42;   
   
  B ∧ input ≤ 41 ∧ y = input − 42  
   
  void main(int x){  1 int z := x + 1;  z =x+1 2 int y := z;  y =x+1 3 z := z+1;  y =x+1 4 assert (x > y);  Fig. 1. program1.  
   
  3 if (y0);   
   
  Fig. 2. program2 (B ≡ (y Positive_RA_Alt_Tresh) from function Non_Crossing_Biased_Climb() is problematic, which causes the error. It should be replaced with ‘≥’ for the implementation to be correct. Our tool ﬁrst inlines all functions into the main() function, which is then analyzed statically. Thus, the complete program has 308 locations in total, and the main() function after inlinement contains 118 locations. Full_AI needs two iterations to terminate by using the BDD domain with four predicates:  
   
  Error Invariants for Fault Localization via Abstract Interpretation  
   
  Fig. 9. An excerpt from an error TCAS implementation.  
   
  205  
   
  206  
   
  A. S. Dimovski  
   
  B1 ≡ (need_upward_RA=1) ∧ (need_downward_RA=1) B2 ≡ (Own_Tracked_Alt < Other_Tracked_Alt) B3 ≡ (own_above_threat=0) ∧ (Curr_Vertical_Sep ≥ MINSP)∧ (Up_Separation ≥ Positive_RA_Alt_Tresh) B4 ≡ (own_below_threat=0)∨ (own_below_threat=1 ∧ ¬(Down_Separation > Positive_RA_Alt_Tresh)) The slice computed by Full_AI approach contains 44 locations relevant for the error. Some of these statements are shown underlined in Fig. 9. Note that not underlined else branches are classiﬁed as irrelevant. The reported relevant statements are suﬃcient to understand the origins of the error. The generated slice depends only on 15 variables instead of 37 variables in the original program. The number of input variables is also reduced from 12 to 6. Thus, we conclude that the obtained slice signiﬁcantly reduces the search space for error statements. The single backward analysis Single_AI reports a slice containing only 28 locations. However, the slice does not contain any statement from the buggy Non_Crossing_Biased_Climb(), thus missing the real reasons for the error. On the other hand, the BugAssist tool reports as potential bugs only 2 locations, the condition ‘if (enabled=1) . . .’ and the assertion, both from alt_sep_test() function. Similarly as in the case of Single_AI, none of these locations is from the buggy Non_Crossing_Biased_Climb(). Performances. Table 1 shows the result of running our tool Full_AI, the single backward analysis Single_AI, and the BugAssist tool on the benchmarks considered so far. The column “LOC#” is the total number of locations in the program, “Time” shows the run-time in seconds, “Slice#” is the number of potential (relevant) fault locations, and “Perc%” is the percentage precision of the given approach to locate the relevant statements for the error. This is the ratio of the sum of correctly classiﬁed erroneous and non-erroneous locations by an approach to the total number of locations in the program. A classiﬁcation of a location as erroneous given by the concrete semantics is considered correct. We conclude that our technique, Full_AI, gives more precise information about potential error statements than simply performing a backward analysis and BugAssist. In fact, Full_AI pin-pointed the correct error locations for all examples. On average, the number of locations to check for potential error (Slice#) is reduced to 47.6% of the total code (LOC#). On the other hand, the precision of Single_AI is 70% and the precision of BugAssist is 64%, on average. Although our technique Full_AI is the most precise, it is slower than Single_AI due to the several iterations it needs to produce the fully reﬁned error invariants. Full_AI and BugAssist have often comparable running times, except for the loop benchmarks when BugAssist is slower due to the need to unwind the loops. Moreover, Full_AI reports more ﬁne-grained information by identifying relevant variables for the error, whereas BugAssist reports only potential bug locations. Finally, we should note that the run-time of our technique Full_AI in all examples is still signiﬁcantly smaller than our human eﬀort required to isolate the fault.  
   
  Error Invariants for Fault Localization via Abstract Interpretation  
   
  207  
   
  Table 1. Performance results of Full_AI vs. Single_AI vs. BugAssist. Full_AI and Single_AI use Polyhedra domain. All times in sec. Bench.  
   
  LOC# Full_AI  
   
  Single_AI  
   
  BugAssist  
   
  Time Slice# Perc% Time Slice# Prec% Time Slice# Perc%  
   
  6  
   
  program1  
   
  6  
   
  0.056 2  
   
  100% 0.013 2  
   
  100% 0.031 1  
   
  66%  
   
  program2  
   
  9  
   
  0.187 5  
   
  100% 0.013 4  
   
  83%  
   
  0.031 2  
   
  50%  
   
  program2-a 9  
   
  0.088 2  
   
  100% 0.015 6  
   
  33%  
   
  0.030 2  
   
  66%  
   
  0.453 3  
   
  100% 0.016 3  
   
  100% 2.954 5  
   
  71%  
   
  program3  
   
  10  
   
  easy2-1  
   
  15  
   
  2.401 10  
   
  100% 0.011 3  
   
  41%  
   
  8.441 9  
   
  66%  
   
  Mysore-1  
   
  9  
   
  0.050 4  
   
  100% 0.014 6  
   
  66%  
   
  0.210 3  
   
  83%  
   
  TCASv.1  
   
  118  
   
  57.78 44  
   
  100% 0.225 28  
   
  86%  
   
  0.095 1  
   
  62%  
   
  Related Work  
   
  Fault localization has been an active area of research in recent years [4,5,20, 26,33]. The most successful approaches for fault localization are based on logic formulae [4,5,20,26,33]. They represent an error trace using an SMT formula and analyze it to ﬁnd suspicious locations. Hence, they assume the existence of error traces on input. The error traces are usually obtained either from failing test cases or from counterexamples produced by external veriﬁcation tools. In contrast, our approach is directly applied on (error) programs, thus it needs no speciﬁc error traces from other tools making it more self-contained. This way, the two phases of error-detection and error-localization are integrated by our approach. The closest to our approach for inferring error invariants applied to fault localization is the work proposed by Ermis et al. [4,20]. They use Craig interpolants and SMT queries to calculate error invariants in an error trace. Another similar approach that uses error traces and SAT queries is BugAssist [26]. It uses MAX-SAT based algorithm to identify a maximal subset of statements from an error trace that are not needed to prove the unsatisﬁability of the logical formula representing the error trace. One limitation of BugAssist is that control-dependent variables and statements are not considered relevant. Moreover, BugAssist do not report error invariants, which can be especially useful for dense errors where the error program cannot be sliced signiﬁcantly. Hence, BugAssist cannot identify relevancy of variables. Other logic formula-based approaches include using weakest preconditions [5], and syntactic information in the form of graphs for static and dynamic dependency relations [33] to localize the errors. Rival [32] uses abstract interpretation static analyzer ASTREE [9] to investigate the found alarms and to classify them as true errors or false errors. It uses an reﬁning sequence of forward-backward analyses to obtain an approximation of a subset of traces that may lead to an error. Hence, the above work aims to ﬁnd a set of traces resulting in an error, thus deﬁning so-called tracewise semantic slicing. In contrast, our approach aims to ﬁnd statements that  
   
  208  
   
  A. S. Dimovski  
   
  are reasons for the error, thus deﬁning the statement-wise semantic slicing. The under-approximated backward analysis proposed by Mine [28] infers suﬃcient preconditions ensuring that the target property holds for all non-deterministic choices. It would produce the under-approximations of concrete error invariants if applied to our approach. We could then combine the results of under- and overapproximating error invariants, so that if both are the same for some locations we can be certain that the corresponding statements are either error-relevant or error-irrelevant. The work [19] also uses forward-backward analyses to estimate the probability that a target assertion os satisﬁed/violated. Decision-tree domains have been used in abstract interpretation community recently [3,10,35]. Segmented decision tree abstract domains have enabled path dependent static analysis [3,10]. Their elements contain decision nodes that are determined either by values of program variables [10] or by the if conditions [3], whereas the leaf nodes are numerical properties. Urban and Miné [35] use decision tree abstract domains to prove program termination. Decision nodes are labelled with linear constraints that split the memory space and leaf nodes contain aﬃne ranking functions for proving termination. Recently, specialized decision tree lifted domains have been proposed to analyze program families (or any other conﬁgurable software system) [11,12,16,18]. Decision nodes partition the conﬁguration space of possible feature values (or statically conﬁgurable options), while leaf nodes provide analysis information of program variants (family members) corresponding to each partition. The work [11] uses lifted BDD domains to analyze program families with Boolean features. Subsequently, the lifted decision tree domain has been proposed to handle program families with both Boolean and numerical features [18], as well as dynamic program families with features changing during run-time [16]. Once a set of statements relevant for the error has been found, we need to replace those statements in order to ﬁx the error. Recently, abstract interpretation has been successfully applied to program sketching [13,15,17]. The above works leverage a lifted (family-based) static analysis to synthesize program sketches, which represent partial programs with some missing integer holes in them [34]. We can combine our approach for fault localization with the techniques for program sketches to develop an automatic procedure for program repair.  
   
  7  
   
  Conclusion  
   
  In this work, we have proposed error invariants for reasoning about the relevancy of portions of an error program. They provide a semantic argument why certain statements are irrelevant for the cause of an error. We have presented an algorithm that infers error invariants via abstract interpretation and uses them to obtain compact slices of error programs relevant for the error. Our evaluation demonstrates that our algorithm provides useful error explanations.  
   
  Error Invariants for Fault Localization via Abstract Interpretation  
   
  209  
   
  References 1. Bourdoncle, F.: Abstract debugging of higher-order imperative languages. In: Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation (PLDI), pp. 46–55. ACM (1993). https://doi.org/10. 1145/155090.155095 2. Bryant, R.E.: Graph-based algorithms for Boolean function manipulation. IEEE Trans. Comput. 35(8), 677–691 (1986). https://doi.org/10.1109/TC.1986.1676819 3. Chen, J., Cousot, P.: A binary decision tree abstract domain functor. In: Blazy, S., Jensen, T. (eds.) SAS 2015. LNCS, vol. 9291, pp. 36–53. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48288-9_3 4. Christ, J., Ermis, E., Schäf, M., Wies, T.: Flow-sensitive fault localization. In: Giacobazzi, R., Berdine, J., Mastroeni, I. (eds.) VMCAI 2013. LNCS, vol. 7737, pp. 189–208. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-358739_13 5. Christakis, M., Heizmann, M., Mansur, M.N., Schilling, C., Wüstholz, V.: Semantic fault localization and suspiciousness ranking. In: Vojnar, T., Zhang, L. (eds.) TACAS 2019, Part I. LNCS, vol. 11427, pp. 226–243. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-17462-0_13 6. Clarke, E., Kroening, D., Lerda, F.: A tool for checking ANSI-C programs. In: Jensen, K., Podelski, A. (eds.) TACAS 2004. LNCS, vol. 2988, pp. 168–176. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-24730-2_15 7. Cousot, P., Cousot, R.: Abstract interpretation: a uniﬁed lattice model for static analysis of programs by construction or approximation of ﬁxpoints. In: Conference Record of the Fourth ACM Symposium on POPL, pp. 238–252. ACM (1977). https://doi.org/10.1145/512950.512973, http://doi.acm.org/10.1145/512950.512973 8. Cousot, P., et al.: The ASTREÉ analyzer. In: Sagiv, M. (ed.) ESOP 2005. LNCS, vol. 3444, pp. 21–30. Springer, Heidelberg (2005). https://doi.org/10.1007/978-3540-31987-0_3 9. Cousot, P., Cousot, R., Feret, J., Mauborgne, L., Miné, A., Rival, X.: Why does ASTRÉE scale up? Formal Methods Syst. Design 35(3), 229–264 (2009). https:// doi.org/10.1007/s10703-009-0089-6 10. Cousot, P., Cousot, R., Mauborgne, L.: A scalable segmented decision tree abstract domain. In: Manna, Z., Peled, D.A. (eds.) Time for Veriﬁcation. LNCS, vol. 6200, pp. 72–95. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-137549_5 11. Dimovski, A.S.: A binary decision diagram lifted domain for analyzing program families. J. Comput. Lang. 63, 101032 (2021). https://doi.org/10.1016/j.cola.2021. 101032 12. Dimovski, A.S.: Lifted termination analysis by abstract interpretation and its applications. In: GPCE 2021: Concepts and Experiences, Chicago, IL, USA, October 2021, pp. 96–109. ACM (2021). https://doi.org/10.1145/3486609.3487202 13. Dimovski, A.S.: Quantitative program sketching using lifted static analysis. In: FASE 2022. LNCS, vol. 13241, pp. 102–122. Springer, Cham (2022). https://doi. org/10.1007/978-3-030-99429-7_6 14. Dimovski, A.S.: Artifact for the paper “error invariants for fault localization via abstract interpretation”. Zenodo (2023). https://doi.org/10.5281/zenodo.8167960 15. Dimovski, A.S.: Quantitative program sketching using decision tree-based lifted analysis. J. Comput. Lang. 75, 101206 (2023). https://doi.org/10.1016/j.cola.2023. 101206  
   
  210  
   
  A. S. Dimovski  
   
  16. Dimovski, A.S., Apel, S.: Lifted static analysis of dynamic program families by abstract interpretation. In: 35th European Conference on Object-Oriented Programming, ECOOP 2021. LIPIcs, vol. 194, pp. 14:1–14:28. Schloss Dagstuhl Leibniz-Zentrum für Informatik (2021). https://doi.org/10.4230/LIPIcs.ECOOP. 2021.14 17. Dimovski, A.S., Apel, S., Legay, A.: Program sketching using lifted analysis for numerical program families. In: Dutle, A., Moscato, M.M., Titolo, L., Muñoz, C.A., Perez, I. (eds.) NFM 2021. LNCS, vol. 12673, pp. 95–112. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-76384-8_7 18. Dimovski, A.S., Apel, S., Legay, A.: Several lifted abstract domains for static analysis of numerical program families. Sci. Comput. Program. 213, 102725 (2022). https://doi.org/10.1016/j.scico.2021.102725 19. Dimovski, A.S., Legay, A.: Computing program reliability using forward-backward precondition analysis and model counting. In: FASE 2020. LNCS, vol. 12076, pp. 182–202. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-45234-6_9 20. Ermis, E., Schäf, M., Wies, T.: Error invariants. In: Giannakopoulou, D., Méry, D. (eds.) FM 2012. LNCS, vol. 7436, pp. 187–201. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-32759-9_17 21. Graves, T.L., Harrold, M.J., Kim, J., Porter, A.A., Rothermel, G.: An empirical study of regression test selection techiques. ACM Trans. Softw. Eng. Methodol. 10(2), 184–208 (2001). https://doi.org/10.1145/367008.367020 22. Greitschus, M., Dietsch, D., Heizmann, M., Nutz, A., Schätzle, C., Schilling, C., Schüssele, F., Podelski, A.: Ultimate taipan: trace abstraction and abstract interpretation. In: Legay, A., Margaria, T. (eds.) TACAS 2017, Part II. LNCS, vol. 10206, pp. 399–403. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3662-54580-5_31 23. Harris, W.R., Sankaranarayanan, S., Ivancic, F., Gupta, A.: Program analysis via satisﬁability modulo path programs. In: Proceedings of the 37th ACM SIGPLANSIGACT Symposium on Principles of Programming Languages, POPL 2010, Madrid, Spain, 17–23 January 2010, pp. 71–82. ACM (2010). https://doi.org/10. 1145/1706299.1706309 24. Jeannet, B.: Relational interprocedural veriﬁcation of concurrent programs. In: Seventh IEEE International Conference on Software Engineering and Formal Methods, SEFM 2009, pp. 83–92. IEEE Computer Society (2009). https://doi.org/10. 1109/SEFM.2009.29 25. Jeannet, B., Miné, A.: Apron: a library of numerical abstract domains for static analysis. In: Bouajjani, A., Maler, O. (eds.) CAV 2009. LNCS, vol. 5643, pp. 661– 667. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-02658-4_52 26. Jose, M., Majumdar, R.: Cause clue clauses: error localization using maximum satisﬁability. In: Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2011, pp. 437–446. ACM (2011). https://doi.org/10.1145/1993498.1993550 27. King, J.C.: Symbolic execution and program testing. Commun. ACM 19(7), 385– 394 (1976). https://doi.org/10.1145/360248.360252 28. Miné, A.: Backward under-approximations in numeric abstract domains to automatically infer suﬃcient program conditions. Sci. Comput. Program. 93, 154–182 (2014). https://doi.org/10.1016/j.scico.2013.09.014 29. Miné, A.: Tutorial on static inference of numeric invariants by abstract interpretation. Found. Trends Program. Lang. 4(3–4), 120–372 (2017). https://doi.org/10. 1561/2500000034  
   
  Error Invariants for Fault Localization via Abstract Interpretation  
   
  211  
   
  30. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78800-3_24 31. Nguyen, H.D.T., Qi, D., Roychoudhury, A., Chandra, S.: SemFix: program repair via semantic analysis. In: 35th International Conference on Software Engineering, ICSE 2013, pp. 772–781. IEEE Computer Society (2013). https://doi.org/10.1109/ ICSE.2013.6606623 32. Rival, X.: Understanding the origin of alarms in ASTRÉE. In: Hankin, C., Siveroni, I. (eds.) SAS 2005. LNCS, vol. 3672, pp. 303–319. Springer, Heidelberg (2005). https://doi.org/10.1007/11547662_21 33. Rothenberg, B.-C., Grumberg, O.: Must fault localization for program repair. In: Lahiri, S.K., Wang, C. (eds.) CAV 2020, Part II. LNCS, vol. 12225, pp. 658–680. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-53291-8_33 34. Solar-Lezama, A.: Program sketching. STTT 15(5–6), 475–495 (2013). https:// doi.org/10.1007/s10009-012-0249-7 35. Urban, C., Miné, A.: A decision tree abstract domain for proving conditional termination. In: Müller-Olm, M., Seidl, H. (eds.) SAS 2014. LNCS, vol. 8723, pp. 302–318. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10936-7_19 36. Yin, B., Chen, L., Liu, J., Wang, J., Cousot, P.: Verifying numerical programs via iterative abstract testing. In: Chang, B.-Y.E. (ed.) SAS 2019. LNCS, vol. 11822, pp. 247–267. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-32304-2_13  
   
  Generalized Program Sketching by Abstract Interpretation and Logical Abduction Aleksandar S. Dimovski(B) Mother Teresa University, st. Mirche Acev nr. 4, 1000 Skopje, North Macedonia [email protected]  https://aleksdimovski.github.io/ Abstract. This paper presents a new approach for synthesizing missing parts from imperative programs by using abstract interpretation and logical abduction. Given a partial program with missing arbitrary expressions, our approach synthesizes concrete expressions that are strong enough to prove the assertions in the given program. Furthermore, the synthesized elements by our approach are the simplest and the weakest among all possible that guarantee the validity of assertions. In particular, we use a combination of forward and backward numerical analyses based on abstract interpretation to generate constraints that are solved by using the logical abduction technique. We have implemented our approach in a prototype synthesis tool for C programs, and we show that the proposed approach is able to successfully synthesize arithmetic and boolean expressions for various C programs. Keywords: Program Synthesis Abduction  
   
  1  
   
  · Abstract interpretation · Logical  
   
  Introduction  
   
  Program synthesis [2] is a task of inferring a program that satisﬁes a given speciﬁcation. A sketch [32,33] is a partial program with missing arithmetic and boolean expressions called holes, which need to be discovered by the synthesizer. Previous approaches for program sketching [19,32,33] automatically synthesize only integer “constants” for the holes so that the resulting complete program satisﬁes given assertions for all possible inputs. However, it is more challenging to deﬁne a synthesis algorithm that infers arbitrary arithmetic and boolean expressions in program sketches. We refer to this as generalized sketching problem. In this paper, we propose a new approach for solving the generalized sketching problem by using abstract interpretation [5,29,34] and logical abduction c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  M. V. Hermenegildo and J. F. Morales (Eds.): SAS 2023, LNCS 14284, pp. 212–230, 2023. https://doi.org/10.1007/978-3-031-44245-2_11  
   
  Generalized Program Sketching  
   
  213  
   
  [1,8,12]. Assume that we have a program sketch with an unknown expression indicated by ??. Our synthesis algorithm computes an expression E over program variables such that, when ?? is replaced by E, all assertions within the resulting complete program become valid. Our synthesis algorithm proceeds in two phases, consisting of constraint generation and constraint solving. The constraint generation phase is based on abstract interpretation, whereas the constraint solving phase is based on logical abduction. In particular, we use forward and backward static analyses based on abstract interpretation to simultaneously compute the invariant postcondition before and suﬃcient precondition that ensures assertion validity after the unknown hole. The forward analysis computes an invariant P representing the facts known at the program location before the hole, whereas the backward analysis provides a suﬃcient precondition C that guarantees that the code after the hole satisﬁes all assertions. Then, we use abduction to ﬁnd missing hypothesis in a logical inference task. In more detail, assume we have a premise P and a desired conclusion C for an inference, where P and C are constraints generated using forward and backward analyses, respectively. The abduction infers the simplest and most general explanation E such that P ∧ E |= C and P ∧ E |= false. The ﬁrst condition states that the abduction solution E together with premise P should imply conclusion C, while the second condition states that the abduction solution E should not contradict premise P . Finally, we use explanation E to synthesize a concrete expression for hole ??. We have implemented our approach in a prototype program synthesizer. Our tool uses the numerical abstract domains (e.g., intervals, octagons, polyhedra) from the APRON library [25] for static analysis, as well as the EXPLAIN tool [8] for logical abduction in the combination SMT theory of linear arithmetic and propositional logic, and the MISTRAL tool [9] for SMT solving. We illustrate this approach for automatic completion of various numerical C sketches from the Sketch project [32,33], SV-COMP (https://sv-comp.sosy-lab.org/), and the literature [28]. We compare performances of our approach against the most popular sketching tool Sketch [32,33] and the FamilySketcher [15,19] that are used for synthesizing program sketches with missing integer constants. This work makes several contributions: (1) We explore the idea of automatically synthesizing arbitrary expressions in program sketches; (2) We show how this generalized program sketching problem can be solved using abstract interpretation and logical abduction; (3) We build a synthesizer using tools for static analysis by abstract interpretation and logical abduction, and present the synthesis results for the domain of numerical (linear arithmetic) programs.  
   
  2  
   
  Motivating Examples  
   
  We now present an overview of our approach using motivating examples. Consider the code intro.c shown in Fig. 1, where the unknown hole ?? is an arith-  
   
  214  
   
  A. S. Dimovski  
   
  Fig. 1. intro.c.  
   
  Fig. 2. abs.c.  
   
  Fig. 3. while.c.  
   
  metic expression in an assignment. The goal is to complete the unknown hole in 3 so that the assertion is always valid. loc.  Our approach starts by performing forward and backward static analyses that compute numerical invariants and suﬃcient conditions. They are abstract interpretation-based static analyses implemented using the Polyhedra abstract domain [7] from the APRON library [25]. The (over-approximating) forward 3 before the hole. The analysis infers the invariant (z=x+1 ∧ y=x+1) at loc.  subsequent (under-approximating) backward analysis starts with the assertion 5 and by propagating it backwards it infers the precondition fact (z>y) at loc. , 4 after the hole.1 We use these inferred facts to construct the (z+2>y) at loc.  following abduction query: (z=x+1 ∧ y’=x+1) ∧ R(y,y’,x,z) =⇒ (z+2>y) which is solved by calling the EXPLAIN tool [8]. The left-hand side of the implication encodes the generated constraints up to the hole ??, where y’ denotes 3 and the unknown predicate R(y,y’,x,z) the value of variable y before loc.  3 The rightencodes the constraint over all program variables in scope at loc. . hand side of this implication encodes the postcondition ensuring that the assertion must be valid. Hence, this abduction query is looking for a constraint over program variables that guarantees the validity of this implication. Among the class of all solutions, we prefer abductive solutions containing the variable y over others. For this reason, we specify in the abduction query the lowest cost to variable y, while y’, x and z have higher costs. The logically weakest and simplest2 solution containing y is: R(y,y’,x,z) ≡ (y ≤ y’+1). This represents a weakest speciﬁcation for the hole, so we can use it to synthesize the unknown 3 with: y = y+1 (other solutions expression. That is, we can ﬁll the hole in loc.  are also possible, e.g. y = y-1). Consider abs.c shown in Fig. 2, where the unknown hole ?? is a boolean expression in the if-guard. The forward analysis infers the invariant (abs=n) 2 The backward analysis starts with the assertion satisfaction, i.e. by at loc. . 1  
   
  2  
   
  This condition guarantees that the assertion is valid for any non-deterministic choice [2, 3]. If we have used an over-approximating backward analysis, it would infer the necessary condition (z+3>y) that may lead to the assertion satisfaction for some nondeterministic choices of [2, 3] (e.g., the execution where the non-deterministic choice [2, 3] returns 3). A solution is simplest if it contains the fewest number of variables.  
   
  Generalized Program Sketching  
   
  215  
   
  propagating the assertion fact (abs ≥ 0) backwards. After the if-guard, it infers that the precondition of then branch is (n ≤ 0) and the precondition of else branch is (abs ≥ 0). Thus, we construct two abduction queries: (1) (abs=n) ∧ Rtrue (n,abs) =⇒ (n ≤ 0), and (2) (abs=n) ∧ Rf alse (n,abs) =⇒ (abs ≥ 0), which are solved by EXPLAIN tool [8]. The reported weakest solutions are: Rtrue (n,abs) ≡ (n ≤ 0) and Rf alse (n,abs) ≡ (n ≥ 0). Subsequently, we check whether ¬Rtrue (n,abs) =⇒ Rf alse (n,abs) using the MISTRAL SMT 2 with the solver [9]. Since ¬(n ≤ 0) =⇒ (n ≥ 0) is valid, we ﬁll the hole at loc.  boolean guard (n ≤ 0) as a suﬃcient condition for the assertion to hold. Consider the code while.c given in Fig. 3, where the unknown hole ?? is a boolean expression in the while-guard. The forward analysis infers the invariant h We construct the abduction query (x ≤ 10) ∧ (x ≤ 10) ∧ (x+y=10) at loc. . (x+y=10) ∧ Rf alse (x,y) =⇒ (y=10). The reported solution by EXPLAIN is Rf alse (x,y) ≡ (x=0). We compute Rtrue (x,y) ≡ ¬Rf alse (x,y) ≡ (x0). Hence, we perform two backward analysis of while.c in which the while-guard is (x0 ∧ ??2 ), respectively. They start with the fact (x=0 ∧ y=10) 5 at loc. . The ﬁrst backward analysis for (x0∧??2 ) infers (1 ≤ x ≤ 11)∧(x+y=10) after the guard at loc. . 1 (x,y) =⇒ false, and (2) two abduction queries: (1) (x ≤ 10 ∧ x+y=10) ∧ Rtrue 2 (x,y) =⇒ (1 ≤ x ≤ 11) ∧ (x+y=10). The obtained (x ≤ 10 ∧ x+y=10) ∧ Rtrue 1 2 (x,y) ≡ true. Hence, we ﬁll the hole solutions are Rtrue (x,y) ≡ false and Rtrue with (x>0). 5 of while.c is assert (y ≤ 10). In this Assume that the assertion at loc.  case, the solution of the ﬁrst abduction query is Rf alse (x,y) ≡ (x ≥ 0). We ﬁnd one interpretation (x=n), where n ≥ 0, of the formula (x ≥ 0) using MISTRAL SMT solver. So, we obtain Rtrue (x,y) ≡ ¬Rf alse (x,y) ≡ (xn). Then, we proceed analogously to above (basically replacing (x=0) by (x=n)).  
   
  3  
   
  Language and Semantics  
   
  This section introduces the target language of our approach as well as its concrete and abstract semantics. They will be employed for designing the invariance (reachability) and suﬃcient condition static analyses using the abstract interpretation theory [5,29,34]. Moreover, we formally deﬁne the logical abduction problem. 3.1  
   
  Syntax  
   
  We use a simple C-like imperative language for writing general-purpose programs. The program variables Var are statically allocated and the only data type is the set Z of mathematical integers. To encode unknown holes, we use the construct ??. The hole constructs ??i are placeholders that the synthesizer must replace with suitable (arithmetic and boolean) expressions, such that the resulting program will avoid any assertion failures. The syntax is given below.  
   
  216  
   
  A. S. Dimovski  
   
  − → Fig. 4. Deﬁnitions of [[s]] : P(Σ) → P(Σ).  
   
  s (s ∈ Stm) ::= skip | x=ae | s; s | if (be) s else s | while (be) do s | assert(be),  
   
  ae (ae ∈ AExp) ::= ??i | ae , ae ::= n | [n, n ] | x ∈ Var | ae ⊕ae , be (be ∈ BExp) ::= ??i | be , be ::= ae  ae | ¬be | be ∧ be | be ∨ be  
   
  where n ranges over integers Z, [n, n ] over integer intervals, x over program variables Var, and ⊕ ∈ {+, −, ∗, /}, ∈ { n) ∧ ??i ]]] (dF ) ; ←−−−−−−−−−−−−−−−−−− Cond,2 dF = [[p[??i → (x < n) ∧ ??i ]]] (dF ) ; 1 h b ), Cond,1 x) ; Rtrue (x) := Abduce(InvdI (), dF (  ,2 2 h b ), ), ConddF ( x) ; Rtrue (x) := Abduce(InvdI ( if (unsat(R1 (x)) ∨ unsat(R2 (x))) then return ∅ ; 1 2 return (x > n ∧ Rtrue (x)) ∨ (x < n ∧ Rtrue (x))  
   
  l l’ ”. reason by the structure of the statement “s For assignments and if-s, we ←− f call a backward abstract analysis [[p]] dF , where dF = FILTERD (bef , InvdI ()),  to compute the suﬃcient conditions ConddF of program p. When s is an assignment “x = ??” (lines 2–7), we construct an abduction query where the premise  l l’ is InvdI ()[x /x], the desired conclusion is ConddF (), and the unknown predicate (abducible) R(x) is deﬁned over all variables x that are in scope of si including x and x . We denote by [x /x] the renaming of x as x . Moreover, we conﬁgure the call to Abduce so that the variable x has the highest priority to occur in the solution R(x) of the given abduction query. Then we call SolveAsg(R(x)) to ﬁnd one expression ei such that x = ei satisﬁes the predicate R(x). This is realized by asking an SMT solver for one interpretation (model) of the formula R(x). Finally, we return the expression ei [x/x ] as solution of this case. We recall the example intro.c in Sect. 2 to see how this case works in practice. t e When s is a conditional statement “if (??) then s 1 else s 2 ” (lines 8– l 13), we construct two abduction queries in which the premise is InvdI () and  
   
  222  
   
  A. S. Dimovski  
   
  the unknown predicate is deﬁned over all variables x that are in scope of s. t e and ConddF () in the ﬁrst and second abducThe conclusions are ConddF () tion query, respectively. We then call SolveCond(Rtrue (x), Rf alse (x)) to check if ¬Rtrue (x) =⇒ Rf alse (x) by an SMT solver, where Rtrue (x) and Rf alse (x) are solutions of the ﬁrst and second abduction query. If this is true, then Rtrue (x) is returned as solution for this case. Otherwise, Rtrue (x) is strengthen until ¬Rtrue (x) =⇒ Rf alse (x) holds, in which case the found Rtrue (x) is returned as solution. For example, see how abs.c in Sect. 2 is resolved. h b do s Similarly, we handle the case when s is an iteration “while (??) 1” (lines 14–22). First, we construct an abduction query where the premise h is InvdI (), the desired conclusion is dF , and the unknown predicate (abducible) is Rf alse (x). Then, we ﬁnd one interpretation (x=n) of the formula Rf alse (x), which is obtained by ﬁnding a model M of Rf alse (x) by an SMT solver and setting x = M (x). Next, we perform two backward analy←−−−−−−−−−−−−−−−−−− ,2 ses deﬁned as follows: Cond,1 dF = [[p[??i → (x > n) ∧ ??i ]]] (dF ) and ConddF = ←−−−−−−−−−−−−−−−−−− [[p[??i → (x < n) ∧ ??i ]]] (dF ). We create two abduction queries using: (1) 1 2 h b h b ), ), InvdI (), (x); and (2) InvdI (), (x). Cond,1 Rtrue Cond,2 and Rtrue dF ( dF ( 1 2 Finally, the solution is (x > n ∧ Rtrue (x)) ∨ (x < n ∧ Rtrue (x)). For example, see how while.c in Sect. 2 works. Correctness. The following theorem states correctness of the GenSketching algorithm. Theorem 1. GenSketching(p, H) is correct and terminates. Proof. The procedure GenSketching(p, H) terminates since all steps in it are terminating. The correctness of GenSketching(p, H) follows from the soundness of InvdI and ConddF (see Proposition 1) and the correctness of Abduce (see Proposition 2). The correctness proof is by structural induction on statements s in programs i s;  f assert (bef )”. We consider the case of assignment x=??. p of the form “ i f i ). Since there is one hole in p, we call Solve(p, x=?? We infer InvdI () = D  f f and ConddF () = be , so we obtain the abduction query Abduce(true, bef , x). The solution is R(x) ≡ bef , thus we call SolveAsg(bef , x) to ﬁnd one expression ae such that x=ae satisﬁes bef . By construction of ae, it follows that the program i x=ae;  f assert (bef )” is valid. Similarly, we handle the other cases.   “  
   
  5  
   
  Evaluation  
   
  We now evaluate our approach for generalized program sketching. The evaluation aims to show that we can use our approach to eﬃciently resolve various C program sketches with numerical data types.  
   
  Generalized Program Sketching  
   
  Fig. 8. max.c.  
   
  Fig. 9. cond.c.  
   
  223  
   
  Fig. 10. mine.c.  
   
  Implementation. We have implemented our synthesis algorithm in a prototype tool. The abstract operations and transfer functions of the numerical abstract domains (e.g. Polyhedra [7]) are provided by the APRON library [25], while the abduction and SMT queries are solved by the EXPLAIN [8] and the MISTRAL [9] tools. Our tool is written in OCaml and consists of around 7K lines of code. Currently, it provides only a limited support for arrays, pointers, struct and union types. Experiment Setup and Benchmarks. All experiments are executed on a 64-bit Intel CoreT M i5 CPU, Lubuntu VM, with 8 GB memory, and we use a timeout value of 300 sec. All times are reported as average over ﬁve independent executions. We report times needed for the actual synthesis task to be performed. The implementation, benchmarks, and all obtained results are available from [16]: https://zenodo.org/record/8165119. We compare our approach GenSketching based on the Polyhedra domain with program sketching tool Sketch version 1.7.6 that uses SAT-based inductive synthesis [32,33], as well as with the FamilySketcher that uses lifted (family-based) static analysis by abstract interpretation (the Polyhedra domain) [19]. Note that Sketch and FamilySketcher can only solve the standard sketching problem, where the unknown holes represent some integer constants. Therefore, they cannot resolve our benchmarks. We need to do some simpliﬁcations, so that the unknown holes refer to integer constants. Moreover, their synthesis times depend on the sizes of hole domains (and inputs for Sketch). Hence, for Sketch and FamilySketcher we report synthesis times to resolve simpliﬁed sketches with 5-bits and 10-bits sizes of unknown holes. On the other hand, GenSketching can synthesize arbitrary expressions and its synthesis time does not depend on the sizes of holes. For GenSketching, we report Time which is the total time to resolve a given sketch, and AbdTime which is the time to solve the abduction queries in the given synthesis task. The evaluation is performed on several C numerical sketches collected from the Sketch project [32,33], SV-COMP (https://sv-comp.sosy-lab.org/), and the literature [28]. In particular, we use the following benchmarks: intro.c (Fig. 1), abs.c (Fig. 2), while.c (Fig. 3), max.c (Fig. 8), cond.c (Fig. 9), and mine.c (Fig. 10).  
   
  224  
   
  A. S. Dimovski  
   
  Table 1. Performance results of GenSketching vs. Sketch vs. FamilySketcher. All times in sec. Bench.  
   
  GenSketching Sketch FamilySketcher Time AbdTime 5-bits 10-bits 5-bits 10-bits  
   
  intro.c 0.0022 0.0011  
   
  0.208  
   
  0.239  
   
  0.0013  
   
  0.0016  
   
  0.0021 0.0013  
   
  0.204  
   
  0.236  
   
  0.0020  
   
  0.0021  
   
  while.c 0.0055 0.0013  
   
  0.213  
   
  0.224  
   
  0.0047  
   
  0.0053  
   
  abs.c max.c  
   
  0.0042 0.0011  
   
  0.229  
   
  24.25  
   
  0.0025  
   
  0.0026  
   
  max2.c  
   
  0.0040 0.0015  
   
  0.227  
   
  31.88  
   
  0.0022  
   
  0.0033  
   
  cond  
   
  0.0019 0.0011  
   
  1.216  
   
  2.362  
   
  0.0021  
   
  0.0023  
   
  mine.c  
   
  0.0757 0.0012  
   
  0.236  
   
  1.221  
   
  0.0035  
   
  0.0042  
   
  mine2.c 0.0059 0.0013  
   
  0.215  
   
  1.217  
   
  0.0024  
   
  0.0031  
   
  Performance Results. Table 1 shows the performance results of synthesizing our benchmarks. To handle intro.c using Sketch and FamilySketcher, we need to simplify it so that the hole represents an integer constant. This is done by replacing y = ?? by y = y-?? or y = y+??. Moreover, they cannot handle non-deterministic choices, so we use constants instead. Sketch and FamilySketcher resolve the simpliﬁed intro.c in 0.208 sec and 0.0013 sec for 5-bit sizes of holes, while GenSketching resolves the intro.c in 0.0022 sec. To resolve abs.c using Sketch and FamilySketcher, we use the if-guard (n ≤ ??). In this case Sketch and FamilySketcher terminate in 0.204 sec and 0.0022 sec for 5-bit sizes of holes, while GenSketching terminates in 0.0021 sec. Still, FamilySketcher reports “I don’t know” answer due to the precision loss. In particular, it infers the invariant (2*??-n+abs ≥ 0 ∧ 0 ≤ ?? ≤ 31 ∧ n+abs ≥ 0) 4 thus it is unable to conclude for which values of ?? the assertion at loc. , (abs ≥ 0) will hold. For similar reasons, FamilySketcher cannot successfully resolve the other simpliﬁed sketches that contain holes in if-guards (see max2.c and cond.c below). We simplify the while-guard of while.c to (x>??). Sketch still cannot resolve this example, since it uses only 8 unrollments of the loop by default. If the loop is unrolled 10 times, Sketch terminates in 0.213 sec for 5-bit sizes of holes. FamilySketcher terminates in 0.0047 sec for 5-bits, while GenSketching terminates in 0.0055 sec. Consider the program sketch max.c in Fig. 8 that ﬁnds a maximum of three 3 The forward analysis integers. It contains one hole in the assignment at loc. . of GenSketching generates the invariant (max ≥ n1 ∧ max ≥ n2 ∧ n3 ≥ max ) before the hole (where max denotes the value of max before the hole), while the backward analysis infers the suﬃcient condition (max ≥ n1 ∧ max ≥ n2 ∧ max ≥ n3) after the hole. The result of the corresponding abduction query is (max = n3), so we ﬁll the hole with the assignment max = n3. For Sketch and FamilySketcher, we simplify the hole to max = n3-??. Since there are three  
   
  Generalized Program Sketching  
   
  225  
   
  inputs and one hole, Sketch takes 24.25 sec to resolve this simpliﬁed sketch for 10-bit sizes and timeouts for bigger sizes. Consider a variant of max.c, denoted max2.c, where the commented state3 so the hole is if-guard. The forward analysis ment in Fig. 8 is placed at loc. , of GenSketching infers the invariant (max ≥ n1 ∧ max ≥ n2) before the hole, while the backward analysis infers the suﬃcient conditions (n3 ≥ n1 ∧ n3 ≥ n2) at the then branch after the hole and (max ≥ n1 ∧ max ≥ n2 ∧ max ≥ n3) at the else branch after the hole. We construct two abduction queries, and the results are (max ≤ n3) and (max ≥ n3), respectively. Hence, we ﬁll the hole with the boolean expression (max ≤ n3). For Sketch and FamilySketcher, we use the simpliﬁed sketch where the if-guard is (max  

 Report "Static Analysis. 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings 9783031442445, 9783031442452"  
 ×    

 --- Select Reason ---  Pornographic  Defamatory  Illegal/Unlawful  Spam  Other Terms Of Service Violation  File a copyright complaint     

 Close  Submit    

    Contact information  
 Michael Browner   
   [email protected]    
   
   Address:   
 1918 St.Regis, Dorval, Quebec, H9P 1H6, Canada.   
   
 Support & Legal  
  O nas 
  Skontaktuj się z nami 
  Prawo autorskie 
  Polityka prywatności 
  Warunki 
  FAQs 
  Cookie Policy 
    
 Subscribe to our newsletter  
  Be the first to receive exclusive offers and the latest news on our products and services directly in your inbox.  
   Subscribe     

 Copyright © 2024 DOKUMEN.PUB. All rights reserved.        

 Unsere Partner sammeln Daten und verwenden Cookies zur Personalisierung und Messung von Anzeigen. Erfahren Sie, wie wir und unser Anzeigenpartner Google Daten sammeln und verwenden  .   Cookies zulassen