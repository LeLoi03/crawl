Image Analysis and Processing – ICIAP 2023 | springerprofessional.de  Skip to main content    Menü   Fachgebiete Chevron down icon     Chevron up icon        Automobil + Motoren    Bauwesen + Immobilien    Business IT + Informatik    Elektrotechnik + Elektronik    Energie + Nachhaltigkeit    Finance + Banking    Management + Führung    Marketing + Vertrieb    Maschinenbau + Werkstoffe    Versicherung + Risiko      
   
 DE     
 EN      
   
 Bücher       
   
 Zeitschriften       
   
 Themenseiten Chevron down icon     Chevron up icon        Organisationspsychologie    Projektmanagement    Marketing    Smart Manufacturing      

 Weitere Formate Chevron down icon     Chevron up icon        Podcasts    Webinare Technik    Webinare Wirtschaft    Kongresse    Veranstaltungskalender    Awards    MyNewsletter      

 Jetzt Einzelzugang starten       
   
 Zugang für Unternehmen       
   
 Referenzkunden       
   
 Sustainability in Automotive       

 Gesamtlebenszyklus wird immer wichtiger  
 In der Digitalkonferenz am 5. Dezember 2024 geht es um die wachsende Bedeutung der Kreislaufwirtschaft in der Automobilindustrie und wie nachhaltige Werkstoffe sowie wiederverwendbare Komponenten dazu beitragen können.  

 Springer Professional     
   
   Suche   Suchbegriffe eingeben       Suchen     
  
 Erweiterte Suche      

  Anmelden      

 Springer Professional  

 JAVASCRIPT BENÖTIGT    
 Bitte aktivieren Sie Java-Script in Ihrem Browser, damit Sie alle Vorteile und Funktionen dieser Website nutzen können.   

  INTERNET EXPLORER WIRD NICHT MEHR UNTERSTÜZT    
 Der Internet Explorer wird als Browser seitens Microsoft nicht mehr unterstützt. Für Ihr optimales Nutzungserlebnis wählen Sie bitte Microsoft Edge, Safari, Chrome oder Firefox als Browser.   

 nach oben    

 2023 | Buch  

 Kapitel lesen  Erstes Kapitel lesen     
   
 Image Analysis and Processing – ICIAP 2023  
 22nd International Conference, ICIAP 2023, Udine, Italy, September 11–15, 2023, Proceedings, Part I  
 herausgegeben von: Gian Luca Foresti, Andrea Fusiello, Edwin Hancock   
   
 Verlag: Springer Nature Switzerland    
 Buchreihe : Lecture Notes in Computer Science    
 Enthalten in: Springer Professional "Wirtschaft+Technik"  ,   Springer Professional "Technik"  ,   Springer Professional "Wirtschaft"    
   
 Inhaltsverzeichnis    

      Suchen      
 insite    
 SUCHEN    

 Über dieses Buch  
 This two-volume set LNCS 14233-14234 constitutes the refereed proceedings of the 22nd International Conference on Image Analysis and Processing, ICIAP 2023, held in Udine, Italy, during September 11–15, 2023.  
  The 85 full papers presented together with 7 short papers were carefully reviewed and selected from 144 submissions. The conference focuses on video analysis and understanding; pattern recognition and machine learning; deep learning; multi-view geometry and 3D computer vision; image analysis, detection and recognition; multimedia; biomedical and assistive technology; digital forensics and biometrics; image processing for cultural heritage; and robot vision.  

 MyTopic Alert   
  Loggen Sie sich ein, um Ihre Alerts zu aktualisieren und Neue anzulegen.  

 Anzeige   

 Inhaltsverzeichnis  
   
 Frontmatter  
  
 Image Retrieval in Semiconductor Manufacturing  
  Content-Based Image Retrieval has a lot of applications in the industry, where large collections of data from manufacturing need to be automatically queried e.g. for quality inspection purposes. In this work we design an image retrieval solution over IMAGO, a dataset of Transmission Electron Microscopy (TEM) images of nano-sized silicon structures collected in the production site of STMicroelectronics, in Agrate Brianza, Italy. Image retrieval in imago is challenging because: i) only a limited portion of images are provided with labels, namely type of semiconductor structure, ii) most images refer to unseen classes that are not represented in the training set, and iii) images of the same class can be acquired at different magnification levels of the electronic microscope. Our main contribution is the design of a deep-learning based image retrieval system that leverages a training procedure that alternates between siamese loss, assessed on annotated samples, and reconstruction loss, assessed on unlabelled samples. Our solution exploits the whole information in the IMAGO dataset, and our experiments confirm we can successfully retrieve images of unseen classes that exhibit the same structure of the query ones. Our solution is currently deployed in STMicroelectronics production sites.  
   
 Giuseppe Gianmarco Gatta, Diego Carrera, Beatrice Rossi, Pasqualina Fragneto, Giacomo Boracchi   

 Continual Source-Free Unsupervised Domain Adaptation  
  Source-free Unsupervised Domain Adaptation (SUDA) approaches inherently exhibit catastrophic forgetting. Typically, models trained on a labeled source domain and adapted to unlabeled target data improve performance on the target while dropping performance on the source, which is not available during adaptation. In this study, our goal is to cope with the challenging problem of SUDA in a continual learning setting, i.e., adapting to the target(s) with varying distributional shifts while maintaining performance on the source. The proposed framework consists of two main stages: i) a SUDA model yielding cleaner target labels—favoring good performance on target, and ii) a novel method for synthesizing class-conditioned source-style images by leveraging only the source model and pseudo-labeled target data as a prior. An extensive pool of experiments on major benchmarks, e.g., PACS, Visda-C, and DomainNet demonstrates that the proposed Continual SUDA (C-SUDA) framework enables preserving satisfactory performance on the source domain without exploiting the source data at all.  
   
 Waqar Ahmed, Pietro Morerio, Vittorio Murino   

 Self-Similarity Block for Deep Image Denoising  
  Non-Local Self-Similarity (NLSS) is a widely exploited prior in image denoising algorithms. The first deep Convolutional Neural Networks (CNNs) for image denoising ignored NLSS and were made of a sequence of convolutional layers trained to suppress noise. The first denoising CNNs leveraging NLSS prior were performing non-learnable operations outside the network. Then, pre-defined similarity measures were introduced and finally learnable, but scalar, similarity scores were adopted inside the network. We propose the Self-Similarity Block (SSB), a novel differentiable building block for CNN denoisers to promote the NLSS prior. The SSB is trained in an end-to-end manner within convolutional layers and learns a multivariate similarity score to improve image denoising by combining similar vectors in an activation map. We test SSB on additive white Gaussian noise suppression, and we show it is particularly beneficial when the noise level is high. Remarkably, SSB is mostly effective in image regions presenting repeated patterns, which most benefit from the NLSS prior.  
   
 Edoardo Peretti, Diego Stucchi, Diego Carrera, Giacomo Boracchi   

 A Request for Clarity over the End of Sequence Token in the Self-Critical Sequence Training  
  The Image Captioning research field is currently compromised by the lack of transparency and awareness over the End-of-Sequence token (<Eos>) in the Self-Critical Sequence Training. If the <Eos> token is omitted, a model can boost its performance up to +4.1 CIDEr-D using trivial sentence fragments. While this phenomenon poses an obstacle to a fair evaluation and comparison of established works, people involved in new projects are given the arduous choice between lower scores and unsatisfactory descriptions due to the competitive nature of the research. This work proposes to solve the problem by spreading awareness of the issue itself. In particular, we invite future works to share a simple and informative signature with the help of a library called SacreEOS. Code available at: https://github.com/jchenghu/sacreeos .  
   
 Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi   

 Shallow Camera Pipeline for Night Photography Enhancement  
  Enhancing night photography images is a challenging task that requires advanced processing techniques. While CNN-based methods have shown promising results, their high computational requirements and limited interpretability can pose challenges. To address these limitations, we propose a camera pipeline for rendering visually pleasing photographs in low-light conditions. Our approach is characterized by a shallow structure, explainable steps, and a low parameter count, resulting in computationally efficient processing. We compared the proposed pipeline with recent CNN-based state-of-the-art approaches for low-light image enhancement, showing that our approach produces more aesthetically pleasing results. The psycho-visual comparisons conducted in this work show how our proposed solution is preferred with respect to the other methods (in about 44% of the cases our solution has been chosen, compared to only about 15% of the cases for the state-of-the-art best method).  
   
 Simone Zini, Claudio Rota, Marco Buzzelli, Simone Bianco, Raimondo Schettini   

 GCK-Maps: A Scene Unbiased Representation for Efficient Human Action Recognition  
  Human action recognition from visual data is a popular topic in Computer Vision, applied in a wide range of domains. State-of-the-art solutions often include deep-learning approaches based on RGB videos and pre-computed optical flow maps. Recently, 3D Gray-Code Kernels projections have been assessed as an alternative way of representing motion, being able to efficiently capture space-time structures. In this work, we investigate the use of GCK pooling maps, which we called GCK-Maps, as input for addressing Human Action Recognition with CNNs. We provide an experimental comparison with RGB and optical flow in terms of accuracy, efficiency, and scene-bias dependency. Our results show that GCK-Maps generally represent a valuable alternative to optical flow and RGB frames, with a significant reduction of the computational burden.  
   
 Elena Nicora, Vito Paolo Pastore, Nicoletta Noceti   

 Autism Spectrum Disorder Identification from Visual Exploration of Images  
  Autism Spectrum Disorder (ASD) affects 1 in 77 children in Italy, but the diagnostic process is slow and costly. As autistic individuals exhibit different gaze patterns from healthy controls in visual exploration of images and semantic interpretation, these are promising biomarkers to exploit in diagnosis. This study aims at developing a model to assist in the diagnosis of ASD using gaze data when static images are presented to the subjects. We first propose a set of features, each one motivated by psychological studies and findings. Then we apply a feature selection mechanism based on Boruta algorithm with SHAP values. Finally we use CatBoost to perform binary classification, and a strategy to optimize model hyperparameters using a multivariate Tree Parzen Estimator. We validated our model on the popular Saliency4ASD dataset, outperforming state of the art models tested with the same protocol by more than 3% in accuracy. We also provide an in-depth analysis of the feature importance and we show how these results are in line with the psychological literature.  
   
 Marco Bolpagni, Francesco Setti   

 Target-Driven One-Shot Unsupervised Domain Adaptation  
  In this paper, we introduce a novel framework for the challenging problem of One-Shot Unsupervised Domain Adaptation (OS-UDA), which aims to adapt to a target domain with only a single unlabeled target sample. Unlike existing approaches that rely on large labeled source and unlabeled target data, our Target-Driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation strategy guided by the target sample’s style to align the source distribution with the target distribution. Our method consists of three modules: an augmentation module, a style alignment module, and a classifier. Unlike existing methods, our augmentation module allows for strong transformations of the source samples, and the style of the single target sample available is exploited to guide the augmentation by ensuring perceptual similarity. Furthermore, our approach integrates augmentation with style alignment, eliminating the need for separate pre-training on additional datasets. Our method outperforms or performs comparably to existing OS-UDA methods on the Digits and DomainNet benchmarks.  
   
 Julio Ivan Davila Carrazco, Suvarna Kishorkumar Kadam, Pietro Morerio, Alessio Del Bue, Vittorio Murino   

 Combining Identity Features and Artifact Analysis for Differential Morphing Attack Detection  
  Due to the importance of the Morphing Attack, the development of new and accurate Morphing Attack Detection (MAD) systems is urgently needed by private and public institutions. In this context, D-MAD methods, i.e. detectors fed with a trusted live image and a probe tend to show better performance with respect to S-MAD approaches, that are based on a single input image. However, D-MAD methods usually leverage the identity of the two input face images only, and then present two main drawbacks: they lose performance when the two subjects look alike, and they do not consider potential artifacts left by the morphing procedure (which are instead typically exploited by S-MAD approaches). Therefore, in this paper, we investigate the combined use of D-MAD and S-MAD to improve detection performance through the fusion of the features produced by these two MAD approaches.  
   
 Nicolò Di Domenico, Guido Borghi, Annalisa Franco, Davide Maltoni   

 SynthCap: Augmenting Transformers with Synthetic Data for Image Captioning  
  Image captioning is a challenging task that combines Computer Vision and Natural Language Processing to generate descriptive and accurate textual descriptions for input images. Research efforts in this field mainly focus on developing novel architectural components to extend image captioning models and using large-scale image-text datasets crawled from the web to boost final performance. In this work, we explore an alternative to web-crawled data and augment the training dataset with synthetic images generated by a latent diffusion model. In particular, we propose a simple yet effective synthetic data augmentation framework that is capable of significantly improving the quality of captions generated by a standard Transformer-based model, leading to competitive results on the COCO dataset.  
   
 Davide Caffagni, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara   

 An Effective CNN-Based Super Resolution Method for Video Coding  
  The reference picture resampling (RPR) technique in Versatile Video Coding allows the input video to be down-sampled before encoding, and then the decoded video will be up-sampled. Compared with the handcrafted up-sampling interpolation filter, the convolutional neural network based super resolution (SR) method has better restoration effect. In this paper, we introduce the RPR technique into the SR method, to effectively restore the details lost in the down-sampling and encoding process. Meanwhile, the normalized attention module in the network is designed to adaptively determine the weight of the extracted features. Moreover, the prediction picture generated during the codec process is fed into the network as side information, which can provide the directional and texture information of the original picture, and implicitly derive the lost details together with the reconstruction picture. Compared with VTM-11.0-NNVC, the experimental results show that the proposed method achieves 12.79% and 10.53% BD-rate reductions, under all intra and random access configurations, respectively.  
   
 Jun Yin, Shuang Peng, Jucai Lin, Dong Jiang, Cheng Fang   

 Medical Transformers for Boosting Automatic Grading of Colon Carcinoma in Histological Images  
  Developing computer-aided approaches for cancer diagnosis and grading is receiving an uprising demand since this could take over intra- and inter-observer inconsistency, speed up the screening process, allow early diagnosis, and improve the accuracy and consistency of the treatment planning processes. The third most common cancer worldwide and the second most common in women is ColoRectal Cancer (CRC). Grading CRC is a key task in planning appropriate treatments and estimating the response to them. Automatic systems have the potential to speed up and make it more robust but, unfortunately, the most recent and promising machine learning techniques have not been applied for automatic CRC grading so far. For example, there is no work exploiting transformer networks, which outperform convolutional neural networks (CNN) and are replacing them in many applications, for CRC detection and grading at a large scale. To fill this gap, in this work, a transformer-based network endowed with an additional control mechanism in the self-attention module is exploited to understand discriminative regions in large histological images. These relevant regions have been used to train the most suited Convolutional Neural Network (as emerged from recent research findings) for the automatic grading of CRC. The experimental proofs on the largest publicly available CRC dataset demonstrated marked improvement with respect to the leading state-of-the-art approaches relying on CNN.  
   
 Pierluigi Carcagnì, Marco Leo, Luca Signore, Cosimo Distante   

 FERMOUTH: Facial Emotion Recognition from the MOUTH Region  
  People use various nonverbal communicative channels to convey emotions, among which facial expressions are considered the most important ones. Consequently, automatic Facial Expression Recognition (FER) is a crucial task for enhancing computers’ perceptive abilities, particularly in human-computer interaction. Although state-of-the-art FER systems can identify emotions from the entire face, situations may arise where occlusions prevent the entire face from being visible. During the COVID-19 pandemic, many FER systems have been developed for recognizing emotions from the eye region due to the obligation to wear a mask. However, in many situations, the eyes may be covered, for instance, by sunglasses or virtual reality devices. In this paper, we faced the problem of developing a FER system that solely considers the mouth region and classifies emotions using only the lower part of the face. We tested the effectiveness of this FER system in recognizing emotions from the lower part of the face and compared the results to a FER system trained on the same datasets using the same approach on the entire face. As expected, emotions primarily associated with the mouth region (e.g., happiness, surprise) were recognized with minimal loss compared to the entire face. Nevertheless, even though most negative emotions were not accurately detected using only the mouth region, in cases where the face is partially covered, this area may still provide some information about the displayed emotion.  
   
 Berardina De Carolis, Nicola Macchiarulo, Giuseppe Palestra, Alberto Pio De Matteis, Andrea Lippolis   

 Consensus Ranking for Efficient Face Image Retrieval: A Novel Method for Maximising Precision and Recall  
  Efficient face image retrieval, i.e. searching for existing photographs of a person in unlabelled photo collections using a query photo, is evaluated for a novel method to find the top n results for Consensus Ranking. The approach aims to maximise precision and recall by using the retrieved photos, all ranked on similarity. The proposed method aims to retrieve all photos of the queried person while excluding images of other individuals. To achieve this, the method uses the top n results as temporary queries, recalculates similarities, and combines the obtained ranked lists to produce a better overall ranking. The method includes a novel and reliable procedure for selecting n, which is evaluated on two datasets, and considers the impact of age variation in the datasets.  
   
 Anders Hast   

 Towards Explainable Navigation and Recounting  
  Explainability and interpretability of deep neural networks have become of crucial importance over the years in Computer Vision, concurrently with the need to understand increasingly complex models. This necessity has fostered research on approaches that facilitate human comprehension of neural methods. In this work, we propose an explainable setting for visual navigation, in which an autonomous agent needs to explore an unseen indoor environment while portraying and explaining interesting scenes with natural language descriptions. We combine recent advances in ongoing research fields, employing an explainability method on images generated through agent-environment interaction. Our approach uses explainable maps to visualize model predictions and highlight the correlation between the observed entities and the generated words, to focus on prominent objects encountered during the environment exploration. The experimental section demonstrates that our approach can identify the regions of the images that the agent concentrates on to describe its point of view, improving explainability.  
   
 Samuele Poppi, Roberto Bigazzi, Niyati Rawal, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara   

 Towards Facial Expression Robustness in Multi-scale Wild Environments  
  Facial expressions are dynamic processes that evolve over temporal segments, including onset, apex, offset, and neutral. However, previous works on automatic facial expression analysis have mainly focused on the recognition of discrete emotions, neglecting the continuous nature of these processes. Additionally, facial images captured from videos in the wild often have varying resolutions due to fixed-lens cameras. To address these problems, our objective is to develop a robust facial expression recognition classifier that provides good performance in such challenging environments. We evaluated several state-of-the-art models on labeled and unlabeled collections and analyzed their performance at different scales. To improve performance, we filtered the probabilities provided by each classifier and demonstrated that this improves decision-making consistency by more than 10%, leading to accuracy improvement. Finally, we combined the models’ backbones into a temporal-sequence classifier, leveraging this consistency-performance trade-off and achieving an additional improvement of 9.6%.  
   
 David Freire-Obregón, Daniel Hernández-Sosa, Oliverio J. Santana, Javier Lorenzo-Navarro, Modesto Castrillón-Santana   

 Depth Camera Face Recognition by Normalized Fractal Encodings  
  Face recognition is a thriving topic in biometrics literature. In most cases, it is performed on RGB images using deep learning approaches. However, due to the wider scenarios in which face recognition can be applied, it is necessary to operate when illumination conditions are not in favor of the RGB image analysis. To overcome this problem, we present DLIF (Depth Landmark Identity by Fractal), a new depth face recognition method that uses fractal encoding to treat faces as depth features. A facial landmark predictor is developed to detect and extract the face, making DLIF completely in-depth. Comparisons of the fractal-encoded faces are obtained by the Canberra distance, which makes DLIF simple but effective. Statistical analysis is performed to improve DLIF, resulting in DLIF+, which takes into account the source of the image. DLIF and DLIF+ are tested on four challenging datasets and on a fifth dataset, named BIPS, with 72 subjects. The results obtained show that DLIF and DLIF+ are competitive with the state of the art and enhance the robustness of this method in relation to the number of subjects considered.  
   
 Umberto Bilotti, Carmen Bisogni, Michele Nappi, Chiara Pero   

 Automatic Generation of Semantic Parts for Face Image Synthesis  
  Semantic image synthesis (SIS) refers to the problem of generating realistic imagery given a semantic segmentation mask that defines the spatial layout of object classes. Most of the approaches in the literature, other than the quality of the generated images, put effort in finding solutions to increase the generation diversity in terms of style i.e. texture. However, they all neglect a different feature, which is the possibility of manipulating the layout provided by the mask. Currently, the only way to do so is manually by means of graphical users interfaces. In this paper, we describe a network architecture to address the problem of automatically manipulating or generating the shape of object classes in semantic segmentation masks, with specific focus on human faces. Our proposed model allows embedding the mask class-wise into a latent space where each class embedding can be independently edited. Then, a bi-directional LSTM block and a convolutional decoder output a new, locally manipulated mask. We report quantitative and qualitative results on the CelebMask-HQ dataset, which show our model can both faithfully reconstruct and modify a segmentation mask at the class level. Also, we show our model can be put before a SIS generator, opening the way to a fully automatic generation control of both shape and texture. Code available at https://github.com/TFonta/Semantic-VAE .  
   
 Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati   

 Improved Bilinear Pooling for Real-Time Pose Event Camera Relocalisation  
  Traditional methods for estimating camera pose have been replaced by more advanced camera relocalization methods that utilize both CNNs and LSTMs in the field of simultaneous localization and mapping. However, the reliance on LSTM layers in these methods can lead to overfitting and slow convergence. In this paper, a novel approach for estimating the six degree of freedom (6DOF) pose of an event camera using deep learning is presented. Our method begins by preprocessing the events captured by the event camera to generate a set of images. These images are then passed through two CNNs to extract relevant features. These features are multiplied using an outer product and aggregated across different regions of the image after adding L2 normalization to normalize the combining vector. The final step of the model is a regression layer that predicts the position and orientation of the event camera. The effectiveness of this approach has been tested on various datasets, and the results demonstrate its superiority compared to existing state-of-the-art methods.  
   
 Ahmed Tabia, Fabien Bonardi, Samia Bouchafa   

 End-to-End Asbestos Roof Detection on Orthophotos Using Transformer-Based YOLO Deep Neural Network  
  Asbestos, a hazardous material associated with severe health issues, requires accurate identification for safe management and removal. This study presents a novel end-to-end deep learning approach using a transformer-based YOLOv5 network for detecting asbestos roofs in high-resolution orthophotos, filling a gap in the scientific literature where end-to-end solutions are lacking. The model is trained on a dataset containing orthophotos with various roof types and conditions around Pisa in Italy. The transformer-based YOLO architecture enhances the detection capabilities compared to traditional CNNs. The proposed method demonstrates high accuracy in asbestos roof detection, outperforming traditional remote sensing techniques, and offers an effective, automated solution for targeting removal efforts and mitigating associated health risks. This end-to-end approach fills a gap in the existing literature and presents a promising direction for future research in asbestos roof detection.  
   
 Cesare Davide Pace, Alessandro Bria, Mariano Focareta, Gabriele Lozupone, Claudio Marrocco, Giuseppe Meoli, Mario Molinara   

 OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data  
  The inexorable growth of online shopping and e-commerce demands scalable and robust machine learning-based solutions to accommodate customer requirements. In the context of automatic tagging classification and multimodal retrieval, prior works either defined a low generalizable supervised learning approach or more reusable CLIP-based techniques while, however, training on closed source data. In this work, we propose OpenFashionCLIP, a vision-and-language contrastive learning method that only adopts open-source fashion data stemming from diverse domains, and characterized by varying degrees of specificity. Our approach is extensively validated across several tasks and benchmarks, and experimental results highlight a significant out-of-domain generalization capability and consistent improvements over state-of-the-art methods both in terms of accuracy and recall. Source code and trained models are publicly available at: https://github.com/aimagelab/open-fashion-clip .  
   
 Giuseppe Cartella, Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara   

 UAV Multi-object Tracking by Combining Two Deep Neural Architectures  
  Detecting and tracking multiple objects from unmanned aerial vehicle (UAV) videos is an high challenging task in a wide range of practical applications. Almost all traditional trackers meet some issues on UAV images due to camera movements causing view change in a 3D directions. In this work, we propose a Convolutional Neural Network specialized in multi-object tracking (MOT) for images captured from UAV. The architecture we introduced is composed by two main blocks: i) an object detection block based on YOLOv8 architecture; ii) an association block based on strongSORT architecture. We investigated different versions of YOLOv8 architectures with the strongSORT as association trackers. Experimental results on the VisDrone2019 dataset show that the proposed solution outperforms the up to date state-of-the-art tracking algorithms performance on UAV videos reaching the 42.03% in Multi-Object Tracking Accuracy (MOTA).  
   
 Pier Luigi Mazzeo, Alessandro Manica, Cosimo Distante   

 GLR: Gradient-Based Learning Rate Scheduler  
  Training a neural network is a complex and time-consuming process because of many combinations of hyperparameters that have to be adjusted and tested. One of the most crucial hyperparameters is the learning rate which controls the speed and direction of updates to the weights during training. We proposed an adaptive scheduler called Gradient-based Learning Rate scheduler (GLR) that significantly reduces the tuning effort thanks to a single user-defined parameter. GLR achieves competitive results in a very wide set of experiments compared to the state-of-the-art schedulers and optimizers. The computational cost of our method is trivial and can be used to train different network topologies.  
   
 Maria Ausilia Napoli Spatafora, Alessandro Ortis, Sebastiano Battiato   

 A Large-scale Analysis of Athletes’ Cumulative Race Time in Running Events  
  Action recognition models and cumulative race time (CRT) are practical tools in sports analytics, providing insights into athlete performance, training, and strategy. Measuring CRT allows for identifying areas for improvement, such as specific sections of a racecourse or the effectiveness of different strategies. Human action recognition (HAR) algorithms can help to optimize performance, with machine learning and artificial intelligence providing real-time feedback to athletes. This paper presents a comparative study of HAR algorithms for CRT regression, examining two important factors: the frame rate and the regressor selection. Our results indicate that our proposal exhibits outstanding performance for short input footage, achieving a mean absolute error of 11 min when estimating CRT for runners that have been on the course for durations ranging from 8 to 20 h.  
   
 David Freire-Obregón, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana   

 Uncovering Lies: Deception Detection in a Rolling-Dice Experiment  
  Deception detection is a challenging and interdisciplinary field that has garnered the attention of researchers in psychology, criminology, computer science, and even economics. While automated deception detection presents more obstacles than traditional polygraph tests, it also offers opportunities for novel economic applications. In this study, we propose a novel multimodal approach that combines deep learning with discriminative models to automate deception detection. We tested our approach on two datasets: the Rolling-Dice Experiment, an economically motivated experiment, and a real-life trial dataset for comparison. We utilized video and audio modalities, with video modalities generated through end-to-end learning (CNN). However, for actual deception detection, we employed discriminative approaches due to limited training data in this field. Our results show that the use of multiple modalities and feature selection improves detection results, particularly in the Rolling-Dice Experiment. Furthermore, we observed that due to minimized reactions, deception detection is much more difficult in the Rolling-Dice Experiment than in the high-stake dataset, quantified with an AUC of 0.65 compared to 0.86. Our study highlights the challenges and opportunities of automated deception detection for economic experiments, and our novel multimodal approach shows promise for future research in this field.  
   
 Laslo Dinges, Marc-André Fiedler, Ayoub Al-Hamadi, Ahmed Abdelrahman, Joachim Weimann, Dmitri Bershadskyy   

 Active Class Selection for Dataset Acquisition in Sign Language Recognition  
  Dataset collection for Sign Language Recognition (SLR) represents a challenging and crucial step in the development of modern automatic SLR systems. Typical acquisition protocols do not follow specific strategies, simply trying to gather equally represented classes. In this paper we provide some empirical evidences that alternative, more clever, strategies can be really beneficial, leading to a better performance of classification systems. In particular, we investigate the exploitation of ideas and tools of Active Class Selection (ACS), a peculiar Active Learning (AL) context specifically devoted to scenarios in which new data is labelled at the same time it is generated. In particular, differently from standard AL where a strategy asks for a specific label from an available set of unlabelled data, ACS strategies define from which class it is more convenient to acquire a new sample. In this paper, we show the beneficial effect of these methods in the SLR scenario, where these concepts have never been investigated. We studied both standard and novel ACS approaches, with experiments based on a challenging dataset recently collected for an ECCV challenge. We also preliminary investigate other possible exploitations of ACS ideas, for example to select which would be, for the classification system, the most beneficial signer.  
   
 Manuele Bicego, Manuel Vázquez-Enríquez, José L. Alba-Castro   

 MC-GTA: A Synthetic Benchmark for Multi-Camera Vehicle Tracking  
  Multi-camera vehicle tracking (MCVT) aims to trace multiple vehicles among videos gathered from overlapping and non-overlapping city cameras. It is beneficial for city-scale traffic analysis and management as well as for security. However, developing MCVT systems is tricky, and their real-world applicability is dampened by the lack of data for training and testing computer vision deep learning-based solutions. Indeed, creating new annotated datasets is cumbersome as it requires great human effort and often has to face privacy concerns. To alleviate this problem, we introduce MC-GTA - Multi Camera Grand Tracking Auto, a synthetic collection of images gathered from the virtual world provided by the highly-realistic Grand Theft Auto 5 (GTA) video game. Our dataset has been recorded from several cameras recording urban scenes at various crossroads. The annotations, consisting of bounding boxes localizing the vehicles with associated unique IDs consistent across the video sources, have been automatically generated by interacting with the game engine. To assess this simulated scenario, we conduct a performance evaluation using an MCVT SOTA approach, showing that it can be a valuable benchmark that mitigates the need for real-world data. The MC-GTA dataset and the code for creating new ad-hoc custom scenarios are available at https://github.com/GaetanoV10/GT5-Vehicle-BB .  
   
 Luca Ciampi, Nicola Messina, Gaetano Emanuele Valenti, Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro   

 A Differentiable Entropy Model for Learned Image Compression  
  In an end-to-end learned image compression framework, an encoder projects the image on a low-dimensional, quantized, latent space while a decoder recovers the original image. The encoder and decoder are jointly trained with standard gradient backpropagation to minimize a rate-distortion (RD) cost function accounting for both distortions between the original and reconstructed image and the quantized latent space rate. State-of-the-art methods rely on an auxiliary neural network to estimate the rate R of the latent space. We propose a non-parametric entropy model that estimates the statistical frequencies of the quantized latent space during training. The proposed model is differentiable, so it can be plugged into the cost function to be minimized as a rate proxy and can be adapted to a given context without retraining. Our experiments show comparable performance with a learned rate estimator and better performance when is adapted over a temporal context.  
   
 Alberto Presta, Attilio Fiandrotti, Enzo Tartaglione, Marco Grangetto   

 Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation  
  This paper presents a novel approach for generating 3D talking heads from raw audio inputs. Our method grounds on the idea that speech related movements can be comprehensively and efficiently described by the motion of a few control points located on the movable parts of the face, i.e., landmarks. The underlying musculoskeletal structure then allows us to learn how their motion influences the geometrical deformations of the whole face. The proposed method employs two distinct models to this aim: the first one learns to generate the motion of a sparse set of landmarks from the given audio. The second model expands such landmarks motion to a dense motion field, which is utilized to animate a given 3D mesh in neutral state. Additionally, we introduce a novel loss function, named Cosine Loss, which minimizes the angle between the generated motion vectors and the ground truth ones. Using landmarks in 3D talking head generation offers various advantages such as consistency, reliability, and obviating the need for manual-annotation. Our approach is designed to be identity-agnostic, enabling high-quality facial animations for any users without additional data or training. Code and models are available at: S2L+S2D .  
   
 Federico Nocentini, Claudio Ferrari, Stefano Berretti   

 SCENE-pathy: Capturing the Visual Selective Attention of People Towards Scene Elements  
  We present SCENE-pathy, a dataset and a set of baselines to study the visual selective attention (VSA) of people towards the 3D scene in which they are located. In practice, VSA allows to discover which parts of the scene are most attractive for an individual. Capturing VSA is of primary importance in the fields of marketing, retail management, surveillance, and many others. So far, VSA analysis focused on very simple scenarios: a mall shelf or a tiny room, usually with a single subject involved. Our dataset, instead, considers a multi-person and much more complex 3D scenario, specifically a high-tech fair showroom presenting machines of an Industry 4.0 production line, where 25 subjects have been captured for 2 min each when moving, observing the scene, and having social interactions. Also, the subjects filled out a questionnaire indicating which part of the scene was most interesting for them. Data acquisition was performed using Hololens 2 devices, which allowed us to get ground-truth data related to people’s tracklets and gaze trajectories. Our proposed baselines capture VSA from the mere RGB video data and a 3D scene model, providing interpretable 3D heatmaps. In total, there are more than 100K RGB frames with, for each person, the annotated 3D head positions and the 3D gaze vectors. The dataset is available here: https://intelligolabs.github.io/scene-pathy .  
   
 Andrea Toaiari, Federico Cunico, Francesco Taioli, Ariel Caputo, Gloria Menegaz, Andrea Giachetti, Giovanni Maria Farinella, Marco Cristani   

 Not with My Name! Inferring Artists’ Names of Input Strings Employed by Diffusion Models  
  Diffusion Models (DM) are highly effective at generating realistic, high-quality images. However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time. Is it acceptable to generate images reminiscent of an artist, employing his name as input? This imply that if the DM is able to replicate an artist’s work then it was trained on some or all of his artworks thus violating copyright. In this paper, a preliminary study to infer the probability of use of an artist’s name in the input string of a generated image is presented. To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists. Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability. Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image. Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .  
   
 Roberto Leotta, Oliver Giudice, Luca Guarnera, Sebastiano Battiato   

 Benchmarking of Blind Video Deblurring Methods on Long Exposure and Resource Poor Settings  
  This paper presents a benchmark evaluation of blind video deblurring methods in specific challenging settings. The employed videos are affected by severe deblurring artifacts and acquisition conditions (e.g., low resolution, high exposure, camera motion, complex scene motion, etc.). An in depth state of the art investigation has been carried out. Then, a specific set of methods based on mathematical optimization with image priors has been involved in our benchmark evaluation. The selected methods have been evaluated quantitatively and qualitatively.  
   
 Maria Ausilia Napoli Spatafora, Massimo O. Spata, Luca Guarnera, Alessandro Ortis, Sebastiano Battiato   

 LieToMe: An LSTM-Based Method for Deception Detection by Hand Movements  
  The ability to detect lies is a crucial skill in essential situations like police interrogations and court trials. At present, several devices, such as polygraphs and magnetic resonance, can ease the deception detection task. However, the effectiveness of these tools can be compromised by intentional behavioral changes due to the subject awareness of such appliances, suggesting that alternative ways must be explored to detect lies without using physical devices. In this context, this paper presents an approach focused on the extraction of meaningful features from hand gestures. The latter provide cues on the person’s behavior and are used to address the deception detection task in RGB videos of trials. Specifically, the proposed system extracts hands skeletons from an RGB video sequence and generates novel handcrafted features from the extrapolated keypoints to reflect the subject behavior through hand movements. Then, a long short-term memory (LSTM) neural network is used to classify these features and estimate whether the person is lying or not. Extensive experiments were performed to assess the quality of the derived features on a public collection of famous real-life trials. On this dataset, the proposed system sets new state-of-the-art performance on the unimodal hand-gesture deception detection task, demonstrating the effectiveness of the proposed approach and its handcrafted features.  
   
 Danilo Avola, Luigi Cinque, Maria De Marsico, Angelo Di Mambro, Alessio Fagioli, Gian Luca Foresti, Romeo Lanzino, Francesco Scarcello   

 Spatial Transformer Generative Adversarial Network for Image Super-Resolution  
  High-resolution images play an essential role in the performance of image analysis and pattern recognition methods. However, the expensive setup required to generate them and the inherent limitations of the sensors in optics manufacturing technology leads to the restricted availability of these images. In this work, we exploit the information retrieved in feature maps using the notable VGG networks and apply a transformer network to address spatial rigid affine transformation invariances, such as translation, scaling, and rotation. To evaluate and compare the performance of the model, three publicly available datasets were used. The model achieved very gratifying and accurate performance in terms of image PSNR and SSIM metrics against the baseline method.  
   
 Pantelis Rempakos, Michalis Vrigkas, Marina E. Plissiti, Christophoros Nikou   

 Real-Time GAN-Based Model for Underwater Image Enhancement  
  Enhancing image quality is crucial for achieving an accurate and reliable image analysis in vision-based automated tasks. Underwater imaging encounters several challenges that can negatively impact image quality, including limited visibility, color distortion, contrast sensitivity issues, and blurriness. Among these, depending on how the water filters out the different light colors at different depths, the color distortion results in a loss of color information and a blue or green tint to the overall image, making it difficult to identify different underwater organisms or structures accurately. Improved underwater image quality can be crucial in marine biology, oceanography, and oceanic exploration. Therefore, this paper proposes a novel Generative Adversarial Network (GAN) architecture for underwater image enhancement, restoring good perceptual quality to obtain a more precise and detailed image. The effectiveness of the proposed method is evaluated on the EUVP dataset, which comprises underwater image samples of various visibility conditions, achieving remarkable results. Moreover, the trained network is run on the RPi4B as an embedded system to measure the time required to enhance the images with limited computational resources, simulating a practical underwater investigation setting. The outcome demonstrates the presented method applicability in real-world underwater exploration scenarios.  
   
 Danilo Avola, Irene Cannistraci, Marco Cascio, Luigi Cinque, Anxhelo Diko, Damiano Distante, Gian Luca Foresti, Alessio Mecca, Ivan Scagnetto   

 HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains  
  We present HERO, an artificial assistant designed to communicate with users with both natural language and images to aid them carrying out procedures in industrial contexts. Our system is composed of five modules: 1) the input module retrieves user utterances and collects raw data, such as text and images, 2) the Natural Language Processing module processes text from user utterances, 3) the object detector module extracts entities by analyzing images captured by the user, 4) the Question Answering module generates responses to users’ specific questions on procedures, and 5) the output module selects the final response to give to the user. We deployed and evaluated the system in an industrial laboratory furnished with different tools and equipment for carrying out repair and test operations on electrical boards. In this setting, the HERO system allows the user to retrieve information on tools, equipment, procedures, and safety rules. Experiments on domain-specific labeled data, as well as a user study suggest that the design of our system is robust and that its use can be beneficial for users over classic methods for retrieving information and guide workers, such as printed manuals.  
   
 Claudia Bonanno, Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella   

 A Computer Vision-Based Water Level Monitoring System for Touchless and Sustainable Water Dispensing  
  In recent years, the need for contactless and sustainable systems has become increasingly relevant. The traditional water dispensers, which require contact with the dispenser and often involve single-use plastic cups or bottles, are not only unhygienic but also contribute to environmental pollution. This paper presents a touchless water dispenser system that uses artificial intelligence (AI) to control the dispensing of water or any liquid beverage. The system is designed to fill a container under the nozzle, dispense water when the container is aligned with the flow, and stop dispensing when the container is full, all without requiring any physical contact. This approach ensures compliance with hygiene regulations and promotes environmental sustainability by eliminating the need for plastic bottles or cups, making it a “plastic-free” and “zero waste” system. The prototype is based on a computer vision approach that employs an RGB camera and a Raspberry Pi board, which allows for real-time image processing and machine learning operations. The system uses image processing techniques to detect the presence of a container under the nozzle and then utilizes AI algorithms to control the flow of liquid. The system is trained using machine learning models and optimized to ensure accuracy and efficiency. We discuss the development and implementation of the touchless water dispenser system, including the hardware and software components used, the algorithms employed, and the testing and evaluation of the system. The results of our experiments show that the touchless water dispenser system is highly accurate and efficient, and it offers a safe and sustainable alternative to traditional water dispensers. The system has the potential to be used in a variety of settings, including public spaces, hospitals, schools, and offices, where hygiene and sustainability are of utmost importance.  
   
 Andrea Felicetti, Marina Paolanti, Rocco Pietrini, Adriano Mancini, Primo Zingaretti, Emanuele Frontoni   

 Smoothing and Transition Matrices Estimation to Learn with Noisy Labels  
  In recent years, there has been impressive progress in learning with noisy labels, particularly in leveraging a small set of clean data. Meta-learning-based label correction techniques have further advanced performance by correcting noisy labels during training. However, these methods require multiple back-propagation steps, which considerably slows down the training process. Alternatively, some researchers have attempted to estimate the label transition matrix on-the-fly to address the issue of noisy labels. These approaches are more robust and faster than meta-learning-based techniques. The use of the transition matrix makes the classifier skeptical about all corrected samples, thereby mitigating the problem of label noise. We propose a novel three-head architecture that can efficiently estimate the label transition matrix and two new label smoothing matrices at each iteration. Our approach enables the estimated matrices to closely follow the shifting noise and reduce over-confidence on classes during classifier model training. We report extensive experiments on synthetic and real world noisy datasets, achieving state of the art performance on synthetic variants of CIFAR-10/100 and on the challenging Clothing1M datasets. Code at https://github.com/z3n0e/STM .  
   
 Simone Ricci, Tiberio Uricchio, Alberto Del Bimbo   

 Semi-supervised Classification for Remote Sensing Datasets  
  Deep semi-supervised learning (DSSL) is a rapidly-growing field that takes advantage of a limited number of labeled examples to leverage massive amounts of unlabeled data. The underlying idea is that training on small yet well-selected examples can perform as effectively as a predictor trained on a larger number chosen at random [14]. In this study, we explore the most relevant approaches in DSSL literature like FixMatch [19], CoMatch [13], and, the class aware contrastive SSL (CCSSL) [25]. Our objective is to perform an initial comparative study of these methods and assess them on two remote sensing (RS) datasets: UCM [27] and AID [22]. The performance of these methods was determined based on their accuracy in comparison to a supervised benchmark. The results highlight that the CoMatch framework achieves the highest accuracy for both the UCM and AID datasets, with accuracies of 95.52% and 93.88% respectively. Importantly, all DSSL algorithms outperform the supervised benchmark, emphasizing their effectiveness in leveraging a limited number of labeled examples to enhance classification accuracy for remote sensing scene classification tasks. The code used in this study was adapted from CCSSL [25] and the detailed implementation will be accessible at https://github.com/itzahs/SSL-for-RS .  
   
 Itza Hernandez-Sequeira, Ruben Fernandez-Beltran, Yonghao Xu, Pedram Ghamisi, Filiberto Pla   

 Exploiting Exif Data to Improve Image Classification Using Convolutional Neural Networks  
  In addition to photo data, many digital cameras and smartphones capture Exif metadata which contain information about the camera parameters used when a photo was captured. While most semantic image recognition approaches only use pixel data for classification decisions, this work aims to examine whether Exif data can improve image classification performed by Convolutional Neural Networks (CNNs). We compare the classification performance and training time of fusion models that use both, image data and Exif metadata, for image classification in contrast to models that use only image data. The most promising result was obtained with a fusion model which was able to increase the classification accuracy for the selected target concepts by $$7.5\%$$ 7.5 % compared to the baseline, while the average total training time of all fusion models was reduced by $$7.9\%$$ 7.9 % .  
   
 Ralf Lederer, Martin Bullin, Andreas Henrich   

 Weak Segmentation-Guided GAN for Realistic Color Edition  
  Editing the color of images in a realistic way finds many applications such as changing the perception of an image, data augmentation or film post processing. The design of an automatic tool is a complex and long addressed challenge. In particular, two properties are difficult to meet altogether: generating realistic results without artifacts and the possibility to precisely choose the future color. Conventional methods using segmentation and histogram matching maximize the controllability but also introduce a lack of realism or are complex to automate. On the contrary, GANs that specialize in realism are difficult to control. To overcome these challenges, we propose a novel GAN architecture leveraging any differentiable segmentation model. We demonstrate the genericness of our framework that presents state of the art results on different use cases. It generates images that look realistic while offering a precise color control.  
   
 Vincent Auriau, Emmanuel Malherbe, Matthieu Perrot   

 Hand Gesture Recognition Exploiting Handcrafted Features and LSTM  
  Hand gesture recognition finds application in several heterogeneous fields, such as Human-Computer Interaction, serious games, sign language interpretation, and more. Modern recognition approaches use Deep Learning methods due to their ability in extracting features without human intervention. The drawback of this approach is the need for huge datasets which, depending on the task, are not always available. In some cases, handcrafted features increase the capability of a model in achieving the proposed task, and usually require fewer data with respect to Deep Learning approaches. In this paper, we propose a method that synergistically makes use of handcrafted features and Deep Learning for performing hand gesture recognition. Concerning the features, they are engineered from hand joints, while for Deep Learning, a simple LSTM together with a multilayer perceptron is used. The tests were performed on the DHG dataset, comparing the proposed method with both state-of-the-art methods that use handcrafted features and methods that use learned features. Our approach overcomes the state-of-the-art handcrafted features methods in both 14 and 28 gestures recognition tests, while we overcome the state-of-the-art learned features methods for the 14 gesture recognition test, proving that it is possible to use a simpler model with well engineered features.  
   
 Danilo Avola, Luigi Cinque, Emad Emam, Federico Fontana, Gian Luca Foresti, Marco Raoul Marini, Daniele Pannone   

 An Optimized Pipeline for Image-Based Localization in Museums from Egocentric Images  
  With the increasing interest in augmented and virtual reality, visual localization is acquiring a key role in many downstream applications requiring a real-time estimate of the user location only from visual streams. In this paper, we propose an optimized hierarchical localization pipeline by specifically tackling cultural heritage sites with specific applications in museums. Specifically, we propose to enhance the Structure from Motion (SfM) pipeline for constructing the sparse 3D point cloud by a-priori filtering blurred and near-duplicated images. We also study an improved inference pipeline that merges similarity-based localization with geometric pose estimation to effectively mitigate the effect of strong outliers. We show that the proposed optimized pipeline obtains the lowest localization error on the challenging Bellomo dataset [11]. Our proposed approach keeps both build and inference times bounded, in turn enabling the deployment of this pipeline in real-world scenarios.  
   
 Nicola Messina, Fabrizio Falchi, Antonino Furnari, Claudio Gennaro, Giovanni Maria Farinella   

 Annotating the Inferior Alveolar Canal: The Ultimate Tool  
  The Inferior Alveolar Nerve (IAN) is of main interest in the maxillofacial field, as an accurate localization of such nerve reduces the risks of injury during surgical procedures. Although recent literature has focused on developing novel deep learning techniques to produce accurate segmentation masks of the canal containing the IAN, there are still strong limitations due to the scarce amount of publicly available 3D maxillofacial datasets. In this paper, we present an improved version of a previously released tool, iacat (Inferior Alveolar Canal Annotation Tool), today used by medical experts to produce 3D ground truth annotation. In addition, we release a new dataset, ToothFairy, which is part of the homonymous MICCAI2023 challenge hosted by the Grand-Challenge platform, as an extension of the previously released Maxillo dataset, which was the only publicly available. With ToothFairy, the number of annotations has been increased as well as the quality of existing data.  
   
 Luca Lumetti, Vittorio Pipoli, Federico Bolelli, Costantino Grana   

 A Learnable EVC Intra Predictor Using Masked Convolutions  
  The Enhanced Video Coding (EVC) workgroup of the Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) organization aims at enhancing traditional video codecs by improving or replacing traditional encoding tools with AI-based counterparts. In this work, we explore enhancing MPEG Essential Video Coding (EVC) intra prediction with a learnable predictor: we recast the problem as a hole inpainting task that we tackle via masked convolutions. Our experiments in standard test conditions show BD-rate reductions in excess of 6% over the EVC baseline profile reference with some sequences in excess of 12%.  
   
 Gabriele Spadaro, Roberto Iacoviello, Alessandra Mosca, Giuseppe Valenzise, Attilio Fiandrotti   

 Enhancing PFI Prediction with GDS-MIL: A Graph-Based Dual Stream MIL Approach  
  Whole-Slide Images (WSI) are emerging as a promising resource for studying biological tissues, demonstrating a great potential in aiding cancer diagnosis and improving patient treatment. However, the manual pixel-level annotation of WSIs is extremely time-consuming and practically unfeasible in real-world scenarios. Multi-Instance Learning (MIL) have gained attention as a weakly supervised approach able to address lack of annotation tasks. MIL models aggregate patches (e.g., cropping of a WSI) into bag-level representations (e.g., WSI label), but neglect spatial information of the WSIs, crucial for histological analysis. In the High-Grade Serous Ovarian Cancer (HGSOC) context, spatial information is essential to predict a prognosis indicator (the Platinum-Free Interval, PFI) from WSIs. Such a prediction would bring highly valuable insights both for patient treatment and prognosis of chemotherapy resistance. Indeed, NeoAdjuvant ChemoTherapy (NACT) induces changes in tumor tissue morphology and composition, making the prediction of PFI from WSIs extremely challenging. In this paper, we propose GDS-MIL, a method that integrates a state-of-the-art MIL model with a Graph ATtention layer (GAT in short) to inject a local context into each instance before MIL aggregation. Our approach achieves a significant improvement in accuracy on the “Ome18” PFI dataset. In summary, this paper presents a novel solution for enhancing PFI prediction in HGSOC, with the potential of significantly improving treatment decisions and patient outcomes.  
   
 Gianpaolo Bontempo, Nicola Bartolini, Marta Lovino, Federico Bolelli, Anni Virtanen, Elisa Ficarra   

 Backmatter  

 Metadaten   

 Titel  Image Analysis and Processing – ICIAP 2023    
 herausgegeben von  Gian Luca Foresti  
  Andrea Fusiello  
  Edwin Hancock  
     
 Copyright-Jahr  2023    
 Verlag  Springer Nature Switzerland     
   
 Electronic ISBN  978-3-031-43148-7    
 Print ISBN  978-3-031-43147-0    
 DOI  https://doi.org/10.1007/978-3-031-43148-7     

 Premium Partner  

 » zur Fachgebietsseite Business IT + Informatik 

 Bildnachweise  Rittal/© Rittal, NTT Data/© NTT Data, Wildix/© Wildix, Ceyoniq Technology GmbH/© Ceyoniq Technology GmbH, arvato Systems GmbH/© arvato Systems GmbH, Ninox Software GmbH/© Ninox Software GmbH, Everyday Software S.L./© Everyday Software S.L., Redgate/© Redgate, CELONIS Labs GmbH, DC-Datacenter-Group GmbH/© DC-Datacenter-Group GmbH, all for one/© all for one, G Data CyberDefense/© G Data CyberDefense   

 Weitere Formate | Podcasts 
  Webinare Technik 
  Webinare Wirtschaft 
  Kongresse 
  Veranstaltungskalender 
  Awards 
  MyNewsletter 
  Fachgebiete | Automobil + Motoren 
  Bauwesen + Immobilien 
  Business IT + Informatik 
  Elektrotechnik + Elektronik 
  Energie + Nachhaltigkeit 
  Finance + Banking 
  Management + Führung 
  Marketing + Vertrieb 
  Maschinenbau + Werkstoffe 
  Versicherung + Risiko 
  Bücher 
  Zeitschriften 
  Themenseiten | Organisationspsychologie 
  Projektmanagement 
  Marketing 
  Smart Manufacturing 
  Jetzt Einzelzugang starten 
  Zugang für Unternehmen 
  Referenzkunden 
  Sustainability in Automotive 
   MyNewsletter 

  Über uns:     
  
 In eigener Sache    Das Team    Redaktionelles Leitbild    Hilfe    Referenzen      
 Unsere Produkte:     
  
 Einzelzugang    Zugang für Unternehmen    PatentFit    MyAlerts    Professional Book Archive    MyNewsletter    Carl Hanser Verlag - Bücher    KI-gestützte Suche      
 Rechtliche Informationen:     
  
 Impressum    AGB    Datenschutzerklärung    Cookies    Cookies verwalten    Verträge hier kündigen    Zahlungsarten      
 Weiterführende Links:     
  
 RSS-Feeds    Social Media    Mediadaten    Corporate Solutions    Whitepaper    Gabler Wirtschaftslexikon    Gabler Banklexikon    Versicherungsmagazin      
   
 Springer Nature Logo    © Springer Fachmedien Wiesbaden GmbH  
   
 Version: 0.3926.0