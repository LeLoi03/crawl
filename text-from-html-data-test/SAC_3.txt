Anmelden 
  Registrierung 
  Deutsch  English 
  Español 
  Português 
  Français 

     Dom 
  Najlepsze kategorie | CAREER & MONEY 
  PERSONAL GROWTH 
  POLITICS & CURRENT AFFAIRS 
  SCIENCE & TECH 
  HEALTH & FITNESS 
  LIFESTYLE 
  ENTERTAINMENT 
  BIOGRAPHIES & HISTORY 
  FICTION 
  Najlepsze historie 
  Najlepsze historie 
  Dodaj historię 
  Moje historie 

 Home 
  Selected Areas in Cryptography – SAC 2023. 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers 9783031533679, 9783031533686 

 Selected Areas in Cryptography – SAC 2023. 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers 9783031533679, 9783031533686   
   
  205    109    16MB    
  English   Pages 457   Year 2024    
  Report DMCA / Copyright    
  DOWNLOAD FILE   
   
 Polecaj historie   

 Selected Areas in Cryptography – SAC 2018: 25th International Conference, Calgary, AB, Canada, August 15–17, 2018, Revised Selected Papers [1st ed.] 978-3-030-10969-1, 978-3-030-10970-7  
 This book contains revised selected papers from the 25th International Conference on Selected Areas in Cryptography, SAC  
  306    69    17MB    Read more   

 Ubiquitous Security: Third International Conference, UbiSec 2023, Exeter, UK, November 1–3, 2023, Revised Selected Papers 9789819712748, 9819712742  
  
  156    97    Read more   

 Parallel Computational Technologies. 17th International Conference, PCT 2023 Saint Petersburg, Russia, March 28–30, 2023 Revised Selected Papers 9783031388637, 9783031388644  
  
  155    55    31MB    Read more   

 Frontiers in Cyber Security: 6th International Conference, FCS 2023, Chengdu, China, August 21–23, 2023, Revised Selected Papers (Communications in Computer and Information Science) 981999330X, 9789819993307  
 This volume constitutes the refereed proceedings of the 6th International Conference on Frontiers in Cyber Security, FCS  
  117    68    Read more   

 Cognitive Systems and Information Processing: 8th International Conference, ICCSIP 2023, Luoyang, China, August 10–12, 2023, Revised Selected Papers, ... in Computer and Information Science) 9819980208, 9789819980208  
 The two-volume set CCIS 1918 and 1919 constitutes the refereed post-conference proceedings of the 8th International Conf  
  171    6    Read more   

 Cognitive Systems and Information Processing: 8th International Conference, ICCSIP 2023, Luoyang, China, August 10–12, 2023, Revised Selected Papers, ... in Computer and Information Science) 9819980178, 9789819980178  
  
  130    22    Read more   

 Computer Science and Educational Informatization: 5th International Conference, CSEI 2023, Kunming, China, August 11–13, 2023, Revised Selected Papers [1] 9819994985, 9789819994984  
 These two volumes constitute the revised selected papers of the 5th International Conference, CSEI 2023, held in Kunming  
  182    56    1MB    Read more   

 Code-Based Cryptography: 11th International Workshop, CBCrypto 2023, Lyon, France, April 22–23, 2023, Revised Selected Papers (Lecture Notes in Computer Science) 303146494X, 9783031464942  
 This book constitutes the refereed proceedings of the 11th International Conference on Code-Based Cryptography, CBCrypto  
  139    119    7MB    Read more   

 High Performance Computing: 10th Latin American Conference, CARLA 2023, Cartagena, Colombia, September 18–22, 2023, Revised Selected Papers 3031521854, 9783031521850  
 This book constitutes the refereed revised selected papers of the 10th Latin American Conference on High Performance Com  
  149    74    24MB    Read more   

 High Performance Computing: 10th Latin American Conference, CARLA 2023, Cartagena, Colombia, September 18–22, 2023, Revised Selected Papers 3031521854, 9783031521850  
 This book constitutes the refereed revised selected papers of the 10th Latin American Conference on High Performance Com  
  100    0    27MB    Read more   

 Author / Uploaded 
  Claude Carlet 
  Kalikinkar Mandal 
  Vincent Rijmen 

 Table of contents :  
  Preface  
  Organization  
  Invited Talks  
  Hardware Security—Directions and Challenges  
  Robust and Non-malleable Threshold Schemes, AMD Codes and External Difference Families  
  A Geometric Approach to Symmetric-Key Cryptanalysis  
  Contents  
  Cryptanalysis of Lightweight Ciphers  
  More Balanced Polynomials: Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
  1 Introduction  
  1.1 Our Contributions  
  1.2 Outline  
  2 Preliminaries  
  2.1 Notation  
  2.2 Boolean Functions and Algebraic Degree  
  2.3 Pseudo-code of Trivium  
  2.4 Cube Attack  
  2.5 The Bit-Based Division Property and Monomial Prediction  
  3 Obtain More Balanced Polynomials  
  3.1 Variable Substitutions  
  3.2 More Balanced Polynomials by Canceling Quadratic Terms  
  3.3 Relationship Between a Cube and Its Subcubes  
  4 Application  
  4.1 A Key-Recovery Attack on 810-Round Trivium with Practical Complexity  
  4.2 A Key-Recovery Attack on 825-Round Trivium with Practical Complexity  
  5 Conclusion  
  References  
  A Closer Look at the S-Box: Deeper Analysis of Round-Reduced ASCON-HASH  
  1 Introduction  
  2 Preliminaries  
  2.1 Notations  
  2.2 Description of ASCON-HASH  
  3 The Attack Frameworks  
  3.1 The Literature and Our New Strategy  
  4 Collision Attacks on 2-Round ASCON-HASH  
  4.1 Optimizing Tk Using Simple Linear Algebra  
  4.2 Finding Valid (M1,M2) with Advanced Techniques  
  4.3 Further Optimizing the Guessing Strategy  
  5 Conclusion  
  A The Algorithmic Description of ASCON-HASH  
  B On Degree-2 S-Box  
  References  
  Improving the Rectangle Attack on GIFT-64  
  1 Introduction  
  2 Preliminary  
  2.1 Description of GIFT-64  
  2.2 The Rectangle Attack  
  2.3 The Rectangle Distinguisher of GIFT-64  
  3 New Rectangle Key Recovery Attack on GIFT-64  
  3.1 The Dedicated Model and New Key Guessing Strategy  
  3.2 New Rectangle Attack on GIFT-64  
  3.3 Complexity Analysis  
  4 Discussion and Conclusion  
  References  
  Side-Channel Attacks and Countermeasures  
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
  1 Introduction  
  2 Mask Compression  
  3 Security Arguments  
  4 Experiment: Order-31 Lattice Signatures  
  4.1 SampleG(z) in Hardware  
  4.2 Implementation Details and Basic Leakage Assessment  
  5 Conclusions and Open Problems  
  References  
  Not so Difficult in the End: Breaking the Lookup Table-Based Affine Masking Scheme  
  1 Introduction  
  2 Preliminaries  
  2.1 Notation  
  2.2 Side-Channel Analysis  
  2.3 Side-Channel Collision Attack  
  2.4 The ANSSI's AES Implementation: ASCADv2  
  3 Related Works  
  4 Vulnerability Analysis  
  4.1 Constant Affine Mask Shares for an Encryption  
  4.2 Zero Input of Affine Masking Scheme  
  5 Attack Results  
  5.1 Side-Channel Collision Attack on Canceling Mask Shares  
  5.2 Correlation Attack on GF(0)  
  6 Discussion and Protection Methods  
  7 Conclusions and Future Work  
  References  
  Threshold Implementations with Non-uniform Inputs  
  1 Introduction  
  2 Preliminaries  
  2.1 Glitch-Extended Bounded-Query Probing Security  
  2.2 Boolean Masking and Threshold Implementations  
  2.3 Linear Cryptanalysis of Maskings  
  3 Analysis of TIs with Non-uniform Inputs  
  3.1 Masked Prince with Non-uniform Inputs  
  3.2 Masked Midori64 with Non-uniform Inputs  
  4 Practical Evaluation and Efficiency  
  4.1 PROLEAD Experiments  
  4.2 FPGA Experiments  
  4.3 Efficiency Comparison  
  5 Conclusion  
  A Prince S-Box Masking  
  B Midori64 S-Box Masking  
  C PROLEAD Experiments  
  C.1 Designs with Uniform Randomness  
  C.2 Midori64, Non-uniform Randomness  
  C.3 Prince, Non-uniform Randomness  
  C.4 Midori64, ``Secure'' Non-uniform Case, Column-Wise  
  References  
  Post-Quantum Constructions  
  SMAUG: Pushing Lattice-Based Key Encapsulation Mechanisms to the Limits  
  1 Introduction  
  1.1 Our Results  
  2 Preliminaries  
  2.1 Notation  
  2.2 Public Key Encryption and Key Encapsulation Mechanism  
  2.3 Fujisaki-Okamoto Transform  
  3 Design Choices  
  3.1 MLWE Public Key and MLWR Ciphertext  
  3.2 Sparse Secret  
  3.3 Discrete Gaussian Noise  
  3.4 Polynomial Multiplication Using Sparsity  
  3.5 FO Transform, FOm  
  4 The SMAUG  
  4.1 Specification of SMAUG.PKE  
  4.2 Specification of SMAUGKEM  
  4.3 Security Proof  
  5 Parameter Selection and Concrete Security  
  5.1 Concrete Security Estimation  
  5.2 Parameter Sets  
  5.3 Decryption Failure Probability  
  6 Implementation  
  6.1 Performance  
  6.2 Comparison to Prior/Related Work  
  6.3 Security Against Physical Attacks  
  References  
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
  1 Introduction  
  2 Preliminaries  
  3 Oblivious Pseudorandom Functions  
  3.1 Security Assumptions  
  4 Countermeasures Against the Unpredictability Attacks  
  5 Countermeasures Against the SIDH Attacks  
  5.1 Adapting the Proof of Isogeny Knowledge  
  6 Verifiability  
  7 A New OPRF Protocol  
  8 Conclusion  
  References  
  Traceable Ring Signatures from Group Actions: Logarithmic, Flexible, and Quantum Resistant  
  1 Introduction  
  1.1 Related Work on Post Quantum Ring Signature  
  1.2 Contribution  
  1.3 Overview of Results  
  2 Preliminaries  
  2.1 Traceable Ring Signature  
  2.2 Security Model  
  2.3 Restricted Pair of Group Actions  
  2.4 Collision-Resistant Hash Function  
  2.5 Sigma Protocol  
  3 General Construction of Traceable Ring Signature  
  3.1 Our Special or Sigma Protocol for Traceable Ring Signature  
  3.2 Traceable Ring Signature from or Sigma Protocol  
  4 Analysis of Our Traceable Ring Signature Scheme  
  4.1 Correctness  
  4.2 Security  
  5 Instantiations  
  5.1 Implementation and Performance  
  6 Conclusion  
  References  
  Symmetric Cryptography and Fault Attacks  
  The Random Fault Model  
  1 Introduction  
  2 Background  
  2.1 Notation  
  2.2 Algorithmic Representation  
  2.3 Random Probing Model  
  3 Random Fault Model  
  3.1 Random Fault Adversary  
  3.2 Security Model: Correctness  
  3.3 Security Model: Privacy  
  4 Case Studies: Random Probing Model  
  4.1 Influence of Duplication  
  4.2 Influence of Masking  
  4.3 Influence of Masked Duplication  
  4.4 Influence of Shuffling  
  5 Case Studies: Random Fault Correctness  
  5.1 Influence of Masking  
  5.2 Influence of Duplication  
  5.3 Influence of Triplication  
  5.4 Influence of Masked Duplication  
  5.5 Influence of Multiplicative Tags  
  5.6 Influence of Shuffling  
  6 Case Studies: Random Fault Privacy  
  6.1 Influence of Duplication  
  6.2 Influence of Masked Duplication  
  6.3 Influence of Shuffling  
  7 Conclusion  
  References  
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
  1 Introduction  
  2 Preliminaries  
  3 Probabilistic Related-Key Statistical Saturation Attack  
  3.1 Introducing a Statistical Model into RKSS Cryptanalysis  
  3.2 Experimental Verification of the Statistical Model  
  4 Improved Key Recovery Attacks on Piccolo Considering Pre- And Post-whitening  
  4.1 Probabilistic RKSS Attack on 10-Round Piccolo-80  
  4.2 Probabilistic RKSS Attack on 16-Round Piccolo-128  
  4.3 Probabilistic RKSS Attack on 17-Round Piccolo-128  
  5 Conclusion and Future Work  
  References  
  Compactly Committing Authenticated Encryption Using Encryptment and Tweakable Block Cipher  
  1 Introduction  
  2 Preliminaries  
  2.1 Tweakable Block Cipher  
  2.2 ccAEAD  
  2.3 Encryptment  
  3 ccAEAD Using Encryptment and TBC  
  3.1 Scheme  
  3.2 Security  
  4 Remotely Keyed ccAEAD  
  4.1 Syntax  
  4.2 Security Requirement  
  5 ECT as RK ccAEAD  
  5.1 Scheme  
  5.2 Security  
  6 Conclusions  
  References  
  Post-Quantum Analysis and Implementations  
  Bit Security Analysis of Lattice-Based KEMs Under Plaintext-Checking Attacks  
  1 Introduction  
  2 Preliminaries  
  3 Side-Channel Information in Plaintext-Checking Attacks  
  3.1 The Meta-Structure of IND-CPA Secure KEM  
  3.2 Model of Plaintext-Checking Attack  
  3.3 Secret Leakage Model in Plaintext-Checking Attack  
  4 Reducing PC-Hint to Perfect Inner-Product Hint  
  4.1 Practical Plaintext-Checking Attack with Theoretical Lower Bound  
  4.2 Message Leakage in Each Query  
  4.3 Integrating Plaintext-Checking Hints into Lattice  
  5 Experiment Results  
  5.1 Kyber  
  5.2 Saber  
  5.3 Frodo  
  5.4 NewHope  
  6 Conclusions and Discussions  
  References  
  Quantum Cryptanalysis of OTR and OPP: Attacks on Confidentiality, and Key-Recovery  
  1 Introduction  
  1.1 Our Contributions  
  2 Preliminaries  
  3 Quantum Attacks on Confidentiality of OTR  
  3.1 Specifications of AES-OTR  
  3.2 IND-qCPA Attack on AES-OTR with Parallel AD Processing  
  3.3 IND-qCPA Attack on AES-OTR with Serial AD Processing  
  4 Quantum Key-Recovery Attack on OPP  
  4.1 Specification of OPP  
  4.2 Our Quantum Key-Recovery Attack  
  References  
  Fast and Efficient Hardware Implementation of HQC  
  1 Introduction  
  1.1 Open-Source Design  
  2 Hardware Design of HQC  
  2.1 Modules Common Across the Design  
  2.2 Encode and Decode Modules  
  2.3 Encrypt and Decrypt Modules  
  2.4 Key Generation  
  2.5 Encapsulation Module  
  2.6 Decapsulation Module  
  3 HQC Joint Design and Related Work  
  3.1 HQC Joint Design  
  3.2 Evaluation and Comparison to Existing HQC Hardware Designs  
  4 Conclusion  
  References  
  Homomorphic Encryption  
  On the Precision Loss in Approximate Homomorphic Encryption  
  1 Introduction  
  2 Preliminaries  
  3 Encoding Analysis  
  3.1 Mapping Theory  
  3.2 Application to Encoding  
  4 Noise Analysis Methods  
  4.1 Bounding Noise Random Variables  
  4.2 Worst-Case Noise Analysis Methods  
  4.3 Average-Case Noise Analysis Method  
  4.4 Summary of Textbook Noise Heuristics  
  5 Application of Methods to RNS-CKKS  
  5.1 Differences from Textbook CKKS  
  5.2 Distribution of Noise Polynomials for the RNS Variant ch16SAC:CHKKS18  
  5.3 Summary Tables of Noise Bounds  
  6 Experimental Results  
  References  
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
  1 Introduction  
  2 Related Work  
  3 Cryptographic Preliminaries  
  3.1 Linear Embeddings of Boolean Functions in Residue Sequences  
  3.2 Prime Numbers with Chosen Residue Symbol Sequences  
  4 Our Cryptosystem  
  4.1 Key Generation  
  4.2 Encryption and Decryption  
  4.3 Correctness of the Evaluation Function  
  5 Semantic Security of CS  
  6 Protocol  
  6.1 Participant Privacy During the Protocol  
  6.2 PB's Privacy  
  7 Results and Discussion  
  7.1 Experiments  
  7.2 An Example Case of Secure Function Evaluation Using  
  8 Conclusion  
  References  
  Public-Key Cryptography  
  Generalized Implicit Factorization Problem  
  1 Introduction  
  2 Notations and Preliminaries  
  2.1 Lattices, SVP, and LLL  
  2.2 Coppersmith's Method  
  3 Generalized Implicit Factorization Problem  
  3.1 Description of GIFP  
  3.2 Algorithm for GIFP  
  4 Experimental Results  
  5 Conclusion and Open Problem  
  A Details of calculations in Section3.2  
  References  
  Differential Cryptanalysis  
  CLAASP: A Cryptographic Library for the Automated Analysis of Symmetric Primitives  
  1 Introduction  
  1.1 Related Works  
  1.2 Our Contribution  
  2 Symmetric Primitives in CLAASP  
  2.1 The Component Class  
  2.2 The Cipher Class  
  3 Library: Evaluation Modules  
  4 Library: Test Modules  
  4.1 Component Analysis  
  4.2 Statistical and Avalanche Tests  
  4.3 Constraint Solvers for Differential and Linear Cryptanalysis  
  4.4 Continuous Diffusion Tests  
  4.5 Algebraic Module  
  4.6 Neural Aided Cryptanalysis Module  
  5 Benchmark Comparison with Other Libraries  
  5.1 TAGADA  
  5.2 CASCADA  
  6 Conclusion  
  References  
  Parallel SAT Framework to Find Clustering of Differential Characteristics and Its Applications  
  1 Introduction  
  2 Preliminaries  
  2.1 Definitions of Differential Characteristic and Differential  
  2.2 SAT-Based Automatic Search for Differential Characteristics  
  3 A New SAT Framework to Find the Best Differential  
  3.1 Our Approach  
  3.2 Incremental SAT Problem  
  3.3 Finding a Good Differential  
  3.4 Optimizing the Efficiency by a Multi-threading Technique  
  3.5 A More Efficient Algorithm to Find a Good Differential  
  4 Applications to PRINCE and QARMA  
  4.1 Good Differentials for PRINCE  
  4.2 Good Differentials for QARMA  
  4.3 Discussion: Comparison with PRINCE and QARMA  
  5 Conclusion  
  References  
  Deep Learning-Based Rotational-XOR Distinguishers for AND-RX Block Ciphers: Evaluations on Simeck and Simon  
  1 Introduction  
  1.1 Related Works  
  1.2 Our Contribution  
  1.3 Outline  
  2 Preliminaries  
  2.1 AND-RX Ciphers  
  2.2 Rotational-XOR (RX) Cryptanalysis  
  2.3 Deep Learning and Its Application on Symmetric Cryptography  
  3 Identification of Optimal RX Distinguishers in Cryptanalysis with Evolutionary Algorithm  
  3.1 AI Tools and Model Development  
  3.2 Training DL-Based RX Distinguishers  
  3.3 Evolutionary Optimization of Deep Learning RX Differential Distinguishers  
  4 Results and Discussion  
  4.1 Simeck Cipher  
  4.2 Simon Cipher  
  5 Impact of the Diffusion Layer and Optimal Rotation Parameters  
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  LNCS 14201  
   
  Claude Carlet Kalikinkar Mandal Vincent Rijmen (Eds.)  
   
  Selected Areas in Cryptography – SAC 2023 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers  
   
  Lecture Notes in Computer Science Founding Editors Gerhard Goos Juris Hartmanis  
   
  Editorial Board Members Elisa Bertino, Purdue University, West Lafayette, IN, USA Wen Gao, Peking University, Beijing, China Bernhard Steffen , TU Dortmund University, Dortmund, Germany Moti Yung , Columbia University, New York, NY, USA  
   
  14201  
   
  The series Lecture Notes in Computer Science (LNCS), including its subseries Lecture Notes in Artificial Intelligence (LNAI) and Lecture Notes in Bioinformatics (LNBI), has established itself as a medium for the publication of new developments in computer science and information technology research, teaching, and education. LNCS enjoys close cooperation with the computer science R & D community, the series counts many renowned academics among its volume editors and paper authors, and collaborates with prestigious societies. Its mission is to serve this international community by providing an invaluable service, mainly focused on the publication of conference and workshop proceedings and postproceedings. LNCS commenced publication in 1973.  
   
  Claude Carlet · Kalikinkar Mandal · Vincent Rijmen Editors  
   
  Selected Areas in Cryptography – SAC 2023 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers  
   
  Editors Claude Carlet Université Paris 8 Saint-Denis Cedex, France  
   
  Kalikinkar Mandal University of New Brunswick Fredericton, NB, Canada  
   
  University of Bergen Bergen, Norway Vincent Rijmen KU Leuven Heverlee, Belgium University of Bergen Bergen, Norway  
   
  ISSN 0302-9743 ISSN 1611-3349 (electronic) Lecture Notes in Computer Science ISBN 978-3-031-53367-9 ISBN 978-3-031-53368-6 (eBook) https://doi.org/10.1007/978-3-031-53368-6 © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland Paper in this product is recyclable.  
   
  Preface  
   
  Selected Areas in Cryptography (SAC) is Canada’s annual research conference on cryptography, held since 1994. The 30th edition of SAC took place at the University of New Brunswick in Fredericton, New Brunswick, Canada, during August 16–18, 2023. There are four areas covered at each SAC conference. Three of the areas are permanent: – Design and analysis of symmetric key primitives and cryptosystems, including block and stream ciphers, hash functions, MAC algorithms, cryptographic permutations, and authenticated encryption schemes. – Efficient implementations of symmetric, public key, and post-quantum cryptography. – Mathematical and algorithmic aspects of applied cryptology, including post-quantum cryptology. The fourth area is selected as a special topic for each edition. The special topic for SAC 2023 was – Counter-measures against side channel attacks in symmetric cryptography. We received 48 submissions, among which 3 were withdrawn before reviewing. The remainng submissions were reviewed in a double-blind review process. Regular submissions received three reviews whereas submissions by Program Committee (PC) members were reviewed by five PC members. All in all, 139 reviews were written by our Program Committee, consisting of 39 members, and with the help of 30 subreviewers. Eventually, 21 papers were accepted for publication in these proceedings and presentation at the conference. There were three invited talks at SAC 2023. The Stafford Tavares Invited Lecture, entitled “Robust and Non-malleable Threshold Schemes, AMD Codes and External Difference Families” was given by Doug Stinson. The first invited talk was given by Tim Güneysu on the topic of “Hardware Security—Directions and Challenges.” The second invited talk was given by Tim Beyne on the topic of “A Geometric Approach to Symmetric-Key Cryptanalysis.” The program of SAC 2023 was completed by a preceding two-day summer school on August 14–15, 2023. During the summer school, there was one day of lectures about “Lattice-Based Cryptography” held by Adeline Roux-Langlois and “Postquantum Cryptography” held by David Jao. The second day focused on “Linear and Differential Cryptanalysis” with lectures by Tim Beyne, and “Physical Attacks and Countermeasures” with lectures by Tim Güneysu. We would like to thank all our colleagues who helped to make SAC 2023 a success. Especially, we would like to thank the Program Committee members and their subreviewers, for their hard work, and the invited speakers and the summer school lecturers  
   
  vi  
   
  Preface  
   
  who nicely enhanced the conference. We also thank all colleagues who submitted a paper and all speakers. December 2023  
   
  Claude Carlet Kalikinkar Mandal Vincent Rijmen  
   
  Organization  
   
  Program Chairs Claude Carlet Kalikinkar Mandal Vincent Rijmen  
   
  Université Paris 8, France and University of Bergen, Norway University of New Brunswick, Canada KU Leuven, Belgium and University of Bergen, Norway  
   
  Program Committee Riham AlTawy Melissa Azouaoui Paulo Barreto Jean-François Biasse Olivier Blazy Wouter Castryck Joan Daemen Maria Eichlseder Aurore Guillevic Ryan Henry Takanori Isobe Yunwen Liu Subhamoy Maitra Barbara Masucci Cuauhtemoc Mancillas-López Bart Mennink Ruben Niederhagen Svetla Nikova Colin O’Flynn Stjepan Picek Elizabeth A. Quaglia Håvard Raddum Matthieu Rivain  
   
  University of Victoria, Canada NXP Semiconductors, Germany University of Washington, Tacoma, USA University of South Florida, USA Ecole Polytechnique, France KU Leuven, Belgium Radboud University, The Netherlands Graz University of Technology, Austria Inria Nancy, France and Aarhus University, Denmark University of Calgary, Canada University of Hyogo, Japan Cryptape Technology Co. Ltd, China Indian Statistical Institute, India University of Salerno, Italy CINVESTAV-IPN, Mexico Radboud University, The Netherlands Academia Sinica, Taiwan and University of Southern Denmark, Denmark KU Leuven, Belgium and University of Bergen, Norway NewAE Technology Inc., Canada Radboud University, The Netherlands Royal Holloway, University of London, UK Simula UiB, Norway CryptoExperts, France  
   
  viii  
   
  Organization  
   
  Raghavendra Rohit Yann Rotella Yu Sasaki Palash Sarkar Nicolas Sendrier Benjamin Smith Sujoy Sinha Roy Douglas Stebila Katsuyuki Takashima Yosuke Todo Yuntao Wang Wen Wang  
   
  Technology Innovation Institute, UAE Paris-Saclay University, Versailles, France NTT Social Informatics Laboratories, Japan Indian Statistical Institute, India Inria, France Inria and Ecole polytechnique, Institut Polytechnique de Paris, France Graz University of Technology, Austria University of Waterloo, Canada Waseda University, Japan NTT Social Informatics Laboratories, Japan Osaka University, Japan Intel Labs, USA  
   
  Additional Reviewers Ismail Afia Aikata Aikata Ravi Anand Nicoleta-Norica B˘acuieti Mario Marhuenda Beltrán Olivier Bronchain Jan-Pieter D’Anvers Pratish Datta Rafaël Del Pino Prastudy Fauzi Paul Frixons Nick Frymann Philippe Gaborit Clemente Galdi John Gaspoz  
   
  Lena Heimberger Akiko Inoue Hilder Vitor Lima Pereira Loïc Masure Kohei Nakagawa Shuhei Nakamura Angel L. Perez Del Pozo Thomas Prest Mostafizar Rahman Shahram Rasoolzadeh Azade Rezaeezade Jacob Schuldt Okan Seker Xiaoling Yu Vincent Zucca  
   
  Invited Talks  
   
  Hardware Security—Directions and Challenges  
   
  Tim Güneysu Abstract. The increasing integration of technology into our daily lives has also increased not only the significance of long-term secure cryptosystems but also its implementation and related aspects of hardware security. This talk explores some of the recent and multifaceted challenges that need to be tackled to ensure the integrity and confidentiality of modern security hardware. Often these vulnerabilities arise from physical attacks such as side-channel attacks (SCA) and fault injection attacks (FIA) that can easily target highly exposed devices of the embedded domain or the Internet of Things (IoT). This talk highlights the many misinterpretations and invalid assumptions of adversary models that are related to such physical attack vectors, revealing often simple vulnerabilities in the real-world that have been considered impossible under the initially assumed model. As technological advancements continue, understanding the achieved level of hardware security become essential – this includes to understand the features and capabilities of contemporary lab equipment, readily available, for example, in the domain of failure testing of integrated circuits, such as Laser Voltage Probing (LSP) or Laser Logic State Imaging (LLSI). This can be used to analyze internal states of modern integrated circuits, including the confidential information contained in the key storage of security devices. Further aspects of the talk highlight the important role of Electronic Design Automation (EDA) for the development of secure hardware devices such as the concept of dynamic reconfiguration of hardware circuits as a moving target defense countermeasure. While the list of challenges in hardware security addressed by this invited talk is far from being complete, understanding and mitigating such aspects are crucial for preserving the confidentiality, integrity, and availability for the next generation of long-term secure hardware systems.  
   
  Robust and Non-malleable Threshold Schemes, AMD Codes and External Difference Families  
   
  Doug Stinson Abstract. We began by reviewing the history of robust threshold schemes, which were introduced by Tampa and Woll in 1988. Various solutions over the years have used mathematical structures related to difference sets, including external difference families and algebraic manipulation detection codes. We presented several constructions for these structures and discussed their application to robust threshold schemes. Finally, we presented some recent (ongoing) work on non-malleable threshold schemes that employ a new variation of AMD codes, namely circular AMD codes.  
   
  A Geometric Approach to Symmetric-Key Cryptanalysis  
   
  Tim Beyne Abstract. I presented recent results that show that linear, differential and integral cryptanalysis are different facets of a single theory. I introduced the basic principles of this point of view, and showed how the choice of base field (complex or p-adic) and basis (diagonalizing an action) correspond to existing techniques. A few applications of this point of view were given, starting with the analysis of invariants of cryptographic primitives and how they relate to (multiple) linear cryptanalysis. As a second application, I discussed how the geometric approach leads to quasidifferential trails and how these can be used to avoid statistical independence assumptions in differential cryptanalysis. Finally, I presented preliminary results on an extension of integral cryptanalysis called ultrametric integral cryptanalysis.  
   
  Contents  
   
  Cryptanalysis of Lightweight Ciphers More Balanced Polynomials: Cube Attacks on 810- And 825-Round Trivium with Practical Complexities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Hao Lei, Jiahui He, Kai Hu, and Meiqin Wang  
   
  3  
   
  A Closer Look at the S-Box: Deeper Analysis of Round-Reduced ASCON-HASH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Xiaorui Yu, Fukang Liu, Gaoli Wang, Siwei Sun, and Willi Meier  
   
  22  
   
  Improving the Rectangle Attack on GIFT-64 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Yincen Chen, Nana Zhang, Xuanyu Liang, Ling Song, Qianqian Yang, and Zhuohui Feng  
   
  43  
   
  Side-Channel Attacks and Countermeasures Mask Compression: High-Order Masking on Memory-Constrained Devices . . . . Markku-Juhani O. Saarinen and Mélissa Rossi Not so Difficult in the End: Breaking the Lookup Table-Based Affine Masking Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Lichao Wu, Guilherme Perin, and Stjepan Picek Threshold Implementations with Non-uniform Inputs . . . . . . . . . . . . . . . . . . . . . . . Siemen Dhooghe and Artemii Ovchinnikov  
   
  65  
   
  82  
   
  97  
   
  Post-Quantum Constructions SMAUG: Pushing Lattice-Based Key Encapsulation Mechanisms to the Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 Jung Hee Cheon, Hyeongmin Choe, Dongyeon Hong, and MinJune Yi A Post-Quantum Round-Optimal Oblivious PRF from Isogenies . . . . . . . . . . . . . 147 Andrea Basso Traceable Ring Signatures from Group Actions: Logarithmic, Flexible, and Quantum Resistant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 Wei Wei, Min Luo, Zijian Bao, Cong Peng, and Debiao He  
   
  xviii  
   
  Contents  
   
  Symmetric Cryptography and Fault Attacks The Random Fault Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 Siemen Dhooghe and Svetla Nikova Probabilistic Related-Key Statistical Saturation Cryptanalysis . . . . . . . . . . . . . . . . 213 Muzhou Li, Nicky Mouha, Ling Sun, and Meiqin Wang Compactly Committing Authenticated Encryption Using Encryptment and Tweakable Block Cipher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 Shoichi Hirose and Kazuhiko Minematsu Post-Quantum Analysis and Implementations Bit Security Analysis of Lattice-Based KEMs Under Plaintext-Checking Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 Ruiqi Mi, Haodong Jiang, and Zhenfeng Zhang Quantum Cryptanalysis of OTR and OPP: Attacks on Confidentiality, and Key-Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 Melanie Jauch and Varun Maram Fast and Efficient Hardware Implementation of HQC . . . . . . . . . . . . . . . . . . . . . . . 297 Sanjay Deshpande, Chuanqi Xu, Mamuri Nawan, Kashif Nawaz, and Jakub Szefer Homomorphic Encryption On the Precision Loss in Approximate Homomorphic Encryption . . . . . . . . . . . . 325 Anamaria Costache, Benjamin R. Curtis, Erin Hales, Sean Murphy, Tabitha Ogilvie, and Rachel Player Secure Function Extensions to Additively Homomorphic Cryptosystems . . . . . . 346 Mounika Pratapa and Aleksander Essex Public-Key Cryptography Generalized Implicit Factorization Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369 Yansong Feng, Abderrahmane Nitaj, and Yanbin Pan  
   
  Contents  
   
  xix  
   
  Differential Cryptanalysis CLAASP: A Cryptographic Library for the Automated Analysis of Symmetric Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 Emanuele Bellini, David Gerault, Juan Grados, Yun Ju Huang, Rusydi Makarim, Mohamed Rachidi, and Sharwan Tiwari Parallel SAT Framework to Find Clustering of Differential Characteristics and Its Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409 Kosei Sakamoto, Ryoma Ito, and Takanori Isobe Deep Learning-Based Rotational-XOR Distinguishers for AND-RX Block Ciphers: Evaluations on Simeck and Simon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 Amirhossein Ebrahimi, David Gerault, and Paolo Palmieri Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451  
   
  Cryptanalysis of Lightweight Ciphers  
   
  More Balanced Polynomials: Cube Attacks on 810- And 825-Round Trivium with Practical Complexities Hao Lei1,2 , Jiahui He1,2 , Kai Hu3 , and Meiqin Wang1,2,4(B) 1  
   
  3  
   
  School of Cyber Science and Technology, Shandong University, Qingdao, Shandong, China {leihao,hejiahui2020}@mail.sdu.edu.cn, [email protected]  2 Key Laboratory of Cryptologic Technology and Information Security, Ministry of Education, Shandong University, Jinan, China School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore, Singapore [email protected]  4 Quan Cheng Shandong Laboratory, Jinan, China Abstract. The key step of the cube attack is to recover the special polynomial, the superpoly, of the target cipher. In particular, the balanced superpoly, in which there exists at least one secret variable as a single monomial and none of the other monomials contain this variable, can be exploited to reveal one-bit information about the key bits. However, as the number of rounds grows, it becomes increasingly diﬃcult to ﬁnd such balanced superpolies. Consequently, traditional methods of searching for balanced superpolies soon hit a bottleneck. Aiming at performing a cube attack on more rounds of Trivium with a practical complexity, in this paper, we present three techniques to obtain suﬃcient balanced polynomials. 1. Based on the structure of Trivium, we propose a variable substitution technique to simplify the superpoly. 2. Obtaining the additional balanced polynomial by combining two superpolies to cancel the two-degree terms. 3. We propose an experimental approach to construct high-quality large cubes which may contain more subcubes with balanced superpolies and a heuristic search strategy for their subcubes whose superpolies are balanced. To illustrate the power of our techniques, we search for balanced polynomials for 810- and 825-round Trivium. As a result, we can mount cube attacks against 810- and 825-round Trivium with the time complexity of 244.17 and 253.17 round-reduced Trivium initializations, respectively, which can be veriﬁed in 48 min and 18 days on a PC with one A100 GPU. For the same level of time complexity, this improves the previous best results by 2 and 5 rounds, respectively. Keywords: Trivium property  
   
  · cube attack · key-recovery attack · division  
   
  Due to the page limit, the appendix of this paper is included in the full version [15]. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 3–21, 2024. https://doi.org/10.1007/978-3-031-53368-6_1  
   
  4  
   
  H. Lei et al.  
   
  1  
   
  Introduction  
   
  The cube attack, proposed by Dinur and Shamir at EUROCRYPT 2009 [7], is one of the most powerful cryptanalysis techniques for symmetric ciphers. It has been successfully used to attack various stream ciphers such as Kreyvium, Acorn and Trivium [2,4,5,18,23,26]. Trivium was designed by Canni`ere and Preneel, as a bit-oriented synchronous stream cipher which was selected as one of the eSTREAM hardware-oriented ﬁnalists, and the international standard under ISO/IEC 29192-3:2012 [4]. Trivium has attracted extensive attention because of its simple structure and high level of security. Since the cube attack was proposed, it has become one of the most eﬀective cryptanalytic techniques to analyze the reduced-round variants of Trivium. Currently, the best cube attacks on Trivium are those enhanced by divsion-properties [23], which have reached 848 rounds [11] but with an impractical complexity that is very close to the exhaustive search. At the same time, the cube attacks on reduced-round Trivium with practical complexities also attract much attention, for the practical attacks have the potential to reveal more internal structural properties of Trivium and inspire new techniques for cube attacks. In [7], the authors proposed the random walk method to attack 767-round Trivium with about 245 initializations. Next, Fouque et al. found many cubes with linear superpolies by improving the time complexity of computing cubes, then the 784-round of Trivium could be attacked with about 239 initializations [8]. Later, in [29], Ye et al. proposed an eﬀective method to construct cubes for linear superpolies and they gave a practical attack against 805-round Trivium with about 241.4 initializations. At FSE 2021, Sun proposed a new heuristic method to reject cubes without independent secret variables and they could perform practical attacks against 806- and 808-round Trivium with time complexity of 239.88 and 244.58 initializations, respectively [21]. Recently, Cheng et al. gave attacks on 815- and 820-round Trivium with 247.32 and 253.17 initializations, respectively [6]. These results are summarized in Table 1. 1.1  
   
  Our Contributions  
   
  This paper focuses on practical key-recovery attacks against reduced-round Trivium. In order to attack a higher number of rounds, we propose the following methods. Simplify Superpolies by Variable Substitutions. At the cost of adding one extra variable, the variable substitution can greatly simplify the superpoly, so that some unbalanced superpolies can be transformed into balanced ones. Thanks to this technique, more simple and balanced superpolies are utilized, from which more information about secret variables can be extracted. More Balanced Polynomials by Canceling the Quadratic Terms. Balanced superpolies are important for practical attacks on round-reduced Trivium. However, for Trivium with a higher number of rounds, it is hard to ﬁnd cubes  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  5  
   
  with balanced superpolies. We propose a method to obtain one additional balanced polynomial by canceling the quadratic terms in two superpolies. The new balanced polynomial is the sum of these two superpolies. A Modified Algorithm to Construct a Better Mother Cube. Finding a large cube that contains many subcubes with balanced superpolies is important for the practical attack on round-reduced Trivium. In the following paper, this large cube is called a mother cube. By examining the relationship between the superpoly of a mother cube and the superpolies of its subcubes, we can more accurately judge whether a mother cube is suitable for a practical attack. A Heuristic Strategy for Searching for Balanced Subcubes. It often takes a long time to recover the superpoly of a cube for high rounds of Trivium. Moreover, a high-dimensional mother cube has a large number of subcubes. To reduce the search space, we propose two strategies for dividing the search space by examining the relationship between the superpoly of a cube and the superpolies of its subcubes, which allows us to obtain suﬃcient balanced superpolies for a practical key-recovery attack with the reduced search space. As an application, we apply our methods to 810-round Trivium and 825-round Trivium. The complexities of the cube attacks on 810 and 825 rounds of Trivium are respectively 244.58 and 253.09 round-reduced Trivium initializations. We list our attacks as well as the previous practical/theoretical cube attacks on Trivium for a better comparison in Table 1. We also implemented these two attacks on a PC with an A100 GPU. The experimental results showed that the whole keys can be recovered within 48 min and 18 days for 810 and 825 rounds of Trivium, respectively. All source codes for our algorithms in this paper are provided at the git repository https://github.com/lhoop/ObtainMoreBS. 1.2  
   
  Outline  
   
  In Sect. 2, some related concepts and deﬁnitions are introduced. In Sect. 3, we propose three techniques to obtain a lot of balanced polynomials for roundreduced Trivium and a heuristic search strategy that reduces the cube search space of round-reduced Trivium. In Sect. 4, we mount cube attacks against 810and 825-round Trivium by our techniques. Finally, we draw our conclusions in Sect. 5.  
   
  2 2.1  
   
  Preliminaries Notation  
   
  We use bold italic lowercase letters to represent bit vectors, such as x = (𝑥0 , 𝑥1 , . . . , 𝑥 𝑛−1 ) where 𝑥𝑖 is the 𝑖-th element of x. For any 𝑛-bit vectors u and v, we deﬁne u  v if 𝑢 𝑖 ≥ 𝑣 𝑖 for all 0 ≤ 𝑖 < 𝑛. Similarly, we deﬁne u  v if 𝑢 𝑖 ≤ 𝑣 𝑖 for all 0 ≤ 𝑖 < 𝑛. Blackboard bold uppercase letters (e.g. X, K, L, . . .) are used to represent sets of bit vectors. And we use the composition operator (◦) to compose two functions. For example, 𝑔 ◦ 𝑓 (𝑥) = 𝑔( 𝑓 (𝑥)).  
   
  6  
   
  H. Lei et al. Table 1. A summary of cube attacks on Trivium Type  
   
  2.2  
   
  # of rounds  
   
  Cube size  
   
  # of key bits  
   
  Total time  
   
  Ref.  
   
  672  
   
  12  
   
  63  
   
  218.56  
   
  [7]  
   
  709  
   
  22–23  
   
  79  
   
  229.14  
   
  [17]  
   
  767  
   
  28–31  
   
  35  
   
  245.00  
   
  [7]  
   
  784  
   
  30–33  
   
  42  
   
  239.00  
   
  [8]  
   
  805  
   
  32–38  
   
  42  
   
  241.40  
   
  [29]  
   
  806  
   
  33–37  
   
  45  
   
  239.88  
   
  [21]  
   
  808  
   
  39–41  
   
  37  
   
  244.58  
   
  810  
   
  40–42  
   
  39  
   
  244.17  
   
  815  
   
  44–46  
   
  35  
   
  247.32  
   
  820  
   
  48–51  
   
  30  
   
  253.17  
   
  825  
   
  49–52  
   
  31  
   
  253.09  
   
  Theoretical 799  
   
  32–37  
   
  18  
   
  262.00  
   
  [8]  
   
  802  
   
  34–37  
   
  8  
   
  272.00  
   
  [27]  
   
  805  
   
  28  
   
  7  
   
  273.00  
   
  [16]  
   
  806  
   
  34–37  
   
  16  
   
  264.00  
   
  [29]  
   
  835  
   
  35  
   
  5  
   
  275.00  
   
  [16]  
   
  832  
   
  72  
   
  1  
   
  279.01  
   
  [25]  
   
  832  
   
  72  
   
  >1  
   
  < 279.01  
   
  [28]  
   
  840  
   
  78  
   
  1  
   
  279.58  
   
  [9]  
   
  840  
   
  75  
   
  3  
   
  277.32  
   
  [14]  
   
  841  
   
  78  
   
  1  
   
  279.58  
   
  [9]  
   
  841  
   
  76  
   
  2  
   
  278.58  
   
  [14]  
   
  842  
   
  78  
   
  1  
   
  279.58  
   
  [10]  
   
  842  
   
  76  
   
  2  
   
  278.58  
   
  [14]  
   
  843  
   
  54–57, 76  
   
  5  
   
  276.58  
   
  [13]  
   
  843  
   
  78  
   
  1  
   
  279.58  
   
  [21]  
   
  844  
   
  54–55  
   
  2  
   
  278.00  
   
  [13]  
   
  845  
   
  54–55  
   
  2  
   
  278.00  
   
  [13]  
   
  846  
   
  51–54  
   
  1  
   
  279.00  
   
  [11]  
   
  847  
   
  52–53  
   
  1  
   
  279.00  
   
  [11]  
   
  848  
   
  51–54  
   
  1  
   
  279.00  
   
  [11]  
   
  Practical  
   
  [21] Section 4.1 [6] [6] Section 4.2  
   
  Boolean Functions and Algebraic Degree  
   
  Boolean Function. Let 𝑓 : F2𝑛 → F2 be a Boolean function whose algebraic normal form (ANF) is 𝑓 (x) = 𝑓 (𝑥0 , 𝑥1 , . . . , 𝑥 𝑛−1 ) =  
   
   u ∈F𝑛 2  
   
  𝑎u  
   
  𝑛−1  𝑖=0  
   
  𝑥𝑖𝑢𝑖 ,  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  7  
   
  where 𝑎 u ∈ F2 , and xu = 𝜋u (x) =  
   
  𝑛−1   
   
  𝑥𝑖𝑢𝑖 with 𝑥𝑖𝑢𝑖 =  
   
  𝑖=0  
   
    
   
  𝑥𝑖 , if 𝑢 𝑖 = 1, 1, if 𝑢 𝑖 = 0,  
   
  is called a monomial. We use the notation xu → 𝑓 to indicate that the coefﬁcient 𝑎 u of xu in 𝑓 is 1, i.e., xu appears in 𝑓 . Otherwise, xu  𝑓 . In this work, we will use xu and 𝜋u (x) interchangeably to avoid the awkward notation u ( 𝑗)  
   
  x (𝑖) when both x and u have superscripts. One important feature of a Boolean function is its algebraic degree which is denoted by deg( 𝑓 ) and deﬁned as deg( 𝑓 ) = max {𝑤𝑡 (u) | 𝑎 u ≠ 0} , where 𝑤𝑡 (u) is the Hamming weight of u, i.e., 𝑤𝑡 (u) =  
   
  𝑛−1 𝑖=0  
   
  𝑢𝑖 .  
   
  Vectorial Boolean Function. Let 𝑓 : F2𝑛 → F2𝑚 be a vectorial Boolean function with y = (𝑦 0 , 𝑦 1 , . . . , 𝑦 𝑚−1 ) = f (x) = ( 𝑓0 (x), 𝑓1 (x), . . . , 𝑓𝑚−1 (x)). For v ∈ F2𝑚 , we use y v to denote the product of some coordinates of y: yv =  
   
  𝑚− 1 𝑖=0  
   
  𝑦 𝑖𝑣𝑖 =  
   
  𝑚− 1  
   
  ( 𝑓𝑖 (x)) 𝑣𝑖 ,  
   
  𝑖=0  
   
  which is a Boolean function in x. 2.3  
   
  Pseudo-code of Trivium  
   
  Trivium is a bit oriented synchronous stream cipher, the main building block of Trivium is a 288-bit nonlinear feedback shift register which is divided into three small registers. For initialization, the 80 bit secret variables k = (𝑘 0 , 𝑘 1 , . . . , 𝑘 79 ) is loaded into the ﬁrst register, and the 80-bit IV (i.e., public variables) v = (𝑣 0 , 𝑣 1 , . . . , 𝑣 79 ) is loaded into the second register. For the third register, all state bits are set to 0 except the last three bits. After 1152 state updates, Trivium starts to output keystream bits. The pseudo-code of Trivium is described in Algorithm 1. 2.4  
   
  Cube Attack  
   
  The cube attack was ﬁrst proposed by Dinur and Shamir in EUROCRYPT 2009 [7]. It is a powerful cryptanalytic technique against stream ciphers. For a cipher with 𝑛 public variables and 𝑚 secret variables, each output bit of the cipher can be represented as a polynomial in these secret and public variables. Let 𝑧 be an output bit, x = (𝑥0 , 𝑥1 , ..., 𝑥 𝑛−1 ) as the public variables and k = (𝑘 0 , 𝑘 1 , ..., 𝑘 𝑚−1 ) as the secret variables. 𝑧 can be expressed as 𝑧 = 𝑓 (k, x).  
   
  8  
   
  H. Lei et al.  
   
  Algorithm 1: Pseudo-code of Trivium 1 2 3 4 5 6 7 8 9 10 11 12 13 14  
   
  𝑠0 , 𝑠1 , ..., 𝑠92 ← (𝑘 0 , ..., 𝑘 79 , 0, ..., 0) 𝑠93 , 𝑠94 , ..., 𝑠176 ← (𝑣 0 , ..., 𝑣 79 , 0, ..., 0) 𝑠177 , 𝑠178 , ..., 𝑠287 ← (0, ..., 0, 1, 1, 1) for i from 1 to N (Number of initialization rounds) do 𝑡𝑎 ← 𝑠65 ⊕ 𝑠92 ⊕ 𝑠90 𝑠91 ⊕ 𝑠170 𝑡𝑏 ← 𝑠161 ⊕ 𝑠176 ⊕ 𝑠174 𝑠175 ⊕ 𝑠263 𝑡𝑐 ← 𝑠242 ⊕ 𝑠287 ⊕ 𝑠285 𝑠286 ⊕ 𝑠68 if 𝑖 > 1152 then 𝑧 𝑖−1152 ← 𝑠65 ⊕ 𝑠92 ⊕ 𝑠161 ⊕ 𝑠176 ⊕ 𝑠242 ⊕ 𝑠287 end 𝑠0 , 𝑠1 , ..., 𝑠92 ← (𝑡𝑏, 𝑠0 , 𝑠1 , ..., 𝑠91 ) 𝑠93 , 𝑠94 , ..., 𝑠176 ← (𝑡𝑎, 𝑠93 , 𝑠94 , ..., 𝑠175 ) 𝑠177 , 𝑠178 , ..., 𝑠287 ← (𝑡𝑐, 𝑠177 , 𝑠178 , ..., 𝑠286 ) end  
   
    ⊂ {𝑥0 , 𝑥1 , . . . , 𝑥 𝑛−1 } be a subset of public variables, Let 𝐼 = 𝑥 𝑐1 , 𝑥 𝑐2 , . . . , 𝑥 𝑐𝑑 assume xu = 𝑥 ∈𝐼 𝑥 is its corresponding term. Then, 𝑓 (x, k) can be uniquely expressed as 𝑓 (x, k) = 𝑝(x, k) · xu + 𝑞(x, k), where 𝑞(x, k) misses at least one variable in 𝐼. A cube determined by 𝐼 is denoted as 𝐶𝐼 and contains all 2𝑑 possible combinations of the values of variables in 𝐼. 𝐶𝐼 is a 𝑑-dimensional cube because the size of 𝐼 is 𝑑. xu is called a cube term. The public variables in 𝐼 are called cube variables and the remaining public variables are called non-cube variables. 𝑝(x, k) is called the superpoly of 𝐶𝐼 which can be computed by  𝑓 (x, k) = 𝑝(x, k). x ∈𝐶𝐼  
   
  Therefore, we can get the value of the superpoly 𝑝(x, k) by 2 |𝐼 | calls to the initialization oracle. For a superpoly 𝑃, if a variable 𝑘 𝑖 that appears only in a one-degree monomial, we say 𝑃 is a balanced superpoly for the balanced variable 𝑘 𝑖 . Moreover, if all variables in 𝑃 are balanced variables, we say 𝑃 is a linear superpoly. For example, 𝑃1 = 𝑘 1 ⊕ 𝑘 2 ⊕ 𝑘 3 𝑘 4 is a balanced superpoly for balanced variables 𝑘 1 and 𝑘 2 , 𝑃2 = 𝑘 1 ⊕ 𝑘 2 is a linear superpoly. We say a cube is balanced if its superpoly is balanced for some balanced variables. For a balanced superpoly whose value is known, we can deduce its one balanced variable by enumerating the values of other variables. 2.5  
   
  The Bit-Based Division Property and Monomial Prediction  
   
  The division property is a generalization of integral property, which was proposed by Todo in [22,24]. The conventional bit-based division property can be used to evaluate the algebraic degree of a cube and the three-subset division property  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  9  
   
  without unknown subset can be used to recover superpoly. The deﬁnitions of the conventional bit-based division property and three-subset division property are provided in [15, Appendix A]. The monomial prediction, proposed by Hu et al. in [14], is another language of division property from a pure algebraic perspective. By counting the so-called monomial trails, the monomial prediction can determine if a monomial of IV appears in the polynomial of the output of the cipher. Let f : F2𝑛0 → F2𝑛𝑟 be a composite vectorial Boolean function of a sequence of 𝑟 smaller function f (𝑖) : F2𝑛𝑖 → F2𝑛𝑖+1 , 0 ≤ 𝑖 ≤ 𝑟 − 1 as f = f (𝑟−1) ◦ f (𝑟−2) ◦ · · · ◦ f ( 0) . F2𝑛𝑖  
   
  Let x (𝑖) ∈ and x (𝑖+1) ∈ F2𝑛𝑖+1 be the input and output variables of f (𝑖) , respectively. Suppose 𝜋u (0) (x ( 0) ) is a monomial of 𝑥 ( 0) , it is easy to ﬁnd all monomials of 𝑥 ( 1) satisfying 𝜋u (0) (x ( 0) ) → 𝜋u (1) (x ( 1) ). For every such 𝜋u (1) (x ( 1) ), we then ﬁnd all the 𝜋u (2) (x ( 2) ) satisfying 𝜋u (1) (x ( 1) ) → 𝜋u (2) (x ( 2) ). Finally, if we are interested in whether 𝜋u (0) (x ( 0) ) → 𝜋u (𝑟 ) (x (𝑟) ), we collect some transitions from 𝜋u (0) (x ( 0) ) to 𝜋u (𝑟 ) (x (𝑟) ) as 𝜋u (0) (x ( 0) ) → 𝜋u (1) (x ( 1) ) → · · · → 𝜋u (𝑟 ) (x (𝑟) ). Every such transition is called a monomial trail from 𝜋u (0) (x ( 0) ) → 𝜋u (𝑟 ) (x (𝑟) ), denoted by 𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) ). All the trails from 𝜋u (0) (x ( 0) ) to 𝜋u (𝑟 ) (x (𝑟) ) are denoted by 𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) ), which is the set of all trails. Then whether 𝜋u (0) (x ( 0) ) → 𝜋u (𝑟 ) (x (𝑟) ) is determined by the size of 𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) ), represented as |𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) )|. If there is no trail from 𝜋u (0) (x ( 0) ) to 𝜋u (𝑟 ) (x (𝑟) ), we say 𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) ) and accordingly |𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) )| = 0. Theorem 1 (Integrated from [9,10,12,14]). Let f = f (𝑟−1) ◦ f (𝑟−2) ◦ · · · ◦ f ( 0) defined as above. 𝜋u (0) (x ( 0) ) → 𝜋u (𝑟 ) (x (𝑟) ) if and only if |𝜋u (0) (x ( 0) )  𝜋u (𝑟 ) (x (𝑟) )| ≡ 1 (mod2). Propagation Rules of the Monomial Prediction. Any component of a symmetric cipher can be regarded as a vectorial Boolean function. To model the propagation of the monomial prediction for a vectorial Boolean function, a common method is to list all the possible (input, output) tuples according to the deﬁnition of the monomial prediction [14]. These tuples can be transformed into a set of linear inequalities and thus modeled with MILP [3,19,20]. Since any symmetric cipher can be decomposed into a sequence of the basic operations XOR, AND and COPY. Their concrete propagation rules for these basic functions and MILP models are provided in [15, Appendix B].  
   
  3  
   
  Obtain More Balanced Polynomials  
   
  To mount a practical key-recovery attack on high rounds of Trivium, we need a lot of small cubes whose superpolies are simple and balanced. However, small cubes for high rounds of Trivium tend to have complex and unbalanced superpolies.  
   
  10  
   
  H. Lei et al.  
   
  In order to obtain more simple and balanced polynomials, we propose a method based on the variable substitution technique to simplify the superpolies and a method to obtain more balanced polynomials by canceling some quadratic terms between superpolies. We introduce these two methods in Sects. 3.1 and 3.2, respectively. Additionally, according to an observation, we modify the algorithm in [29, Section 3.1] to obtain a mother cube containing more subcubes whose superpolies are balanced and provide a heuristic search strategy to make a tradeoﬀ between the search space and the search quality. They are introduced in Sect. 3.3. 3.1  
   
  Variable Substitutions  
   
  The Principle of Variable Substitutions. Notice that after the state of Trivium was updated 24 times, the last 11 bits of the secret variables will only exist in the state in a speciﬁc form. (𝑠0 , 𝑠1 , . . . , 𝑠92 ) ← (𝑘 0 , 𝑘 1 , . . . , 𝑘 79 , 0, . . . , 0); (𝑠93 , 𝑠94 , . . . , 𝑠176 ) ← (𝑣 0 , 𝑣 1 , . . . , 𝑣 79 , 0, . . . , 0); (𝑠177 , 𝑠178 , . . . , 𝑠287 ) ← (0, . . . , 0, 1, 1, 1). After 24 rounds of updates,  ) ← (𝑡𝑏 , 𝑡𝑏 , . . . , 𝑡𝑏 , 𝑘 , . . . , 𝑘 ); (𝑠0 , 𝑠1 , . . . , 𝑠92 23 22 0 0 68  , 𝑠  , . . . , 𝑠  ) ← (𝑡𝑎 , 𝑡𝑎 , . . . , 𝑡𝑎 , 𝑣 , . . . , 𝑣 ); (𝑠93 23 22 0 0 59 94 176  , 𝑠  , . . . , 𝑠  ) ← (𝑡𝑐 , 𝑡𝑐 , . . . , 𝑡𝑐 , 0, . . . , 0), (𝑠177 23 22 0 178 287  
   
  where 𝑡𝑎 𝑖 , 𝑡𝑏 𝑖 , 𝑡𝑐 𝑖 are three bits updated by the round function of the 𝑖-th round of Trivium.  , we ﬁnd that 𝑘 , 𝑘 , . . . , 𝑘 From the expressions of 𝑠0 , 𝑠1 , ..., 𝑠287 69 70 79 are only    involved in 𝑠94 , 𝑠95 , .., 𝑠105 .  = 𝑡𝑎 𝑠94 23 = 𝑘 42 ⊕ 𝑘 69 ⊕ 𝑘 68 𝑘 67 ⊕ 𝑣 54 ,  = 𝑡𝑎 𝑠95 22 = 𝑘 43 ⊕ 𝑘 70 ⊕ 𝑘 69 𝑘 68 ⊕ 𝑣 55 ,  
   
  .. .  𝑠105 = 𝑡𝑎 12 = 𝑘 53 ⊕ 0 ⊕ 𝑘 79 𝑘 78 ⊕ 𝑣 65 .  
   
  We use new variables 𝑝 69 , . . . , 𝑝 80 to replace these polynomials of secret variables. 𝑝 69 = 𝑘 42 ⊕ 𝑘 69 ⊕ 𝑘 68 𝑘 67 , 𝑝 70 = 𝑘 43 ⊕ 𝑘 70 ⊕ 𝑘 69 𝑘 68 , .. . 𝑝 80 = 𝑘 53 ⊕ 0 ⊕ 𝑘 79 𝑘 78 .  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  11  
   
  For the sake of clarity, we also use 𝑝 0 , . . . , 𝑝 68 to replace the other secret variables (𝑝 0 = 𝑘 0 , 𝑝 1 = 𝑘 1 , . . . , 𝑝 68 = 𝑘 68 ). Then all state bits of round 24 are only related to the 81 new secret variables p = ( 𝑝 0 , 𝑝 1 , . . . , 𝑝 80 ) and the 80 public variables v = (𝑣 0 , 𝑣 1 , . . . , 𝑣 79 ).  ) can be written as the output of a vectorial Boolean function of p (𝑠0 , . . . , 𝑠287 and v, namely,  y = (𝑠0 , . . . , 𝑠287 ) = g(v, p) = (𝑔0 (v, p), 𝑔1 (v, p), . . . , 𝑔287 (v, p)).  
   
  And the output bit after 𝑟 rounds (𝑟 > 24) can be expressed as follows, 𝑧𝑟 = ℎ(s) ◦ g(v, p),  ) to 𝑧 𝑟 . where ℎ(s) is a Boolean function from (𝑠0 , . . . , 𝑠287 This implies that we can represent the superpoly as a polynomial of only p. Since polynomials of degree 2 in k (e.g., 𝑘 42 + 𝑘 69 + 𝑘 68 𝑘 67 ) are now replaced by single variables (e.g., 𝑝 69 ), the p-representation of the superpoly is likely to have a lower algebraic degree and thus, is simpler. Therefore, after representing superpolies as polynomials of p, more simple and balanced superpolies may be obtained. We show this in Example 1.  
   
  Example 1. Let 𝐴 = 𝑘 42 ⊕ 𝑘 69 ⊕ 𝑘 68 𝑘 67 and 𝐵 = 𝑘 45 ⊕ 𝑘 1 (𝑘 45 ⊕ 𝑘 72 ⊕ 𝑘 71 𝑘 70 ) be a balanced superpoly and an unbalanced superpoly of k, respectively. After variable substitutions, we have 𝐴 = 𝑝 69 , 𝐵 = 𝑝 45 ⊕ 𝑝 1 𝑝 72 , which means 𝐴 becomes a linear superpoly of p and 𝐵 becomes a balanced superpoly of p. To illustrate the power of this technique, we search for balanced superpolies among the same set of cubes for 825-round Trivium before and after variable substitutions, respectively. We evaluate the simplicity of each superpoly by the number of variables it contains and record the number of occurrences of balanced superpolies at diﬀerent levels of simplicity. See Table 2 for details. Table 2. Distribution of balanced superpolies (B.S.) involving diﬀerent numbers of variables. # variables involved #B.S. of k #B.S. of p ≤5  
   
  34  
   
  42  
   
  ≤10  
   
  42  
   
  67  
   
  ≤20  
   
  90  
   
  122  
   
  ≤40  
   
  187  
   
  235  
   
  ≤60  
   
  275  
   
  318  
   
  ≤80  
   
  322  
   
  354  
   
  Substituting Variables Back. The variable substitution technique simpliﬁes superpolies at the cost of adding one extra variable, i.e., 𝑝 80 . Notice that the values of 𝑘 0 , . . . , 𝑘 79 can be derived from the values of 𝑝 0 , . . . , 𝑝 79 as follows,  
   
  12  
   
  H. Lei et al.  
   
  𝑝 0 , . . . , 𝑝 68 ⇒𝑘 0 , . . . , 𝑘 68 , 𝑝 69 , 𝑘 42 , 𝑘 67 ,𝑘 68 ⇒ 𝑘 69 , 𝑝 70 , 𝑘 43 , 𝑘 68 ,𝑘 69 ⇒ 𝑘 70 , .. . 𝑝 79 , 𝑘 52 , 𝑘 78 ,𝑘 77 ⇒ 𝑘 79 . Therefore, 𝑝 80 is actually redundant and can be expressed as a polynomial of 𝑝 0 , . . . , 𝑝 79 . In other words, 𝑓 : (𝑘 0 , . . . , 𝑘 79 ) → ( 𝑝 0 , . . . , 𝑝 79 ) is actually a bijective function. In this paper, we always consider the superpolies represented as polynomials of p. Once the values of all 81 new secret variables (i.e., 𝑝 0 , . . . , 𝑝 80 ) are obtained, we ﬁrst derive the values of 𝑘 0 , . . . , 𝑘 79 from 𝑝 0 , . . . , 𝑝 79 , then check whether the value of 𝑝 80 is correct. The concrete process of substituting p back to obtain k is shown in Algorithm 2.  
   
  Algorithm 2: Recovering k from p  
   
  1 2 3 4 5 6 7 8 9  
   
  3.2  
   
  Input: ( 𝑝 0 , 𝑝 1 , . . . , 𝑝 80 ) Output: (𝑘 0 , 𝑘 1 . . . , 𝑘 79 ) or 𝑓 𝑎𝑙𝑠𝑒 for i from 0 to 68 do 𝑘 𝑖 ← 𝑝𝑖 ; for i from 69 to 79 do 𝑘 𝑖 ← 𝑝 𝑖 ⊕ 𝑘 𝑖−27 ⊕ 𝑘 𝑖−1 𝑘 𝑖−2 ; 𝑐ℎ𝑒𝑐𝑘 ← 𝑘 53 ⊕ 𝑘 78 𝑘 79 ; if 𝑝 80 == 𝑐ℎ𝑒𝑐𝑘 then return (𝑘 0 , 𝑘 1 . . . , 𝑘 79 ) else return false  
   
  More Balanced Polynomials by Canceling Quadratic Terms  
   
  After constructing a mother cube, we can obtain a large number of subcubes, but only a very small fraction of subcubes have balanced superpolies. More balanced polynomials are required when we perform the practical attack on high rounds of Trivium. In order to obtain more balanced polynomials, we provide an algorithm to obtain additional balanced polynomials by canceling the common quadratic terms between superpolies. First, we deﬁne two types of superpolies. Definition 1 (Type-I Superpoly). If there exists a single monomial 𝑝 𝑖 and a quadratic monomial 𝑝 𝑖 𝑝 𝑗 , and none of the other monomials contain 𝑝 𝑖 , then we call this superpoly a type-I superpoly of a variable 𝑝 𝑖 .  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  13  
   
  Definition 2 (Type-II Superpoly). If there exists a variable 𝑝 𝑖 that appears in a quadratic monomial 𝑝 𝑖 𝑝 𝑗 , and none of the other monomials contain this variable, then we call this superpoly a type-II superpoly of a variable 𝑝 𝑖 . For example, a superpoly 𝐴 = 𝑝 73 ⊕ 𝑝 73 𝑝 25 is a type-I superpoly of a variable 𝑝 73 and a superpoly 𝐵 = 𝑝 73 𝑝 25 is a type-II superpoly of variables 𝑝 73 and 𝑝 25 . Combining Rule. For a speciﬁc variable 𝑝 𝑖 , if we can ﬁnd both a type-I superpoly of 𝑝 𝑖 and a type-II superpoly of 𝑝 𝑖 and the quadratic monomials containing 𝑝 𝑖 in these two superpolies are the same, then we can get a balanced polynomial by adding these two superpolies. For example, we can obtain a balanced polynomial 𝐶 with 𝑝 73 being a balanced variable by adding the type-I superpoly 𝐴 and the type-II superpoly 𝐵, 𝐶 = 𝐴 ⊕ 𝐵 = 𝑝 73 ⊕ 𝑝 73 𝑝 25 ⊕ 𝑝 73 𝑝 25 = 𝑝 73 . In the superpolies recovery process, we use SageMath [1] to determine whether a superpoly is a type-I superpoly or a type-II superpoly. For 825-round Trivium, we ﬁnd 540 cubes with type-I superpolies or type-II superpolies. By the combining rule, we obtained 782 additional balanced polynomials. This increases our number of balanced polynomials from 451 to 1323. 3.3  
   
  Relationship Between a Cube and Its Subcubes  
   
  In our practical cube attacks on Trivium, we focus on balanced superpolies rather than superpolies of low degrees. For example, consider two superpolies 𝐴 and 𝐵. 𝐴 : 𝑝1 𝑝2 ⊕ 𝑝2 𝑝3 = 𝑐1 , 𝐵 : 𝑝 1 ⊕ 𝑝 3 𝑝 10 𝑝 4 𝑝 6 = 𝑐 2 . The degrees of 𝐴 and 𝐵 are 2 and 4, respectively. We prefer 𝐵 to 𝐴 because 𝐵 is balanced so that we can use it to deduce 𝑝 1 by enumerating 𝑝 3 , 𝑝 10 , 𝑝 4 , 𝑝 6 . Inspiration. In the search process, we obtain a set of cubes whose superpolies are estimated to have low degrees based on the division property, but the superpolies of these cubes are almost all unbalanced. While for another set of cubes, we ﬁnd that a lot of their superpolies are balanced, although their superpolies are estimated to have high degrees. This inspires us to investigate how to locate the balanced superpolies more accurately. With experiments, we have the following observation. Observation 1. For a given 𝑥-dimensional cube 𝐼, if its superpoly is balanced, then the superpolies of its (𝑥 − 1)-dimensional subcubes have a greater probability of being balanced. There are some data in [15, Appendix D] to support this observation. Based on this observation, we propose a method for constructing a better mother cube that may contain more subcubes with balanced superpolies and a heuristic search method to reduce the search space.  
   
  14  
   
  H. Lei et al.  
   
  A Modified Algorithm to Construct a Mother Cube for Balanced Superpolies. We modify the algorithm for constructing the mother cube in [29, Section 3.1] to obtain a better mother cube that may have more subcubes with balanced superpolies. First, at the stage of determining the starting cube, Ye et al. predicted a preference bit 𝑠𝜆(𝑟) for 𝑟-round Trivium. 𝑠𝜆(𝑟) can be written as 𝑠𝜆(𝑟) = 𝑠𝑖(𝑟−𝜆) · 𝑠𝑖(𝑟−𝜆) ⊕ 𝑠𝑖(𝑟−𝜆) ⊕ 𝑠𝑖(𝑟−𝜆) ⊕ 𝑠𝑖(𝑟−𝜆) , 𝜆 𝜆 𝜆 𝜆 𝜆 1  
   
  2  
   
  3  
   
  4  
   
  5  
   
  and 𝑠𝑖(𝑟−𝜆) are the it have ﬁve state bits from (𝑟 − 𝜆)-round Trivium. Next, 𝑠𝑖(𝑟−𝜆) 𝜆 𝜆 1  
   
  2  
   
  dominant parts in determining whether 𝑠𝜆(𝑟) contributes to linear terms in the superpoly. Therefore, the starting cube should have a linear superpoly in 𝑠𝑖(𝑟−𝜆) 𝜆 1  
   
  or 𝑠𝑖(𝑟−𝜆) . We are less strict about the starting cube because we focus on balanced 𝜆 2  
   
  superpolies, so our starting cube only need to have a simple balanced superpoly or 𝑠𝑖(𝑟−𝜆) . in 𝑠𝑖(𝑟−𝜆) 𝜆 𝜆 1  
   
  2  
   
  Second, at the stage of extending the starting cube, we will obtain a lot of candidate mother cubes after the expansion. The criteria for selection from candidate mother cubes by Tian et al. is choosing the mother cube which has the most subcubes of degree less than 5 [6]. However, it may not be accurate enough to judge whether superpoly is more likely to be balanced by its degree. We propose a more accurate way to determine the best mother cube from the candidate mother cubes, i.e., the 𝑥-dimensional candidate mother cube that contains the most (𝑥 − 1)-dimensional subcubes with simple balanced superpolies is selected. If an 𝑥-dimensional mother cube contains most (𝑥 − 1)-dimensional subcubes with simple balanced superpolies. Then by Observation 1, the superpolies of their (𝑥 − 2)-dimensional subcubes have a greater probability of being balanced and so do the superpolies of for (𝑥 − 3)- and (𝑥 − 4)-dimensional subcubes. As an example, for 825-round Trivium, we select 𝐼2 = {𝑝 3 , 𝑝 4 , 𝑝 5 , 𝑝 8 , 𝑝 9 , 𝑝 10 , 𝑝 16 , 𝑝 17 , 𝑝 20 , 𝑝 22 , 𝑝 25 , 𝑝 26 , 𝑝 28 , 𝑝 29 , 𝑝 32 , 𝑝 34 , 𝑝 37 , 𝑝 39 , 𝑝 40 , 𝑝 43 , 𝑝 44 , 𝑝 45 , 𝑝 46 , 𝑝 47 , 𝑝 48 , 𝑝 49 , 𝑝 51 , 𝑝 52 , 𝑝 53 , 𝑝 54 , 𝑝 55 , 𝑝 56 , 𝑝 57 , 𝑝 58 , 𝑝 59 , 𝑝 60 , 𝑝 61 , 𝑝 62 , 𝑝 64 , 𝑝 66 , 𝑝 67 , 𝑝 69 , 𝑝 70 , 𝑝 71 , 𝑝 72 , 𝑝 74 , 𝑝 76 , 𝑝 78 , 𝑝 79 , 𝑝 80 } as our mother cube. The size of 𝐼2 is 53 and it has 7 52-dimensional subcubes with simple balanced superpolies. Among the subcubes of these 7 52-dimensional subcubes, there are 44 51-dimensional subcubes with balanced superpolies. Further more, 87 50-dimensional subcubes with balanced superpolies can be discovered from these 44 51-dimensional subcubes. A Heuristic Search Strategy for Searching Balanced Cubes. At ASIACRYPT 2022 [11], He et al. proposed an eﬃcient method based on the core monomial prediction to recover the superpolies for up to 848 rounds of Trivium. We utilize this method as a tool to search for balanced subcubes (i.e., subcubes  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  15  
   
  with balanced superpolies) among the subcubes of a mother cube by directly recovering the superpolies of subcubes. According to our experiments, if the superpoly of a subcube is complex, it often takes a long time to recover it. Therefore, when recovering the superpoly for a speciﬁc subcube, we set a time limit. If the superpoly of a subcube is not recovered within the time limit, we consider the superpoly to be complex and discard this subcube. This simple trick speeds up the search for balanced subcubes of a mother cube. However, for a 53-dimensional mother cube, it has 53 52-dimensional subcubes, 1378 51-dimensional subcubes, and 23426 50-dimensional subcubes. Recovering the superpolies for these subcubes one by one is still impossible in a reasonable time, but reducing the search space for subcubes means some balanced superpolies may be missed. To deal with this problem, we give two heuristic strategies based on Observation 1 to reject potentially useless subcubes in advance. – First strategy. If an 𝑥-dimensional cube is determined to have an unbalanced and non-zero superpoly, all (𝑥 − 1)-dimensional subcubes of this cube are considered to have unbalanced superpolies and are rejected. – Second strategy. If an 𝑥-dimensional cube is determined to have a balanced or zero superpoly, all (𝑥 − 1)-dimensional subcubes of this cube are considered to possibly have balanced superpolies and will be checked. The diﬀerence between the ﬁrst strategy and the second strategy is that when an (𝑥 − 1)-dimensional cube is simultaneously the subcube of several 𝑥dimensional cubes and one of these 𝑥-dimensional cubes has an unbalanced and non-zero superpoly, then the (𝑥 − 1)-dimensional cube will be rejected according to the ﬁrst strategy, but will be checked according to the second strategy. In other words, the ﬁrst strategy is more aggressive in narrowing down the search space. To further illustrate the eﬀect of reducing the search space on the number of balanced superpolies, we give speciﬁc experimental data using the ﬁrst strategy, the second strategy, and no strategy (i.e., the search over the whole (𝑥 − 1)dimensional subcubes, called full search) in Table 3. For an 𝑥-dimensional mother cube, we usually search over its (𝑥 − 1)- and (𝑥 − 2)-dimensional subcubes by the full search, then the second strategy is used to search over its (𝑥 −3)-dimensional subcubes and (𝑥 −4)-dimensional subcubes. Finally, we use the ﬁrst strategy to search for (𝑥−5)-dimensional subcubes. These search strategies help us ﬁnd a suﬃcient number of balanced superpolies with an acceptable computational eﬀort.  
   
  4  
   
  Application  
   
  In this section, we apply our improved methods to 810- and 825-round Trivium. Due to the page limit, superpolies used in this section are provided at https:// github.com/lhoop/ObtainMoreBS/tree/main/data.  
   
  16  
   
  H. Lei et al.  
   
  Table 3. A comparison between search space and balanced cubes for 50-dimensional subcubes of a 53-dimensional mother cube 𝐴0 for 825-round Trivium, where 𝐴0 = {𝑣 2 , 𝑣 5 , 𝑣 8 , 𝑣 10 , 𝑣 12 , 𝑣 15 , 𝑣 17 , 𝑣 19 , 𝑣 23 , 𝑣 29 , 𝑣 31 , 𝑣 41 , 𝑣 44 , 𝑣 46 , 𝑣 51 , 𝑣 55 , 𝑣 63 , 𝑣 66 , 𝑣 72 , 𝑣 78 , 𝑣 3 , 𝑣 0 , 𝑣 69 , 𝑣 6 , 𝑣 26 , 𝑣 7 , 𝑣 50 , 𝑣 68 , 𝑣 25 , 𝑣 48 , 𝑣 33 , 𝑣 4 , 𝑣 21 , 𝑣 76 , 𝑣 36 , 𝑣 16 , 𝑣 14 , 𝑣 37 , 𝑣 38 , 𝑣 39 , 𝑣 59 , 𝑣 61 , 𝑣 18 , 𝑣 53 , 𝑣 34 , 𝑣 74 , 𝑣 40 , 𝑣 1 , 𝑣 57 , 𝑣 9 , 𝑣 13 , 𝑣 22 , 𝑣 35 } Method  
   
  Search space # of balanced cubes Space rate† Balanced cubes rate‡  
   
  first strategy  
   
  1365  
   
  second strategy 12201  
   
  63  
   
  5.83%  
   
  40.38%  
   
  156  
   
  52.1%  
   
  96.30%  
   
  no strategy 23426 162 100% 100% †: Space rate = (search space)/(the whole search space). ‡: Balanced cubes rate = (balanced cubes)/(balanced cubes from the whole search space).  
   
  4.1  
   
  A Key-Recovery Attack on 810-Round Trivium with Practical Complexity  
   
  ( 810) as Determine a Mother Cube. Using the algorithm in [29], we predict 𝑠66 the preference bit for 810-round Trivium. We choose many cubes of size 19 and ( 744) are balanced. Then, the cube search for cubes whose superpolies in 𝑠285  
   
  𝑆𝑎 1 = {𝑣 2 , 𝑣 6 , 𝑣 8 , 𝑣 10 , 𝑣 11 , 𝑣 15 , 𝑣 19 , 𝑣 21 , 𝑣 25 , 𝑣 29 , 𝑣 30 , 𝑣 32 , 𝑣 34 , 𝑣 36 , 𝑣 39 , 𝑣 41 , 𝑣 43 , 𝑣 45 , 𝑣 50 } is obtained. Its superpoly is 𝑝 56 . Also, we search for cubes whose superpolies in ( 744) 𝑠286 are balanced. We get the cube 𝑆𝑎 2 = {𝑣 0 , 𝑣 2 , 𝑣 4 , 𝑣 8 , 𝑣 10 , 𝑣 11 , 𝑣 17 , 𝑣 19 , 𝑣 25 , 𝑣 29 , 𝑣 30 , 𝑣 32 , 𝑣 34 , 𝑣 36 , 𝑣 39 , 𝑣 41 , 𝑣 43 , 𝑣 45 , 𝑣 50 }. Its superpoly is 𝑝 56 . We ﬁrst choose 𝑆𝑎 1 as the starting cube to extend. The public variables are added to 𝑆𝑎 1 iteratively to make the degree1 of the superpoly decrease to a minimum value other than 0. Then 𝑆𝑎 3 is obtained. 𝑆𝑎 3 = {𝑣 2 , 𝑣 6 , 𝑣 8 , 𝑣 10 , 𝑣 11 , 𝑣 15 , 𝑣 19 , 𝑣 21 , 𝑣 25 , 𝑣 29 , 𝑣 30 , 𝑣 32 , 𝑣 34 , 𝑣 36 , 𝑣 39 , 𝑣 41 , 𝑣 43 , 𝑣 45 , 𝑣 50 , 𝑣 0 , 𝑣 75 , 𝑣 12 , 𝑣 4 , 𝑣 14 , 𝑣 20 , 𝑣 22 , 𝑣 16 , 𝑣 27 , 𝑣 23 , 𝑣 72 , 𝑣 52 , 𝑣 55 , 𝑣 60 , 𝑣 37 , 𝑣 79 , 𝑣 62 , 𝑣 64 , 𝑣 47 , 𝑣 54 , 𝑣 70 }. The upper bound of the degree of its superpoly is 8 and its size is 40. We note that this set contains all elements in 𝑆𝑎 2 except 𝑣 17 . So we replace 𝑣 16 in 𝑆𝑎 3 with 𝑣 17 to make it fully contain 𝑆𝑎 2 . 𝑆𝑎 3 = {𝑣 2 , 𝑣 6 , 𝑣 8 , 𝑣 10 , 𝑣 11 , 𝑣 15 , 𝑣 19 , 𝑣 21 , 𝑣 25 , 𝑣 29 , 𝑣 30 , 𝑣 32 , 𝑣 34 , 𝑣 36 , 𝑣 39 , 𝑣 41 , 𝑣 43 , 𝑣 45 , 𝑣 50 , 𝑣 0 , 𝑣 75 , 𝑣 12 , 𝑣 4 , 𝑣 14 , 𝑣 20 , 𝑣 22 , 𝑣 17 , 𝑣 27 , 𝑣 23 , 𝑣 72 , 𝑣 52 , 𝑣 55 , 𝑣 60 , 𝑣 37 , 𝑣 79 , 𝑣 62 , 𝑣 64 , 𝑣 47 , 𝑣 54 , 𝑣 70 }. 1  
   
  We evaluate the upper bound of the degree of the superpoly based on the division property modeled with MILP.  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  17  
   
  And after adding one element of set 𝐴 = {𝑣 40 , 𝑣 53 , 𝑣 57 , 𝑣 58 , 𝑣 67 , 𝑣 68 , 𝑣 77 , 𝑣 48 }, the degree of 𝑆𝑎 3 is less than 5. Then, we select four elements from set 𝐴 to add to 𝑆𝑎 3 and get 70 44-dimensional candidate mother cubes. We examine the number of balanced superpolies in the 43-dimensional subcubes for each of the 70 candidate mother cubes, then the mother cube 𝑆𝑎 4 = {𝑣 0 , 𝑣 2 , 𝑣 4 , 𝑣 6 , 𝑣 8 , 𝑣 10 , 𝑣 11 , 𝑣 12 , 𝑣 14 , 𝑣 15 , 𝑣 17 , 𝑣 19 , 𝑣 20 , 𝑣 21 , 𝑣 22 , 𝑣 23 , 𝑣 25 , 𝑣 27 , 𝑣 29 , 𝑣 30 , 𝑣 32 , 𝑣 34 , 𝑣 36 , 𝑣 37 , 𝑣 39 , 𝑣 41 , 𝑣 43 , 𝑣 45 , 𝑣 47 , 𝑣 50 , 𝑣 52 , 𝑣 54 , 𝑣 55 , 𝑣 60 , 𝑣 62 , 𝑣 64 , 𝑣 70 , 𝑣 72 , 𝑣 75 , 𝑣 79 , 𝑣 68 , 𝑣 57 , 𝑣 53 , 𝑣 48 } is selected. It has two simple 43-dimensional subcubes whose superpolies are linear. Search for Balanced Subcubes. We add a time limit to the search time. A full search is performed over all 42- and 43-dimensional subcubes of 𝑆𝑎 4 . The 40and 41-dimensional subcubes are searched using the second strategy. The method used to recover superpolies is from [11], and we modify it to recover superpolies for new secret variables. Then several superpolies are obtained. After using SageMath to extract balanced or quadratic superpolies, 405 balanced superpolies and 526 quadratic superpolies are obtained. Then we obtain additional 275 balanced polynomials from 526 quadratic superpolies using the combining rule in Sect. 3.2. Determine the Order of Derivation. We pick 39 polynomials from 680 balanced polynomials. The corresponding cubes and the independent bits contained by these polynomials are listed in [15, Appendix E]. 39 variables can be deduced from these polynomials. The speciﬁc polynomials are provided at https://github. com/lhoop/ObtainMoreBS/tree/main/data/810 superpoly. A Practical Attack on a PC. The size of 𝑆𝑎 4 is 44, so it takes 244 requests to obtain all the values of these 39 polynomials. Next, we need to enumerate the values of 42 variables: {𝑝 0 , 𝑝 2 , 𝑝 3 , 𝑝 4 , 𝑝 6 , 𝑝 7 , 𝑝 9 , 𝑝 10 , 𝑝 17 , 𝑝 18 , 𝑝 20 , 𝑝 23 , 𝑝 25 , 𝑝 28 , 𝑝 30 , 𝑝 33 , 𝑝 35 , 𝑝 38 , 𝑝 39 , 𝑝 40 , 𝑝 42 , 𝑝 44 , 𝑝 45 , 𝑝 47 , 𝑝 48 , 𝑝 49 , 𝑝 51 , 𝑝 52 , 𝑝 56 , 𝑝 57 , 𝑝 58 , 𝑝 59 , 𝑝 60 , 𝑝 63 , 𝑝 66 , 𝑝 69 , 𝑝 70 , 𝑝 73 , 𝑝 77 , 𝑝 78 , 𝑝 79 , 𝑝 80 }. For each enumeration, the values of the remaining 39 variables can be deduced iteratively in the order (𝑝 76 , 𝑝 61 , 𝑝 64 , 𝑝 74 , 𝑝 62 , 𝑝 41 , 𝑝 46 , 𝑝 11 , 𝑝 37 , 𝑝 34 , 𝑝 21 , 𝑝 22 , 𝑝 54 , 𝑝 24 , 𝑝 50 , 𝑝 12 , 𝑝 36 , 𝑝 19 , 𝑝 65 , 𝑝 5 , 𝑝 29 , 𝑝 8 , 𝑝 16 , 𝑝 53 , 𝑝 26 , 𝑝 14 , 𝑝 43 , 𝑝 68 , 𝑝 55 , 𝑝 67 , 𝑝 71 , 𝑝 27 , 𝑝 75 , 𝑝 31 , 𝑝 15 , 𝑝 1 , 𝑝 32 , 𝑝 13 , 𝑝 72 ). There are 242 enumerations, for each enumeration, we use Algorithm 2 to substitute new secret variables back to original secret variables. Half of the enumerations will be excluded because the value of 𝑝 80 does not match. This check only costs constant time. So actually, there are only 241 enumerations of original secret variables. With 241 round-reduced Trivium initializations, the correct key can be ﬁltered out of these 241 candidate keys. To sum up, the whole attack costs 244 + 241 round-reduced Trivium initializations. On a PC with an A100 GPU, we can perform the whole attack in 48 min.  
   
  18  
   
  4.2  
   
  H. Lei et al.  
   
  A Key-Recovery Attack on 825-Round Trivium with Practical Complexity  
   
  ( 825) Determine a Mother Cube. We predict 𝑠66 as the preference bit for 825round Trivium. Then we choose many cubes of sizes 20 and search for cubes ( 759) are balanced. Finally, the cube whose superpolies in 𝑠286  
   
  𝑆𝑏 1 = {𝑣 2 , 𝑣 5 , 𝑣 6 , 𝑣 8 , 𝑣 10 , 𝑣 12 , 𝑣 15 , 𝑣 19 , 𝑣 23 , 𝑣 29 , 𝑣 34 , 𝑣 41 , 𝑣 44 , 𝑣 46 , 𝑣 53 , 𝑣 55 , 𝑣 63 , 𝑣 66 , 𝑣 72 , 𝑣 78 } is selected. Its superpoly is 𝑝 66 ⊕ 𝑝 24 ⊕ 𝑝 22 𝑝 23 ⊕ 1. The public variables are added to 𝑆𝑏 1 iteratively to make the degree of the superpoly decrease to a minimum value other than 0. Then 𝑆𝑏 2 is obtained. 𝑆𝑏 2 = {𝑣 2 , 𝑣 5 , 𝑣 8 , 𝑣 10 , 𝑣 12 , 𝑣 15 , 𝑣 17 , 𝑣 19 , 𝑣 23 , 𝑣 29 , 𝑣 31 , 𝑣 41 , 𝑣 44 , 𝑣 46 , 𝑣 51 , 𝑣 55 , 𝑣 63 , 𝑣 66 , 𝑣 72 , 𝑣 78 , 𝑣 3 , 𝑣 0 , 𝑣 69 , 𝑣 6 , 𝑣 26 , 𝑣 7 , 𝑣 50 , 𝑣 68 , 𝑣 25 , 𝑣 48 , 𝑣 33 , 𝑣 4 , 𝑣 21 , 𝑣 76 , 𝑣 36 , 𝑣 16 , 𝑣 14 , 𝑣 37 , 𝑣 38 , 𝑣 39 , 𝑣 59 , 𝑣 61 , 𝑣 18 , 𝑣 53 , 𝑣 34 , 𝑣 74 , 𝑣 40 , 𝑣 1 , 𝑣 57 , 𝑣 9 }. The upper bound of the degree of its superpoly is 6 and its size is 50. And after adding one variable of set 𝐵 = {𝑣 40 , 𝑣 53 , 𝑣 57 , 𝑣 58 , 𝑣 67 , 𝑣 68 , 𝑣 77 , 𝑣 48 }, the degree of 𝑆𝑏 2 is less than 5. Then we select three variables from set 𝐵 to add to 𝑆𝑏 2 and obtain 280 53-dimensional candidate mother cubes. For each of the 280 candidate mother cubes, we examine the number of balanced superpolies that can be generated by its 52-dimensional subcubes, then the mother cube 𝑆𝑏 3 = {𝑣 2 , 𝑣 5 , 𝑣 8 , 𝑣 10 , 𝑣 12 , 𝑣 15 , 𝑣 17 , 𝑣 19 , 𝑣 23 , 𝑣 29 , 𝑣 31 , 𝑣 41 , 𝑣 44 , 𝑣 46 , 𝑣 51 , 𝑣 55 , 𝑣 63 , 𝑣 66 , 𝑣 72 , 𝑣 78 , 𝑣 3 , 𝑣 0 , 𝑣 69 , 𝑣 6 , 𝑣 26 , 𝑣 7 , 𝑣 50 , 𝑣 68 , 𝑣 25 , 𝑣 48 , 𝑣 33 , 𝑣 4 , 𝑣 21 , 𝑣 76 , 𝑣 36 , 𝑣 16 , 𝑣 14 , 𝑣 37 , 𝑣 38 , 𝑣 39 , 𝑣 59 , 𝑣 61 , 𝑣 18 , 𝑣 53 , 𝑣 34 , 𝑣 74 , 𝑣 40 , 𝑣 1 , 𝑣 57 , 𝑣 9 , 𝑣 13 , 𝑣 22 , 𝑣 35 } is selected. It has 7 52-dimensional subcubes whose superpolies are simple and balanced. Search for Balanced Subcubes. A full search is performed over all 52- and 51dimensional subcubes of 𝑆𝑏 3 . The 50- and 49-dimensional subcubes are searched using the second strategy, respectively. Then several superpolies are obtained. We use SageMath to extract balanced or quadratic superpolies and obtain 354 balanced superpolies and 422 quadratic superpolies. Then we obtain an extra 872 balanced polynomials from 422 quadratic superpolies using the combining rule in Sect. 3.3. Determine the Order of Derivation. We pick 31 polynomials from 1226 balanced polynomials. The corresponding cubes and the independent bits contained by these polynomials are listed in [15, Appendix E]. 31 variables can be deduced from these polynomials. The speciﬁc polynomials are provided at https://github. com/lhoop/ObtainMoreBS/tree/main/data/825 superpoly. A Practical Attack on a PC. The size of 𝑆𝑏 3 is 53, so it takes 253 requests to obtain all the values of these 31 polynomials. Next, we need to enumerate the values of 50 variables: {𝑝 3 , 𝑝 4 , 𝑝 5 , 𝑝 8 , 𝑝 9 , 𝑝 10 , 𝑝 16 , 𝑝 17 , 𝑝 20 , 𝑝 22 , 𝑝 25 ,  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  19  
   
  𝑝 26 , 𝑝 28 , 𝑝 29 , 𝑝 32 , 𝑝 34 , 𝑝 37 , 𝑝 39 , 𝑝 40 , 𝑝 43 , 𝑝 44 , 𝑝 45 , 𝑝 46 , 𝑝 47 , 𝑝 48 , 𝑝 49 , 𝑝 51 , 𝑝 52 , 𝑝 53 , 𝑝 54 , 𝑝 55 , 𝑝 56 , 𝑝 57 , 𝑝 58 , 𝑝 59 , 𝑝 60 , 𝑝 61 , 𝑝 62 , 𝑝 64 , 𝑝 66 , 𝑝 67 , 𝑝 69 , 𝑝 70 , 𝑝 71 , 𝑝 72 , 𝑝 74 , 𝑝 76 , 𝑝 78 , 𝑝 79 , 𝑝 80 }. For each enumeration, the values of the remaining 31 variables can be deduced iteratively in the order (𝑝 73 , 𝑝 75 , 𝑝 77 , 𝑝 63 , 𝑝 14 , 𝑝 31 , 𝑝 11 , 𝑝 35 , 𝑝 27 , 𝑝 33 , 𝑝 12 , 𝑝 41 , 𝑝 30 , 𝑝 65 , 𝑝 38 , 𝑝 1 , 𝑝 13 , 𝑝 15 , 𝑝 50 , 𝑝 42 , 𝑝 6 , 𝑝 7 , 𝑝 18 , 𝑝 68 , 𝑝 24 , 𝑝 23 , 𝑝 2 , 𝑝 0 , 𝑝 19 , 𝑝 36 , 𝑝 21 ). There are 250 enumerations, for each enumeration, we use Algorithm 2 to substitute new secret variables back to original secret variables. Half of the enumerations will be excluded because the value of 𝑝 80 does not match. This check only costs a constant time. So actually, we only need 249 enumerations of original secret variables. With 249 round-reduced Trivium initializations, the correct key can be ﬁltered out of these 249 candidate keys. To sum up, the whole attack costs 253 + 249 round-reduced Trivium initializations. On a PC with an A100 GPU, we can perform the whole attack in 18 days.  
   
  5  
   
  Conclusion  
   
  In this paper, we focus on practical full key-recovery attacks on Trivium. We propose a variable substitution technique to simplify the superpoly and a new method to obtain a new balanced polynomial by combining two superpolies to cancel out the quadratic terms. Moreover, by an observation that the subcubes of a cube whose superpoly is balanced are more likely to have balanced superpolies, we modify the original algorithm to construct a better mother cube that contains more subcubes with balanced superpolies, and propose a heuristic strategy for searching for cubes with balanced superpolies. As a result, we use our new methods to perform full key-recovery attacks on 810- and 825-round Trivium, which can be done with time complexity 244.17 and 253.09 round-reduced Trivium initializations, respectively. It is experimentally veriﬁed that the two attacks could be completed in 48 min and 18 days on a PC with one A100 GPU (128 × 256 threads), respectively. We also time-test previous attacks on 808- and 820-round Trivium with the same number of threads [6,21]. For the attack on 808-round Trivium in [21], it could be completed in 12 h with our GPU. And for the attack on 820-round Trivium in [6], we estimate that the attack would be completed in 19 days with our GPU. These experiments conﬁrm that we can improve the previous results for 2 and 5 rounds without increasing the time complexity. Acknowledgment. The authors would like to thank Raghvendra Rohit as our shepherd and other anonymous reviewers that have helped us improve the quality of this paper. This research is supported by the National Key Research and Development Program of China (Grant No. 2018YFA0704702), the National Natural Science Foundation of China (Grant No. 62032014), the Major Basic Research Project of Natural Science Foundation of Shandong Province, China (Grant No. ZR202010220025).  
   
  20  
   
  H. Lei et al.  
   
  References 1. Sagemath. https://www.sagemath.org 2. Aumasson, J.-P., Dinur, I., Meier, W., Shamir, A.: Cube testers and key recovery attacks on reduced-round MD6 and trivium. In: Dunkelman, O. (ed.) FSE 2009. LNCS, vol. 5665, pp. 1–22. Springer, Heidelberg (2009). https://doi.org/10.1007/ 978-3-642-03317-9 1 3. Boura, C., Coggia, D.: Eﬃcient MILP modelings for Sboxes and linear layers of SPN ciphers. IACR Trans. Symmetric Cryptol. 2020(3), 327–361 (2020). https:// doi.org/10.13154/tosc.v2020.i3.327-361 4. De Canni`ere, C., Preneel, B.: Trivium. In: Robshaw, M., Billet, O. (eds.) New Stream Cipher Designs. LNCS, vol. 4986, pp. 244–266. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-68351-3 18 5. Canteaut, A., et al.: Stream ciphers: a practical solution for eﬃcient homomorphicciphertext compression. J. Cryptol. 31(3), 885–916 (2018). https://doi.org/10. 1007/s00145-017-9273-9 6. Che, C., Tian, T.: An experimentally veriﬁed attack on 820-round trivium. In: Deng, Y., Yung, M. (eds.) Inscrypt 2022. LNCS, vol. 13837, pp. 357–369. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-26553-2 19 7. Dinur, I., Shamir, A.: Cube attacks on tweakable black box polynomials. In: Joux, A. (ed.) EUROCRYPT 2009. LNCS, vol. 5479, pp. 278–299. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-01001-9 16 8. Fouque, P., Vannet, T.: Improving key recovery to 784 and 799 rounds of Trivium using optimized cube attacks. IACR Cryptol. ePrint Arch. 312 (2015). http:// eprint.iacr.org/2015/312 9. Hao, Y., Leander, G., Meier, W., Todo, Y., Wang, Q.: Modeling for three-subset division property without unknown subset. In: Canteaut, A., Ishai, Y. (eds.) EUROCRYPT 2020. LNCS, vol. 12105, pp. 466–495. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-45721-1 17 10. Hao, Y., Leander, G., Meier, W., Todo, Y., Wang, Q.: Modeling for three-subset division property without unknown subset. J. Cryptol. 34(3), 22 (2021). https:// doi.org/10.1007/s00145-021-09383-2 11. He, J., Hu, K., Preneel, B., Wang, M.: Stretching cube attacks: improved methods to recover massive superpolies. In: Agrawal, S., Lin, D. (eds.) ASIACRYPT 2022, Part IV. LNCS, vol. 13794, pp. 537–566. Springer, Cham (2022). https://doi.org/ 10.1007/978-3-031-22972-5 19 12. Hebborn, P., Lambin, B., Leander, G., Todo, Y.: Lower bounds on the degree of block ciphers. In: Moriai, S., Wang, H. (eds.) ASIACRYPT 2020, Part I. LNCS, vol. 12491, pp. 537–566. Springer, Cham (2020). https://doi.org/10.1007/978-3030-64837-4 18 13. Hu, K., Sun, S., Todo, Y., Wang, M., Wang, Q.: Massive superpoly recovery with nested monomial predictions. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021, Part I. LNCS, vol. 13090, pp. 392–421. Springer, Cham (2021). https://doi. org/10.1007/978-3-030-92062-3 14 14. Hu, K., Sun, S., Wang, M., Wang, Q.: An algebraic formulation of the division property: revisiting degree evaluations, cube attacks, and key-independent sums. In: Moriai, S., Wang, H. (eds.) ASIACRYPT 2020, Part I. LNCS, vol. 12491, pp. 446–476. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-64837-4 15 15. Lei, H., He, J., Hu, K., Wang, M.: More balanced polynomials: cube attacks on 810- and 825-round Trivium with practical complexities. IACR Cryptol. ePrint Arch. 1237 (2023). https://eprint.iacr.org/2023/1237  
   
  Cube Attacks on 810- And 825-Round Trivium with Practical Complexities  
   
  21  
   
  16. Liu, M., Yang, J., Wang, W., Lin, D.: Correlation cube attacks: from weak-key distinguisher to key recovery. In: Nielsen, J.B., Rijmen, V. (eds.) EUROCRYPT 2018, Part II. LNCS, vol. 10821, pp. 715–744. Springer, Cham (2018). https://doi. org/10.1007/978-3-319-78375-8 23 17. Mroczkowski, P., Szmidt, J.: Corrigendum to: the cube attack on stream cipher Trivium and quadraticity tests. IACR Cryptol. ePrint Arch. 32 (2011). http:// eprint.iacr.org/2011/032 18. Salam, M.I., Bartlett, H., Dawson, E., Pieprzyk, J., Simpson, L., Wong, K.K.-H.: Investigating cube attacks on the authenticated encryption stream cipher ACORN. In: Batten, L., Li, G. (eds.) ATIS 2016. CCIS, vol. 651, pp. 15–26. Springer, Singapore (2016). https://doi.org/10.1007/978-981-10-2741-3 2 19. Sasaki, Yu., Todo, Y.: New algorithm for modeling S-box in MILP based diﬀerential and division trail search. In: Farshim, P., Simion, E. (eds.) SecITC 2017. LNCS, vol. 10543, pp. 150–165. Springer, Cham (2017). https://doi.org/10.1007/978-3319-69284-5 11 20. Sun, S., Hu, L., Wang, P., Qiao, K., Ma, X., Song, L.: Automatic security evaluation and (related-key) diﬀerential characteristic search: application to SIMON, PRESENT, LBlock, DES(L) and other bit-oriented block ciphers. In: Sarkar, P., Iwata, T. (eds.) ASIACRYPT 2014. LNCS, vol. 8873, pp. 158–178. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-45611-8 9 21. Sun, Y.: Automatic search of cubes for attacking stream ciphers. IACR Trans. Symmetric Cryptol. 2021(4), 100–123 (2021). https://doi.org/10.46586/tosc.v2021.i4. 100-123 22. Todo, Y.: Structural evaluation by generalized integral property. In: Oswald, E., Fischlin, M. (eds.) EUROCRYPT 2015. LNCS, vol. 9056, pp. 287–314. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-46800-5 12 23. Todo, Y., Isobe, T., Hao, Y., Meier, W.: Cube attacks on non-blackbox polynomials based on division property. In: Katz, J., Shacham, H. (eds.) CRYPTO 2017, Part III. LNCS, vol. 10403, pp. 250–279. Springer, Cham (2017). https://doi.org/10. 1007/978-3-319-63697-9 9 24. Todo, Y., Morii, M.: Bit-based division property and application to Simon family. In: Peyrin, T. (ed.) FSE 2016. LNCS, vol. 9783, pp. 357–377. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-52993-5 18 25. Wang, S., Hu, B., Guan, J., Zhang, K., Shi, T.: MILP-aided method of searching division property using three subsets and applications. In: Galbraith, S.D., Moriai, S. (eds.) ASIACRYPT 2019, Part III. LNCS, vol. 11923, pp. 398–427. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-34618-8 14 26. Wu, H.: ACORN v3. Submission to CAESAR competition (2016) 27. Ye, C., Tian, T.: A new framework for ﬁnding nonlinear superpolies in cube attacks against trivium-like ciphers. In: Susilo, W., Yang, G. (eds.) ACISP 2018. LNCS, vol. 10946, pp. 172–187. Springer, Cham (2018). https://doi.org/10.1007/978-3319-93638-3 11 28. Ye, C., Tian, T.: Algebraic method to recover superpolies in cube attacks. IET Inf. Secur. 14(4), 430–441 (2020). https://doi.org/10.1049/iet-ifs.2019.0323 29. Ye, C.-D., Tian, T.: A practical key-recovery attack on 805-round trivium. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021, Part I. LNCS, vol. 13090, pp. 187–213. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-92062-3 7  
   
  A Closer Look at the S-Box: Deeper Analysis of Round-Reduced ASCON-HASH Xiaorui Yu1 , Fukang Liu2 , Gaoli Wang1(B) , Siwei Sun3 , and Willi Meier4 1  
   
  3  
   
  Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai 200062, China [email protected]  , [email protected]  2 Tokyo Institute of Technology, Tokyo, Japan School of Cryptology, University of Chinese Academy of Sciences, Beijing, China 4 FHNW, Windisch, Switzerland  
   
  Abstract. ASCON, a lightweight permutation-based primitive, has been selected as NIST’s lightweight cryptography standard. ASCONHASH is one of the hash functions provided by the cipher suite ASCON. At ToSC 2021, the collision attack on 2-round ASCON-HASH with time complexity 2103 was proposed. Due to its small rate, it is always required to utilize at least 2 message blocks to mount a collision attack because each message block is only of size 64 bits. This signiﬁcantly increases the diﬃculty of the analysis because one almost needs to analyze equivalently at least 2 rounds of ASCON in order to break  rounds. In this paper, we make some critical observations on the round function of ASCON, especially a 2-round property. It is found that such properties can be exploited to reduce the time complexity of the 2-round collision attack to 262.6 . Although the number of attacked rounds is not improved, we believe our techniques shed more insight into the properties of the ASCON permutation and we expect they can be useful for the future research.  
   
  Keywords: ASCON Technique  
   
  1  
   
  · ASCON-HASH · Collision Attack · Algebraic  
   
  Introduction  
   
  Lightweight cryptography algorithms are a class of ciphers designed for resourceconstrained environments. They typically have low requirements in terms of computing power, storage space, and power consumption, and are suitable for resource-constrained application scenarios such as embedded systems, IoT devices, and sensors. In 2013, NIST started the lightweight cryptography project. Later in 2016, NIST provided an overview of the project and decided to seek for some new algorithms as a lightweight cryptography standard. In 2019, NIST received 57 c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 22–42, 2024. https://doi.org/10.1007/978-3-031-53368-6_2  
   
  A Closer Look at the S-Box  
   
  23  
   
  submissions and 56 of them became the ﬁrst round candidates after the initial review. After the project proceeded into Round 2 [4], NIST selected 32 submissions as Round 2 candidates, including ASCON. After that, ASCON was selected to be one of the ten ﬁnalists of the lightweight cryptography standardization process. On February 7, 2023, NIST announced the selection of the ASCON family for the lightweight cryptography standardization. ASCON [10] is a lightweight permutation-based primitive. It aims to provide eﬃcient encryption and authentication functions while maintaining suﬃciently high security. Advantages. The main advantages of ASCON can be summarized as below: – Lightweight: The design of ASCON is simple and suitable for hardware and software implementation. It is particularly suitable for resource constrained environments, such as IoT devices, embedded systems, and low-power devices. – High security: ASCON provides high security and resists many diﬀerent types of known attacks. – Adjustable: ASCON supports diﬀerent security levels and performance requirements. For example, ASCON-128 and ASCON-128a provide a 128-bit security level, suitable for high security requirements; ASCON-80pq provides an 128-bit security level, suitable for low-power and low-cost scenarios. – Authentication encryption: ASCON can achieve both data encryption and integrity protection. It supports associated data and allows for veriﬁcation of additional information during the encryption process, such as the identities of message senders and receivers. History. ASCON was ﬁrst published as a candidate in Round 1 [6] of the CAESAR competition [1]. This original design (version v1) speciﬁed the permutation as well as the mode for authenticated encryption with two recommended family members: The primary recommendation Ascon-128 as well as a variant Ascon96 with 96-bit key. For the subsequent version V1.1 [7] and V1.2 [8], minor functional tweaks were applied, including a reordering of the round constants and the modiﬁcation of the secondary recommendation to the current Ascon128a. Then, V1.2 [8] and the status update ﬁle [9] were submitted to the NIST Lightweight Cryptography project. The submission to NIST includes not only the authenticated cipher family, but also introduces modes of operation for hashing: ASCON-HASH and ASCON-XOF, as well as a third parameterization for authenticated encryption: Ascon-80pq. For ASCON-HASH and ASCON-XOF, they support 256-bit and arbitrary-length hash values, respectively. On the Collision Resistance of ASCON-HASH. Due to the used sponge structure, the generic time complexity to ﬁnd a collision of ASCON-HASH is 2128 and the memory complexity is negligible with Floyd’s cycle ﬁnding algorithm [11]. Due to its small rate, it is quite challenging to ﬁnd collisions for a large number of rounds. In [21], the ﬁrst 2-round collision attack on ASCON-HASH was presented with time complexity 2125 . However, it is shown that such an attack is  
   
  24  
   
  X. Yu et al.  
   
  invalid because the used 2-round diﬀerential characteristic is invalid according to [13]. Later, at ToSC 2021 [12], a new and valid 2-round diﬀerential characteristic with an optimal diﬀerential probability was found. Based on the same attack strategy as in [21], they gave a 2-round collision with time complexity of 2103 in [12]. Very recently, Qin et al. presented collision attacks on 3 and 4 rounds of ASCON-HASH by turning preimages for ASCON-XOF into collisions for ASCON-HASH [19]. However, it can be found that both the time complexity and memory complexity of the 3/4-round collision attacks are very high, i.e. larger than 2120 . From a practical view, it seems that these attacks may be slower than the generic attack. In any case, all the collision attacks are far from being practical, even for 2 rounds. Table 1. Summary of collision attacks on ASCON-HASH Attack Type  
   
  ∗  
   
  Rounds Time complexity Memory Complexity Reference  
   
  collision attack 2 2 2 3 4 The characteristic used is  
   
  2125∗ 2103 262.6 2121.85 2126.77 invalid.  
   
  negligible negligible negligible 2121 2126  
   
  [21] [12] Sect. 4 [19] [19]  
   
  Our Contributions. We aim to signiﬁcantly improve the time complexity of the 2-round collision attack in [12] such that it can be much closer to a practical attack. Our contributions are summarized below: 1. We found that the 2-round collision attack in [12] is quite straightforward, i.e., the authors found a better characteristic but did not optimize the attack strategy. Hence, we are motivated to take a closer look at the used 2-round differential characteristic and aim to improve the attack by using some algebraic properties of the S-box as in the recent algebraic attack on LowMC [14,17], i.e., we are interested in the relations between the diﬀerence transitions and value transitions. 2. Based on our ﬁndings of the properties of the S-box, we propose to use a better attack framework and advanced algebraic techniques to improve the 2-round collision attack. As a result, the time complexity is reduced from 2103 to 262.6 , as shown in Table 1. Organization of this paper. In Sect. 2, we deﬁne some notations that will be used throughout the paper and brieﬂy describe ASCON-HASH. In Sect. 3, we describe the collision attack framework that will be used in the new attacks. In Sect. 4, we show how to optimize the existing 2-round collision attack with advanced algebraic techniques. Finally, the paper is concluded in Sect. 5.  
   
  A Closer Look at the S-Box  
   
  2  
   
  25  
   
  Preliminaries  
   
  2.1  
   
  Notations  
   
  The notations used in this paper are summarized in Table 2. Table 2. Notations r  
   
  the length of the rate part for ASCON-HASH, r = 64  
   
  c  
   
  the length of the capacity part for ASCON-HASH, c = 256  
   
  Sji i  
   
  S [j]  
   
  the input state of round i when absorbing the message block Mj the j-th word (64-bit) of Si  
   
  S i [j][k] the k-th bit of S i [j], k = 0 means the least signiﬁcant bit and k is within modulo 64  
   
  2.2  
   
  xi  
   
  the i-th bit of a 5-bit value x, x0 represents the most signiﬁcant bit  
   
  M  
   
  message  
   
  Mi  
   
  the i-th block of the padded message  
   
  ≫  
   
  right rotation (circular right shift)  
   
  a%b  
   
  a mod b  
   
  0n  
   
  a string of n zeroes  
   
  Description of ASCON-HASH  
   
  The ASCON family oﬀers 2 important hash functions: ASCON-HASH and ASCON-XOF. ASCON-HASH is a sponge-based hash function [2]. In its core, it is a 12-round permutation P a over a state of 320 bits. The hashing mode is shown in Fig. 1.  
   
  Fig. 1. The mode of ASCON-HASH  
   
  For ASCON-HASH, the state denoted by X is divided into ﬁve 64-bit words, i.e., X = X0 ||X1 ||X2 ||X3 ||X4 . The ﬁrst 64-bit word X0 will be loaded in the rate part while the remaining 4 words (X1 , X2 , X3 , X4 ) are loaded in the capacity part. The round function f = fL ◦ fS ◦ fC is composed of 3 operations: fC is the constant addition, fS is the substitution layer, and fL is the linear diﬀusion layer. For simplicity, the -round ASCON permutation is simply denoted by f  .  
   
  26  
   
  X. Yu et al.  
   
  On the Internal States. When absorbing the message block Mj , denote the 320bit input state at round i (0 ≤ i ≤ 11) by Sji and the state transitions are described below. fC  
   
  fS  
   
  fL  
   
  Sji −→ Sji,a −→ Sji,s −→ Sji+1 . Note that if we only consider one message block, we simply omit j as below: fC  
   
  fS  
   
  fL  
   
  S i −→ S i,a −→ S i,s −→ S i+1 . The corresponding graphic explanations can be referred to Fig. 2 and Fig. 3, respectively.  
   
  Fig. 2. The 1-round state transition when absorbing Mj  
   
  Fig. 3. The 1-round state transition  
   
  Constant Addition fC . For this operation, an 8-bit round constant ci is added to the word X2 , i.e., X2 ← X2 ⊕ ci . The round constants (ci )0≤i≤11 for 12-round ASCON-HASH are shown in Table 3.  
   
  Fig. 4. The substitution layer  
   
  A Closer Look at the S-Box  
   
  27  
   
  Fig. 5. The linear diﬀusion layer Table 3. The round constants ci i  
   
  0  
   
  1  
   
  2  
   
  3  
   
  4  
   
  5  
   
  6  
   
  7  
   
  8  
   
  9  
   
  10  
   
  11  
   
  ci 0xf0 0xe1 0xd2 0xc3 0xb5 0xa5 0x96 0x87 0x78 0x69 0x5a 0x4b  
   
  Substitution Layer fS . At this operation, the state will be updated by 64 parallel applications of a 5-bit S-box. The S-box (y0 , . . . , y4 ) = SB(x0 , . . . , x4 ) is deﬁned as follows: ⎧ y0 = x4 x1 ⊕ x3 ⊕ x2 x1 ⊕ x2 ⊕ x1 x0 ⊕ x1 ⊕ x0 , ⎪ ⎪ ⎪ ⎪ ⎨ y1 = x4 ⊕ x3 x2 ⊕ x3 x1 ⊕ x3 ⊕ x2 x1 ⊕ x2 ⊕ x1 ⊕ x0 , y2 = x4 x3 ⊕ x4 ⊕ x2 ⊕ x1 ⊕ 1, (1) ⎪ ⎪ = x x ⊕ x ⊕ x x ⊕ x ⊕ x ⊕ x ⊕ x , y ⎪ 3 4 0 4 3 0 3 2 1 0 ⎪ ⎩ y4 = x4 x1 ⊕ x4 ⊕ x3 ⊕ x1 x0 ⊕ x1 . As shown in Fig. 4, the input (x0 , . . . , x4 ) and output (y0 , . . . , y4 ) correspond to one column of the state. Linear Diﬀusion Layer fL . This operation is used to diﬀuse each 64-bit  word Xi , as shown in Fig. 5. Speciﬁcally, Xi is updated by the function i where 0 ≤ i ≤ 4, as speciﬁed below: ⎧ X0 ← Σ0 (X0 ) = X0 ⊕ (X0 ≫ 19) ⊕ (X0 ≫ 28), ⎪ ⎪ ⎪ ⎪ ⎨ X1 ← Σ1 (X1 ) = X1 ⊕ (X1 ≫ 61) ⊕ (X1 ≫ 39), X2 ← Σ2 (X2 ) = X2 ⊕ (X2 ≫ 1) ⊕ (X2 ≫ 6), ⎪ ⎪ X ⎪ 3 ← Σ3 (X3 ) = X3 ⊕ (X3 ≫ 10) ⊕ (X3 ≫ 17), ⎪ ⎩ X4 ← Σ4 (X4 ) = X4 ⊕ (X4 ≫ 7) ⊕ (X4 ≫ 41). On the Initial Value and State. The hash function initializes the 320-bit state using a constant IV = 0x00400c0000000000. Then, the 12-round ASCON permutation is applied and we obtain an initial state S10 = f 12 (IV ||0256 ), as speciﬁed below: 0xee9398aadb67f03d 0x8bb21831c60f1002 S10  
   
  ←  
   
  0xb48a92db98d5da62 0x43189921b8f8e3e8 0x348fa5c9d525e140  
   
  28  
   
  X. Yu et al.  
   
  The padding rule of ASCON-HASH is as follows: it appends a single 1 and the smallest number of zeroes to M such that the size of padded message in bits is a multiple of r = 64. The complete description of the hashing function is given in Algorithm 1 in Appendix A.  
   
  3  
   
  The Attack Frameworks  
   
  For diﬀerential-based collision attacks on a sponge-based hash function, one essential step is to ﬁnd a collision-generating diﬀerential characteristic. The second step is to ﬁnd conforming message pairs satisfying this diﬀerential characteristic. With the development of automatic tools, there are many possible methods to search for a desired diﬀerential characteristic. However, when it comes to the second step, i.e., satisfying the conditions of the diﬀerential characteristic, it always involves dedicated eﬀorts and sometimes requires nontrivial techniques. For example, the linearization techniques for the KECCAK round function have been widely used to speed up the diﬀerential-based collision attack on KECCAK, e.g., the 1/2/3-round connectors [5,18,20]. As can be seen from the current record of the Keccak crunchy crypto collision contest1 , it is quite challenging to analyze sponge-based hash functions with a small rate, which is exactly the case of ASCON. It is thus not surprising to see that the best diﬀerential-based collision attack on ASCON could only reach up to 2 rounds. For a sponge-based hash function with a small rate, one main obstacle exists in the available degrees of freedom in each message block. For ASCON, each message block only provides at most 64 free bits. However, for a diﬀerential characteristic used for collision attacks, there may exist more than 128 bit conditions, which directly makes it mandatory to utilize at least 3 message blocks. Let us consider a general case and suppose that we have an -round collisiongenerating diﬀerential characteristic. Furthermore, suppose we will use k message blocks (M1 , . . . , Mk ) to fulﬁll the conditions, i.e., we aim to ﬁnd (M1 , . . . , Mk ) and (M1 , . . . , Mk−1 , Mk ) such that   0 = f  Sj0 ⊕ (Mj ||0256 ) where 1 ≤ j ≤ k − 1, Sj+1     ||0256 = f  Sk0 ⊕ (Mk ||0256 ) ⊕ f  Sk0 ⊕ SB(Mk ||0256 ) , where Mk = Mk and  is an arbitrary r-bit value. From the diﬀerential characteristic, suppose that there are nc bit conditions on the capacity part of Sk0 and the remaining conditions hold with probability 2−nk . Then, a straightforward method to ﬁnd conforming message pairs is as follows: Step 1: Find a solution of (M1 , . . . , Mk−1 ) such that the nc bit conditions on the capacity part of Sk0 can hold. 1  
   
  https://keccak.team/crunchy_contest.html.  
   
  A Closer Look at the S-Box  
   
  29  
   
  Step 2: Exhaust Mk and check whether remaining nk bit conditions can hold. If there is a solution, a collision is found. Otherwise, return to Step 1. For convenience, we call the above procedure the general 2-step attack framework. Note that this has been widely used and it is really not a new idea. For a sponge with rate r, we need to perform Step 2 for about 2nk −r times and hence we need to perform Step 1 for 2nk −r times. Suppose the time complexity to ﬁnd a solution of (M1 , . . . , Mk−1 ) and Mk is Tpre and Tk , respectively. In this way, the total time complexity Ttotal is estimated as Ttotal = (k − 1) · 2nk −r · Tpre + 2nk −r · Tk .  
   
  (2)  
   
  If Tk and Tpre are simply treated as 2r and 2nc , respectively, i.e., only the naive exhaustive search is performed, then Ttotal = (k − 1) · 2nk +nc −r + 2nk . In other words, the total time complexity is directly related to the probability of the diﬀerential characteristic, i.e., 2−nc −nk . In many cases, the attackers can optimize Tk by using some advanced techniques to satisfy partial conditions implied in the diﬀerential characteristic, i.e., Tk can be smaller than 2r . For example, the target diﬀerence algorithm proposed in [5] is one of such techniques. However, to optimize Tpre , one has to solve a problem similar to the −round preimage ﬁnding problem. In most cases, this is not optimized due to the increasing diﬃculty and it is simply treated as Tpre = 2nc . 3.1  
   
  The Literature and Our New Strategy  
   
  It is found that neither Tk nor Tpre has been optimized for the existing 2-round collision attacks on ASCON-HASH [12,21] and they exactly follow the above attack framework. In the collision attack on 6-round GIMLI-HASH [13], the attackers optimized both Tk and Tpre where k = 2. As can be noted in our new attacks on ASCON-HASH, optimizing Tk is indeed quite straightforward after a little deeper analysis of the round function and its 5-bit S-box. However, optimizing Tpre looks infeasible at the ﬁrst glance. Indeed even if Tk is optimized to 1, the improved factor is still quite small. Therefore, to achieve signiﬁcant improvements, it is necessary to optimize Tpre . Our idea to achieve this purpose is to further convert the nc conditions on 0 , as the capacity part of Sk0 into some n1c conditions on the capacity part of Sk−1 Fig. 6 shows. In this way, our attack is stated as follows: Step 1: Find a solution of (M1 , . . . , Mk−2 ) such that the n1c bit conditions on 0 can hold. the capacity part of Sk−1 Step 2: Enumerate all the solutions of Mk−1 such that the conditions on the capacity part of Sk0 can hold. Step 3: Exhaust Mk and check whether remaining nk bit conditions can hold. If there is a solution, a collision is found. Otherwise, return to Step 1. To distinguish this from the general 2-step attack framework, we call the above procedure the general 3-step attack framework.  
   
  30  
   
  X. Yu et al.  
   
  Fig. 6. The general 3-step attack framework  
   
  Analysis of the Time Complexity. For convenience, the time complexity of Step 1, 2 and 3 is denoted by Tpre1 , Tk-1 and Tk , respectively. In this way, the total time complexity becomes Ttotal = (k − 2) · 2nk +nc −2r · Tpre1 + 2nk +nc −2r · Tk-1 + 2nk −r · Tk .  
   
  (3)  
   
  Speciﬁcally, we need on average 2nk −r diﬀerent valid solutions of (M1 , . . . , Mk−1 ). In this sense, we need about 2nk +nc −2r diﬀerent valid solutions of (M1 , . . . , Mk−2 ) because for each valid (M1 , . . . , Mk−2 ), we expect to have 2r−nc valid solutions of Mk−1 . Based on Eq. 3, if nc < r holds, we have 2nk +nc −2r < 2nk −r . Compared with Eq. 2, this case has indicated the possibility to optimize the attack if Tk-1 can be signiﬁcantly optimized and Tpre1 is relatively small, i.e., we  know Tpre1 ≤ 2nc . On the Purpose to Convert Conditions. As stated above, we have to optimize Tpre1 . This is related to the original purpose to introduce conditions on the 0 . Speciﬁcally, we expect that after adding these conditions, capacity part of Sk−1 we can eﬃciently enumerate the solutions of Mk−1 to satisfy the nc conditions on the capacity part of Sk0 . In other words, without these conditions, we still can only perform the naive exhaustive search over Mk−1 and no improvement can be obtained, i.e., the time complexity is (k − 2) · 2nk +nc −2r + 2nk +nc −2r · 2r + 2nk −r · Tk = (k − 2) · 2nk +nc −2r + 2nk +nc −r + 2nk −r · Tk . The Big Picture of Our New Attacks. In our attacks, we do not make more eﬀorts 0 to convert the nc conditions on Sk−1 into conditions on the previous input states due to the increasing diﬃculty. Hence, in our setting, we will make   
   
  Tpre1 = 2nc .  
   
  A Closer Look at the S-Box  
   
  31  
   
  In this way, the total time complexity is estimated as   
   
  Ttotal = (k − 2) · 2nk +nc +nc −2r + 2nk +nc −2r · Tk-1 + 2nk −r · Tk .  
   
  (4)  
   
  In the following, we will describe how to signiﬁcantly optimize Tk-1 and Tk based on an existing 2-round diﬀerential characteristic of ASCON.  
   
  4  
   
  Collision Attacks on 2-Round ASCON-HASH  
   
  The collision attack in this paper is based on the 2-round diﬀerential characteristic proposed in [12], as shown in Table 4. Note that the ﬁrst collision attack on 2-round ASCON-HASH was proposed in [21] but the diﬀerential characteristic is shown to be invalid in [13]. We have veriﬁed with the technique in [13] that the 2-round diﬀerential characteristic in [12] is correct. Table 4. The 2-round diﬀerential characteristic in [12] ΔS 0 (2−54 )  
   
  ΔS 1 (2−102 )  
   
  ΔS 2  
   
  0xbb450325d90b1581 0x2201080000011080 0xbaf571d85e1153d7 0x0  
   
  0x2adf0c201225338a 0x0  
   
  0x0  
   
  0x0  
   
  0x0  
   
  0x0000000100408000 0x0  
   
  0x0  
   
  0x2adf0c211265b38a 0x0  
   
  0x0  
   
  According to [12], there are 27 and 28 active S-boxes in the ﬁrst and second round, respectively. Speciﬁcally, there are 54 bit conditions on the capacity part of the input S 0 and 102 bit conditions on the input state S 1 of the second round. With our notations, there are nc = 54,  
   
  nk = 102.  
   
  With this diﬀerential characteristic, they used the technique in [21] to mount the collision attack with k = 4 message blocks and its time complexity is 2102 . It follows the general 2-step attack framework described above without optimization on Tpre and Tk , i.e., Tpre = 254 ,  
   
  Tk = 264 .  
   
  In this way, the total time complexity can be computed based on Eq. 2, i.e., Ttotal = 2 × 2102−64 × 254 + 2102−64 × 264 = 293 + 2102 ≈ 2102 .  
   
  (5)  
   
  It should be noted that in [12], the authors simply checked whether M3 and M3 ⊕ ΔS30 can follow the 2-round diﬀerential characteristic by exhausting M3 and hence the time complexity in [12] is estimated as 2 × 2102 = 2103 . In other words, they do not take the speciﬁc conditions into account, while in the above, we only check whether the conditions on the S 1 hold for each M3 .  
   
  32  
   
  4.1  
   
  X. Yu et al.  
   
  Optimizing Tk Using Simple Linear Algebra  
   
  Indeed, it is quite straightforward to optimize Tk . However, even if it is reduced to 1, the time complexity is still high, i.e., 292 according to Eq. 5. Let us elaborate on how to signiﬁcantly optimize Tk in this section. First, we need to study some properties of the S-box. Studying the Active S-Boxes in the First Round. First, we describe why there are 54 bit conditions on the capacity part of S 0 . Property 1 [21]. For an input diﬀerence (Δ0 , . . . , Δ4 ) satisfying Δx1 = Δx2 = Δx3 = Δx4 = 0 and Δx0 = 1, the following constraints hold: – For the output diﬀerence: ⎧ ⎨ Δy0 ⊕ Δy4 = 1, Δy1 = Δx0 , ⎩ Δy2 = 0. – For the input value:  

  x1 = Δy0 ⊕ 1, x3 ⊕ x4 = Δy3 ⊕ 1.  
   
  (6)  
   
  (7)  
   
  Based on Property 1 and the 2-round diﬀerential characteristic in Table 4, we can derive 27 + 27 = 54 bit conditions on the capacity part of S 0 , i.e., 27 bit conditions on S 0 [1] and 27 bit conditions on S 0 [3] ⊕ S 0 [4]. This also explains why nc = 54. Studying the Active S-Boxes in the Second Round. As the next step, we further study the 28 active S-boxes in the second round. We observe that from ΔS 1 to ΔS 1,s , there are only 3 diﬀerent possible diﬀerence transitions (Δx0 , . . . , Δx4 ) → (Δy0 , . . . , Δy4 ) through the S-box, as shown below: (1, 1, 0, 0, 1) → (1, 0, 0, 0, 0), (0, 0, 0, 1, 1) → (1, 0, 0, 0, 0), (0, 1, 0, 0, 1) → (1, 0, 0, 0, 0). Similar to the algebraic attacks on LowMC [14,17], we study and exploit the properties of the (x0 , . . . , x4 ) such that SB(x0 , . . . , x4 ) ⊕ SB(x0 ⊕ Δx0 , . . . , x4 ⊕ Δx4 ) = (Δy0 , . . . , Δy4 ) = (1, 0, 0, 0, 0) where (Δx0 , . . . , Δx4 ) ∈ {(1, 1, 0, 0, 1), (0, 0, 0, 1, 1), (0, 1, 0, 0, 1)}. It is found that  
   
  A Closer Look at the S-Box  
   
  33  
   
  – for (Δx0 , . . . , Δx4 ) = (1, 1, 0, 0, 1), all possible (x0 , . . . , x4 ) form an aﬃne subspace of dimension 2, as shown below: x0 ⊕ x4 = 0,  
   
  x1 = 1,  
   
  x3 = 0;  
   
  (8)  
   
  – for (Δx0 , . . . , Δx4 ) = (0, 0, 0, 1, 1), all possible (x0 , . . . , x4 ) form an aﬃne subspace of dimension 2, as shown below: x1 = 0,  
   
  x2 = 0,  
   
  x3 ⊕ x4 = 0;  
   
  (9)  
   
  – for (Δx0 , . . . , Δx4 ) = (0, 1, 0, 0, 1), all possible (x0 , . . . , x4 ) form an aﬃne subspace of dimension 1, as shown below: x0 = 0,  
   
  x1 ⊕ x4 = 1,  
   
  x2 = 0,  
   
  x3 = 0.  
   
  (10)  
   
  As a result, the diﬀerence transitions in the second round, i.e., the 28 active S-boxes, directly impose 102 linear conditions on S 1 . Note that it is unclear whether the probability 2−102 is directly computed according to the diﬀerential distribution table (DDT) of the 5-bit S-box in [12]. At least, we do not see any such related claims in [12] that the probability 2−102 is caused by 102 linear conditions on S 1 , i.e., the conditions may be nonlinear if we do not carefully study the relations between the diﬀerence transitions and values transitions. Indeed, we can simply generalize the above observations for any degree-2 S-box, as shown in Appendix B, i.e. all the conditions on the input bits must be linear for each valid diﬀerence transition of a degree-2 S-box. More Nonlinear Conditions on the Capacity Part of S 0 . As can be noted from Eq. 9 and Eq. 10, there will be conditions on S 1 [2], i.e., the conditions on x2 in Eq. 9 and Eq. 10. However, according to the deﬁnition of the S-box, we know that y2 = x4 x3 ⊕ x4 ⊕ x2 ⊕ x1 ⊕ 1. Hence, after the capacity part of S30 is ﬁxed, S 1 [2] is irrelevant to S 0 [0]. As a result, apart from the 54 linear conditions on the capacity part of S 0 , there are also 21 nonlinear (quadratic) conditions on the capacity part of S 0 . In other words, at the ﬁrst glance, although there are 102 linear conditions on S 1 , there are indeed only 102 − 21 = 81 linear conditions on S 1 depending on S 0 [0] after the capacity part of S 0 is known. Hence, we can equivalently say that nc = 54 + 21 = 75,  
   
  nk = 81.  
   
  With the general 2-step attack framework, the total time complexity is not aﬀected as nc + nk remains the same, i.e., it is still 2102 . Optimizing Tk . After knowing that there are 81 linear conditions on S 1 depending on S 0 [0] after the capacity part of S 0 is known, optimizing Tk is quite straightforward. Recall the general 2-step attack framework described previously. Specifically, by using 3 message blocks (M1 , M2 , M3 ), we ﬁrst generate valid (M1 , M2 ) such that the 75 bit conditions on the capacity part of S30 can hold. Then, since  
   
  34  
   
  X. Yu et al.  
   
  M3 is only added to S30 [0], S31 directly becomes linear in M3 and we know there are 81 linear conditions on S31 . Therefore, we can construct 81 linear equations in M3 , i.e., 64 variables. Similar to the idea in [15], solving this linear equation system is equivalent to exhausting all possible values of M3 and hence Tk is reduced to the time complexity to solve 81 linear equations in 64 variables (Fig. 7) that requires 81 × 81 × 64 ≈ 219 bit operations. As explained before, only optimizing Tk is insuﬃcient to signiﬁcantly improve the attack and we need to further optimize Tpre .  
   
  Fig. 7. Exhaust M3 by solving linear equations  
   
  4.2  
   
  Finding Valid (M1 , M2 ) with Advanced Techniques  
   
  To ﬁnd valid (M1 , M2 ), we are now only simply looping over (M1 , M2 ) and checking whether the 75 bit conditions on the capacity part can hold. To improve the attack, we have to avoid such a naive loop. In what follows, we describe how to use the general 3-step attack framework stated above to overcome this obstacle. The core idea is to utilize a 2-round property of ASCON. Let us explain it step by step. Property 2. For (y0 , . . . , y4 ) = SB(x0 , . . . , x4 ), if x3 ⊕ x4 = 1, y3 will be independent to x0 . Proof. We can rewrite y3 as follows: y3 = (x4 ⊕ x3 ⊕ 1)x0 ⊕ (x4 ⊕ x3 ⊕ x2 ⊕ x1 ). Hence, if x3 ⊕ x4 = 1, y3 is irrelevant to x0 . Property 3. Let (S 1 [0], . . . , S 1 [4]) = f (S 0 [0], . . . , S 0 [4]),  
   
  (S 2 [0], . . . , S 2 [4]) = f (S 1 [0], . . . , S 1 [4]),  
   
  A Closer Look at the S-Box  
   
  35  
   
  where (S 0 [1], S 0 [2], S 0 [3], S 0 [4]) are constants and S 0 [0] is the only variable. Then, it is always possible to make u bits of S 2 [1] linear in S 0 [0] by adding at most 9u bit conditions on S 0 [3] ⊕ S 0 [4].  
   
  Fig. 8. Adding conditions on the capacity part to linearize S 2 [1]  
   
  Proof. First, since S 0 [0] is the only variable, according to the deﬁnition of f , we know that (S 1 [0], S 1 [1], S 1 [3], S 1 [4]) are linear in S 0 [0] while S 1 [2] is still constant. Each bit S 2 [1][i] can be expressed as S 2 [1][i] = S 1,s [1][i] ⊕ S 1,s [1][i + 61] ⊕ S 1,s [1][i + 39]. To make S 2 [1][i] linear in S 0 [0], we need to ensure S 1,s [1][i] ⊕ S 1,s [1][i + 61] ⊕ S 1,s [1][i + 39] is linear in S 0 [0]. According to the deﬁnition of the S-box speciﬁed in Eq. 1, the expression of y1 is y1 = x4 ⊕ x1 x3 ⊕ x3 ⊕ x2 (x3 ⊕ x1 ⊕ 1) ⊕ x1 ⊕ x0 . Hence, if x2 is constant, there is only one quadratic term x1 x3 in the expression of y1 . According to the above analysis, S 1 [2] is always constant. Hence, we have S 1,s [1][i] ⊕ S 1,s [1][i + 61] ⊕ S 1,s [1][i + 39] = S 1 [1][i]S 1 [3][i] ⊕ S 1 [1][i + 61]S 1 [3][i + 61] ⊕ S 1 [1][i + 39]S 1 [3][i + 39] ⊕ Li (S 1 [0], . . . , S 1 [4])  
   
  (11)  
   
  where Li is a linear function. Furthermore, according to Property 2, we can make S 0,s [3][i] (0 ≤ i ≤ 63) irrelevant to S 0 [0] by adding 1 bit condition on S 0 [3] ⊕ S 0 [4]. In this way, we  
   
  36  
   
  X. Yu et al.  
   
  can add at most 9 bit conditions on S 0 [3] ⊕ S 0 [4] to make (S 1 [3][i], S 1 [3][i + 61], S 1 [3][i + 39]) irrelevant to S 0 [0] since each bit of S 1 [3] is linear in 3 bits of S 0,s [3]. Once (S 1 [3][i], S 1 [3][i+61], S 1 [3][i+39]) is irrelevant to S 0 [0], S 1,s [1][i]⊕ S 1,s [1][i + 61] ⊕ S 1,s [1][i + 39] becomes linear in S 0 [0] according to Eq. 11. Hence, to make u bits of S 2 [1] linear in S 0 [0], we need to add at most 9u bit conditions on S 0 [3] ⊕ S 0 [4]. A graphical explanation for Property 3 can be seen from Fig. 8. Property 4. Let (S 1 [0], . . . , S 1 [4]) = f (S 0 [0], . . . , S 0 [4]),  
   
  (S 2 [0], . . . , S 2 [4]) = f (S 1 [0], . . . , S 1 [4]),  
   
  where (S 0 [1], S 0 [2], S 0 [3], S 0 [4]) are constants and S 0 [0] is the only variable. Then, it is always possible to make u bits of S 2 [1] linear in S 0 [0] by guessing 3u linear equations in S 0 [0]. Proof. Similar to the proof of Property 3, we have S 1,s [1][i] ⊕ S 1,s [1][i + 61] ⊕ S 1,s [1][i + 39] = S 1 [1][i]S 1 [3][i] ⊕ S 1 [1][i + 61]S 1 [3][i + 61] ⊕ S 1 [1][i + 39]S 1 [3][i + 39] ⊕ Li (S 1 [0], . . . , S 1 [4]) where Li is a linear function and (S 1 [0], S 1 [1], S 1 [2], S 1 [3], S 1 [4]) are linear in S 0 [0]. Hence, if we guess (S 1 [3][i], S 1 [3][i + 61], S 1 [3][i + 39]), S 2 [1][i] will be linear in S 0 [0]. In other words, by guessing 3 linear equations in S 0 [0], S 2 [1][i] can be linear in S 0 [0]. Improving the Attack. Based on the above discussions, it is now possible to further improve the 2-round collision attack. We utilize the general 3-step attack framework where k = 3, i.e., we use message blocks (M1 , M2 , M3 ). From previous analysis, there are 54 linear conditions on the capacity part of S30 and among them, 27 bit conditions are on S30 [1] (or S22 [1]). Based on Property 3 and Property 4, it is possible to satisfy these 54 linear conditions more eﬃciently with advanced algebraic techniques, i.e., we can improve Tk-1 . We emphasize that there are additional 21 quadratic conditions on the capacity part of S30 , but we will not consider them to speed up the exhaustive search over M2 due to the increasing diﬃculty, i.e., it is required to solve degree-4 Boolean equations. Speciﬁcally, based on Property 3, we can add 9u1 conditions on the capacity part of S20 such that u1 bits of S30 [1] can be linear in M2 after the capacity part of S20 is known. Moreover, based on Property 4, after the capacity part of S20 is known, we can guess 3u2 linear equations in M2 such that u2 bits of S30 [1] can be linear in M2 . In total, we set up u1 +4u2 linear equations in 64 variables to satisfy u1 + u2 out of 27 bit conditions. Then, we perform the Gaussian elimination on these u1 + 4u2 linear equations and obtain u3 = 64 − u1 − 4u2  
   
  A Closer Look at the S-Box  
   
  37  
   
  free variables. Note that the ﬁrst round is always freely linearized and the remaining 54 − u1 − u2 linear conditions on S30 can be expressed as quadratic equations in these u3 free variables. In a word, to eﬃciently exhaust M2 such that the 54 conditions on S30 can hold, we can perform the following procedure: Step 1: Guess 3u2 = 42 bits of M2 and construct 4u2 + u1 linear equations. Step 2: Apply the Gaussian elimination to the system and obtain u3 = 64 − u1 − 4u2 free variables. Step 3: Construct 54 − u1 − u2 quadratic equations in these u3 variables and solve the equations. Step 4: Check whether the remaining 21 quadratic conditions on the capacity part of S30 can hold for each obtained solution. We use a similar method in [3,16] to estimate the time complexity to solve a quadratic equation system. After some calculations, the optimal choice of (u1 , u2 , u3 ) is as follows: u1 = 3,  
   
  u2 = 13  
   
  u3 = 9.  
   
  In other words, we need to perform the Gaussian elimination on 55 linear equations in 64 variables for 23u2 = 239 times. Then, we need to solve 38 quadratic equations in 9 variables for 239 times. The total time complexity is estimated as 239 × (552 × 64 + 382 × 45) ≈ 256.6 bit operations. The cost of Step 4 is negligible since it is expected to perform such a check for about 264−54 = 210 times. Time Complexity Evaluation. Based on the previous general 3-step attack framework using 3 message blocks (M1 , M2 , M3 ), we have 9u1 = 27 conditions on S20 and we need 281+75−128 = 228 diﬀerent valid M1 . The cost of this step can be estimated as 228+27 = 255 calls to the 2-round ASCON permutation. Then, for each valid M1 , i.e., each valid S20 , we can exhaust M1 with 256.6 bit operations. At last, for each valid (M1 , M2 ), we can exhaust M3 with about 219 bit operations. Assume that one round of the ASCON permutation takes about 15 × 64 ≈ 210 bit operations, the total time complexity can be estimated as Ttotal = 228 × 227 + 228 × 256.6−11 + 217 × 219−11 ≈ 273.6 calls to the 2-round ASCON permutation. 4.3  
   
  Further Optimizing the Guessing Strategy  
   
  In the above improved 2-round collision attack, we mainly exploit Property 3 and Property 4 to make some conditional bits of S22 [1] linear in M2 . Speciﬁcally, the core problem is to make (S21 [3][i], S21 [3][i + 61], S21 [3][i + 39])  
   
  38  
   
  X. Yu et al.  
   
  constant by either guessing their values according to Property 4 or adding conditions on S20 [3] ⊕ S20 [4] according to Property 3. However, the two strategies are independently used for diﬀerent bits of S22 [1]. It can be noted that for one speciﬁc conditional bit of S22 [1], i.e., S22 [1][i], we can guess g out of 3 bits of (S21 [3][i], S21 [3][i+61], S21 [3][i+39]) and add 3×(3−g) conditions on S20 [3]⊕S20 [4] to achieve the same goal. In other words, for the same conditional bit, we can use a hybrid guessing strategy. As the next step, we aim to optimize the guessing strategy such that we can obtain a suﬃcient number of linear equations by guessing a smaller number of linear equations or adding a smaller number of extra conditions on S20 [3] ⊕ S20 [4]. For example, for the above naive guess strategy, we need to add 9u1 = 27 bit conditions on S20 [3]⊕S20 [4] and we need to further guess 3u2 = 39 linear equations in order to get u1 + 4u2 = 3 + 52 = 55 linear equations in M2 . Can we guess fewer bits to achieve better results? Note that there are 27 conditional bits in S22 [1]. For completeness, we denote the set of i such that S22 [1][i] is conditional by I and we have I = {0, 7, 8, 10, 12, 16, 17, 19, 24, 27, 28, 30, 31, 32, 34, 37, 40, 41, 48, 50, 54, 56, 57, 59, 60, 61, 63}. For each i ∈ I, let Pi = {i, (i + 61)%64, (i + 39)%64}. Further, let Pi = Pi,g ∪ Pi,a ,  
   
  Pi,g ∩ Pi,a = ∅.  
   
  In other words, to linearize S22 [1][i], we guess S21 [3][j0 ] where j0 ∈ Pi,g and make S21 [3][j1 ] constant where j1 ∈ Pi,a by adding 3 conditions on S20 [3][j1 ]⊕S20 [4][j1 ],  
   
  S20 [3][j1 +10]⊕S20 [4][j1 +10],  
   
  S20 [3][j1 +17]⊕S20 [4][j1 +17],  
   
  We can build a simple MILP model to determine the optimal choice of a subset I  ⊆ I and the corresponding Pi,g and Pi,a where i ∈ I  such that the total time complexity of the attack is optimal. Speciﬁcally, assuming that after adding u4 conditions on S20 [3] ⊕ S20 [4] and guessing u5 bits of S21 [3], we can set up u6 linear equations for u6 conditional bits of S22 [1]. In this way, we have in total u5 + u6 linear equations and after the Gaussian elimination, we can set up 54 − u6 quadratic equations in u7 = 64 − u5 − u6 free variables. After some conﬁgurations, we propose to choose u4 = 31,  
   
  u5 = 28,  
   
  u6 = 27  
   
  as the optimal parameters. In other words, we can make all the 27 conditional bits of S22 [1] linear in M2 by guessing 28 linear equations in S21 [3] and adding 31 bit conditions on S20 [3] ⊕ S20 [4]. In this way, we need to perform the Gaussian elimination to u5 + u6 = 55 linear equations in 64 variables that requires about 217.6 bit operations and then solve 27 quadratic equations in u7 = 64 − 55 = 9  
   
  A Closer Look at the S-Box  
   
  39  
   
  variables. Based on the method [3,16] to estimate the time complexity to solve such an overdeﬁned quadratic equation system, it takes about 272 × 45 + 23 × 122 × 6 ≈ 215.3 bit operations. Hence, the new total time complexity is Ttotal = 228 × 231 + 228 × 228 × (217.6 + 215.3 ) × 2−11 + 217 × 219−11 ≈ 262.6 . In conclusion, with the optimal guess strategy and advanced algebraic techniques, we can improve the best collision attack on 2-round ASCON-HASH by a factor of about 240.4 . For completeness, the required 28 guessed bits of S21 [3] and the 31 condition bits of S20 [3] ⊕ S20 [4] are shown in Table 5. Table 5. The optimal guessing strategy  i∈I  
   
  Pi,g  
   
  {0, 3, 4, 7, 8, 10, 14, 15, 17, 21, 24, 25, 27, 28, 31, 32, 34, 35, 37, 38, 41, 45, 48, 51, 54, 55, 58, 61}  i∈I Pi,a {2, 5, 6, 9, 12, 13, 16, 19, 23, 29, 30, 36, 39, 40, 46, 47, 49, 50, 53, 56, 57, 59, 60, 63}  {j, (j + 10)%64, (j + 17)%64 | j ∈ i∈I Pi,a } {0, 2, 3, 5, 6, 9, 10, 12, 13, 15, 16, 19, 22, 23, 26, 29, 30, 33, 36, 39, 40, 46, 47, 49, 50, 53, 56, 57, 59, 60, 63}  
   
  5  
   
  Conclusion  
   
  By carefully studying the relations between the diﬀerence transitions and values transitions through the S-box, we show that the existing collision attacks on 2-round ASCON-HASH can be signiﬁcantly improved with the aid of advanced algebraic techniques. We expect our close look at the algebraic properties of the S-box can inspire more eﬃcient attacks on ASCON-HASH or ASCON-XOF. Acknowledgements. This work is supported by the National Key Research and Development Program of China (No. 2022YFB2701900); the National Natural Science Foundation of China (Nos. 62072181, 62132005); the Shanghai Trusted Industry Internet Software Collaborative Innovation Center; and the “Digital Silk Roa” Shanghai International Joint Lab of Trustworthy Intelligent Software (No. 22510750100).  
   
  A  
   
  The Algorithmic Description of ASCON-HASH  
   
  The Algorithmic Description of ASCON-HASH is shown in Algorithm 1.  
   
  40  
   
  X. Yu et al.  
   
  Algorithm 1: ASCON-HASH Input: M ∈ {0, 1}∗ Output: hash H ∈ {0, 1}256 Initialization: S10 ← f 12 (IV ||0c ); Absorbing: M1 , . . . , Ms ← M ||1||0∗ ; for i = 1, . . . , sdo  0 Si+1 ← f 12 Si0 ⊕ (Mi ||0c ) ; end Squeezing: 0 ; S 0 ← Ss+1 for i = 1, . . . , t = 256/r do Hi ← S 0 [0]; S 0 ← f 12 (S 0 ); end return H1 || . . . ||Ht 256 ;  
   
  B  
   
  On Degree-2 S-Box  
   
  For an n-bit S-box whose algebraic degree is 2, we can show that for any valid pair of input and output diﬀerence, the inputs satisfying this diﬀerence transition must form an aﬃne subspace. Let (x0 , . . . , xn−1 ) ∈ Fn2 and (y0 , . . . , yn−1 ) ∈ Fn2 be the input and output of the S-box. Further, let yi = fi (x0 , . . . , xn−1 ), 0 ≤ i ≤ n − 1, where the algebraic degree of fi is at most 2. Given any valid input diﬀerence (Δx0 , . . . , Δxn−1 ) and output diﬀerence (Δy0 , . . . , Δyn−1 ), we aim to show that (x0 , . . . , xn−1 ) satisfying the following n equations must form an aﬃne subspace: f0 (x0 , . . . , xn−1 ) ⊕ f0 (x0 ⊕ Δx0 , . . . , xn−1 ⊕ Δxn−1 ) = Δy0 , ··· fn−1 (x0 , . . . , xn−1 ) ⊕ fn−1 (x0 ⊕ Δx0 , . . . , xn−1 ⊕ Δxn−1 ) = Δyn−1 . First, since (Δx0 , . . . , Δxn−1 ) → (Δy0 , . . . , Δyn−1 ) is a valid diﬀerence transition, there must exist solutions to the above n equations. We only need to show that all the n equations are indeed linear in (x0 , . . . , xn−1 ) for each given (Δx0 , . . . , Δxn−1 , Δy0 , . . . , Δyn−1 ) and then the proof is over. Note that the algebraic degree of fi is at most 2. In this case, fi (x0 , . . . , xn−1 ) ⊕ fi (x0 ⊕ Δx0 , . . . , xn−1 ⊕ Δxn−1 ) must be linear in (x0 , . . . , xn−1 ), thus completing the proof.  
   
  A Closer Look at the S-Box  
   
  41  
   
  References 1. The CAESAR committee, CAESAR: competition for authenticated encryption: security, applicability, and robustness (2014). https://competitions.cr.yp.to/ caesar-submissions.html 2. Bertoni, G., Daemen, J., Peeters, M., Van Assche, G.: Sponge functions. In: ECRYPT Hash Workshop, no. 9 (2007) 3. Bouillaguet, C., Delaplace, C., Trimoska, M.: A simple deterministic algorithm for systems of quadratic polynomials over F2 . In: Bringmann, K., Chan, T. (eds.) 5th Symposium on Simplicity in Algorithms, SOSA@SODA 2022, Virtual Conference, 10–11 January 2022, pp. 285–296. SIAM (2022). https://doi.org/10.1137/1. 9781611977066.22 4. Bovy, E., Daemen, J., Mennink, B.: Comparison of the second round candidates of the NIST lightweight cryptography competition. Bachelor thesis, Radboud University (2020) 5. Dinur, I., Dunkelman, O., Shamir, A.: New attacks on Keccak-224 and Keccak256. In: Canteaut, A. (ed.) FSE 2012. LNCS, vol. 7549, pp. 442–461. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-34047-5_25 6. Dobraunig, C., Eichlseder, M., Mendel, F., Schläﬀer, M.: ASCON v1. Submission to round 1 of the CAESAR competition (2014). https://competitions.cr.yp.to/ round1/Asconv1.pdf 7. Dobraunig, C., Eichlseder, M., Mendel, F., Schläﬀer, M.: ASCON v1.1. Submission to round 2 of the CAESAR competition (2015). https://competitions.cr.yp.to/ round2/Asconv11.pdf 8. Dobraunig, C., Eichlseder, M., Mendel, F., Schläﬀer, M.: Ascon v1.2. Submission to round 1 of the NIST lightweight cryptography project (2019). https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/ documents/round-1/spec-doc/Ascon-spec.pdf 9. Dobraunig, C., Eichlseder, M., Mendel, F., Schläﬀer, M.: Status update on ASCON v1. 2 (2020) 10. Dobraunig, C., Eichlseder, M., Mendel, F., Schläﬀer, M.: ASCON v1.2: lightweight authenticated encryption and hashing. J. Cryptol. 34(3), 33 (2021). https://doi. org/10.1007/s00145-021-09398-9 11. Floyd, R.W.: Nondeterministic algorithms. J. ACM 14(4), 636–644 (1967). https:// doi.org/10.1145/321420.321422 12. Gérault, D., Peyrin, T., Tan, Q.Q.: Exploring diﬀerential-based distinguishers and forgeries for ASCON. IACR Trans. Symmetric Cryptol. 2021(3), 102–136 (2021). https://doi.org/10.46586/tosc.v2021.i3.102-136 13. Liu, F., Isobe, T., Meier, W.: Automatic veriﬁcation of diﬀerential characteristics: application to reduced GIMLI. In: Micciancio, D., Ristenpart, T. (eds.) CRYPTO 2020, Part III. LNCS, vol. 12172, pp. 219–248. Springer, Cham (2020). https:// doi.org/10.1007/978-3-030-56877-1_8 14. Liu, F., Isobe, T., Meier, W.: Cryptanalysis of full LowMC and LowMC-M with algebraic techniques. In: Malkin, T., Peikert, C. (eds.) CRYPTO 2021, Part III. LNCS, vol. 12827, pp. 368–401. Springer, Cham (2021). https://doi.org/10.1007/ 978-3-030-84252-9_13 15. Liu, F., Isobe, T., Meier, W., Yang, Z.: Algebraic attacks on round-reduced Keccak. In: Baek, J., Ruj, S. (eds.) ACISP 2021. LNCS, vol. 13083, pp. 91–110. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-90567-5_5  
   
  42  
   
  X. Yu et al.  
   
  16. Liu, F., Meier, W., Sarkar, S., Isobe, T.: New low-memory algebraic attacks on LowMC in the picnic setting. IACR Trans. Symmetric Cryptol. 2022(3), 102–122 (2022). https://doi.org/10.46586/tosc.v2022.i3.102-122 17. Liu, F., Sarkar, S., Wang, G., Meier, W., Isobe, T.: Algebraic meet-in-the-middle attack on LowMC. In: Agrawal, S., Lin, D. (eds.) ASIACRYPT 2022, Part I. LNCS, vol. 13791, pp. 225–255. Springer, Cham (2022). https://doi.org/10.1007/978-3031-22963-3_8 18. Qiao, K., Song, L., Liu, M., Guo, J.: New collision attacks on round-reduced Keccak. In: Coron, J.-S., Nielsen, J.B. (eds.) EUROCRYPT 2017, Part III. LNCS, vol. 10212, pp. 216–243. Springer, Cham (2017). https://doi.org/10.1007/978-3-31956617-7_8 19. Qin, L., Zhao, B., Hua, J., Dong, X., Wang, X.: Weak-diﬀusion structure: meet-inthe-middle attacks on sponge-based hashing revisited. Cryptology ePrint Archive, Paper 2023/518 (2023). https://eprint.iacr.org/2023/518 20. Song, L., Liao, G., Guo, J.: Non-full Sbox linearization: applications to collision attacks on round-reduced Keccak. In: Katz, J., Shacham, H. (eds.) CRYPTO 2017, Part II. LNCS, vol. 10402, pp. 428–451. Springer, Cham (2017). https://doi. org/10.1007/978-3-319-63715-0_15 21. Zong, R., Dong, X., Wang, X.: Collision attacks on round-reduced GIMLIHASH/ASCON-XOF/ASCON-HASH. Cryptology ePrint Archive, Paper 2019/1115 (2019). https://eprint.iacr.org/2019/1115  
   
  Improving the Rectangle Attack on GIFT-64 Yincen Chen1 , Nana Zhang2,3 , Xuanyu Liang1 , Ling Song1(B) , Qianqian Yang2 , and Zhuohui Feng1 1 2  
   
  College of Cyber Security, Jinan University, Guangzhou 510632, China [email protected]  , [email protected]  Key Laboratory of Cyberspace Security Defense, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China {zhangnana,yangqianqian}@iie.ac.cn 3 School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China  
   
  Abstract. GIFT is a family of lightweight block ciphers based on SPN structure and composed of two versions named GIFT-64 and GIFT-128. In this paper, we reevaluate the security of GIFT-64 against the rectangle attack under the related-key setting. Investigating the previous rectangle key recovery attack on GIFT-64, we obtain the core idea of improving the attack—trading oﬀ the time complexity of each attack phase. We ﬂexibly guess part of the involved subkey bits to balance the time cost of each phase so that the overall time complexity of the attack is reduced. Moreover, the reused subkey bits are identiﬁed according to the linear key schedule of GIFT-64 and bring additional advantages for our attacks. Furthermore, we incorporate the above ideas and propose a dedicated MILP model for ﬁnding the best rectangle key recovery attack on GIFT64. As a result, we get the improved rectangle attacks on 26-round GIFT64, which are the best attacks on it in terms of time complexity so far. Keywords: symmetric cryptography · GIFT-64 · rectangle attack key recovery attack · related-key scenario · key guessing strategy  
   
  1  
   
  ·  
   
  Introduction  
   
  Accompanied by the momentous expansion in emerging ubiquitous technologies, securing resource-limited devices has become increasingly important. Lightweight block ciphers came into being in such a situation, which have more advantages in terms of cost, speed, power, and execution time than traditional block ciphers, but still provide a suﬃciently high safety margin for resourcelimited devices. Over the past three decades, researchers have committed to researching algorithms to eﬃciently and accurately evaluate the security of block ciphers. In 1990, Biham and Shamir proposed diﬀerential cryptanalysis [5], which tracks the diﬀerence between a pair of inputs to outputs. One of the essential steps of diﬀerential c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 43–61, 2024. https://doi.org/10.1007/978-3-031-53368-6_3  
   
  44  
   
  Y. Chen et al.  
   
  cryptanalysis is to ﬁnd a high-probability diﬀerential trail over the target cipher. However, this goal is hard to achieve when the cipher contains many rounds. The boomerang attack [28] is an extension of diﬀerential cryptanalysis, which combines two short diﬀerential trails to get a long trail with a high probability. The rectangle attack [4] is a variant of the boomerang attack. The boomerang attack requires chosen plaintexts and chosen ciphertexts, while the rectangle attack only needs to choose plaintexts. Besides, the rectangle attack considers as many diﬀerences as possible in the middle to estimate the probability more accurately. The boomerang and rectangle attacks have been applied to many ciphers, and many good results have been obtained. For example, Biryukov et al. [6] put forward the rectangle attack on full AES-192 and AES-256, and Derbez et al. proposed the boomerang attack on full AES-192 in [12]. In recent years, many strategies emerged to mount key recovery attacks as eﬃciently as possible for the rectangle attack, such as [13,24,31]. Song et al. proposed the most eﬃcient and generic rectangle key recovery algorithm at ASIACRYPT 2022 [24]. This algorithm supports ﬂexible key guessing strategies and is compatible with all the previous rectangle key recovery algorithms. By trading oﬀ the overall complexity, Song et al. obtained the optimal results of rectangle key recovery attacks on a series of block ciphers. GIFT is a family of SPN-based lightweight block ciphers proposed by Banik et al. at CHES’17 [3]. It is composed of two versions named GIFT-64 and GIFT128, where the block sizes are 64 bits and 128 bits, and the numbers of rounds are 28 and 40, respectively. The key lengths of GIFT-64 and GIFT-128 are both 128 bits. As the inheritor of PRESENT [7], GIFT mends its weak points and achieves eﬃciency and security improvements. Because of the comprehensive treatment of the linear layer and the S-box, GIFT receives excellent performance in hardware and software implementations and has become one of the most energy-eﬃcient ciphers. Beneﬁting from these advantages, GIFT plays the role of the underlying primitives of many lightweight authenticated encryption schemes, such as GIFT-COFB [2], HyENA [9], LOTUS-AEAD and LOCUSAEAD [8], and SUNDAE-GIFT [1]. Notably, GIFT-COFB is one of the ﬁnal round ﬁnalists of the NIST Lightweight Cryptography standardization project1 . Thus, the security evaluation of GIFT is of great signiﬁcance. Previous Attacks on GIFT-64. GIFT has attracted the attention of many researchers since its publication and has been the subject of many cryptanalyses. The best result of the meet-in-the-middle attack is from [20], which attacks 15round GIFT-64 under the single key scenario. Sun et al. proposed the best linear attack on GIFT-64 at present, which is a linear attack on 19-round GIFT-64 in [26]. The most eﬃcient diﬀerential analysis for GIFT-64 under the related-key scenario is currently the 26-round diﬀerential attack of Sun et al. [25]. Dong et al. proposed the most eﬃcient attack on GIFT-64 for the moment, which is a 26round rectangle key recovery attack we are interested in [13]. We summarize the state-of-the-art attacks against GIFT-64 in Table 1, where RK and SK denote related-key and single-key settings, respectively, and enc. and m.a. represent time complexity in units of encryption and memory access. 1  
   
  https://csrc.nist.gov/projects/lightweight-cryptography.  
   
  Improving the Rectangle Attack on GIFT-64  
   
  45  
   
  This paper focuses on the rectangle key recovery attack against GIFT-64. In 2019, Chen et al. executed the 23-round rectangle key recovery attack on GIFT64 based on a 19-round related-key boomerang distinguisher [10]. Later, Zhao et al. [31] expanded this attack to 24 rounds with a more eﬃcient key-guessing strategy. In 2020, Ji et al. [15] proposed the 20-round related-key boomerang distinguisher. Based on this distinguisher, they also proposed a 25-round related-key rectangle key recovery attack on GIFT-64 using Zhao et al.’s strategy in [15]. At EUROCRYPT 2022, Dong et al. [13] further improved the key guessing strategy and extended the attack of Ji et al. to 26 rounds, resulting in the most eﬀective rectangle key recovery attack of GIFT-64 so far. Table 1. Summary of relevant analysis results of GIFT-64 Method  
   
  Setting Round Time  
   
  Data  
   
  Memory Source  
   
  Integral  
   
  SK  
   
  14  
   
  296.00 enc.  
   
  263.00  
   
  263.00  
   
  [3]  
   
  MITM  
   
  SK  
   
  15  
   
  2112.00 enc.  
   
  264.00  
   
  216.00  
   
  [20]  
   
  62.96  
   
  60.00  
   
  Linear  
   
  127.11  
   
  SK  
   
  19  
   
  2  
   
  Boomerang RK  
   
  23  
   
  2126.60 enc. 123.23  
   
  enc.  
   
  Diﬀerential RK  
   
  26  
   
  2  
   
  Rectangle  
   
  23 24 25 26 26 26 26 26  
   
  2107.00 m.a. 291.58 enc. 2120.92 enc. 2122.78 enc. 2121.75 enc. 2112.07 enc. 2110.06 enc. and 2115.8 m.a. 2111.51 enc. and 2115.78 m.a.  
   
  RK RK RK RK RK RK RK RK  
   
  enc.  
   
  2  
   
  2  
   
  [26]  
   
  263.30  
   
  –  
   
  [18]  
   
  60.96  
   
  102.86  
   
  2  
   
  2  
   
  260.00 260.00 263.78 263.78 262.715 263.79 263.78 263.78  
   
  260.00 260.32 264.10 263.78 262.715 263.79 264.36 267.8  
   
  [25] [10] [31] [15] [13] [17] [30] Sect. 3 Sect. 3  
   
  Our Contributions. We investigate the previous rectangle attacks on GIFT-64 and ﬁnd that the time complexities of diﬀerent attack phases are not balanced. Inspired by the work of Song et al. [24], we study how to ﬁnd a better strategy for the rectangle key recovery attack on GIFT-64 to trade oﬀ the complexity of each attack phase. GIFT has a bit-wise linear layer and a bit-wise key schedule. We carefully study each component of GIFT and, for the ﬁrst time, apply the generic rectangle key recovery algorithm [24] to such ciphers. For ciphers like GIFT, which mostly have bit-wise operations, ﬁnding the best key-guessing strategy is much more sophisticated than for cell-wise ciphers. In the attack on GIFT-64, we carefully analyzed the key schedule and identiﬁed all the reused key bits. To ﬁnd the best attack for a given rectangle distinguisher, we build a MILP model in which all possible key guessing strategies are allowed and minimize the overall time complexity. As a result, we improve the rectangle attacks on 26-round GIFT64 with new key-guessing strategies. Our attacks on GIFT-64 are better than  
   
  46  
   
  Y. Chen et al.  
   
  the previous rectangle key recovery attacks and are the best attacks on GIFT64 in terms of time complexity to date. The comparison of our attacks with previous works is shown in Table 1. Apart from this, we also study the rectangle key recovery attack on GIFT-128 and eventually reduce the complexity of the attack in [15] by a factor of 22 . Limited by the piece space, the full version of this paper is available on eprint2 . It’s worth noting that Yu et al. [30] proposed remarkable cryptanalysis of GIFT-64 concurrently (available on-line on The Computer Journal on 14th July 2023). They constructed an automatic search model which treats the distinguisher and the key recovery phase as a whole for GIFT. Taking the linear key schedule into account, they also discovered a new boomerang distinguisher of GIFT-64. The complexity of their 26-round rectangle key recovery attack on GIFT-64 is (T, D, M ) = (2112.07 enc., 263.79 , 263.79 ). Organization. The rest of the paper is organized as follows. In Sect. 2, we introduce the structure of GIFT-64 and review the rectangle attack and the key recovery algorithm. In Sect. 3, we propose the dedicated MILP model for ﬁnding the best rectangle key recovery attack on GIFT-64 and describe in detail the rectangle key recovery attack based on a new key guessing strategy. Sect. 4. concludes the paper.  
   
  2  
   
  Preliminary  
   
  2.1  
   
  Description of GIFT-64  
   
  GIFT is a block cipher with Substitution-Permutation-Network, which Banik et al. proposed at CHES’ 2017 [3]. According to the 64-bit and 128-bit block sizes, GIFT has two versions, GIFT-64 and GIFT-128, with round numbers 28 and 40, respectively. Both versions of GIFT use a 128-bit master key. r in this subsection represents the number of rounds, where r ∈ {1, 2, ..., 28}. Round Function. The round function of GIFT-64 consists of three operations. For convenience, we consider the 64-bit round state as 16 4-bit nibbles. The three operations of the round function are as follows: 1. SubCells: Nonlinear S-box substitutions are applied to each nibble, as is shown in Table 2. Denote Xr and Yr as the inputs and outputs of the 16 Sboxes in the round r. 2. PermBits: For each bit of input, linear bit permutation bP (i) ← bi , ∀i ∈ {0, 1, ..., 63} is applied. The permutation P (i) is shown in Table 3. Denote the state which is transformed from Yr by PermBits in round r as Zr . 3. AddRoundKey: This step consists of adding the round key and round constants. At each round, a 32-bit round key is obtained from the master key. Denote round key as RKr = U ||V = u15 , ..., u0 ||v15 , ..., v0 . For each round, U and V are XORed with the cipher state, i.e. b4i+1 ← b4i+1 ⊕ ui , b4i ← 2  
   
  https://eprint.iacr.org/2023/1419.  
   
  Improving the Rectangle Attack on GIFT-64  
   
  47  
   
  Table 2. The S-box of GIFT x  
   
  0 1 2 3 4 5 6 7 8 9 a b c d e f  
   
  GS(x) 1 a 4 c 6 f 3 9 2 d b 7 5 0 8 e Table 3. Speciﬁcations of GIFT-64 Bit Permutation i  
   
  0  
   
  P (i) 0 i  
   
  3  
   
  4  
   
  5  
   
  17 34 51 48 1  
   
  6  
   
  7  
   
  8  
   
  9  
   
  10 11 12 13 14 15  
   
  18 35 32 49 2  
   
  19 16 33 50 3  
   
  21 38 55 52 5  
   
  22 39 36 53 6  
   
  23 20 37 54 7  
   
  32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  
   
  P (i) 8 i  
   
  2  
   
  16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  
   
  P (i) 4 i  
   
  1  
   
  25 42 59 56 9  
   
  26 43 40 57 10 27 24 41 58 11  
   
  48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63  
   
  P (i) 12 29 46 63 60 13 30 47 44 61 14 31 28 45 62 15  
   
  b4i ⊕ vi , ∀i ∈ {0, ..., 15}. A single bit"1" and a 6-bit constant C = c5 c4 c3 c2 c1 c0 are added to each state at bit position 63, 23, 19, 15, 11, 7, 3 respectively, i.e. b63 ← b63 ⊕1, b23 ← b23 ⊕c5 , b19 ← b19 ⊕c4 , b15 ← b15 ⊕c3 , b11 ← b11 ⊕c2 , b7 ← b7 ⊕ c1 , b3 ← b3 ⊕ c0 . RKr is added to the state Zr in each round. Key Schedule. Split the master key K into 8 16-bit subkeys k7 ||k6 ||...||k1 ||k0 ← K. For each round, the round key consists of the last two signiﬁcant subkeys, and then, the key state is updated following k7 ||k6 ||...||k1 ||k0 ← k1 ≫ 2||k0 ≫ 12||...||k3 ||k2 , where ≫ i is an i-bit right rotation within a 16-bit word. 2.2  
   
  The Rectangle Attack  
   
  In this subsection, we review the rectangle attack and the generic rectangle key recovery algorithm [24] and explain the notations used in this paper. Before introducing the rectangle attack, we must ﬁrst review the boomerang attack. The boomerang attack was proposed by Wanger [28] in 1999, which is an adaptive chosen plaintext/ciphertext attack. As is illustrated in Fig. 1, it regards the target cipher as a composition of two sub-ciphers E0 and E1 , i.e. E = E0 ◦ E1 . The diﬀerential trail α → β travels in E0 with probability p, and the diﬀerential trail γ → δ travels in E1 with probability q, which composes the boomerang distinguisher with the probability p2 q 2 . In [16], Kelsey et al. developed a chosen-plaintext variant and formed the ampliﬁed boomerang attack with probability p2 q 2 2−n , where n is the size of each block. The rectangle attack [4] improves the ampliﬁed boomerang attack, which estimates the probability more accurately by considering as many diﬀerences as possible inthe middle. The probability of a rectangle distinguisher is 2−n pˆ2 qˆ2 , where pˆ =  
   
  → βi ), Σi Pr2 (α −  
   
  48  
   
  Y. Chen et al.  
   
  Fig. 1. Boomerang distinguisher  
   
   qˆ = Σj Pr2 (γj − → δ). Later, researchers discovered many methods to compute the probability more accurately and proposed an innovative tool named boomerang connectivity table (BCT) [11,23]. The Generic Key Recovery Algorithm. Another line of research on the rectangle attack is to mount key recovery attacks as eﬃciently as possible. The rectangle key recovery algorithm includes four steps: (1) data collection, (2) pair construction, (3) quartet generation and processing, and (4) exhaustive search. In the past few years, eﬀorts have been made to ﬁnd more eﬀective key guessing strategies to improve the eﬃciency of key recovery attacks, like [13,31]. In the generic algorithm of Song et al. [24], which we are inspired by, one can select part of partial key bits involved in extended rounds to guess. Using the generic algorithm, the adversary can balance the complexities of each attack step by guessing the involved key reasonably. The outline of the key recovery algorithm can be proﬁled in Fig. 2. The notations involved in the upcoming work will be described for a better understanding. As shown in Fig. 2, α is the diﬀerential obtained by the propagation of α through Eb−1 , and δ  is the diﬀerential obtained by the propagation of δ through Ef . Note that not all quartets which satisfy the diﬀerence α and δ  are useful to suggest and extract the right key. However, quartets that do not satisfy such conditions are necessarily useless. rb and rf are the number of unknown bits of input diﬀerential and output diﬀerential. kb and kf denote the subkey bits for verifying the diﬀerential propagation in Eb and Ef , respectively, where mb = |kb | and mf = |kf | are the size of kb and kf . In our attack, we guess part of kb and kf , so we denote kb and kf as the bits in kb and kf which have been guessed. Similarly, mb = |kb |, mf = |kf |, and rb , rf are the number of inactive state bits which can be deduced by guessing key bits. Besides, in order  
   
  Improving the Rectangle Attack on GIFT-64  
   
  49  
   
  Fig. 2. Outline of rectangle key recovery attack [24]  
   
  to clearly describe the new attack, we deﬁne rb∗ = rb − rb and m∗b = mb − mb (resp. rf∗ and m∗f ). Related-Key Rectangle Attack. Under the related-key scenario, the rectangle attack and the rectangle key recovery attack diﬀer slightly from those under the single-key setting. Let ΔK and ∇K be the key diﬀerences for E0 and E1 . In the phase of data collection, the adversary needs to access four related-key oracles with K1 , K2 = K1 ⊕ ΔK, K3 = K1 ⊕ ∇K and K4 = K1 ⊕ ΔK ⊕ ∇K to obtain four plaintext-ciphertexts of (P1 , C1 ), (P2 , C2 ), (P3 , C3 ) and (P4 , C4 ) respectively. The remaining steps should be performed under the related-key oracles as well. The Success Probability. From the method of [22], the success probability of the rectangle key recovery attack is calculated according to the Eq. 1, where SN = pˆ2 qˆ2 /2−n is the signal/noise ratio, with an h-bit or higher advantage. s is the expected number of right quartets. √   sSN − Φ−1 1 − 2−h √ (1) Ps = Φ SN + 1 2.3  
   
  The Rectangle Distinguisher of GIFT-64  
   
  Our attack is based on the 20-round related-key boomerang distinguisher of Ji et al. [15]: α = 00 00 00 00 00 00 a0 00, δ = 04 00 00 00 01 20 10 00, with ΔK = 0004 0000 0000 0800 0000 0000 0000 0010 and ∇K = 2000 0000 0000 0000 0800 0000 0200 0800. The probability of the distinguisher above is Pd = 2−58.557 . Li et al. [17] have increased this probability to 2−57.43 by improving the BCT of the distinguisher (but otherwise using the same parameters as Ji et al.). Note that this paper does not discuss the calculation of the distinguisher probability but focuses on the key guessing strategy in the key recovery phase.  
   
  50  
   
  3  
   
  Y. Chen et al.  
   
  New Rectangle Key Recovery Attack on GIFT-64  
   
  This section proposes the new rectangle key recovery attack on GIFT-64. We begin with the basic idea and describe the speciﬁc parameters of the attack. In Subsect. 3.1, the dedicated MILP model and the key guessing strategies will be presented. In Subsect. 3.2, we will describe the exact process of the attack. The complexity of the attacks will be calculated in Subsect. 3.3. As illustrated in Table 4, we utilise the distinguisher of Ji et al. to attack 26-round GIFT-64, where ? denotes the bit with the unknown diﬀerence, 0 and 1 represent the bit with the ﬁxed diﬀerence. Eb spans the ﬁrst three rounds, and 44 unknown bits in Eb distribute over ΔY1 , thus rb =44 . The number of subkey bits involved in the unknown diﬀerence in Z1 and Z2 is 24 and 6, respectively, so mb =30. Ef spans the last three rounds, and there are 64 unknown bits in Ef distributing over Z26 , thus rf =64 . The involved 32 bits, 24 bits, and 8 bits subkeys are added to state Z26 , Z25 , and Z24 respectively, so mf =64 . Hence, the number of subkey bits involved in kb and kf is mb + mf =94 bits. Table 4. The 26-round related-key rectangle attack on GIFT-64. For round r, ΔXr and ΔYr are the input and output diﬀerences of the S-boxes, and ΔZr is the output diﬀerence of the linear layer. r ∈ {1, 2, ..., 26} input ΔY1 ΔZ1  
   
  ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ??0? 1??0 01?? ?0?? 1?0? ?1?0 0??? ?0?? ??0? ???0 0??? ?0?? ??0? ???0 0??? ?0?? ???? ???? ???? ???? 0000 0000 0000 0000 11?? ???? ???? ???? ???? 11?? ???? ????  
   
  ΔX2 ΔY2 ΔZ2  
   
  ???? ???? ???? ???? 0000 0000 0000 0000 11?? ???? ???? ???? ???? 11?? ???? ???? 0?01 00?0 000? ?000 0000 0000 0000 0000 0100 00?0 000? ?000 ?000 0100 00?0 000? ???? 0000 ?1?? 0000 0000 0000 0000 0000 0001 0000 0000 0000 0000 0000 0000 ?1??  
   
  ΔX3 ΔY3 ΔZ3  
   
  ???? 0000 ?1?? 0000 0000 0000 0000 0000 0001 0000 0000 0000 0000 0000 0000 ?1?? 1000 0000 0010 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0010 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0010 1010 0000 0000 0000  
   
  ΔX4 (α) : ΔX24 (δ) ΔY24 ΔZ24  
   
  0000 0000 0000 0000 0000 0000 0000 0000 ... 0000 0100 0000 0000 0000 0000 0000 0000 0000 ???1 0000 0000 0000 0000 0000 0000 00?0 0000 00?? 0?00 0001 0000 ?00? 00?0  
   
  ΔX25 ΔY25 ΔZ25  
   
  00?0 0000 00?? 0?00 0001 0000 ?00? 00?0 ?010 0000 ??00 000? 0?00 0000 0??0 ?000 ???? 0000 ???? ???? ???? 0000 ???? ???? ???? 0000 ???? ???? ???? 0000 ???? ???? ??0? ??0? ??0? ??0? ???0 ???0 ???0 ???0 0??? 0??? 0??? 0??? ?0?? ?0?? ?0?? ?0??  
   
  ΔX26 ΔY26 ΔZ26  
   
  ??0? ??0? ??0? ??0? ???0 ???0 ???0 ???0 0??? 0??? 0??? 0??? ?0?? ?0?? ?0?? ?0?? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ????  
   
  output  
   
  ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ????  
   
  0000 ... 0000 0000 ?000  
   
  0000 0000 0000 1010 0000 0000 0000 0001 0010 0000 0001 0000 0000 0000 ???? ???? 0000 ???? 0000 0000 0000 0000 ??00 000? 0?00 0000 0??0 ?000  
   
  Thanks to the bit-wise linear key schedule of GIFT-64, the subkey bits are reused in certain rounds. For example, if we guess RK1 , we can obtain RK25 and vice versa. This relationship between subkey bits is also present in RK2 and RK26 . As shown in Table 5, we mark the subkey bits involved in kb and  
   
  Improving the Rectangle Attack on GIFT-64  
   
  51  
   
  kf as bold, the subkey bits shared by RK1 and RK25 as green, and the subkey bits shared in RK2 and RK26 as cyan. Note that only the bolded-and-colored subkey bits are reusable. The number of reusable subkey bits is 26 (20 bits between RK1 and RK25 , say k1 |k0 in Table 5. 6 bits between RK2 and RK26 , say k3 |k2 in Table 5). Therefore, the number of subkey bits involved in the attack is 94 − 26 =68 . Table 5. The relation of the involved subkey bits in the key recovery phase ΔZ1  
   
  ????  
   
  ????  
   
  ????  
   
  ????  
   
  0000 0000 0000 0000 11?? ???? ???? ????  
   
  ????  
   
  11??  
   
  4|4  
   
  3|3  
   
  2|2  
   
  1|1  
   
  0000 0000 0000 0000 0001 0000 0000 0000  
   
  0000  
   
  0000  
   
  0000 ?1??  
   
  3|3  
   
  2|2  
   
  ?0??  
   
  ?0??  
   
  k1 |k0 15|15 14|14 13|13 12|12 11|11 10|10 ΔZ2  
   
  ????  
   
  0000  
   
  ?1??  
   
  0000  
   
  k3 |k2 15|15 14|14 13|13 12|12 11|11 10|10  
   
  9|9 9|9  
   
  ...  
   
  8|8 8|8  
   
  7|7 7|7  
   
  6|6 6|6  
   
  5|5 5|5  
   
  4|4  
   
  ???? ????  
   
  1|1  
   
  0|0 0|0  
   
  ... ...  
   
  ΔZ25  
   
  ??0?  
   
  ??0?  
   
  ??0?  
   
  ??0?  
   
  ???0  
   
  ???0 ???0 ???0 0??? 0??? 0??? 0???  
   
  k1 |k0  
   
  11|7  
   
  10|6  
   
  9|5  
   
  8|4  
   
  7|3  
   
  6|2  
   
  5|1  
   
  4|0  
   
  ΔZ26  
   
  ????  
   
  ????  
   
  ????  
   
  ????  
   
  ????  
   
  ???? ???? ???? ???? ???? ???? ????  
   
  k3 |k2  
   
  11|7  
   
  10|6  
   
  9|5  
   
  8|4  
   
  7|3  
   
  6|2  
   
  5|1  
   
  4|0  
   
  ?0?? ?0??  
   
  3|15 2|14 1|13 0|12 15|11 14|10 13|9 12|8 ????  
   
  ????  
   
  ???? ????  
   
  3|15 2|14 1|13 0|12 15|11 14|10 13|9 12|8  
   
  Based on the observation of the previous rectangle key recovery attack instance and the analysis of the GIFT-64 key schedule, we summarize the attack strategies into the following two observations: – The phases of pair construction and quartet generation and processing dominate the time cost of our attacks. In general, there is a certain extent inverse relationship between the time consumption of pair construction and quartet generation and processing. Both of these parts are related to the number of guessed subkey bits and the number of ﬁltering bits produced. Balancing the time complexity of these two parts with appropriate guessing subkey bits will be the core idea of our improvement. – Our key guessing strategies are based on the idea of ﬁnding a method that requires less guessing time complexity but can obtain more conditional bits, i.e. mb + mf ≤ 2(rb + rf ). The reused subkey bits can provide additional ﬁltering bits for some guesses. However, excessive guessing of these subkey bits will make the complexity of pair construction and quartet generation and processing unbalanced. Under the consideration of trading oﬀ the time complexity, guessing as many reused subkey bits as possible will maximize the advantage of our attack. 3.1  
   
  The Dedicated Model and New Key Guessing Strategy  
   
  As a block cipher with the bit-wise key schedule and bit-wise linear layer, the structure of GIFT-64 allows us to directly reap the beneﬁts of guessing each bit. These advantages provide greater ﬂexibility for our attack. Our attack is under the related-key setting, so the ﬁltering process corresponds to the guessed subkeys and their complement.  
   
  52  
   
  Y. Chen et al.  
   
  Since mouha et al.’s seminal paper [19], Mixed Integer Linear Programming (MILP) has been widely used in automated cryptanalysis and has yielded promising results in numerous cryptographic key recovery attacks [14,21,27,29]. In this paper, we present a dedicated model for automated key recovery attack for GIFT-64 based on our new subkey guessing strategy. We start our modeling with a selection of plaintexts and ciphertexts that satisfy the diﬀerence of Z1 and Z26 , respectively. We use binary variables to indicate whether each bit is known and mark all the output bits of an S-box as known if and only if both its input bits and the 2 bits subkey involved are guessed. Guessing the subkey bits allows the propagation of the knownness to obtain the ﬁltering bits. Note that the diﬀerential propagation in Eb is the exact opposite of the diﬀerential propagation in Ef . We then count the ﬁltering bits of the S-box for which both the input and output diﬀerences are known and calculate the time complexity of each attack step. Naturally, we set our objective function to balance the time complexity of each attack step optimally. The parameters we used are listed in Table 6, and the dedicated model is described in Model 1. Attack I: The guessing of the keys involved in the forward and backward extend rounds, shown in Fig. 3, and described as follows. Note that we have marked in red (resp. blue) the keys guessed in the Eb (resp. Ef ) and the ﬁlters that can be used. Next, the guessed key bits and ﬁlters involved in Eb and Ef are explained. ◦ Involved in Eb : Choose the plaintexts that satisfy ΔZ1 . If k0 [0] and k1 [0] are guessed, we can obtain the value of X2 [3 : 0] and ﬁlter out the pairs of plaintexts that do not satisfy the diﬀerence 000? by using the ﬁlter GS(X2 [3 : 0]) ⊕ GS(X2 [3 : 0] ⊕ ΔX2 [3 : 0]) = ΔY2 [3 : 0]. As shown in Fig. 3, there are none bit in ΔX2 [3 : 0] with ﬁxed diﬀerence and 3 bits in ΔY2 [3 : 0] with ﬁxed diﬀerence. When k1 [0] and k0 [0] are guessed, there exist 3 ﬁltering bits (???? → 000?) via the corresponding S-box. We guess 24 bits k0 [15, 14, 13, 12, 7, 6, 5, 4, 3, 2, 1, 0] and k1 [15, 14, 13, 12, 7, 6, 5, 4, 3, 2, 1, 0] to obtain 34 ﬁltering bits in round 2. We guess 6 bits k2 [15, 13, 0] and k3 [15, 13, 0] to obtain 10 ﬁltering bits in round 3. Therefore, we guess 30 subkey bits and obtain 44 filtering bits in Eb , i.e. mb =30 can verify rb =44 , and rb∗ = rb − rb =0 . ◦ Involved in Ef : The ﬁltering bits are obtained in the same way as described in Eb . We guess 16 bits k2 [8, 7, 6, 5, 4, 3, 2, 1] and k3 [14, 12, 11, 10, 9, 8, 7, 5], combined with the guessed subkey bits k2 [15, 13, 0] and k3 [15, 13, 0] which can be reused, this guess provides 11 ﬁltering bits in round 26. We guess 2 bits k0 [10, 9], combined with k0 [14, 13, 6, 5] and k1 [14, 13, 6, 5, 2, 1] that can be reused. This guess provides 17 ﬁltering bits in round 25. Therefore, we guess 18 subkey bits and obtain 28 filtering bits in Ef , i.e. mf = 18 can verify rf = 28 , and rf∗ = rf − rf = 36 .  
   
  Improving the Rectangle Attack on GIFT-64  
   
  53  
   
  Table 6. Parameters of dedicated model Parameter  
   
  Implication  
   
  R0 = {1, 2, 3}  
   
  Extension rounds in Eb  
   
  R1 = {24, 25, 26}  
   
  Extension rounds in Ef  
   
  r ∈ {R0 ||R1 }  
   
  Round number in extension rounds  
   
  m ∈ {0, ..., 16}  
   
  S-box position in each round  
   
  n ∈ {0, ..., 4}  
   
  Bit position in the input (resp. output) of S-box  
   
  s ∈ {0, ..., 4}  
   
  Number of attack step  
   
  Binary KXr,m,n , KYr,m,n Bit diﬀerence is known or not Binary F Xr,m,n , F Yr,m,n  
   
  Bit diﬀerence is ﬁxed or not  
   
  Binary GKr,m  
   
  Subkey bits is guessed or not  
   
  Integer Ar,m  
   
  Number of ﬁltering bits of each ﬁlter  
   
  General Ts  
   
  Time complexity of each attack step  
   
  General T  
   
  Upper bound of Ts  
   
  Algorithm 1. Optimal key guessing strategy searching Require: R0 rounds in Eb and R1 rounds in Ef , the system of inequalities for linear layer and its inverse, the number of S-box in each round m, the size of each S-box n, multi binary variables KXr,m,n , KYr,m,n , F Xr,m,n , F Yr,m,n , GKr,m , multi integer variables Ar,m , multi general variables Ts and T Ensure: system of inequalities /* Initialization */ Constraints: all KX0 = KY26 = 1 Constraints: F Xr,m,n and F Yr,m,n follow the diﬀerential in Eb and Ef /* Counting the number of ﬁltering bits */ 1: for r in R0 do 2: for i = 0 to m do 3: Constraints: all KYr,i,∗ = 1 if and only if all KXr,i,∗ = GKr,i = GKr,i = 1, otherwise all KYr,i,∗ = 0 4: Constraints: Ar,i = KYr,i,∗ × (sum(F Yr,i ) − sum(F Xr,i )) 5: end for 6: Constraints: Linear permutation from KYr,∗,∗ to KXr+1,∗,∗ 7: end for 8: for r in R1 do 9: for i = 0 to m do 10: Constraints: all KXr,i,∗ = 1 if and only if all KYr,i,∗ = GKr,i = GKr,i = 1, otherwise all KXr,i,∗ = 0 11: Constraints: Ar,i = KXr,i,∗ × (sum(F Xr,i ) − sum(F Yr,i )) 12: end for 13: Constraints: Reversed linear permutation from KXr,∗,∗ to KYr−1,∗,∗ 14: end for /* Computing time complexity */ 15: for s in the number of attack steps do 16: Constraints: Ts follow Eq. 6 17: Constraints: T >= Ts 18: end for /* Objective function */ 19: MINIMIZE T  
   
  54  
   
  Y. Chen et al.  
   
  Fig. 3. Guessed key bits and the corresponding propagation relations for Attack I  
   
  3.2  
   
  New Rectangle Attack on GIFT-64  
   
  In this subsection, we conducted the rectangle key recovery attack against GIFT√ 64 using diﬀerent key guessing strategies. For both attacks, we choose y = s · √ n 2 2 −rb / Pd to be the number of structures that should be pre-constructed, where s is the number of quartets expected to be correct.  
   
  Improving the Rectangle Attack on GIFT-64  
   
  55  
   
  The key guessing strategy is detailed in Subsect. 3.1. The complexities will be calculated in Subsect. 3.3. The attack process is as follows: 1. Collect and store y structures of 2rb plaintexts each. Encrypt these structures under four related keys K1 , K2 , K3 , and K4 . We obtain four datasets containing the plaintext-ciphertexts under four related keys, denote as L1 , L2 , L3 , and L4 . Let D = y · 2rb for convenience, so the time, data, and memory complexity of this step is T0 = M0 = Dtotal = 4 · D = y · 2rb +2 . 2. Guess (mb + mf )-bit key, and for each guess: (a) Initialize a list of key counters for the unguessed subkey bits of kb and ∗ ∗ kf , the memory complexity of key counters is Mc = 2mb +mf . (b) For each plaintext-ciphertext collected in step 1, partially encrypt Pi and partially decrypt Ci under the key bits we guessed, i.e. Pi∗ = Enckb (Pi ), Ci∗ = Deckf (Ci ), where i ∈ {1, 2, 3, 4}. The time complexity of this step is 4 · D = y · 2rb +2 partial encryptions. (c) Choose to construct pairs in the input of Eb . The reason is that constructing pairs in the input of Eb can get more ﬁlters for the phase of constructing pairs and balance the time complexity i. Insert L1 into a hash table indexed by the rb inactive bits of P1∗ , so  ∗ there are 2rb items, each of which comprises 2rb pairs (P1∗ , C1∗ ). For L2 , ﬁnd matches in the hash table according to the rb inactive bits of P2∗ . Each match corresponds to a pair {(P1∗ , C1∗ ), (P2∗ , C2∗ )}. There are  r∗  2b  y ·2rb items in total, and there will be 2· diﬀerent combinations 2 for each item. Perform the same operation to L3 and L4 , so we can get two sets S1 = {(P1∗ , C1∗ ), (P2∗ , C2∗ )} and S2 = {(P3∗ , C3∗ ), (P4∗ , C4∗ )}. The size of each is:  r∗  ∗ 2b rb y·2 ·2· (2) = D · 2rb . 2 ∗  
   
  The memory complexity of this step is M1 = 2 · D · 2rb for storing ∗ sets S1 and S2 . We need D · 2rb look-up tables to construct each set. ∗ The time complexity of this step is 2 · D · 2rb memory accesses. ii. Insert S1 into a hash table indexed by the 2rf inactive bits of both   
   
  ∗  
   
  C1∗ and C2∗ . Since each set contains D · 2rb pairs, there are 22rf ∗  items, each of which comprises D · 2rb −2rf pairs {(P1∗ , C1∗ ), (P2∗ , C2∗ )}. For S2 , ﬁnd matches in the hash table according to the 2rf inactive bits of both C3∗ and C4∗ . Each matching provides a quartet {(P1∗ , C1∗ ), (P2∗ , C2∗ ), (P3∗ , C3∗ ), (P4∗ , C4∗ )} for us. For each item, simi ∗   D · 2rb −2rf lar to step 2(c)i, there are 2 · diﬀerent combinations. 2 So the number of quartets we generate is 2rf  
   
  2  
   
    
   
  ∗  
   
    
   
  D · 2rb −2rf ·2· 2  
   
    
   
  ∗  
   
  ∗  
   
  = D2 · 22rb +2rf −2rf .  
   
  (3)  
   
  56  
   
  Y. Chen et al. ∗  
   
  ∗  
   
  The time complexity of this step is D2 ·22rb +2rf −2rf memory accesses. (d) Utilize the quartets we obtain above to determine the key candidates involved in Eb and Ef and increase the corresponding key counters. Denote the time cost of processing each quartet as . The time com∗ ∗ plexity of this step is D2 · 22rb +2rf −2n · . We will discuss the estimation of in detail in Subsect. 3.3. ∗ ∗ (e) Select the top 2mb +mf −h hits in the counters to be the key candidates, which delivers a h-bit or higher advantage, where 0 < h ≤ m∗b + m∗f . (f) Guess the remaining k − mb − mf bits key according to the key schedule and exhaustively search over them to recover the correct key, where k is   the key size of GIFT-64. The time complexity of this step is 2k−mb −mf −h encryptions. 3.3  
   
  Complexity Analysis  
   
  We will introduce two diﬀerent methods to process quartets and extract key information, i.e. calculation of , which will bring diﬀerent time complexity to our attacks. We apply computed using the method of the pre-constructed hash table into the MILP model and compute the time complexity of Attack I accordingly. We also bring computed using the method of the guess and ﬁlter to the MILP model and compute the time complexity of Attack II accordingly. We mount this approach to our two attacks and take Attack I as an example to introduce it in detail. The complexity analysis of Attack II is calculated in detail in the eprint version. Guess and Filter: Suppose we obtain Nq quartets after step 2d in the ﬁrst attack. Guessing k2 [14, 12] and k3 [6, 4], we can use ﬁlters GS(Y26 [15 : 12]) ⊕ GS(Y26 [15 : 12] ⊕ ΔY26 [15 : 12] =?0?? and GS(Y26 [47 : 44]) ⊕ GS(Y26 [47 : 44] ⊕ ΔY26 [47 : 44] =?0??. Furthermore, combined with the subkey bits k0 [15, 7, 3, 1] and k1 [15, 7, 3], which have been guessed before, we can use ﬁlters GS(Y25 [63 : 60]) ⊕ GS(Y25 [63 : 60] ⊕ ΔY25 [63 : 60] = 00?0 and GS(Y25 [55 : 48]) ⊕ GS(Y25 [55 : 48] ⊕ ΔY25 [63 : 48] = 00??, 0?00. There are 10 ﬁltering bits for pairs, so 20 ﬁltering bits for quartets. Therefore, there are 24 · Nq · 2−20 = Nq · 2−16 quartets remain, and the time cost is 24 · Nq S-box accesses. Note that after this step, the number of quartets remaining is much lower than before. The time consumption of the remaining steps will be far less than Nq · 24 S-box accesses. Therefore, ≈ 24 /26 ≈ 2−0.7 encryption. Pre-construct Hash Tables: As shown in Table 7, the time complexity of step 2d can be reduced by accessing the hash table instead of traversing subkeys. We can obtain the bits deduced according to the inputs of the ﬁlters, i.e. starting bits and subkey bits. Utilize these bits to match pairs in the quartets. Each match can provide the corresponding subkey information. The time and memory cost of each subtable is determined by the underlined bits, and the ﬁltering eﬀect indicates the number of candidate subkeys obtained for each subtable access. Note that these hash tables in our paper are all built for quartets but can also be built for pairs in memory-limited cases.  
   
  Improving the Rectangle Attack on GIFT-64  
   
  57  
   
  Table 7. Precomputation tables for the 26-round attack on GIFT-64 No Starting bits  
   
  Subkey bits  
   
  Bits deduced  
   
  248  
   
  2−16  
   
    k0 [8] Y26 [51 : 48] X26 [49] ⊕ X26 [49] = 0   k1 [8] [35 : 32] [32] = 0 Y26 X26 [32] ⊕ X26   k2 [14, 10, 9] Y26 [19 : 16] [19] = 0 X26 [19] ⊕ X26   k3 [6, 2, 1] X26 [51 : 48] X26 [51 : 48] [15 : 12] X25 [15 : 12] ⊕ X25  X26 [35 : 32] X26 [35 : 32] = 0?00   [19 : 16] [7 : 0] X26 [19 : 16] X26 X25 [7 : 0] ⊕ X25  [15 : 12] X25 [15 : 12] X25 = 0??0, ?000  [7 : 0] X25 [7 : 0] X25 i Y26 [51 : 48], Y26 [35 : 32], Y26 [19 : 16], X26 [3 : 1], i = 1, 2, 3, 4 : k0 [8], k1 [8], k2 [14, 10, 9], k3 [6, 2, 1]  
   
  250  
   
  2−14  
   
  Y26 [47 : 44] k2 [14, 12] Y26 [15 : 12] k3 [6, 4] X26 [63 : 60] X26 [31 : 28]  
   
  2  
   
  Y26 [51 : 48] Y26 [35 : 32] Y26 [19 : 16] X26 [3 : 0]  
   
   Y26 [15 : 12]  X26 [47 : 44]  X26 [15 : 12]  X25 [63 : 60]  X25 [55 : 48]  
   
  T & M Filter eﬀect  
   
   X26 [14] ⊕ X26 [14] = 0  [44] = 0 X26 [44] ⊕ X26  [63 : 60] X25 [63 : 60] ⊕ X25 = 00?0  [55 : 48] X25 [55 : 48] ⊕ X25 = 00??, 0?00 i i Y26 [47 : 44], Y26 [15 : 12], X26 [63 : 60], X26 [31 : 28], i = 1, 2, 3, 4 : k2 [14, 12], k3 [6, 4]  
   
  1  
   
   Y26 [47 : 44] X26 [47 : 44] X26 [15 : 12] X25 [63 : 60] X25 [55 : 48]  
   
  Filter condition  
   
  As shown in Fig. 3, these uncoloured cells of ΔRKi denote the subkey bits not used in previous steps, where i = 24, 25, 26. We should access subtable No.1 in Table 7 ﬁrst. Combining state Y26 [47 : 44], Y26 [15 : 12], X26 [63 : 60], X26 [31 : 28]  [47 : 44], with subkey bits k2 [14, 12], k3 [6, 4], we can deduce the corresponding Y26    Y26 [15 : 12], X26 [47 : 44], X26 [47 : 44], X26 [15 : 12], X26 [15 : 12], X25 [63 :   [63 : 60], X25 [55 : 48], X25 [55 : 48]. We can obtain ﬁltering bits from 60], X25    [44] = 0, X25 [63 : 60] ⊕ X25 [63 : 60] = 00?0, X26 [14] ⊕ X26 [14] = 0, X26 [44] ⊕ X26  X25 [55 : 48] ⊕ X25 [55 : 48] = 00??, 0?00. Speciﬁcally, for a pair of plaintext, there are 10 ﬁltering bits from ΔY26 [47 : 44] =???? → ΔX26 [47 : 44] =???0, ΔY26 [15 : 12] =???? → ΔX26 [15 : 12] =?0??, ΔY25 [63 : 60] =???? → ΔX25 [63 : 60] = 00?0, ΔY25 [31 : 28] =????, ???? → ΔX25 [31 : 28] = 00??, 0?00. Therefore, we can get 20 ﬁltering bits for a quartet using these ﬁlters. According to these ﬁlters, we can extract information of k2 [14, 12], k3 [6, 4] and discard quartets that suggest nothing. We need to store subkey bits k2 [14, 12], k3 [6, 4] into a hash i i i i [47 : 44], Y26 [15 : 12], X26 [63 : 60], X26 [31 : 28] in table indexed by 64-bit Y26 −16 16 total, where i = 1, 2, 3, 4. The ﬁlter eﬀect is 2 , which means 2 quartets will be ﬁltered out, so the time and memory cost to construct such a subtable is 264−16 = 248 . For now, the number of quartets is much lower than before. To process the rest of the quartets and increase key counters, we need to access the remaining subtable No.2. The time cost of the following steps will be far less than that of accessing subtable No.1. Finally, we get ≈ 1 memory access. The Complexity of Attack I: Considering the total complexity and success √ √ n rate, we choose s = 2 and h = 20, so we need construct y = s · 2 2 −rb / Pd = 217.78 structures in step 1 and D = y · 2rb = 261.78 .  
   
  58  
   
  Y. Chen et al.  
   
  • Data complexity: The plaintexts in step 1 are the total data we need to collect in this attack. Dtotal = 4 · D = 263.78  
   
  (4)  
   
  • Memory complexity: We need store data collected in step 1, key counters described in step 2a, and datasets S1 and S2 generated in step 2(c)i. Mtotal = M0 + M1 + Mc ∗  
   
  ∗  
   
  ∗  
   
  = 4 · D + 2 · D · 2rb + 2mb +mf  
   
  (5)  
   
  = 263.78 + 262.78 + 236 ≈ 264.36  
   
  • Time complexity: The time complexity consists of ﬁve parts: (1) data collection in step 1 denote as T0 , (2) partial encryption and partial decryption in step 2b denote as T1 , (3) pairs generation in step 2(c)i denote as T2 , (4) quartets generation and processing in step 2d denote as T3 and (5) the exhaustive search of the remaining key in step 2f denote as T4 . Recall that our key guessing strategy provides the following parameters: mb = 30, rb = rb = 44, rb∗ = 0 and mf = 18, rf = 28, rf∗ = rf − rf = 36. For the last four parts, we need to consider all guessing of (mb + mf ) = 48 subkey bits. T0 = 4 · D = 263.78   
   
    
   
    
   
    
   
  T1 = 2mb +mf · 4 · D = 2111.78 ∗  
   
  T2 = 2mb +mf · 2 · D · 2rb = 2110.78 mb +mf  
   
  T3 = 2  
   
  2rb∗ +2rf∗ −2n  
   
  ·D ·2 2  
   
  mb +mf +k−mb −mf −h  
   
  T4 = 2  
   
  (6)  
   
  · =2  
   
  115.56  
   
  ·  
   
  = 2k−h = 2108  
   
  We obtain ≈ 2−0.7 encryption with the method of the guess and ﬁlter. The 6 · 2111.78 + 2115.56−0.7 + 2108 ≈ time complexity of Attack I is T1 + T3 + T4 = 26 114.86 110.78 encryption and T2 = 2 memory access. 2 We obtain ≈ 1 memory access with the method of the pre-constructed 6 · 2111.78 + 2108 ≈ hash table. The time complexity of Attack I is T1 + T4 = 26 111.78−2.11 108 110.06 +2 ≈ 2 26-round encryptions and T2 + T3 = 2110.78 + 2 115.56 115.80 ≈ 2 memory accesses. 2 Attack II: The key guessing strategy of this attack is mb =26 , rb =39 , rb∗ = rb −rb =5 and mf =22 , rf =34 , rf∗ = rf −rf =30 . We choose s = 2 and h = 20, so D = 261.78 . The data complexity is Dtotal = 263.78 , memory complexity is  = 267.8 . We obtain that ≈ 2−2.7 encryptions with the method of the Mtotal guess and ﬁlter, so time complexity is 2111.51 26-round encryptions and 2115.78 memory accesses. We obtain that ≈ 1 memory access with the method of the pre-construct hash table. Thence, the time complexity is 2110.06 26-round encryptions and 2116.06 memory accesses. The detail of Attack II can be found in the eprint version.  
   
  Improving the Rectangle Attack on GIFT-64  
   
  59  
   
  In summary, we obtained that the best current rectangle key recovery attack on GIFT-64 is Attack I, which pre-constructs hash tables to process quartets. The attack complexity is: data complexity is Dtotal = 263.78 , memory complexity is Mtotal = 264.36 , time complexity is Ttotal = 2110.06 26-round encryption and 2115.80 memory accesses. Further, calculating using the probability ∗ = 263.22 , which improved in [17], the overall complexity of the attack is: Dtotal ∗ 63.8 ∗ 109.5 115.24 26-round encryption and 2 memory accesses. Mtotal = 2 , Ttotal = 2 According to Eq. 1, the success rate of Attack I, II are both around 75%. Reevaluate the Security of GIFT-128. We also re-evaluate the ability of GIFT-128 to resist rectangle key recovery attack. In [15], Ji et al. proposed the rectangle key recovery attack on 23-round GIFT-128. They guessed all the subkey bits involved in Eb , non-guessed any subkey bits in Ef . For improvement, we guess 2 fewer subkey bits in Eb than [15] and roughly reduce its total time complexity by a factor of 22 ,i.e. 2123.1 encryptions, and T2 = 2126.31 memory accesses. The data and memory complexity is the same as [15]. The details are given in eprint version.  
   
  4  
   
  Discussion and Conclusion  
   
  We propose new key guessing strategies to improve the attack of 26-round GIFT64. The MILP model of the best key guessing strategy for GIFT-64 rectangle key recovery attack is constructed, and thus the optimal key guessing strategy for our attack scenario is obtained by searching and comparing. Our attacks against GIFT-64 are the best in terms of time complexity so far. For GIFT-64, its bit-wise linear permutation gives us great ﬂexibility, and the reused subkey bits derived by the bit-wise key schedule provide additional ﬁlter bits for each guessing. Our attack starts from the bit-wise operations of GIFT-64 and considers the more accurate selection of the subkey bits involved in the attack so as to guess the key more eﬀectively. This idea also provides a new possibility for better attacks on block ciphers structured by bit-wise operations. We have observed that the designer of GIFT had not claimed the relatedkey security. For the community of symmetric cryptography, however, we believe that cryptographic security analysis under related-key settings is indispensable. Acknowledgements. We would like to thank the anonymous reviewers for their helpful comments and suggestions. This paper is supported by the National Key Research and Development Program (No. 2018YFA0704704, No.2022YFB2701900, No.2022YFB2703003) and the National Natural Science Foundation of China (Grants 62022036, 62132008, 62202460, 62172410, 62372213).  
   
  References 1. Banik, S., et al.: SUNDAE-GIFT. In: Submission to the NIST Lightweight Cryptography Project (2019) 2. Banik, S., et al.: GIFT-COFB. In: Cryptology ePrint Archive (2020)  
   
  60  
   
  Y. Chen et al.  
   
  3. Banik, S., Pandey, S.K., Peyrin, T., Sasaki, Yu., Sim, S.M., Todo, Y.: GIFT: a small present - towards reaching the limit of lightweight encryption. In: Fischer, W., Homma, N. (eds.) CHES 2017. LNCS, vol. 10529, pp. 321–345. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66787-4_16 4. Biham, E., Dunkelman, O., Keller, N.: The rectangle attack—rectangling the serpent. In: Pﬁtzmann, B. (ed.) EUROCRYPT 2001. LNCS, vol. 2045, pp. 340–357. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-44987-6_21 5. Biham, E., Shamir, A.: Diﬀerential cryptanalysis of DES-like cryptosystems. J. Cryptol. 4(1), 3–72 (1991). https://doi.org/10.1007/BF00630563 6. Biryukov, A., Khovratovich, D.: Related-key cryptanalysis of the full AES-192 and AES-256. In: Matsui, M. (ed.) ASIACRYPT 2009. LNCS, vol. 5912, pp. 1–18. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-10366-7_1 7. Bogdanov, A., et al.: PRESENT: an ultra-lightweight block cipher. In: Paillier, P., Verbauwhede, I. (eds.) CHES 2007. LNCS, vol. 4727, pp. 450–466. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-74735-2_31 8. Chakraborti, A., Datta, N., Jha, A., Lopez, C.M., Nandi, M., Sasaki, Y.: LOTUSAEAD and LOCUS-AEAD. In: Submission to the NIST Lightweight Cryptography project (2019) 9. Chakraborti, A., Datta, N., Jha, A., Nandi, M.: HYENA. In: Submission to the NIST Lightweight Cryptography project (2019) 10. Chen, L., Wang, G., Zhang, G.: MILP-based related-key rectangle attack and its application to GIFT, Khudra, MIBS. Comput. J. 62(12), 1805–1821 (2019). https://doi.org/10.1093/comjnl/bxz076 11. Cid, C., Huang, T., Peyrin, T., Sasaki, Yu., Song, L.: Boomerang connectivity table: a new cryptanalysis tool. In: Nielsen, J.B., Rijmen, V. (eds.) EUROCRYPT 2018, Part II. LNCS, vol. 10821, pp. 683–714. Springer, Cham (2018). https://doi. org/10.1007/978-3-319-78375-8_22 12. Derbez, P., Euler, M., Fouque, P., Nguyen, P.H.: Revisiting related-key boomerang attacks on AES using computer-aided tool. In: Agrawal, S., Lin, D. (eds.) ASIACRYPT 2022, Part III. LNCS, vol. 13793, pp. 68–88. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-22969-5_3 13. Dong, X., Qin, L., Sun, S., Wang, X.: Key guessing strategies for linear keyschedule algorithms in rectangle attacks. In: Dunkelman, O., Dziembowski, S. (eds.) EUROCRYPT 2022, Part III. LNCS, vol. 13277, pp. 3–33. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-07082-2_1 14. Fu, K., Wang, M., Guo, Y., Sun, S., Hu, L.: MILP-based automatic search algorithms for diﬀerential and linear trails for speck. In: Peyrin, T. (ed.) FSE 2016. LNCS, vol. 9783, pp. 268–288. Springer, Heidelberg (2016). https://doi.org/10. 1007/978-3-662-52993-5_14 15. Ji, F., Zhang, W., Zhou, C., Ding, T.: Improved (related-key) diﬀerential cryptanalysis on GIFT. In: Dunkelman, O., Jacobson Jr., M.J., O’Flynn, C. (eds.) SAC 2020. LNCS, vol. 12804, pp. 198–228. Springer, Cham (2021). https://doi.org/10. 1007/978-3-030-81652-0_8 16. Kelsey, J., Kohno, T., Schneier, B.: Ampliﬁed boomerang attacks against reducedround MARS and serpent. In: Goos, G., Hartmanis, J., van Leeuwen, J., Schneier, B. (eds.) FSE 2000. LNCS, vol. 1978, pp. 75–93. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-44706-7_6 17. Li, C., Wu, B., Lin, D.: Generalized boomerang connectivity table and improved cryptanalysis of gift. In: Deng, Y., Yung, M. (eds.) Inscrypt 2022. LNCS, vol. 13837, pp. 213–233. Springer, Cham (2023). https://doi.org/10.1007/978-3-03126553-2_11  
   
  Improving the Rectangle Attack on GIFT-64  
   
  61  
   
  18. Liu, Y., Sasaki, Y.: Related-key boomerang attacks on GIFT with automated trail search including BCT eﬀect. In: Jang-Jaccard, J., Guo, F. (eds.) ACISP 2019. LNCS, vol. 11547, pp. 555–572. Springer, Cham (2019). https://doi.org/10.1007/ 978-3-030-21548-4_30 19. Mouha, N., Wang, Q., Gu, D., Preneel, B.: Diﬀerential and linear cryptanalysis using mixed-integer linear programming. In: Wu, C.-K., Yung, M., Lin, D. (eds.) Inscrypt 2011. LNCS, vol. 7537, pp. 57–76. Springer, Heidelberg (2012). https:// doi.org/10.1007/978-3-642-34704-7_5 20. Sasaki, Yu.: Integer linear programming for three-subset meet-in-the-middle attacks: application to GIFT. In: Inomata, A., Yasuda, K. (eds.) IWSEC 2018. LNCS, vol. 11049, pp. 227–243. Springer, Cham (2018). https://doi.org/10.1007/ 978-3-319-97916-8_15 21. Sasaki, Yu., Todo, Y.: New impossible diﬀerential search tool from design and cryptanalysis aspects. In: Coron, J.-S., Nielsen, J.B. (eds.) EUROCRYPT 2017, Part III. LNCS, vol. 10212, pp. 185–215. Springer, Cham (2017). https://doi.org/ 10.1007/978-3-319-56617-7_7 22. Selçuk, A.A.: On probability of success in linear and diﬀerential cryptanalysis. J. Cryptol. 21(1), 131–147 (2008). https://doi.org/10.1007/s00145-007-9013-7 23. Song, L., Qin, X., Hu, L.: Boomerang connectivity table revisited. Application to SKINNY and AES. IACR Trans. Symmetric Cryptol. 2019(1), 118–141 (2019). https://doi.org/10.13154/tosc.v2019.i1.118-141 24. Song, L., et al.: Optimizing rectangle attacks: a uniﬁed and generic framework for key recovery. In: ASIACRYPT 2022, Part I. LNCS, vol. 13791, pp. 410–440. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-22963-3_14 25. Sun, L., Wang, W., Wang, M.: Accelerating the search of diﬀerential and linear characteristics with the SAT method. IACR Trans. Symmetric Cryptol. 2021(1), 269–315 (2021). https://doi.org/10.46586/tosc.v2021.i1.269-315 26. Sun, L., Wang, W., Wang, M.: Improved attacks on GIFT-64. In: AlTawy, R., Hülsing, A. (eds.) SAC 2021. LNCS, vol. 13203, pp. 246–265. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-99277-4_12 27. Sun, S., Hu, L., Wang, P., Qiao, K., Ma, X., Song, L.: Automatic security evaluation and (related-key) diﬀerential characteristic search: application to SIMON, PRESENT, LBlock, DES(L) and other bit-oriented block ciphers. In: Sarkar, P., Iwata, T. (eds.) ASIACRYPT 2014, Part I. LNCS, vol. 8873, pp. 158–178. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-45611-8_9 28. Wagner, D.: The boomerang attack. In: Knudsen, L. (ed.) FSE 1999. LNCS, vol. 1636, pp. 156–170. Springer, Heidelberg (1999). https://doi.org/10.1007/3-54048519-8_12 29. Xiang, Z., Zhang, W., Bao, Z., Lin, D.: Applying MILP method to searching integral distinguishers based on division property for 6 lightweight block ciphers. In: Cheon, J.H., Takagi, T. (eds.) ASIACRYPT 2016. LNCS, vol. 10031, pp. 648–678. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-53887-6_24 30. Yu, Q., Qin, L., Dong, X., Jia, K.: Improved related-key rectangle attacks On GIFT. Comput. J. bxad071 (2023). https://doi.org/10.1093/comjnl/bxad071 31. Zhao, B., Dong, X., Meier, W., Jia, K., Wang, G.: Generalized related-key rectangle attacks on block ciphers with linear key schedule: applications to SKINNY and GIFT. Des. Codes Cryptogr. 88(6), 1103–1126 (2020). https://doi.org/10.1007/ s10623-020-00730-1  
   
  Side-Channel Attacks and Countermeasures  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices Markku-Juhani O. Saarinen1(B) and M´elissa Rossi2 1  
   
  PQShield Ltd., Oxford, UK [email protected]  2 ANSSI, Paris, France [email protected]   
   
  Abstract. Masking is a well-studied method for achieving provable security against side-channel attacks. In masking, each sensitive variable is split into d randomized shares, and computations are performed with those shares. In addition to the computational overhead of masked arithmetic, masking also has a storage cost, increasing the requirements for working memory and secret key storage proportionally with d. In this work, we introduce mask compression. This conceptually simple technique is based on standard, non-masked symmetric cryptography. Mask compression allows an implementation to dynamically replace individual shares of large arithmetic objects (such as polynomial rings) with κ-bit cryptographic seeds (or temporary keys) when they are not in computational use. Since κ does not need to be larger than the security parameter (e.g., κ = 256 bits) and each polynomial share may be several kilobytes in size, this radically reduces the memory requirement of highorder masking. Overall provable security properties can be maintained using appropriate gadgets to manage the compressed shares. We describe gadgets with Non-Interference (NI) and composable Strong-Non Interference (SNI) security arguments. Mask compression can be applied in various settings, including symmetric cryptography, code-based cryptography, and lattice-based cryptography. It is especially useful for cryptographic primitives that allow quasilinear-complexity masking and are practically capable of very high masking orders. We illustrate this with a d = 32 (Order-31) implementation of the recently introduced lattice-based signature scheme Raccoon on an FPGA platform with limited memory resources. Keywords: Side-Channel Security · Mask Compression Signature Scheme · Post-Quantum Cryptography  
   
  1  
   
  · Raccoon  
   
  Introduction  
   
  Physical side-channel attacks exploit sensitive information leaked by a cryptography system via externally observable characteristics such as Timing [20], Power consumption (SPA/DPA) [21,22], and Electromagnetic emissions [30]. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 65–81, 2024. https://doi.org/10.1007/978-3-031-53368-6_4  
   
  66  
   
  M.-J. O. Saarinen and M. Rossi  
   
  Currently, NIST and the cryptographic community are engaged in a widereaching transition eﬀort to use Post-Quantum Cryptography (PQC) algorithms such as Kyber [2] (a lattice-based key establishment scheme) and Dilithium [4] (a lattice-based signature scheme) to replace older quantum-vulnerable RSA and Elliptic Curve based cryptography [1,26]. In many prominent use cases, this transition requires physical side-channel security from PQC implementations: Authentication tokens, Mobile/IoT device platform security (secure boot, ﬁrmware update, attestation), smart cards, and other secure elements. Masking. Masking is a general technique to attain side-channel security by splitting sensitive variables into d randomized shares, where t = d − 1 is the masking order. Each share individually appears uniformly random, and all d shares are required to determine their sum, which is the actual masked quantity. We write x to denote a masked representation of x. The relationship may be either an exclusive-or operation (“Boolean masking”) or modular (“Arithmetic masking”): Boolean masking: x = x0 ⊕ x1 ⊕ . . . ⊕ xt Arithmetic masking: x = x0 + x1 + . . . + xd−1 (mod q).  
   
  (1) (2)  
   
  PQC algorithm side-channel countermeasures are primarily based on masking. For example, see [6,14] for details about masking Kyber, and [3,24] for Dilithium. High-order computation on the shares is relatively complex in the case of these two algorithms, requiring both Boolean and Arithmetic masking. Complexity of Attack and Defence. In addition to practicality, one main advantage of masking over more ad-hoc approaches is that it allows one to prove sidechannel security properties of implementations. In pioneering work, Chari et al. [7] showed that in the presence of Gaussian noise, the number of side-channel observations required to determine x from individual bits grows exponentially with the number of shares d. The understanding of this exponential relationship has since been made more precise both theoretically and in practice [13,18,23]. In [15], Ishai et al. introduced the probing model: the notion of t-probing security states that the joint distribution of any set of at most t internal intermediate values should be independent of any of the secrets. Thus, a circuit is t-probing secure iﬀ it is secure against observations of t = d−1 wires. Reductions from the probing model to the noisy leakage model [12,29] exist and allow to link t-probing security with realistic leakage models. In addition, [15] showed that any circuit can be transformed into a t-probing secure circuit of size O(nt2 ). It has since been demonstrated that quasilinear O(t log t) masking complexity can be achieved for some primitives, including the Lattice-based signature scheme Raccoon [27,28]. Structure of this Paper and Our Contributions. The mask compression technique is introduced in Sect. 2, which also discusses how it can be applied in practice. Further security discussion is given in Sect. 3, including requirements for composability (strong non-interference). Section 4 gives a practical example of a very  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  67  
   
  high-order PQC scheme (Raccoon [28] with d = 32 shares) implemented with Mask Compression on FPGA with 128kB of physical SRAM, instead of several megabytes that would be required without it.  
   
  2  
   
  Mask Compression  
   
  Mask compression in a group G (Eqs. 1 or 2) requires a symmetric cryptography primitive SampleG (z) that maps short binary keys z to elements in G. The function is used to manipulate sensitive variables, but thanks to the way it is used, SampleG (z) itself does not need to be masked. Its input and output variables are generally ephemeral (single-use) individual shares. Definition 1. (Informal). The function x ← SampleG (z) uses the input seed z ∈ {0, 1}κ to sample a pseudorandom element x ∈ G deterministically. We assume that SampleG is cryptographically secure under a suitable deﬁnition. For a technical discussion of pseudorandomness, see [19, Section 3] (the deﬁnitions oﬀered for binary strings can be easily extended to other uniform distributions.) Intuitively, we assume that the task of distinguishing x from a uniformly random element in set G is computationally hard. Typically key size κ is selected to match the overall security level of the system. In this case, distinguishing x should not be substantially easier than an exhaustive search for z. Practical Instantiation. We can implement SampleG (z) with an extendable output function (XOF) such as SHAKE [25]1 The function can also be instantiated with a stream cipher or a block cipher (in counter mode). If a mapping from XOF output to non-binary uniform distributions is required, one may use rejection sampling since each XOF(z) deﬁnes an arbitrarily long bit sequence. Examples of sampled |G|  2κ include large-degree polynomials that are ring elements Zq [x]/(xn + 1) in Kyber [2] and Dilithium [4]. Note that implementations Kyber and Dilithium already have subroutines that generate uniform polynomial coeﬃcients in Z/qZ from XOF output via rejection sampling. In common lattice algorithms, an eﬃcient (unmasked) method for this task is required to create polynomials for A generator matrix on the ﬂy. This is the reason why a PQC hardware implementation (such as the one discussed in Sect. 4) will often have an eﬃcient instance of SampleG (z) available. Definition 2 (Compressed Mask Set). A compressed mask set consists of a t tuple xz = (x0 , z1 , · · · , zt ) satisfying x ≡ x0 + i=1 SampleG (zi ) with x0 ∈ G and zi ∈ {0, 1}κ for i ∈ [1, t]. Theorem 1. It is computationally infeasible to determine information about x from any subset of t = d − 1 elements in compressed masking d-tuple xz . 1  
   
  FIPS 202 presents a SHAKE speciﬁcally as an extensible output function (XOF), which is deﬁned as a hash function with arbitrary-length input and output.  
   
  68  
   
  M.-J. O. Saarinen and M. Rossi  
   
  Proof. If x0 is not known, x can be any value. If one of zi is unavailable, the indistinguishability property of SampleG (zi ) makes x similarly indistinguishable (Fig. 1).  
   
  Fig. 1. Illustrating ﬁrst-order (t = 1, d = 2) mask compression. Let x = (x0 , x1 ) consist of a pair of degree-n polynomials (n = 256 for Kyber, Dilithium) with integer coeﬃcients ∈ Zq . Function SampleZnq (z) takes a 256-bit key z and uniformly samples a polynomial from it (similarly to ExpandA(z) in Dilithium and Parse(XOF(z)) in Kyber.) On the left-hand side, a “compression” algorithm (analogous to Algorithm 1) creates a 256-bit random z1 and samples a random polynomial x1 using it. It then subtracts x1 from x0 and then adds x1 to the result, producing x0 . This construction is exactly like a trivial ﬁrst-order refresh algorithm, except that instead of (x0 , x1 ), we store (x0 , z1 ), which has a signiﬁcantly smaller since z1 is only 256 bits. While x1 ← SampleZnq (z1 ) would suﬃce for decompression (once), on the right-hand side, we present a simultaneous refresh mechanism (analogous to Algorithm 2) that allows repeated extractions.  
   
  Encoding Size. From Theorem 1, we observe that the compressed masking inherits the basic security properties of regular masked encoding. However, the size of the representation is only log2 |G| + dκ bits, while a regular representation requires (d + 1) log2 |G| bits. In the case of Kyber, polynomials are typically packed in 12 ∗ 256 = 3072 bits, while Dilithium ring elements require 23 ∗ 256 = 5888 bits. In compressed masking, this is the size of the xz0 element only, while zi variables are κ = 256 bits.  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  69  
   
  Algorithm 1: xz = MaskCompress(x) (Proved t-NI in Th. 2) Input: Masking x = (x0 , x1 , · · · , xt ).   Output: Compressed masking xz with xz0 + ti=1 SampleG (zi ) = ti=0 xi . z 1: x0 = x0 2: for i = 1, 2, · · · , t do  Random Bit Generator, κ bits. 3: zi ← Random(κ) 4: xz0 ← xz0 − SampleG (zi ) 5: xz0 ← xz0 + xi 6: return xz = (xz0 , z1 , z2 , · · · , zt )  
   
  Conversions. We obtain a trivial mapping from compressed encoding xz to the general masked encoding x by setting x0 = xz0 and xi = SampleG (zi ) for i ∈ [1, d]. Security follows from the observation that this conversion is “linear” in the sense that there is no interaction between shares. Mapping from regular to compressed format requires interaction between the shares since SampleG is not invertible. Algorithm 1 MaskCompress presents one way of performing this conversion. We note its resemblance to the RefreshMasks algorithm of Rivain and Prouﬀ ([31, Algorithm 4]); its NI security follows similarly (see Sect. 3 for more details). While it is secure if used appropriately, combining it with other algorithms may expose leakage, as demonstrated in [9]. Depending on requirements, it can be combined with additional refresh steps to build an SNI [5] algorithm (also see Sect. 3 for more details).  
   
  Algorithm 2: xi = LoadShare(xz , i)  
   
   Input: Compressed masking xz satisfying x = xz0 + ti=1 SampleG (zi ) Input: Index i for the share to be accessed. Output: If read in order, i = 0, 1, · · · t, the returned {xi } is a fresh masking x. 1: if i = 0 then ← xz0  Should be accessed ﬁrst, the rest i > 0 only once. 2: xout i 3: else ← SampleG (zi )  Expand the current zi . 4: xout i  Update zi with a Random Bit Generator. 5: zi ← Random(κ) 6: xz0 ← xz0 − SampleG (zi )  Update xz0 accordingly. 7: xz0 ← xz0 + xout i out 8: return xi  
   
  70  
   
  M.-J. O. Saarinen and M. Rossi  
   
  Algorithm 3: x = FullLoadShare(xz ) (Proved t-NI Th. 3)  
   
   Input: Compressed masking xz satisfying x = xz0 + ti=1 SampleG (zi ) Input: Index i for the share to be accessed. Output: If read in order, i = 0, 1, · · · t, the returned {xi } is a fresh masking x. 1: for i = 0, 1, · · · , t do 2: xi ← LoadShare(xz , i) 3: return (x0 , x1 , · · · , xt )  
   
  Computing with Compressed Masking. A key observation for memory conservation is that one does not need to uncompress all shares to perform computations with the compressed masked representation. One can decompress a single share, perform a transformation on that share, compress it, and proceed to the next one. Masked lattice cryptography implementations generally operate sequentially on each share, performing complex linear operations such as Number Theoretic Transforms (NTT) on individual shares without interaction with others. Furthermore, they require individual masked secret key shares only once (or a limited number of times) during a private key operation. Algorithm 2, LoadShare(xz , i) decodes a share xi ∈ G from a compressed like premasking xz . If the shares are accessed in the sequence i = 0, 1, 2, · · · , t, t sented in Algorithm 3, it is easy to show that their sum will satisfy x = i=0 xi . The compressed masking is refreshed simultaneously (albeit not necessarily in an SNI-composable manner). Subsequent accesses to the same indices will return a diﬀerent encoding x .  
   
  3  
   
  Security Arguments  
   
  Let us introduce some standard, intermediate security properties used in security proofs [5,9,31]. Definition 3 (t-Non Interference [5]). An algorithm is said to be t-noninterfering (written t-NI for short) iﬀ any set of at most t observed internal intermediate variables can be perfectly simulated from at most t shares of each input. One can see that t-non interference implies t-probing security. Such a precise deﬁnition allows simulation proofs for sequential compositions of non-interferent parts. Note that stronger security notions have been introduced in [5] like the t-strong non-interference to handle more than sequential compositions. Definition 4 (t-Strong Non Interference[5]). An algorithm is said tstrongly-non-interfering (written t-SNI for short) if and only if any set of at most t = tint + tout observed variables where tint are made on internal data and tout are made on the outputs can be perfectly simulated from at most tint shares of each input.  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  71  
   
  We observe that t-strong non-interference implies t-non interference. Any non-interferent algorithm can achieve strong non-interference with an extra mask refreshing of its output [5]. Considering MaskCompress (Algorithm 1), we propose the following Theorem and prove it below. Theorem 2. Algorithm 1 is d-Non Interferent under the Pseudorandom Function hypothesis on the SampleG function (Deﬁnition 1). Hence, it is also t-probing secure. Let us ﬁrst assume that there exists an index i∗ ∈ {1, ..., d} such that both the seed zi∗ and the input xi∗ are left unobserved by the probing attacker. With a hybrid argument, under the pseudorandomness hypothesis on the SampleG (zi∗ ) function, SampleG (zi∗ ) may be replaced by a uniform random value in G, denoted y ∗ . Hence, all the intermediate values that intervene in the i∗ -th iteration can be simulated with uniform random. Therefore, the distribution of the observations can be simulated with at most t shares of the input (xi for i = i∗ ) under the computational assumption. Now assume that it is not possible to ﬁnd such an index i∗ ∈ {1, ..., d}. In that case, all the t observations are made on a combination of xi for i ∈ {1, t} and zi for i ∈ {1, t}. Let us note that in that case, the input x0 is always left unobserved. The distribution of xz0 over all iterations is then statistically indistinguishable from uniform random in G. Hence, the distribution of the observations can be simulated with at most t shares of the input (xi for i ∈ {1, t}). Algorithm 4: xz = SNIMaskCompress(x) (Proved t-SNI in Th. 4) Input: Masking x = (x0 , x1 , · · · , xt ).   Output: Compressed masking xz with xz0 + ti=1 SampleG (zi ) = ti=0 xi . 1: xz0 = x0 2: for i = 1, 2, · · · , t do 3: zi ← Random(κ) 4: xz0 ← xz0 − SampleG (zi )  Compared to Alg. 6, xi is directly accessed 5: xz0 ← xz0 + xi 6: for j = 1, 2, · · · , t do 7: for i = 1, 2, · · · , t do 8: xi ← SampleG (zi ) 9: zi ← Random(κ) 10: xz0 ← xz0 − SampleG (zi ) 11: xz0 ← xz0 + xi 12: return xz = (xz0 , z1 , z2 , · · · , zt )  
   
  Let us now consider the LoadShare algorithm. As noted above, the full version of Algorithm 2, presented in Algorithm 3, is very similar to the Non-Interferent RefreshMasks algorithm introduced in [31]. Hence, we introduce the following Theorem.  
   
  72  
   
  M.-J. O. Saarinen and M. Rossi  
   
  Theorem 3. Algorithm 3 is d-Non Interferent and thus t-probing secure under the pseudorandomness hypothesis on the SampleG function (Deﬁnition 1). Since there are t+1 iterations and at most t observations, there exists an index i∗ ∈ {0, ..., t} designating an iteration that is left unobserved by the probing attacker. Hence both the input seed zi∗ and the value xz0 (of the i∗ -th iteration) are left unobserved. In that case, all the subsequent updates of xz0 can be replaced with uniform random under the same pseudorandomness hypothesis of SampleG . Finally, all the attacker’s observations may be simulated with (x0 , (zi )i=i∗ ) if i∗ = 0 and all the (zi ) otherwise. There are no more than t shares of the input, which concludes the proof.  
   
  Algorithm 5: xi = SNILoadShare(xz , i)  
   
   Input: Compressed masking xz satisfying x = xz0 + ti=1 SampleG (zi ) Input: Index i for the share to be accessed. Output: If read in order, i = 0, 1, · · · t, the returned {xi } is a fresh masking x. 1: if i = 0 then ← xz0  Should be accessed ﬁrst, the rest i > 0 only once. 2: xout i 3: else ← SampleG (zi )  Expand the current zi . 4: xout i 5: for j = 1, 2, · · · , t do 6: xj ← SampleG (zj )  Update zi with a Random Bit Generator. 7: zj ← Random(κ) 8: xz0 ← xz0 − SampleG (zj )  Update xz0 accordingly. 9: xz0 ← xz0 + xj out 10: return xi  
   
  Strong Non Interference. Our mask compression design does not immediately reach the strong non-interference security notion; thus, it cannot be directly composed in complex designs. As outlined above, for safe composition properties, applying a Strong Non-Interferent mask refreshing like introduced in [8] is important. We present in Algorithm 6 an SNI refresh procedure on compressed masks. Applying Algorithm 6 at the beginning of FullLoadShare and at the end MaskCompress allows one to easily reach the strong Non-Interference property. However, it is also possible to slightly save some randomness and directly transform both our algorithms such that they reach the SNI property. We introduce them in Algorithm 4, 5 and 7.  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  73  
   
  Please note that these three SNI gadgets will not be used in Sect. 4 for Raccoon, but they are provided here for potential other applications.  
   
  Algorithm 6: xz = SNIRefresh(x) (Proved t-SNI in Th. 4) Input: Compressed masking xz Output: Compressed masking xz with fresh shares. 1: for j = 0, 1, · · · , t do 2: for i = 1, 2, · · · , t do 3: xi ← SampleG (zi ) 4: zi ← Random(κ) 5: xz0 ← xz0 − SampleG (zi ) 6: xz0 ← xz0 + xi 7: return xz = (xz0 , z1 , z2 , · · · , zt )  
   
  Algorithm 7: x = FullLoadShare(xz ) (Proved t-SNI Th. 4)  
   
   Input: Compressed masking xz satisfying x = xz0 + ti=1 SampleG (zi ) Input: Index i for the share to be accessed. Output: If read in order, i = 0, 1, · · · t, the returned {xi } is a fresh masking x. 1: for i = 0 · · · , t do 2: xi ← SNILoadShare(xz , i) 3: return (x0 , x1 , · · · , xt )  
   
  Theorem 4. Algorithm 4, 6, and 7 are d-Strongly Non-Interferent under the Pseudorandomness hypothesis of SampleG function (Deﬁnition Algorithm 1). They may be safely composed in complex designs. In Algorithm 4 and 6, since there are t + 1 = d iterations (with one outside of the loop with index j for Algorithm 4) and t observations, at least one iteration is left unobserved. All the observations (including the observations on the output) performed after the unobserved iteration can be simulated with uniform random (under the same pseudorandomness hypothesis). All the observations performed before the unobserved iteration can be simulated with at most t shares of the input (inherited from the NI property of Algorithm 1). For Algorithm 7, one can switch the loops for i and j and apply the same reasoning.  
   
  74  
   
  M.-J. O. Saarinen and M. Rossi  
   
  4  
   
  Experiment: Order-31 Lattice Signatures  
   
  We illustrate Mask Compressions with Raccoon2 at very high masking order 31 (number of shares d = t + 1 = 32) [28]. The unit performs all of the masked arithmetic in KeyGen(), and Sign(), and also implements Verif(). We will focus on the masked signing process, reproduced in Algorithm 8. Overview of the Hardware. The FPGA implementation contains an RV32C controller, a 24-cycle Keccak accelerator, and a lattice unit with direct memory access via a 64-bit interface. The lattice unit has hard-coded support for Raccoon’s mod q arithmetic. It can perform arbitrary-length vector arithmetic operations such as polynomial addition, coeﬃcient multiplication, NTT butterﬂy operations, and shifts on 64-bit words. The FPGA implementation has a 5-cycle modular multiplier with a 64-to-49 bit ﬁxed-modulus reduction circuit. All variants of Raccoon utilize the same modulus q, allowing “hard-coded” reduction circuitry to be used to implement them all. Since the implementation is designed for masking, the circuitry also has a fast “random ﬁll” function that generates non-deterministic masking randomness rapidly. In a production implementation, this function would require special attention to guarantee that the randomness used in each share is genuinely independent, but trivial entropy sources with simple ASCON [11] -based mixing function was used in the prototype. 4.1  
   
  SampleG (z) in Hardware  
   
  Crucially, the hardware can directly perform mod q rejection sampling from streaming SHAKE output to memory. Since a full Keccak round is implemented in hardware, it produces output at a very high rate, theoretically an entire block (136 bytes for SHAKE-256) every 24 cycles. This function works in parallel with other operations. We found that bus access and arithmetic steps tend to be the performance bottlenecks rather than the rejection sampling component. In addition to implementing SampleG (z), the rejection sampler eliminates perhaps the most signiﬁcant performance bottleneck in microcontroller latticebased PQC implementations: It was initially intended to generate the k ×  polynomial matrix A on the ﬂy (Lines 2 and 12 in Algorithm 8, similar requirement in key generation and veriﬁcation functions.) Such on-the-ﬂy generation of A is also required in Kyber and Dilithium implementations. Hence, a rejection sampler of this type can be expected to be available in dedicated PQC hardware.  
   
  2  
   
  The discussion applies to the version of Raccoon published at IEEE S&P 2023 [28]. There are diﬀerences to the Raccoon version submitted to the NIST PQC Call [27].  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  75  
   
  Share Access Gadgets. For this implementation, we used gadgets based on Algorithm 1 and 2, implemented as a library call with an “API” for loading and storing mask sets consistently (so that leakage characteristics would be uniform). Note that while the introduced SNI gadgets are not used in Raccoon – this would violate the quasilinear complexity requirement – the NI gadgets in Algorithm 1 and 2 suﬃce to ensure the probing security of Raccoon. One polynomial (x0 in Deﬁnition 2) was held as full 64-bit integers to facilitate fast hardware arithmetic, while the rest (t = 31 shares) were stored as κ = 256 bit seeds. Note that each arithmetic step utilized the shares one at a time (thanks to the requirements of the quasilinear lattice cryptography) i = 0, 1, · · · , t. When a share i was required for arithmetic, an implementation of Algorithm 2 gadget was called. For storing i = 0, 1, · · · , t, the share i = 0 was stored in full, while the rest utilized Lines 3–6 of Algorithm 1 to update it. The implementation of Decode function does not t require simultaneous refresh, so it is suﬃcient to compute x0 + i=1 SampleG (zi ). Memory Footprint. Algorithm 8 has been annotated with the share-access gadgets used in each stage, which allow the implementation to use mask compression on each sensitive variable. All of these are vectors of polynomial rings Rq , with dimension depending on the security level λtarget ∈ {128, 192, 256} [28, Table 3]. Focusing on the “Category 1” Raccoon-128 parameter sets the vector length is either  = 3 (for r, s, and z) or k = 8 (for u and w.) For the masked variables, only the secret key s needs to be retained for repeated use. Not all internal variables are used concurrently, and hence e.g., u and w can occupy the same memory as r and z. Hence this Raccoon implementation requires  = 3 masked polynomials for persistent storage (secret key), and additional  + k = 11 for working memory. To estimate the minimum memory requirement at t = 31 without mask compression, we assume that each polynomial coeﬃcient is bit-packed into log2 q = 49 bits; hence a masked polynomial requires d×n×49 = 802, 816 bits. For both secret key and working memory, this comes to roughly 1.4 megabytes (close to 2 MB if coeﬃcients are stored in an access-friendly manner as 64-bit integers.) With mask compression, the size of each masked polynomial drops to n × 49 + t × κ = 33, 024 bits, or 4.1% of the uncompressed mask size. This is only a 31.6% increase over completely unmasked implementation, even for the very high masking order of 31; one can well say that the storage cost of masking becomes negligible with mask compression. The physical FPGA implementation operated well with 128 kB of SRAM, while at least 2000 kB would have been required without compression. The secret  
   
  76  
   
  M.-J. O. Saarinen and M. Rossi  
   
  key s size also shrunk from 294 kB to 12.1 kB, which is important as nonvolatile storage can be more scarce than working memory. Algorithm 8: Sign(sk, vk, msg): “IEEE SP ’23” Raccoon signing [28, Algorithm 7] with applicable mask compression gadgets annotated in the comments. (Note: There are diﬀerences to the “NIST” version [27].) Input: A masked signing key sk, a message msg Output: A signature sig of msg under sk 1: r ← (Rq )d  In the implementation: A random mask set! 2: u := A · r  Access: NI Alg. 1, 2 or SNI Alg. 4, 5. 3: u ← Refresh(u)  Implicit with NI or SNI with Alg. 6. 4: w := ApproxShiftq→qw (u)  Access: NI Alg. 1, 2 or SNI Alg. 4, 5. 5: w := Decode(w)  Commitment. NI: Alg. 3 or SNI Alg. 7. 6: chash := H(w, msg)  Challenge hash. (Not masked.) 7: cpoly := ChalPoly(chash )  Challenge polynomial. (Not masked.) 8: s ← Refresh(s)  Implicit with NI or SNI with Alg. 6. 9: r ← Refresh(r)  Implicit with NI or SNI with Alg. 6. 10: z := cpoly · s + r  Access: NI Alg. 1, 2 or SNI Alg. 4, 5. 11: z := Decode(z)  Response. NI: Alg. 3 or SNI Alg. 7. 12: y := A · z − pt · cpoly · t  (The rest is not masked.) 13: ytop := y q→qw 14: h := w − ytop  Hint. 15: if ( h 2 > B2 ) or ( h ∞ > B∞ ) then 16: goto Line 1  Check the hint’s norms. 17: return sig := (chash , z, h)  
   
  4.2  
   
  Implementation Details and Basic Leakage Assessment  
   
  On an XC7A100T (Xilinx Artix 7) FPGA target, this size-optimized design (including a control Core, Keccak unit, lattice coprocessor, masking random number generator, and communication peripherals) was 10,638 Slice LUTs (16.78%), 4,140 Slice registers/Flip Flops, (3.26%) and only 3 DSPs (as logic was used for multipliers – the design is ASIC-oriented). The design was rated for 78.3 MHz. Table 1 summarizes its performance at various masking levels. Table 1. FPGA cycle counts at various side-channel security levels. Algorithm  
   
  Shares Keygen()  
   
  Sign()  
   
  Verif()  
   
  Raccoon-128 d = 2  
   
  1,366,000  
   
  2,402,000 1,438,000  
   
  Raccoon-128 d = 4  
   
  2,945,000  
   
  3,714,230 1,433,034  
   
  Raccoon-128 d = 8  
   
  6,100,000  
   
  6,345,000 1,389,000  
   
  Raccoon-128 d = 16 12,413,000 11,605,000 1,389,000 Raccoon-128 d = 32 25,073,000 22,160,000 1,393,000  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  77  
   
  Leakage Assessments. We ran a TVLA/17825:2022(E) [17] type leakage assessment on all orders from d = 2 up to d = 32, with N = 200, 000 traces at d = 2 showing no leakage. Such detection mechanisms are generally limited to ﬁrst-order leakage, so testing a high-order implementation can be seen as unnecessary. However, in this particular case, there is a risk that the mask compression gadgets have unforeseen leaks. Fixed vs. Random Test. A non-speciﬁc t-test [32] was conducted on the signing function to assess leakage of secret key s. The ﬁxed set of traces consisted of signing operations using synthetic keypairs where the secret s component was ﬁxed (but refreshed for every operation), and the public A was randomized. For the signing operation, a synthetic t is derived with the ﬁxed s and randomized A. The second random set of traces used completely random keypairs. The message to be signed was constant in both tests. Critical Value. At order d = 32, the leakage assessment was carried out with N = 20, 000 full traces and passed well under a threshold value matching α = 10−5 . As noted by several authors, for example, Ding et al. [10] and Oswald et al. [33], the common “TVLA” threshold value 4.5 needs to be adjusted for long traces (the overall false positive rate with millions of points would be close to 1.) The threshold value corresponding to signiﬁcance level α = 10−5 with l = 2.59 × 106 time points is C = 6.94, using the methodology of [10]. Signal Acquisition and Post-processing. Power signal was acquired from the FPGA chip on the CW305 board [16, Sect. C.3] with a PicoScope 2208B oscilloscope. The test was run with a 24ns (41.7 MHz) clock cycle. Power samples were gathered at the same rate. Each trace of the signature operation contained more than 22 million samples at d = 32. The DUT generated a cycle-precise trigger. Random delays and other non-masking countermeasures were disabled. We applied post-processing steps to improve detection. The waveforms were computationally normalized so that each 1 ms sliding window had μ = 0 and σ 2 = 1 (eﬀectively, a 1 kHz high-pass ﬁlter and dynamic amplitude control). This allowed the traces to match more closely on the vertical axis. The traces were also aligned horizontally using the start and end triggers. Results. At N=20,000 traces, the maximum t-value was 5.55 (Fig. 2), well under the threshold and corresponding to P-value 0.47. At N=10,000 traces, the test result was t = 5.43. We also veriﬁed that leakage detection is functional by disabling countermeasures in various ways; spikes rapidly appear in those cases.  
   
  78  
   
  M.-J. O. Saarinen and M. Rossi  
   
  Fig. 2. On top, t-trace of Raccoon-128 (d = 32) signature function from N = 20, 000 waveforms, each with 22.16 × 106 measurements (time on the horizontal axis). No leakage spikes were detected; the t-statistic values are within the critical value boundaries (thin red lines). This test only detects ﬁrst-order leakage, so it is merely oﬀered as additional evidence related to the implementation of the mask compression gadgets. The bottom ﬁgure has N = 500 traces of the same implementation with mask randomization disabled; this demonstrates that leakage detection was operational. (Color ﬁgure online)  
   
  5  
   
  Conclusions and Open Problems  
   
  We have introduced Mask Compression, a method to reduce the memory cost of high-order masking side-channel countermeasures using non-masked symmetric cryptography. This simple technique allows a set of t-order mask shares to have a storage requirement equivalent to a single share and t symmetric keys. The technique nearly halves the memory requirement for ﬁrst-order Kyber and Dilithium, but its beneﬁts are most signiﬁcant in higher-order masking. We present security arguments in the well-known NI and SNI frameworks. To illustrate the technique’s utility, we describe an Order-31 implementation of the Raccoon signature scheme [28] where the size of the secret keys is reduced from 294kB to 12kB. The overall memory requirement is reduced from two megabytes to 128 kB, allowing the scheme to be implemented on a resourceconstrained FPGA target while maintaining a quasilinear masking complexity and a high level of non-invasive side-channel security. However, this experiment was with NI gadgets only. As an open problem, we are working on closing SNI  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  79  
   
  composability gaps for some of the components and providing SNI gadgets with quasilinear complexity.  
   
  References 1. Alagic, G., et al.: Status report on the third round of the NIST post-quantum cryptography standardization process. Interagency or Internal Report NISTIR 8413-upd1, National Institute of Standards and Technology (2022). https://doi. org/10.6028/NIST.IR.8413-upd1. https://csrc.nist.gov/publications/detail/nistir/ 8413/ﬁnal 2. Avanzi, R., et al.: CRYSTALS-Kyber: Algorithm speciﬁcations and supporting documentation (version 3.02). NIST PQC Project, 3rd Round Submission Update (2021). https://pq-crystals.org/kyber/data/kyber-speciﬁcation-round3-20210804. pdf 3. Azouaoui, M., et al.: Leveling Dilithium against leakage: revisited sensitivity analysis and improved implementations. IACR ePrint 2022/1406 (2022). https://eprint. iacr.org/2022/1406, fourth PQC Standardization Conference, NIST (Virtual) 29 Nov–1 Dec 2022 4. Bai, S., et al.: CRYSTALS-Dilithium: algorithm speciﬁcations and supporting documentation (version 3.1). NIST PQC Project, 3rd Round Submission Update (2021). https://pq-crystals.org/dilithium/data/dilithium-speciﬁcationround3-20210208.pdf 5. Barthe, G., et al.: Strong non-interference and type-directed higher-order masking. In: Weippl, E.R., Katzenbeisser, S., Kruegel, C., Myers, A.C., Halevi, S. (eds.) CCS 2016: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, 24–28 October 2016, pp. 116–129. ACM (2016). https://doi.org/10.1145/2976749.2978427. http://dl.acm. org/citation.cfm?id=2976749 6. Bos, J.W., Gourjon, M., Renes, J., Schneider, T., van Vredendaal, C.: Masking kyber: ﬁrst- and higher-order implementations. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2021(4), 173–214 (2021). https://doi.org/10.46586/tches.v2021.i4. 173-214 7. Chari, S., Jutla, C.S., Rao, J.R., Rohatgi, P.: Towards sound approaches to counteract power-analysis attacks. In: Wiener [34], pp. 398–412. https://doi.org/10. 1007/3-540-48405-1 26 8. Coron, J.-S.: Higher order masking of look-up tables. In: Nguyen, P.Q., Oswald, E. (eds.) EUROCRYPT 2014. LNCS, vol. 8441, pp. 441–458. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-55220-5 25 9. Coron, J.-S., Prouﬀ, E., Rivain, M., Roche, T.: Higher-order side channel security and mask refreshing. In: Moriai, S. (ed.) FSE 2013. LNCS, vol. 8424, pp. 410–424. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-43933-3 21 10. Ding, A.A., Zhang, L., Durvaux, F., Standaert, F.-X., Fei, Y.: Towards sound and optimal leakage detection procedure. In: Eisenbarth, T., Teglia, Y. (eds.) CARDIS 2017. LNCS, vol. 10728, pp. 105–122. Springer, Cham (2018). https://doi.org/10. 1007/978-3-319-75208-2 7 11. Dobraunig, C., Eichlseder, M., Mendel, F., Schl¨ aﬀer, M.: Ascon v1.2. Submission to NIST (Lightweight Cryptography Project) (2021). https://csrc.nist.gov/CSRC/ media/Projects/lightweight-cryptography/documents/ﬁnalist-round/updatedspec-doc/ascon-spec-ﬁnal.pdf  
   
  80  
   
  M.-J. O. Saarinen and M. Rossi  
   
  12. Duc, A., Dziembowski, S., Faust, S.: Unifying leakage models: from probing attacks to noisy leakage. J. Cryptol. 32(1), 151–177 (2019). https://doi.org/10.1007/ s00145-018-9284-1 13. Duc, A., Faust, S., Standaert, F.-X.: Making masking security proofs concrete. In: Oswald, E., Fischlin, M. (eds.) EUROCRYPT 2015. LNCS, vol. 9056, pp. 401– 429. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-46800-5 16. https://eprint.iacr.org/2015/119, extended version is available as IACR ePrint Report 2015/015 14. Heinz, D., Kannwischer, M.J., Land, G., P¨ oppelmann, T., Schwabe, P., Sprenkels, D.: First-order masked Kyber on ARM Cortex-M4. IACR ePrint 2022/058 (2022). https://eprint.iacr.org/2022/058 15. Ishai, Y., Sahai, A., Wagner, D.: Private circuits: securing hardware against probing attacks. In: Boneh, D. (ed.) CRYPTO 2003. LNCS, vol. 2729, pp. 463–481. Springer, Heidelberg (2003). https://doi.org/10.1007/978-3-540-45146-4 27 16. ISO: IT security techniques - test tool requirements and test tool calibration methods for use in testing non-invasive attack mitigation techniques in cryptographic modules - part 2: Test calibration methods and apparatus. Standard ISO/IEC 20085–2:2020(E), International Organization for Standardization (2020). https:// www.iso.org/standard/70082.html 17. ISO: Information technology - security techniques - testing methods for the mitigation of non-invasive attack classes against cryptographic modules. Draft International Standard ISO/IEC DIS 17825:2022(E), International Organization for Standardization (2023) 18. Ito, A., Ueno, R., Homma, N.: On the success rate of side-channel attacks on masked implementations: information-theoretical bounds and their practical usage. In: Yin, H., Stavrou, A., Cremers, C., Shi, E. (eds.) Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, CCS 2022, Los Angeles, CA, USA, 7–11 November 2022, pp. 1521–1535. ACM (2022). https:// doi.org/10.1145/3548606.3560579. https://eprint.iacr.org/2022/576 19. Katz, J., Lindell, Y.: Introduction to Modern Cryptography, 3rd edn. CRC Press, Boca Raton (2021). https://doi.org/10.1201/9781351133036 20. Kocher, P.C.: Timing attacks on implementations of Diﬃe-Hellman, RSA, DSS, and other systems. In: Koblitz, N. (ed.) CRYPTO 1996. LNCS, vol. 1109, pp. 104–113. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-68697-5 9 21. Kocher, P.C., Jaﬀe, J., Jun, B.: Diﬀerential power analysis. In: Wiener [34], pp. 388–397. https://doi.org/10.1007/3-540-48405-1 25 22. Kocher, P.C., Jaﬀe, J., Jun, B., Rohatgi, P.: Introduction to diﬀerential power analysis. J. Cryptogr. Eng. 1(1), 5–27 (2011). https://doi.org/10.1007/s13389-0110006-y 23. Masure, L., Rioul, O., Standaert, F.X.: A nearly tight proof of Duc et al.’s conjectured security bound for masked implementations. In: Buhan, I., Schneider, T. (eds.) CARDIS 2022. LNCS, vol. 13820, pp. 69–81. Springer, Cham (2023). https:// doi.org/10.1007/978-3-031-25319-5 4. https://eprint.iacr.org/2022/600 24. Migliore, V., G´erard, B., Tibouchi, M., Fouque, P.-A.: Masking Dilithium - eﬃcient implementation and side-channel evaluation. In: Deng, R.H., Gauthier-Uma˜ na, V., Ochoa, M., Yung, M. (eds.) ACNS 2019. LNCS, vol. 11464, pp. 344–362. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-21568-2 17 25. NIST: SHA-3 standard: Permutation-based hash and extendable-output functions. Federal Information Processing Standards Publication FIPS 202 (2015). https:// doi.org/10.6028/NIST.FIPS.202  
   
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
   
  81  
   
  26. NSA: Announcing the commercial national security algorithm suite 2.0. National Security Agency, Cybersecurity Advisory (2022). https://media.defense.gov/2022/ Sep/07/2003071834/-1/-1/0/CSA CNSA 2.0 ALGORITHMS .PDF 27. del Pino, R., et al.: Raccoon: a side-channel secure signature scheme. Submission to NIST Standardization Call for Additional PQC Signature Schemes (2023). https:// github.com/masksign/raccoon/blob/main/doc/raccoon.pdf 28. del Pino, R., Prest, T., Rossi, M., Saarinen, M.J.O.: High-order masking of lattice signatures in quasilinear time. In: 44th IEEE Symposium on Security and Privacy, SP 2023, San Francisco, CA, USA, 22–25 May 2023, pp. 1168–1185. IEEE (2023). https://doi.org/10.1109/SP46215.2023.00160 29. Prouﬀ, E., Rivain, M.: Masking against side-channel attacks: a formal security proof. In: Johansson, T., Nguyen, P.Q. (eds.) EUROCRYPT 2013. LNCS, vol. 7881, pp. 142–159. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3642-38348-9 9 30. Quisquater, J.-J., Samyde, D.: ElectroMagnetic analysis (EMA): measures and counter-measures for smart cards. In: Attali, I., Jensen, T. (eds.) E-smart 2001. LNCS, vol. 2140, pp. 200–210. Springer, Heidelberg (2001). https://doi.org/10. 1007/3-540-45418-7 17 31. Rivain, M., Prouﬀ, E.: Provably secure higher-order masking of AES. In: Mangard, S., Standaert, F.-X. (eds.) CHES 2010. LNCS, vol. 6225, pp. 413–427. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15031-9 28 32. Schneider, T., Moradi, A.: Leakage assessment methodology. In: G¨ uneysu, T., Handschuh, H. (eds.) CHES 2015. LNCS, vol. 9293, pp. 495–513. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48324-4 25 33. Whitnall, C., Oswald, E.: A critical analysis of ISO 17825 (‘testing methods for the mitigation of non-invasive attack classes against cryptographic modules’). In: Galbraith, S.D., Moriai, S. (eds.) ASIACRYPT 2019. LNCS, vol. 11923, pp. 256– 284. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-34618-8 9 34. Wiener, M. (ed.): CRYPTO 1999. LNCS, vol. 1666. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48405-1  
   
  Not so Diﬃcult in the End: Breaking the Lookup Table-Based Aﬃne Masking Scheme Lichao Wu1 , Guilherme Perin2 , and Stjepan Picek1(B) 1  
   
  Radboud University, Nijmegen, The Netherlands [email protected]  2 Leiden University, Leiden, The Netherlands  
   
  Abstract. The lookup table-based masking countermeasure is prevalent in real-world applications due to its potent resistance against side-channel attacks and low computational cost. The ASCADv2 dataset, for instance, ranks among the most secure publicly available datasets today due to two layers of countermeasures: lookup table-based aﬃne masking and shufﬂing. Current attack approaches rely on strong assumptions. In addition to requiring access to the source code, an adversary would also need prior knowledge of random shares. This paper forgoes reliance on such knowledge and proposes two attack approaches based on the vulnerabilities of the lookup table-based aﬃne masking implementation. As a result, the ﬁrst attack can retrieve all secret keys’ reliance in less than a minute without knowing mask shares. Although the second attack is not entirely successful in recovering all keys, we believe more traces would help make such an attack fully functional. Keywords: Side-channel analysis Correlation  
   
  1  
   
  · Side-channel collision attack ·  
   
  Introduction  
   
  Side-channel analysis (SCA) on symmetric-key cryptography implementations is typically divided into non-proﬁling [20,31] and proﬁling attacks [10,37], depending on the availability of a replica of the device under attack (proﬁling device). Non-proﬁling attacks operate without this assumption, and an adversary collects measurements that encode secret information and subsequently perform a statistical analysis to form a guess about the secrets. In contrast, proﬁling attacks assume that the adversary has unrestricted control over a duplicate of the targeted device. Using this duplicate, the adversary identiﬁes and understands the device’s side-channel behavior, subsequently leveraging this knowledge to extract secret information from the device under attack. Recent advancements, especially deep learning-based side-channel analysis, have signiﬁcantly improved proﬁling attacks. Today, researchers can compromise various protected targets using a single measurement, underscoring the impressive progress in this ﬁeld [24]. Such results were achieved on datasets only a few years ago considered diﬃcult to c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 82–96, 2024. https://doi.org/10.1007/978-3-031-53368-6_5  
   
  Not so Diﬃcult in the End  
   
  83  
   
  break: ASCAD with ﬁxed key and ASCAD with random keys, both protected with the ﬁrst-order Boolean masking. However, one dataset is yet to be broken without prior knowledge about the random shares: ASCADv2 [2]. Indeed, this secure AES-128 implementation published by the Agence Nationale de la S´ecurit´e des Syst`emes d’Information (ANSSI) has been protected with multiple layers of hiding (shuﬄing) and masking countermeasures. Speciﬁcally, a lookup table-based masking scheme [9,13,28] is adopted, wherein a masked Sbox is pre-computed with random shares before the cryptographic operation. Consequently, any intermediate data leakages relating to the non-linear operation of the cipher are eﬀectively eliminated. Although the weaknesses of lookup table-based masking schemes have been discussed in [32], to our knowledge, a direct key-recovery attack without any prior knowledge of mask shares never succeeded. One of the most signiﬁcant challenges in overcoming this implementation is the masking schemes that incorporate both multiplicative mask computation with ﬁnite ﬁeld multiplication over GF(28 ) and additive (Boolean) masks. Traditional attacks on the Boolean masked implementation depend on the ability of the proﬁling model to combine mask shares and masked data to retrieve the sensitive data. For ASCADv2, even though the multiplicative masks signiﬁcantly leak [23], the additive masked Galois ﬁeld multiplication is complex for a proﬁling model to comprehend, even when leveraging deep learning [8]. Therefore, all existing attacks on this dataset are performed in a white-box setting with prior knowledge of the random shares, at least in the proﬁling phase or in both proﬁling and attack phases [22,23]. We argue that such assumptions could not be practical even in secure evaluation labs that perform white-box evaluations. Although cryptographic algorithms are evaluated with all implementation details (e.g., source code of the cryptographic library and hardware design details), the random shares would rarely be accessible to an evaluator. The reason is straightforward: the registers that store the random values for the system protection would never be accessible from the outside world unless severe implementation ﬂaws exist. Although it is possible to predict the output of some weak pseudo-random number generators (PRNG) with modeling techniques [1], we consider it diﬃcult considering the unknown random seeds and complexity (e.g., high-order polynomials) of PRNG. Other ways of bypassing such protections are monitoring the random number leakages on data bus with probes or forcing the PRNG stuck at some ﬁxed value with fault injection. However, it highly depends on the implementation, and it is out of the scope of this paper that focus solely on SCA. This paper presents two vulnerabilities in ASCADv2’s aﬃne masking implementation that could lead to successful key recovery without knowledge of the mask shares. With the knowledge of plaintexts, the implementation could be broken down in less than a minute with a CPU only. Note that we disable the shuﬄing countermeasure and concentrate solely on masking schemes. Although turning on this countermeasure would cause the proposed attack to fail with the number of traces we have, we expect it could be circumvented with, for instance, more leakage measurements [39]. For instance, without further means  
   
  84  
   
  L. Wu et al.  
   
  to overcome the shuﬄing, one would need approximately 120 times more traces to overcome a true random permutation. Our main contributions are: 1. We conduct an in-depth investigation into two vulnerabilities inherent in implementing the lookup table-based aﬃne masking scheme, substantiating our ﬁndings with theoretical analysis. 2. We propose several strategies to execute second-order attacks that leverage the identiﬁed vulnerabilities. 3. We demonstrate two attack methodologies that lead to eﬃcient key recovery without the knowledge of the mask shares. 4. We discuss several protection methods that would be resilient to our attack. The rest of this paper is organized as follows. In Sect. 2, we provide the necessary background information. Section 3 discusses related works. Section 4 details the identiﬁed vulnerabilities. In Sect. 5, we provide experimental results. Section 6 discusses the identiﬁed vulnerability from a higher level, then oﬀer possible protection methods to defend against proposed attacks. Finally, in Sect. 7, we conclude the paper and discuss potential future research directions.  
   
  2  
   
  Preliminaries  
   
  This section introduces the notation we follow. Afterward, the relevant information about the side-channel analysis, collision attack, and the targeted ASCADv2 dataset is discussed. 2.1  
   
  Notation  
   
  We utilize calligraphic letters such as X to represent sets, while the corresponding uppercase letters X denote random variables and random vectors X deﬁned over X . Realizations of X and X are denoted by lowercase letters x and x, respectively. Functions, such as f, are presented in a sans-serif font. The symbol k represents a candidate key byte in a key space K. The notation k ∗ refers to the correct key byte or the key byte assumed to be correct by the adversary1 . A dataset T comprises traces ti , which are associated with plaintext/ciphertext pairs di in plaintext/ciphertext space D and keys ki , or ki,j and di,j when considering partial key recovery on byte j. Throughout this work, we focus on a ﬁxed key scenario where ki remains constant for each trace ti , resulting in the utilization of byte vector notation exclusively in equations. 2.2  
   
  Side-Channel Analysis  
   
  As brieﬂy mentioned in the introduction section, side-channel analysis (SCA) can be broadly classiﬁed into two types, proﬁling SCA and non-proﬁling SCA, based on the availability of a fully-controlled cloned device. Non-proﬁling sidechannel analysis exploits the correlation between key-related intermediate values 1  
   
  It is important to note that subkey candidates can involve guessing any number of bits. Although we assume the AES cipher here, the concept remains algorithmindependent.  
   
  Not so Diﬃcult in the End  
   
  85  
   
  and leakage measurements. An adversary collects a series of traces generated during the encryption of diﬀerent plaintexts. The adversary can guess a key portion by examining the correlation between the key-related intermediate values and the leakage measurements. The attack strategy typically involves a “divideand-conquer” approach. First, an adversary divides the traces into groups based on the predicted intermediate value corresponding to the current key guess. If the groups exhibit noticeable diﬀerences (the deﬁnition of “diﬀerence” depends on the attack method), it suggests that the current key guess is likely correct. The non-proﬁling attacks assume relatively weaker adversaries who do not have access to a cloned device. Consequently, these attacks may require many measurements (potentially millions) to extract conﬁdential information. Examples of non-proﬁling attacks include simple power analysis (SPA), diﬀerential power analysis (DPA) [20]/correlation power analysis (CPA) [7], and some machine learning-based attacks [16,31,38]. Note that side-channel collision attack [5,29] and its deep learning version [30] are also considered a non-proﬁling SCA but follows a slightly diﬀerent strategy, discussed in the next section. Proﬁling side-channel attacks aim to map a set of inputs (e.g., side-channel traces) to outputs (e.g., a probability vector of key hypotheses). Proﬁling attacks involve two phases. In the proﬁling phase, the adversary constructs a proﬁling model fθM , parameterized by a leakage model M and a set of learning parameters θ. This model maps the inputs (side-channel measurements) to the outputs (classes obtained by evaluating the leakage model during a sensitive operation) using a set of N proﬁling traces. The notations fθM and fθ are used interchangeably. Then, in the attack phase, the trained model processes each attack trace ti and produces a vector of probabilities pj , representing the likelihood of the associated leakage value or label j. The adversary determines the best key candidate based on this vector of probabilities. If the adversary constructs an eﬀective proﬁling model, only a few measurements from the target device may be suﬃcient to break its security. Examples of proﬁling attacks include the template attack [10], stochastic models [27], and supervised machine learning-based attacks [19,21,25]. 2.3  
   
  Side-Channel Collision Attack  
   
  Side-channel Collision Attack (SCCA) represents a class of non-proﬁling attacks that leverage information dependence leaked during cryptographic processes. Traditional collision attacks capitalize on situations where two distinct inputs into a cryptographic algorithm yield an identical output, a circumstance known as a “collision”. Since collisions are generally infrequent in robustly designed cryptographic systems, SCCA explicitly targets the internal state, which is more likely to be identical. In SCCA, an adversary observes the side-channel information as the system processes diﬀerent inputs. The adversary then scans for patterns or similarities in the side-channel data that indicate a collision has occurred. Upon identifying a collision, the adversary can utilize this knowledge to deduce information about the inter-dependencies of diﬀerent key portions or the algorithm’s internal state, thereby signiﬁcantly reducing the remaining key space.  
   
  86  
   
  L. Wu et al.  
   
  As an illustration, let us consider the SubBytes operation of the Advanced Encryption Standard (AES) [15]. The same intermediate data (Sbox input or output) could be processed if two diﬀerent Sbox operations result in an identical side-channel pattern. Since the Sbox operation is bijective (i.e., a one-to-one correspondence between two sets), we have the following equations: Sbox(ki ⊕ pi ) = Sbox(kj ⊕ pj ) =⇒ ki ⊕ pi = kj ⊕ pj  
   
  (1)  
   
  =⇒ ki ⊕ kj = pi ⊕ pj . Note that a collision of Sbox input would also satisfy ki ⊕kj = pi ⊕pj . Indeed, in contrast to other SCA methods concentrating on key recovery, SCCA aims to uncover the linear diﬀerence between various keys. By making guesses on a single subkey, an adversary can leverage this linear diﬀerence to compute the remainder of the key. This essentially reduces the remaining key space to the equivalent of a single byte, 28 . 2.4  
   
  The ANSSI’s AES Implementation: ASCADv2  
   
  ANSSI has published a library implementing a secure AES-128 on an ARM Cortex-M4 architecture [2] together with 800 000 power measurements focusing on the full AES encryption. This implementation is equipped with several layers of countermeasures, such as aﬃne secret-sharing [17] and random shuﬄing of independent operations [36]. We brieﬂy discuss their implementations in this section. More implementation details can be found on the corresponding GitHub page [2] and paper [8,23]. An overview of generating a mask state Ci with an AES state Xi is shown in Eq. 2. (2) Ci = (Xi ⊗ α) ⊕ β, where i stands for byte indices. Two random shares realize the aﬃne masking scheme: the multiplicative share α and additive share β. Finite ﬁeld multiplication over GF(28 ) and xor are denoted by ⊗ and ⊕, respectively. Note that β may denote Sbox’s input mask rin , Sbox’s output mask rout , or rl , the mask used in the linear operation of AES2 . To ensure there is no direct leakage on the AES state, a masked Sbox, denoted as Sboxm , is pre-computed for all bytes based on rin , rout and α, enabling the processing of the masked data in the non-linear part of AES, illustrated in Eq. 3. Note that rl is removed after rin is applied and added before rout is canceled. Therefore, sensitive states are masked during the entire AES process. Sbox  
   
  → (Sbox(Xi ) ⊗ α) ⊕ rout . (Xi ⊗ α) ⊕ rin −−−−m  
   
  (3)  
   
  The random shares α, rin , and rout remain the same during the computations of each byte and are refreshed in the next AES operation. 2  
   
  The proposed attack target the intermediate data when β = rin and β = rout .  
   
  Not so Diﬃcult in the End  
   
  87  
   
  Random permutations are applied to ShiftRows, MixColumns, and Sbox executions; the permutations indices for each byte are generated based on random seeds.  
   
  3  
   
  Related Works  
   
  Side-channel analysis has been widely researched and applied to diﬀerent cryptographic algorithms during past decades. Multiple attack methods have been developed, such as direct (non-proﬁled) attacks like Simple Power Analysis (SPA), Diﬀerential Power Analysis (DPA) [20], and two-stage (proﬁling) attacks like the template attack [10]. Machine learning-based attacks have been actively researched in recent years and could be used in both proﬁling [19,24,25,37,41] and non-proﬁling settings [16,31,38]. ASCAD [3], a ﬁrst-order masked AES-128 implementation running on an 8-bit AVR microcontroller, is one of the most studied datasets by the sidechannel community. While there are two versions of this ASCAD dataset (one with a ﬁxed key and the other one with random keys), there is little diﬀerence in attacking those two datasets, see, e.g., [24], which is also discussed in more generic terms of portability diﬃculty in [4]. During only a few years of active research, the secret key of this dataset managed to be retrieved from around a thousand attack traces [3] to one trace [24]. For an overview of novel attack methodologies based on the publicly available implementations and the corresponding leakage measurements, as well as for the details on those datasets, we refer readers to [26]. Considering that almost all of the available datasets can be “easily” broken, there is a strong demand from the SCA community to have more robust open-source implementations and leakage measurements. Indeed, knowing the complexity of modern devices, we see a large disbalance between the realistic implementations and those studied in academia. The release of new cryptographic implementations implemented with diﬀerent hardware, software, and protections ﬁlls the gap between academics and the real world. Lookup table-based masking is a common countermeasure against SCA. This strategy, particularly when applied to mask the Sbox, stands out for its computational eﬃciency. The pre-computed Sbox notably reduces the computational load during operations. The initial provably secure ﬁrst-order lookup table-based masking scheme was proposed by Chari et al. [9]. A randomized Sbox lookup table undergoes a shift and receives protection with an output mask. Following this seminal work, enhancements have been made in terms of enhancing its secure order [11,12,28] and decreasing its memory requirement [33,34]. In 2019, ANSSI publicly released a hardened AES-128 implementation. This secure variant employs a lookup table-based aﬃne masking scheme in line with [17], incorporating both multiplicative and additive masking. This combination poses a signiﬁcant challenge to the proﬁling attack on ﬁrst-order leakages. Initial security analysis of this implementation was undertaken by Bronchain et al., who proposed several attack strategies given knowledge of the source code, the secret key, and the random shares processed during the proﬁling phase [8]. Following  
   
  88  
   
  L. Wu et al.  
   
  this, Cristiani et al. [14] uses non-proﬁled SCA with Joint Moments Regression to break the ASCADv2 dataset with 100M traces. Masure et al. conducted partial attacks on various shares and permutation indices [23]. The knowledge they garnered from these attacks was subsequently used to orchestrate a global attack on the protected data. Marquet et al. further contributed to the ﬁeld by highlighting the superiority of multi-task learning over single-task learning when the analysis is focused exclusively on secret data [22]. Recently, Vasselle et al. published an AES implementation that included both masking and artiﬁcially implemented shuﬄing as countermeasures [35]. They successfully breached the target using a spatial dependency analysis. Their research has helped to further our understanding of the strengths and weaknesses of these countermeasures and oﬀers new avenues for exploration in securing AES implementations.  
   
  4  
   
  Vulnerability Analysis  
   
  This section ﬁrst discusses the constant aﬃne mask shares used in ASCADv2. Afterward, we discuss the zero input to the aﬃne masking scheme. 4.1  
   
  Constant Aﬃne Mask Shares for an Encryption  
   
  As mentioned, the ASCADv2 implementation is protected by an aﬃne masking scheme consisting of independent additive and multiplicative mask shares (see Eq. 2). This implementation increases the security level of the implementation [8,23]. Nonetheless, upon analyzing the code, we observe that the same pre-computation table is used for all state bytes, meaning that additive and multiplicative masks remain constant throughout a single AES encryption. Random values are pre-loaded into mask registers before encryption and are retrieved during mask calculations. Such an implementation presents the opportunity to bypass these masking schemes altogether. Formally, assuming Ci = Cj during an AES processing, we have: (Xi ⊗ α) ⊕ β = (Xj ⊗ α) ⊕ β =⇒ Xi = Xj , α = 0.  
   
  (4)  
   
  Lemma 1. Given Xi ⊗ α ⊕ β = Xj ⊗ α ⊕ β, we xor both sides of the equation with β to cancel it out (5) Xi ⊗ α = Xj ⊗ α. Since we work with finite field multiplication (in GF(28 )), each element has a unique inverse (except the element 0). Since α is non-zero (otherwise, both sides of the original equation would equal β, which would not provide any information), we can multiply both sides of the equation by the multiplicative inverse of α, denoted as α−1 : Xi ⊗ α ⊗ α−1 = Xj ⊗ α ⊗ α−1 , α = 0.  
   
  (6)  
   
  Not so Diﬃcult in the End  
   
  89  
   
  Applying the associative property of finite field multiplication over GF(28 ), we have: Xi ⊗ (α ⊗ α−1 ) = Xj ⊗ (α ⊗ α−1 ) =⇒ Xi ⊗ 1 = Xj ⊗ 1  
   
  (7)  
   
  =⇒ Xi = Xj . Therefore, a collision between Xi and Xj is created without the knowledge of α and β. Equation 4 illustrates the vulnerability of this AES implementation. Indeed, a ﬁxed mask can be easily canceled by comparing intermediate data protected by the same mask shares. Note that Xi and Xj could be key-related intermediate data, represented by Sbox(kj ⊕ pj ). In this case, Eq. 1 is satisﬁed if Xi equals Xj . Since the plaintext is known, we adopt a side-channel collision attack to retrieve ki ⊕ kj for all keys, detailed in Sect. 5.1. 4.2  
   
  Zero Input of Aﬃne Masking Scheme  
   
  As discussed in Eq. 7, the multiplicative mask α is non-zero, so each element has a unique inverse. However, it is also possible that Xi is zero (e.g., Sbox(·) = 0). Formally speaking, when Xi = 0, Eq. 2 can be rewritten as: Ci = (Xi ⊗ α) ⊕ β =0⊕β  
   
  (8)  
   
  = β. The masked state Ci only relies on β, and the multiplicative mask α is disabled in this scenario, links to the zero-value 2nd -order leakage mentioned in [17]. To exploit this attack path, an adversary would try all possible keys to calculate Xi and select the traces that satisfy Xi = 0. Then, the chosen traces are correlated with β. The traces set with the highest correlation would indicate the correct key. There are two ways to perform such an attack. The ﬁrst attack path requires the knowledge of β, indicating that an adversary should, for instance, access the output of a PRNG that provides the mask value. In this case, one could conduct the attack in the proﬁling SCA setting similar to other researches [8,22,23], namely learning β on the cloned and fully controlled device and predict β on a victim device, ﬁnally performing correlation analysis using the predicted values and leakage measurements. Since this attack path relies on the knowledge of the mask shares, it is less interesting considering the scope of this paper that aims at breaking ASCADv2 with no assumption on prior knowledge about the mask shares. The second attack path is similar to a side-channel collision attack in which an adversary compares two trace segments. Instead of correlating with the β value, an adversary could correlate with the leakage segments that process β.  
   
  90  
   
  L. Wu et al.  
   
  According to the source code, since β is handled in plaintext (which makes sense as there is no need to protect a random value from side-channel leakages), we expect signiﬁcant leakages of the β processing. The relevant features would correlate well with the trace segments that process SubBytes with zero Sbox inputs. The attack results are shown in Subsect. 5.2.  
   
  5  
   
  Attack Results  
   
  This section provides experimental results, ﬁrst the collision attack on canceling mask shares, followed by correlation attack on GF(0). Instead of regenerating leakage traces [8,23], the original traces provided by ANSSI are used for attacks3 . 5.1  
   
  Side-Channel Collision Attack on Canceling Mask Shares  
   
  The collision attacks require the trace segments of each intermediate data processing. Therefore, the leakage analysis is crucial for the success of such an attack. Figure 1a presents an averaged trace representing the ﬁrst round of the AES. Y-axis stands for the leakage amplitude. The sixteen SubBytes operations are highlighted in red. Repeated patterns can be observed when zooming in on each SubBytes operation, as shown in Fig. 1b. The trace segments for each operation (T 0 , T 1 , · · · T 15 ) are selected based on the lowest value of each repetitive pattern (e.g., the end of T 0 , T 1 , and T 2 interval in Fig. 1b). Note that the selection of the trace segment is neither restricted to the highlighted ranges nor requires any additional knowledge regarding the data being processed or random shares. For instance, one could include intervals between T 0 , T 1 , and T 2 (according to the source code, these intervals could represent operations such as register writing). Based on our preliminary experiments, such a setting would also break the target. Algorithm 1. Side-channel collision attack on ASCADv2 Input: trace segments Ti and Tj , plaintext bytes di and dj Output: most-likely key diﬀerence δ ∗ 1: for δ in K do 2: indices =arg where(di ⊕ δ ==dj ) 3: diﬀ δ = E(Tiindices − Tjindices ) 4: end for 5: δ ∗ = arg minδ diﬀ  
   
  Following Algorithm 1, we perform a side-channel collision attack with the selected trace segments. Given trace segments Ti and Tj and plaintext bytes di and dj , we ﬁrst ﬁnd the trace indices that satisﬁes ki ⊕ kj = pi ⊕ pj (Eq. 1) with the current ki ⊕kj guess in an AES encryption, denoted as δ. Then, the similarity 3  
   
  https://github.com/ANSSI-FR/ASCAD/tree/master/STM32 AES v2.  
   
  Not so Diﬃcult in the End  
   
  91  
   
  Fig. 1. An overview of the leakage trace and the target time interval.  
   
  of the two trace segments is measured with squared Euclidean distance [6] and averaged (represented by E in Algorithm 1) over indices. After looping through all possible δ candidates, the δ guess that leads to the lowest averaging diﬀerence would be the most likely candidate δ ∗ . The experimental result is shown in Fig. 2. When attacking with 30 000 traces, only k1 ⊕ k2 and k13 ⊕ k14 cannot be successfully recovered (δ rank are 3 and 12, respectively). In this case, one could adopt error correction methods [18,40] to recover the true key diﬀerences. With around 70 000 attack traces, all δ ∗ that represents the correct subkey diﬀerence can be recovered. Given this information, the entropy of the key is reduced to 256 and can be easily brute-forced.  
   
  Fig. 2. Side-channel collision attack on all bytes.  
   
  92  
   
  5.2  
   
  L. Wu et al.  
   
  Correlation Attack on GF(0)  
   
  The same trace segments used in the previous section, namely T0 to T15 , are adopted for the attack presented in this section. Based on the source code, Sbox’s output mask rout is loaded right after the SubBytes operation is ﬁnished. Therefore, the time interval of β is selected similarly to the selection of SubBytes operations with the same pattern gap, for instance, T14 and T15 shown in Fig. 3.  
   
  Fig. 3. The selected time intervals including the additive mask (β).  
   
  The attack method is presented in Algorithm 2. Since the goal is to correlate the β leakages with the trace segments of SubBytes, the pairwise correlation corr is performed column-wise. Note that each column in trace segments Ti represents a leakage feature at a speciﬁc time location; the pairwise correlation ensures the dissimilarity of traces segments, due to diﬀerent operation steps and data handling methods, less inﬂuence the correlation results. After averaging the output correlation matrix with E, the k guess that leads to the highest correlation value would be the most likely candidate k ∗ . Algorithm 2. Correlation attack on ASCADv2 Input: trace segments Ti and Tβ , plaintext bytes di Output: most-likely key k∗ 1: for k in K do 2: indices = arg where(Sbox(di ⊕ k) == 0) 3: corrk = E(corr(Tiindices , Tβindices )) 4: end for 5: k∗ = arg max corr  
   
  The experimental result is shown in Fig. 4. Although most of the correct key does not reach a key rank of zero (the most-likely key), we see a clear convergence of the key rank with increased attack traces. Table 1 shows the detailed key rank of each subkey with 500 000 attack traces. Two subkeys are successfully recovered, and the rest (except k5 and k6 ) reach low values of the key rank. As a rough estimation, eight times more traces would lead to successful δ recovery of all subkeys.  
   
  Not so Diﬃcult in the End  
   
  93  
   
  Fig. 4. Correlation attack results on all bytes. Table 1. The rank of each subkey with 500 000 attack traces. k0 k1 k2 k3 k4 k5 Key rank 1  
   
  6  
   
  22 0  
   
  3  
   
  1  
   
  k6  
   
  k7 k8 k9 k10 k11 k12 k13 k14 k15  
   
  112 113 0  
   
  37 1  
   
  21  
   
  23  
   
  12  
   
  24  
   
  24  
   
  8  
   
  Discussion and Protection Methods  
   
  The implementation of AES by ANSSI provides an excellent example of a secure AES execution. It employs masking schemes that protect the entire AES process while shuﬄing serves to minimize potential leakages further. From the viewpoint of a ﬁrst-order attack focusing on the leakage of a single intermediate data, this implementation exhibits robust security, only breakable under strong attack assumptions. However, this masking scheme could be easily compromised with straightforward techniques when examining second-order leakages. A solitary shuﬄing countermeasure could be defeated by employing more traces. Analyzing its design reveals that the reliance on a single Sboxm facilitates the attacks discussed in this paper. Despite all AES states being masked and unknown to an adversary, the deterministic association between the Sbox input and output leaves the computation of each byte susceptible to second-order attacks. One might propose the generation of sixteen distinct Sboxm to facilitate byte substitution. However, in the speciﬁc case of the aﬃne masking scheme outlined in [17], it’s crucial that the multiplicative share must remain consistent across every byte in the state. Deviating from this principle would result in linear operations, such as AddRoundKey and MixColumns, losing their homomorphic property with the aﬃne encoding. This, in turn, implies that each operation would necessitate 16 pre-computed lookup tables, each with a size of 2562 , which makes the cryptographic implementation prohibitively resource-intensive. Alternatively, implementing hiding countermeasures may be a simple and eﬀective strategy against the proposed attacks. The proposed attacks require the comparison of trace segments. The original implementation’s random shuﬄing signiﬁcantly increases the required attack traces. Adding to this, countermeasures introducing temporal randomnesses, such as clock jitters and random branch insertion, could further complicate the process of identifying and comparing the target operations, enhancing the security of the implementation. Addressing the second vulnerability would involve carefully redesigning the implementation, ensuring that GF(0) results in a random output. A more  
   
  94  
   
  L. Wu et al.  
   
  straightforward solution would involve randomizing the timing β process, thereby reducing the correlation between the Sbox operation and β leakages.  
   
  7  
   
  Conclusions and Future Work  
   
  In this paper, we evaluate two vulnerabilities in lookup table-based aﬃne masking implementation, then leverage them to perform eﬃcient second-order attacks on the ASCADv2 dataset. Speciﬁcally, we notice that some mask shares remain constant during an AES encryption, which leads to an easy cancellation of masks with a side-channel collision attack. Another vulnerability relies on implementing the Galois ﬁeld multiplication, which always outputs zero when one input is zero. In this case, an adversary could choose speciﬁc traces that generate zero input. In this case, the aﬃne masking scheme is signiﬁcantly weakened, as only additive mask shares remain as the output. Multiple aspects would be interesting to consider in future research. First, the proposed attacks rely on the single masked SBox used during encryption. It will be interesting to investigate the applicability of the proposed attack when two or more masked SBoxm are used in a cryptographic operation. Next, the proposed attacks are grounded on the squared Euclidean distance and Pearson correlation coeﬃcient for similarity assessment. It would be interesting to explore deep learning in initiating attacks under more noisy circumstances, such as those involving desynchronization. Further, it would be compelling to study and augment the attack performance hinging on our second identiﬁed vulnerability: the zero output of the ﬁnite ﬁeld multiplication. Finally, an optimal objective would be to devise innovative techniques to overcome the complexity inherent in ﬁnite ﬁeld multiplication, enabling direct attacks on this dataset’s intermediate data.  
   
  References 1. Amigo, G., Dong, L., Ii, R.J.M.: Forecasting pseudo random numbers using deep learning. In: 2021 15th International Conference on Signal Processing and Communication Systems (ICSPCS), pp. 1–7. IEEE (2021) 2. Benadjila, R., Khati, L., Prouﬀ, E., Thillard, A.: Hardened library for AES-128 encryption/decryption on ARM Cortex M4 architecture (2019). https://github. com/ANSSI-FR/SecAESSTM32 3. Benadjila, R., Prouﬀ, E., Strullu, R., Cagli, E., Dumas, C.: Deep learning for sidechannel analysis and introduction to ASCAD database. J. Cryptogr. Eng. 10(2), 163–188 (2020) 4. Bhasin, S., Chattopadhyay, A., Heuser, A., Jap, D., Picek, S., Ranjan, R.: Mind the portability: a warriors guide through realistic proﬁled side-channel analysis. In: Network and Distributed System Security Symposium, NDSS 2020, pp. 1–14 (2020) 5. Bogdanov, A.: Improved side-channel collision attacks on AES. In: Adams, C., Miri, A., Wiener, M. (eds.) SAC 2007. LNCS, vol. 4876, pp. 84–95. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-77360-3 6  
   
  Not so Diﬃcult in the End  
   
  95  
   
  6. Bogdanov, A., Kizhvatov, I.: Beyond the limits of DPA: combined side-channel collision attacks. IEEE Trans. Comput. 61(8), 1153–1164 (2011) 7. Brier, E., Clavier, C., Olivier, F.: Correlation power analysis with a leakage model. In: Joye, M., Quisquater, J.-J. (eds.) CHES 2004. LNCS, vol. 3156, pp. 16–29. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-28632-5 2 8. Bronchain, O., Standaert, F.X.: Side-channel countermeasures’ dissection and the limits of closed source security evaluations. IACR Trans. Cryptographic Hardware Embed. Syst. 1–25 (2020) 9. Chari, S., Jutla, C.S., Rao, J.R., Rohatgi, P.: Towards sound approaches to counteract power-analysis attacks. In: Wiener, M. (ed.) CRYPTO 1999. LNCS, vol. 1666, pp. 398–412. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-484051 26 10. Chari, S., Rao, J.R., Rohatgi, P.: Template attacks. In: Kaliski, B.S., Ko¸c, K., Paar, C. (eds.) CHES 2002. LNCS, vol. 2523, pp. 13–28. Springer, Heidelberg (2003). https://doi.org/10.1007/3-540-36400-5 3 11. Coron, J.-S.: Higher order masking of look-up tables. In: Nguyen, P.Q., Oswald, E. (eds.) EUROCRYPT 2014. LNCS, vol. 8441, pp. 441–458. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-55220-5 25 12. Coron, J.S., Rondepierre, F., Zeitoun, R.: High order masking of look-up tables with common shares. Cryptology ePrint Archive (2017) 13. Coron, J.S., Rondepierre, F., Zeitoun, R.: High order masking of look-up tables with common shares. IACR Trans. Cryptographic Hardware Embed. Syst. 40–72 (2018) 14. Cristiani, V., Lecomte, M., Hiscock, T., Maurine, P.: Fit the joint moments: how to attack any masking scheme. IEEE Access 10, 127412–127427 (2022) 15. Daemen, J., Rijmen, V.: AES proposal: Rijndael (1999) 16. Dol, N.T., Le, P.C., Hoang, V.P., Doan, V.S., Nguyen, H.G., Pham, C.K.: MODLSCA: deep learning based non-proﬁled side channel analysis using multi-output neural networks. In: 2022 International Conference on Advanced Technologies for Communications (ATC), pp. 245–250. IEEE (2022) 17. Fumaroli, G., Martinelli, A., Prouﬀ, E., Rivain, M.: Aﬃne masking against higherorder side channel analysis. In: Biryukov, A., Gong, G., Stinson, D.R. (eds.) SAC 2010. LNCS, vol. 6544, pp. 262–280. Springer, Heidelberg (2011). https://doi.org/ 10.1007/978-3-642-19574-7 18 18. G´erard, B., Standaert, F.-X.: Uniﬁed and optimized linear collision attacks and their application in a non-proﬁled setting. In: Prouﬀ, E., Schaumont, P. (eds.) CHES 2012. LNCS, vol. 7428, pp. 175–192. Springer, Heidelberg (2012). https:// doi.org/10.1007/978-3-642-33027-8 11 19. Hospodar, G., Gierlichs, B., De Mulder, E., Verbauwhede, I., Vandewalle, J.: Machine learning in side-channel analysis: a ﬁrst study. J. Cryptogr. Eng. 1(4), 293–302 (2011) 20. Kocher, P., Jaﬀe, J., Jun, B.: Diﬀerential power analysis. In: Wiener, M. (ed.) CRYPTO 1999. LNCS, vol. 1666, pp. 388–397. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48405-1 25 21. Maghrebi, H., Portigliatti, T., Prouﬀ, E.: Breaking cryptographic implementations using deep learning techniques. In: Carlet, C., Hasan, M.A., Saraswat, V. (eds.) SPACE 2016. LNCS, vol. 10076, pp. 3–26. Springer, Cham (2016). https://doi. org/10.1007/978-3-319-49445-6 1 22. Marquet, T., Oswald, E.: A comparison of multi-task learning and single-task learning approaches. Cryptology ePrint Archive (2023)  
   
  96  
   
  L. Wu et al.  
   
  23. Masure, L., Strullu, R.: Side-channel analysis against ANSSI’s protected AES implementation on ARM: end-to-end attacks with multi-task learning. J. Cryptographic Eng. 1–19 (2023) 24. Perin, G., Wu, L., Picek, S.: Exploring feature selection scenarios for deep learningbased side-channel analysis. IACR Trans. Cryptographic Hardware Embed. Syst. 828–861 (2022) 25. Picek, S., et al.: Side-channel analysis and machine learning: a practical perspective. In: 2017 International Joint Conference on Neural Networks, IJCNN 2017, Anchorage, AK, USA, 14–19 May 2017, pp. 4095–4102 (2017) 26. Picek, S., Perin, G., Mariot, L., Wu, L., Batina, L.: SoK: deep learning-based physical side-channel analysis. ACM Comput. Surv. 55(11), 1–35 (2023) 27. Schindler, W., Lemke, K., Paar, C.: A stochastic model for diﬀerential side channel cryptanalysis. In: Rao, J.R., Sunar, B. (eds.) CHES 2005. LNCS, vol. 3659, pp. 30–46. Springer, Heidelberg (2005). https://doi.org/10.1007/11545262 3 28. Schramm, K., Paar, C.: Higher order masking of the AES. In: Pointcheval, D. (ed.) CT-RSA 2006. LNCS, vol. 3860, pp. 208–225. Springer, Heidelberg (2006). https:// doi.org/10.1007/11605805 14 29. Schramm, K., Wollinger, T., Paar, C.: A new class of collision attacks and its application to DES. In: Johansson, T. (ed.) FSE 2003. LNCS, vol. 2887, pp. 206– 222. Springer, Heidelberg (2003). https://doi.org/10.1007/978-3-540-39887-5 16 30. Staib, M., Moradi, A.: Deep learning side-channel collision attack. IACR Trans. Cryptographic Hardware Embed. Syst. 422–444 (2023) 31. Timon, B.: Non-proﬁled deep learning-based side-channel attacks with sensitivity analysis. IACR Trans. Cryptographic Hardware Embed. Syst. 107–131 (2019) 32. Tunstall, M., Whitnall, C., Oswald, E.: Masking tables—an underestimated security risk. In: Moriai, S. (ed.) FSE 2013. LNCS, vol. 8424, pp. 425–444. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-43933-3 22 33. Vadnala, P.K.: Time-memory trade-oﬀs for side-channel resistant implementations of block ciphers. In: Handschuh, H. (ed.) CT-RSA 2017. LNCS, vol. 10159, pp. 115–130. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-52153-4 7 34. Valiveti, A., Vivek, S.: Second-order masked lookup table compression scheme. IACR Trans. Cryptographic Hardware Embed. Syst. 129–153 (2020) 35. Vasselle, A., Thiebeauld, H., Maurine, P.: Spatial dependency analysis to extract information from side-channel mixtures: extended version. J. Cryptographic Eng. 1–17 (2023) 36. Veyrat-Charvillon, N., Medwed, M., Kerckhof, S., Standaert, F.-X.: Shuﬄing against side-channel attacks: a comprehensive study with cautionary note. In: Wang, X., Sako, K. (eds.) ASIACRYPT 2012. LNCS, vol. 7658, pp. 740–757. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-34961-4 44 37. Wu, L., Perin, G., Picek, S.: The best of two worlds: Deep learning-assisted template attack. IACR Trans. Cryptographic Hardware Embed. Syst. 413–437 (2022) 38. Wu, L., Perin, G., Picek, S.: Hiding in plain sight: non-proﬁling deep learning-based side-channel analysis with plaintext/ciphertext. Cryptology ePrint Archive (2023) 39. Wu, L., Picek, S.: Remove some noise: on pre-processing of side-channel measurements with autoencoders. IACR Trans. Cryptographic Hardware Embed. Syst. 389–415 (2020) 40. Wu, L., Tiran, S., Perin, G., Picek, S.: An end-to-end plaintext-based side-channel collision attack without trace segmentation. Cryptology ePrint Archive (2023) 41. Zaid, G., Bossuet, L., Habrard, A., Venelli, A.: Methodology for eﬃcient CNN architectures in proﬁling attacks. IACR Trans. Cryptographic Hardware Embed. Syst. 1–36 (2020)  
   
  Threshold Implementations with Non-uniform Inputs Siemen Dhooghe(B)  
   
  and Artemii Ovchinnikov  
   
  COSIC, KU Leuven, Leuven, Belgium {siemen.dhooghe,artemii.ovchinnikov}@esat.kuleuven.be  
   
  Abstract. Modern block ciphers designed for hardware and masked with Threshold Implementations (TIs) provide provable security against ﬁrst-order attacks. However, the application of TIs leaves designers to deal with a trade-oﬀ between its security and its cost, for example, the process to generate its required random bits. This generation cost comes with an increased overhead in terms of area and latency. Decreasing the number of random bits for the masking allows to reduce the aforementioned overhead. We propose to reduce the randomness to mask the secrets, like the plaintext. For that purpose, we suggest relaxing the requirement for the uniformity of the input shares and reuse randomness for their masking in ﬁrst-order TIs. We apply our countermeasures to ﬁrst-order TIs of the Prince and Midori64 ciphers with three shares. Since the designs with non-uniform masks are no longer perfect ﬁrst-order probing secure, we provide further analysis by calculating bounds on the advantage of a noisy threshold-probing adversary. We then make use of the PROLEAD tool, which implements statistical tests verifying the robust probing security to compare its output with our estimates. Finally, we evaluate the designs on FPGA to highlight the practical security of our solution. We observe that their security holds while requiring four times less randomness over uniform TIs. Keywords: FPGA · Masking Implementations · Uniformity  
   
  1  
   
  · Probing Security · Threshold  
   
  Introduction  
   
  For the past two decades after Kocher et al. [16] presented diﬀerential power analysis in 1999, one example of a side-channel attack, the development of protected cryptographic hardware devices has turned into an important goal for researchers and designers. To that end, masking has become a reliable countermeasure. In masking, a secret is split into several parts to confound the correlation between its value and some physical characteristics, like its power consumption. However, to eliminate the mentioned statistical dependence, the shares of the masking have to be uniform random such that they are independent c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 97–123, 2024. https://doi.org/10.1007/978-3-031-53368-6_6  
   
  98  
   
  S. Dhooghe and A. Ovchinnikov  
   
  of the secret. Threshold Implementations (TIs), introduced in 2006 by Nikova et al. [21], allow to preserve this uniformity of the masks and provide for ﬁrstorder security. As a result, one only needs some ﬁxed number of random bits to be generated at the initial stage and no fresh random bits are needed for further re-masking. The process of randomness generation needed for this uniformity lacks attention in terms of cost and security from the research community. As a result, to be on the safe side, researchers recommend heavy-duty random number generators based on standardised cryptographic primitives. However, in practice, lighter non-cryptographic generators are used whose security remains unknown. Instead of deducting which lightweight random number generator can be used, designers of masking schemes focus on reducing the total number of random bits required for the masking to be secure. The security of this question can more easily be veriﬁed by using the probing model by Ishai et al. [15] which has become the standard model for masking countermeasures. The research on reducing the randomness cost for maskings has signiﬁcantly progressed. The work by Shahmirzadi and Moradi [22] reduces the fresh randomness needed for ﬁrst-order designs using only two shares while their follow-up work [23] does the same for second-order designs. The work by Beyne et al. [7] in 2020 introduces the bounded-query probing model which allows to obtain a concrete bound on the adversary’s advantage by means of a security framework based on linear cryptanalysis. In 2021, Beyne et al. [6] applied their scheme to make a low-randomness masked AES. This work was followed up in 2022 [5] to include the noise on probes to improve the bounds on the adversary and to improve the eﬃciency of their designs. For all three previous works, the security framework was always applied to second-order masked designs but never to the ﬁrst-order designs. Its application to ﬁrst-order designs provides an opportunity of using non-uniform randomness to generate secret shares. Via careful analysis, it can provide a way to reduce the cost of the initial randomness for TIs. Contributions. In this paper, we investigate TIs of lightweight ciphers and their security when given non-uniform inputs. We provide a framework, based on the works by Beyne et al. [5,7], to show when the implementations are secure and when they are not, and provide practical evidence to support it. For the analysis, we work with the Prince [10] and Midori64 [1] ciphers and take previously established TIs of them, namely the one by Bozilov et al. [11] for Prince and by Moradi et al. [18] for Midori64. The goal of the analysis is to reduce the initial randomness needed to mask the plaintext for both ciphers. To that end, we investigate cases where we can re-use randomness for the initial masking and provide, for each cipher, insecure and secure cases using the same number of total random bits. This shows that it is not the total entropy of the masked input which counts, but instead, its relation to the properties of the masking of the cipher. We demonstrate that using the security framework via a trail based approach where we obtain bounds on a probing adversary’s advantage. In order to complete the research, we provide practical experiments on top of the previous mentioned theoretical analysis. We test the designs with nonuniform masked inputs using two diﬀerent approaches. First, we use the PRO-  
   
  Threshold Implementations with Non-uniform Inputs  
   
  99  
   
  LEAD tool by M¨ uller and Moradi [19] which allows for a noiseless statistical leakage evaluation based on the glitch-extended probing model. Second, we provide the results of a practical evaluation of our designs on FPGA. For the observed power consumption, we apply ﬁrst-order statistical ﬁxed vs. random t-tests [2]. The results of those tests are closer to practice as the noise is included in the sampled traces. As an end result, we show that the TIs by Bozilov et al. and by Moradi et al. can be used with a non-uniform masked input reducing the total needed randomness four times over while still providing practical security of up to 50M traces.1  
   
  2  
   
  Preliminaries  
   
  In this section, we introduce the probing security model together with the basics of masking and threshold implementations. We also introduce the cryptanalysisbased evaluation we use to determine which non-uniform inputs of threshold implementations are secure. 2.1  
   
  Glitch-Extended Bounded-Query Probing Security  
   
  We ﬁrst introduce the bounded-query probing model [7] with a noisy probing variant [5]. The security model is depicted in Fig. 1. In this model, the security of a circuit C with input k against a t-thresholdprobing adversary is quantiﬁed as follows. The challenger picks a random bit b and provides an oracle Ob (the masked circuit with b hardwired), to which adversary A is given query access. The adversary queries the oracle by choosing up to t wires of the masked circuit to probe, we denote this set by P, and sends it to the oracle along with the inputs (for a cipher, both key and plaintext) k0 and k1 . The oracle responds by giving back a noisy leakage function f (following the deﬁnition given in [5]) of the glitch-extended probed wire values of the masked circuit with input kb . After a total of q queries, the adversary responds to the challenger with a guess for b. For b ∈ {0, 1}, denote the result of the adversary b after interacting with the oracle Ob using q queries by AO . For left-or-right security, the advantage of the adversary A is then deﬁned as 0  
   
  1  
   
  Advt-thr (A) = | Pr[AO = 1] − Pr[AO = 1]|. Since we are working on hardware, we make use of the glitch-extended probing model by Faust et al. [13]. Whereas one of the adversary’s probes normally results in the value of a single wire, a glitch-extended probe allows obtaining the values of all wires in a bundle, with the limit that glitches do not propagate through memory gates.  
   
  1  
   
  The HDL representations of the constructed ciphers will be made publicly available on https://github.com/KULeuven-COSIC/TIs with Non-Uniform Inputs.  
   
  100  
   
  S. Dhooghe and A. Ovchinnikov  
   
  Fig. 1. [5] The privacy model for the glitch-extended t-threshold-probing security consisting of a challenger C, an adversary A, a left-right oracle Ob , two inputs k0 , k1 , a set of probes P, and a noisy leakage function f (v b ) of the probed wire values v b in the circuit C(kb ).  
   
  2.2  
   
  Boolean Masking and Threshold Implementations  
   
  Boolean masking is a technique based on splitting each secret variable sx −1 xi ∈ F2 in the circuit into shares x ¯ = (x0 , x1 , . . . , xsx −1 ) such that x = i=0 x over F2 . A random Boolean masking of a ﬁxed secret is uniform if all maskings of that secret are equally likely. There are several approaches to masking a circuit. In this work, we make use of threshold implementations proposed by Nikova et al. [21]. Let F¯ be a masked function in the threshold implementation corresponding to ¯ the unmasked function F : Fn2 → Fm 2 . Then F can have the following properties. Deﬁnition 1 (Threshold implementations [21]). Let F : Fn2 → Fm 2 be a msy x ¯ is said to be → F be a masking of F . The masking F function and F¯ : Fns 2 2 sy −1 i 0 sx −1 F (x , . . . , xsx −1 ) = F ( i=0 xi ), 1. correct if ∀x0 , . . . , xsx −1 ∈ Fn2 , i=0 2. non-complete if any function share F i depends on at most sx −1 input shares, 3. uniform if F¯ maps a uniform random masking of any x ∈ Fn2 to a uniform random masking of F (x) ∈ Fm 2 . 2.3  
   
  Linear Cryptanalysis of Maskings  
   
  To prove the ﬁrst-order probing security of a circuit, it is suﬃcient to show that the probed values consist only of uniform randomness and public values. To that end, we make use of the theory by Beyne et al. [7] which bounds the distribution of probed values (in a bounded query model) by their Fourier distribution. We also make use of the later work [5], where this framework was expanded to include the noise on the traces. We recall the main results of these works. Bound on the Advantage. We ﬁrst discuss the bound on a bounded-query probing adversary which uses λ-noisy probes. We refer to [5] on the speciﬁc deﬁnition of the used noisy leakage function. In the rest of this description, the noise parameter λ characterises the level of the noise. In principle, the noise parameters  
   
  Threshold Implementations with Non-uniform Inputs  
   
  101  
   
  could be computed empirically from estimates of the probability distributions of the leakage (i.e. trace points) under all possible secrets. We assume that any probed wire value can be labelled as ‘good’ or ‘bad’. The values labelled ‘good’ jointly reveal nothing about the secret. The ‘bad’ values may reveal secret information, but the leakage can be bounded in terms of λ and ε. The parameter λ is determined by physical aspects such as the leakage model and noise level. The parameter ε is instead determined by the mathematical properties of the masking. Speciﬁcally, it will be shown later how these parameters can be determined using linear cryptanalysis. Below is the deﬁnition of the bound on a ﬁrst-order noisy probing adversary given a bound on ε and λ. Theorem 1 ([5]). Let A be a noisy threshold-probing adversary for a circuit C. Take λ ≥ 1, and ε ≤ 1 as non-negative real numbers. Assume that for every query made by A on the oracle Ob with result z, there exists a partitioning (depending only on the probe positions) of the probed wire values into two random variables x (‘good’) and y (‘bad’) such that 1. The noisy leakage function f such that z = f (x, y) is λ-noisy. 2. The conditional probability distribution py|x satisfies Ex  py|x 22 ≤ ε. 3. Any t-threshold-probing adversary for the same circuit C and making the same oracle queries as A, but which only receives the ‘good’ wire values ( i.e. corresponding to x) for each query, has advantage zero. The advantage of A can be upper bounded as  Advnoisy (A) ≤ 2q ε/λ, where q is the number of queries to the oracle Ob . The security bound obtained in Theorem 1 depends on the parameter ε. This value will be determined by performing linear cryptanalysis of the masked cipher. Essentially, this follows regular linear cryptanalysis, except that masking schemes naturally incorporate linear relations (namely that the sum of the shares form the secret). As a result, the basic deﬁnitions of linear cryptanalysis need to be adapted to work over a quotient space where, in short, the last share is removed to avoid the previously mentioned linear relation. Viewing linear cryptanalysis over this quotient space is justiﬁed by the non-completeness property of threshold implementations, namely that a probe does not view all shares of a secret at once, and as a result, we only investigate relations over non-complete sets of shares. Correlation of Maskings. For any linear masking scheme, there exists a vector space V ⊂ F2 of valid maskings of zero. More speciﬁcally, an F2 -linear secret sharing scheme is an algorithm that maps a secret x ∈ Fn2 to a random element of a corresponding coset of the vector space V. Let ρ : Fn2 → F2 be a map that sends secrets to their corresponding coset representative. For convenience, we denote Va = a + V. We use the following deﬁnition of correlation matrices of a masking.  
   
  102  
   
  S. Dhooghe and A. Ovchinnikov  
   
  Deﬁnition 2 (Correlation matrix). For a subspace V ⊆ F2 , let F : V → V be a function. The correlation matrix C F of F is a real |V| × |V| matrix with coordinates indexed by elements u, v ∈ Fn2 /V⊥ and equal to F = Cv,u  
   
     1  (−1)u x+v F (x) . |V|  
   
  x∈V  
   
  for a function F  : Va → Vb with F  (x) = F (x + a) + b. The link between ε from Theorem 1 and linear cryptanalysis is completed by the theorem below. It shows that the coordinates of pz are entries of the correlation matrix of the state-transformation between the speciﬁed probe locations. In Theorem 2, the restriction of x ∈ Va to an index set I = {i1 , . . . , im } is denoted |I| by xI = (xi1 , . . . , xim ) ∈ F2 . This deﬁnition depends on the speciﬁc choice of the representative a, but the result of the theorem does not. Theorem 2 ([7], §5.2). Let F : Va → Vb be a function with V ⊂ F2 and I, J ⊂ {1, . . . , }. For x uniform random on Va and y = F (x), let z = (xI , yJ ). The Fourier transformation of the probability mass function of z then satisfies , v ∈ F2 /V⊥ are such that u I = u, u []\I = 0, vJ = v | pz (u, v)| = |Cv˜F, u˜ |, where u and v[]\J = 0. The above theorem relates the linear approximations of F to pz (u, v) and hence provides a method to upper bound ε based on linear cryptanalysis. Applying the Bound with Non-uniform Inputs. The analysis by Beyne et al. originally applied to threshold implementations working on a uniform input or consisting of uniform functions. In this work, we extend this framework by analysing threshold implementations with a non-uniform input, namely an input which is shared via a non-uniform function. More speciﬁcally, we use a limited number of random bits to mask the input of the threshold implementation and analyse the impact on its ﬁrst-order security. While previous works focus on reducing the online randomness of a masking, we instead propose to use the cryptanalysis technique to reduce the randomness requirement at the start of the masking. We model this limited-random input by considering a non-uniform input encoder Enc (shown in Fig. 2) which takes in the circuit’s input k (e.g. plaintext and key) and random bits r (modelled as shares of zero and as ‘bad’ values), and provides a shared input for the masked circuit. In particular, where this shared input is larger in size than the randomness that was given as input. Due to Enc being a non-uniform function, the correlation matrix’s entries  ∈ F2 /V⊥ are non-zero. As a result, when | pyJ (v)| = |CvH,0 | = 0 for CvEnc ,0 for v H = F ◦ Enc (some non-uniform function which maps the limited randomness r to the probed values), vJ = v and v[]\J = 0 where a single probe is placed on pyJ (v)| yJ , it is possible that this probe reveals a secret. The speciﬁc value ε = | determines the advantage of the ﬁrst-order probing adversary via Theorem 1. In other words, due to the threshold implementation using a non-uniform input,  
   
  Threshold Implementations with Non-uniform Inputs  
   
  103  
   
  Fig. 2. Depiction of the non-uniform encoder Enc masking the input (e.g., plaintext and key) k using a few uniform random bits r.  
   
  a ﬁrst-order probing attack is possible. However, we show the probability of success is limited in function of ε. In the rest of the work, we will look at trails over F (which is a uniform function) where we can pick non-zero input linear masks with certain conditions depending on how the input of the cipher was masked. We thus analyse the trails of the masked cipher ending in a single probe position with conditions on the input mask related to how it is masked. Cautionary Note. In this work, we use the piling-up principle [17,25] to obtain estimates of ε using a trail-based approach (which is often used in the ﬁeld of cryptanalysis). However, since Enc is a non-uniform (read non-balanced) function, a zero input mask will correlate to several output masks. As a result, and due to the ﬁrst S-box layer, many input masks of F from Fig. 2 are related to an output mask. As a result, the actual correlations may diﬀer from the trailbased estimates. More speciﬁcally, we make the assumption that the correlation of the probed values is determined by a trail with an outstanding value. In case a trail with high correlation is found, we can assume that there is an attack. However, we emphasise that the absence of such trails does not trivially imply that no attack is possible. For that reason, we use this piling-up principle as a heuristic to ﬁnd and verify promising non-uniform inputs for the threshold implementations. We then base ourselves on a practical veriﬁcation to analyse the promising candidates via tools like PROLEAD [19] for a noiseless veriﬁcation and via FPGA experiments for more realistic and noisy veriﬁcation.  
   
  3  
   
  Analysis of TIs with Non-uniform Inputs  
   
  We analyse threshold implementations of the Midori64 and Prince ciphers which are given non-uniform masked inputs. The threshold implementations consist of uniform functions which do not need fresh randomness for their computation. The security calculations of the non-uniform inputs are done via the cryptanalytic technique explained in Sect. 2.3. In essence, this analysis allows to bound the deviation of a masking from uniform. When this deviation is low, the masking is “almost uniform” and we can expect the masking to still be ﬁrst-order  
   
  104  
   
  S. Dhooghe and A. Ovchinnikov  
   
  secure. The analysis is mainly based on trail-based techniques where we investigate the activity patterns of the non-uniform inputs to discover whether there are weak probing points in the masked implementation. We then estimate the number of queries (or traces) a bounded-query probing adversary needs in order to get advantage one. 3.1  
   
  Masked Prince with Non-uniform Inputs  
   
  We start the analysis with a threshold implementation of the Prince cipher with a non-uniform input. We provide the details of the cipher and its masking as well as a secure and an insecure example of a non-uniform masked input. We provide the theoretical analysis of both cases. The experimental analysis is found in Sect. 4. Prince Cipher. Prince [10] is an AES-like cipher which consists of a 64-bit state divided in 4 × 4 rosters of nibbles and a 128-bit key. The S-box is a 4-bit cubic function and the linear layer consists of a MixColumns operation with a quasi-MDS matrix and a ShiftRows operation. The key schedule is simple where the ﬁrst and last whitening keys contain the ﬁrst 64-bits of the master key and the other round keys form the last 64-bits of the master key. The cipher consists of 12 nonlinear layers, where the ﬁrst half applies the S-box and the second half applies the inverse S-box. The S-box and its inverse are aﬃne equivalent. Masking Details. Consider the threshold implementation of the Prince cipher, we choose the design presented in work by Bozilov et al. [11], where the S-box is decomposed using three identical quadratic functions S = A4 ◦ Q294 ◦ A3 ◦ Q294 ◦ A2 ◦ Q294 ◦ A1 with A1 , A2 , A3 , A4 aﬃne layers and Q294 a quadratic representative of a particular aﬃne equivalence class as described by Bilgin et al. [8,9]. Since the inverse S-box of Prince used in the second part of the algorithm is an aﬃne equivalent of the regular S-box, only two additional aﬃne layers are required to implement it. The masking of the above functions is achieved via direct sharing using three shares. Similarly, the masking of Prince’s linear layer is done share-wise. Both are non-complete and uniform, and such that a register layer is placed after the only nonlinear layer, namely after Q294 . Since the cipher is implemented in parallel and calculates regular and inverse diﬀusion layers simultaneously, the chosen approach may signiﬁcantly decrease the security with non-uniform inputs against noisy-probing adversary. To reduce the probability of leakage, we add two more registers in the design. One register is added before inverse diﬀusion to disable it, as it is unnecessary until the end of ﬁfth round. The other register is added after ShiftRows operation. The details on the S-box and its inverse decomposition and maskings of its quadratic substitutions are given in Appendix A. The architecture of the masking follows a round-based design. In total, the original version needs 36 cycles and does not require fresh randomness for its  
   
  Threshold Implementations with Non-uniform Inputs  
   
  105  
   
  computation. Our modiﬁed version needs 48 clock cycles, due to the presence of new registers. The schematic of the design can be found in Figure 5 in [11]. The data path is 64 × 3 bits width indicating a masked state. Round constants and the key are XORed with the ﬁrst share of the state. The threshold implementation of the S-box has interesting cryptanalytic properties which we can use when evaluating the security of the cipher with non-uniform inputs. Namely, the masked S-box has a nontrivial upper bound on the maximum absolute correlation. Lemma 1 (Correlation of the Masked Prince S-box). Let S¯ : Va → Vb be any restriction of the sharing of the masked Prince S-box S. Denote its absolute ¯ matrix by |C S |. For any u, v ∈ F2 /V⊥ such that u = 0, it holds that correlation  ¯ S  Cu,v ≤ 2−1.41 . The above result will be used in the analysis of non-uniform inputs. For simplicity of the analysis, we keep the key as a constant (as a result, we do not need to consider trails including the key schedule). Meaning that for the theoretical and experimental analysis, we do not consider the inﬂuence of the key and instead only consider a non-uniform masking of the plaintext. We note that with this analysis, the security of the masked cipher including the key (given that the key is uniformly masked) is also included (with the possible eﬀect that the masked key improves the security due to the increased entropy). An Insecure Non-uniform Input. We ﬁrst detail the non-uniform masking of the plaintext which shows negative experimental results in PROLEAD and on FPGA as featured in Sect. 4. For the masking of the plaintext we use 32 random bits (versus 128 bits for a uniform three-sharing), and we re-use this randomness row-by-row. We depict this as follows ⎞ ⎛ r1 r2 r3 r4 ⎜r1 r2 r3 r4 ⎟ ⎟ ⎜ (1) ⎝r1 r2 r3 r4 ⎠ , r1 r2 r3 r4 with ri eight bits of (ideal) randomness. Namely, each cell in a column of the state is masked using the same randomness. We analyse the eﬀect of this non-uniform input masking using the linear cryptanalytic techniques detailed in Sect. 2.3. To recap, we use a trail based approach. To that end, we study the activity patterns through the masked Prince given the non-uniform input (or given the uniform input with the non-uniform encoder Enc from Fig. 2). When we call some parts of the state active, we mean the non-zero linear approximations are applied to those parts. We consider (masked) cells of the cipher’s state as main indicators of the activity propagation. Using the resulting trails, we ﬁnd the dependency between the probed values and the initially masked input secret. Due to the way the plaintext is shared, when activating a cell masked with ri , we have to activate at least one  
   
  106  
   
  S. Dhooghe and A. Ovchinnikov  
   
  other cell masked with the same randomness ri . This results in the constraint that each row either has no cells activated or at least two cells activated. We then analyse the resulting activity patterns. We stop the activity pattern when a single probe can cover the output activity. Namely, in Prince, this is when only one input of a single MixColumns function is active or when only one S-box is active. R1, SB  
   
  R1, MC  
   
  Fig. 3. The trail for the threshold implementation of Prince with the non-uniform input from Eq. (1) activating at most 3 masked S-boxes in the ﬁrst round. SB stands for SubCell, MC for MixColumn. The lighting indicates a single-bit probe in the active cell before it and will be omitted on the following pictures.  
   
  Considering the described approach, we can see in Fig. 3 that the input of the ﬁrst MixColumns function is already non-uniform. As a result, at most three masked S-boxes are in the trail (the branch number of Prince’s MixColumns minus one), each with a maximum absolute correlation of 2−1.41 following Lemma 1. When probing the S-box, a glitch-extended probe can view up to 16 bits due to the parallel architecture. A probe placed after MixColumns reveals 9 bits, because due to aﬃne layers, each glitch-extended probe can view several bits of cells in the ﬁrst round. Considering the bound from Theorem 1, we ﬁnd that ε ≈ 1. As a result, the non-uniformity is so high that no relevant bound can be found. Thus, this method should be insecure and, in practice, it should leak. In Sects. 4.1 and 4.2, we have implemented this case study in PROLEAD and on FPGA and observed this leakage. A Secure Non-uniform Input. The previous example already shows that badly chosen non-uniform inputs can lead to insecurities. We now detail a nonuniform masking (with the same entropy as the insecure example) of the plaintext which shows positive experimental results in PROLEAD and on FPGA as detailed in Sects. 4.1 and 4.2. For this example, we again need 32 random bits which are re-used now in a diﬀerent, row-wise, manner. We mask the plaintext as follows ⎞ ⎛ r1 r1 r1 r1 ⎜r2 r2 r2 r2 ⎟ ⎟ ⎜ (2) ⎝r3 r3 r3 r3 ⎠ , r4 r4 r4 r4 with ri eight bits of (ideal) randomness.  
   
  Threshold Implementations with Non-uniform Inputs  
   
  107  
   
  Applying the same strategy as for the insecure case, we ﬁnd that the trail activating the least number of masked S-boxes ends in an S-box operation of the round three and activates 12 masked S-boxes. The trail is depicted in Fig. 4. Round 1  
   
  Round 2  
   
  Round 3  
   
  Fig. 4. The best trail for the threshold implementation of Prince with the non-uniform input from Eq. (2) activating 12 masked S-boxes.  
   
  Considering the above trail, we calculate the advantage of a bounded-query noisy-probing adversary. A probe in a masked S-box views at most 16 bits, namely when propagating through the aﬃne layers and branching, the probe returns two shares of two clock cycles. Moreover, the best trail activates 12 masked S-boxes each with a maximum absolute correlation of 2−1.41 following Lemma 1. As a result, ε :=  pz − δ0 22 ≤ |supp pz |  pz − δ0 2∞ ≤ 216 2−33.84 = 2−17.84 . The above calculation gives the following bound on the advantage of a noisyprobing adversary.  q , Adv2-thr (A) ≤ λ216.84 where λ is the addition of noise that would be observed during practical experiments. It was mentioned in the paper of Beyne et al. that the noise that was observed during evaluation on an FPGA was bounded by λ < 29 . Given that we take around 50M ≈ 225 traces, the above bound looks to be a promising candidate to be tested. In Sect. 4, we provide PROLEAD and FPGA experimental results. Given that Prince’s MixColumns works on four cells at a time and since we require the input of this operation to be (close-to) uniform, using less than 32-bits of randomness to mask the input would likely lead to insecure designs. Nevertheless, a carefully chosen masking of the S-box and the input might allow for a further reduction in cost. Such an optimisation would be non-trivial and we leave it as an open problem. 3.2  
   
  Masked Midori64 with Non-uniform Inputs  
   
  We apply the analysis on a second case study. Namely, we investigate nonuniform inputs to a threshold implementation of the Midori64 cipher. Like in the Prince example, we use no less than 32 bits to mask the plaintext because we aim to preserve the uniformity of the ﬁrst MixColumns operation.  
   
  108  
   
  S. Dhooghe and A. Ovchinnikov  
   
  Midori64 Cipher. Midori [1] is a block cipher optimised for low-energy usage. The S-box is also used in other block ciphers including CRAFT [4] and MANTIS [3]. In this work, we speciﬁcally look at the Midori64 variant which has a 128-bit key and a 64-bit state that is split into 4-bit cells. An involutive binary quasi-MDS matrix together with a permutation of the 4-bit cells form the diﬀusion layer and it uses a 4-bit cubic S-box as the non-linear layer. Midori64 has a simple key schedule where each round either the left or right half of the master key is XORed to the state of the cipher. Masking Details. To create the ﬁrst-order secure masking of Midori64 we adopt the approach described in the work by Moradi et al. [18] with a change of the decomposition choice where we switch the quadratic class Q12 for the classes Q294 and Q299 . The architecture comprises only the encryption of the Midori64 and follows a pipelined structure as depicted in Fig. 5.  
   
  Fig. 5. Midori64 encryption round-based architecture.  
   
  Midori64 S-box is aﬃne equivalent to the cubic class C266 and can be decomposed into two quadratic bijections. The decomposition we choose for our experiments is S = A3 ◦ Q299 ◦ A2 ◦ Q294 ◦ A1 with the aﬃne functions A1 , A2 , A3 and the quadratic classes Q299 , Q294 . More details on the decomposition and its masking are given in Appendix B. The pipelined architecture implies two register stages: one placed before the S-box and another one inside of it to split the nonlinear layers. It increases the latency of the implementation; nevertheless allowing to encrypt two plaintexts at the same time. The chosen design needs 32 clock cycles to perform one encryption and does not require fresh randomness for its computation. Similar to the design of the threshold implementation of Prince, the key is considered a constant. As a result, the secure example of a non-uniform input below would also be secure in case the key is (uniformly) masked. The threshold implementation of the S-box has the following cryptanalytic property. Lemma 2 (Correlation of the Masked Midori64 S-box). Let S¯ : Va → Vb be any restriction of the masking of S defined above. Denote its absolute  
   
  Threshold Implementations with Non-uniform Inputs  
   
  109  
   
  ¯  
   
  correlation matrix by |C S |. For any u, v ∈ F2 /V⊥ such that u = 0, it holds that  S¯  Cu,v  ≤ 2−2 . An Insecure Non-uniform Input. An insecure example is designed following the same principle as for the corresponding case in Sect. 3.1 for Prince. We initialise the state with 32-bits of randomness placing them as follows ⎛ ⎞ r1 r2 r3 r4 ⎜r2 r1 r4 r3 ⎟ ⎜ ⎟ (3) ⎝r3 r4 r1 r2 ⎠ . r4 r3 r2 r1 With the above masking of the plaintext, an input to an S-box on the second round is non-uniform. The trail to such an S-box activates three cells with the same randomness because of the diﬀusion layer. To verify whether the advantage of the noisy probing adversary is high, we use the analysis from Sect. 2.3. Recall from Lemma 2 that a masked S-box has a maximum absolute correlation of 2−2 and that we activate only three of them. A probe placed in the S-box after the ﬁrst round will reveal at most 8 bits of information because of the Q299 quadratic layer. If the probe is placed after the MixColumn operation, it views at most 8 · 3 = 24 bits due to the quasi-MDS matrix and the absence of a register between Q299 and the MixColumns layer. Thus, we ﬁnd that ε ≈ 1. As a result, we expect to quickly observe leakage in this case. In Sects. 4.1 and 4.2, we have implemented this case study in PROLEAD and on FPGA and observed this leakage. A Secure Non-uniform Input. For the secure non-uniform masking of the plaintext, we need again a total of 32 random bits and reuse the randomness over the rows the same way as it was done in Sect. 3.1 for the secure non-uniform input example. We represent the masking of the plaintext as a matrix ⎞ ⎛ r1 r1 r1 r1 ⎜r2 r2 r2 r2 ⎟ ⎟ ⎜ (4) ⎝r3 r3 r3 r3 ⎠ , r4 r4 r4 r4 with ri eight bits of randomness. We analyse activity patterns for Midori64 which end in a single S-box or a single active column and which start with the constraint that a row either has no activations or at least two active cells. The best trail with these constraints activates at least 12 masked S-boxes and is depicted in Fig. 6. As we already mentioned for the insecure case, a probe placed after MixColumn can observe up to 24 wires and the masked S-box has a maximum absolute correlation of 2−2 . From the above, we ﬁnd that pz − δ0 2∞ ≤ 224 2−48 = 2−24 , ε :=  pz − δ0 22 ≤ |supp pz |   
   
  110  
   
  S. Dhooghe and A. Ovchinnikov R1, SB  
   
  R1, SC  
   
  R1, MC  
   
  R2, SB  
   
  R2, SC  
   
  R2, MC  
   
  Fig. 6. An example of the best trail for the masked Midori64 with a non-uniform input activating 12 masked S-boxes in 2 rounds (denoted R1 and R2). SB stands for SubCell, SC for ShuﬄeCell, and MC for MixColumn.  
   
  which provides the following advantage for a non-uniform input masking  q Adv2-thr (A) ≤ . λ223 Similar to Prince’s secure case, the above advantage shows the masking is a promising candidate for practical veriﬁcation. Its results are found in Sect. 4. We also tested another way to re-use the same amount of initial random bits (32-bits), namely a column-wise masking similar to the masking in Eq. (1) of Prince (which was an insecure example). The case of a column-wise masking for Midori64 provides the same security bound as for the row-wise masking which is detailed above. This case was also tested in PROLEAD and lead to a secure result. We provide these test results in Appendix C.4.  
   
  4  
   
  Practical Evaluation and Eﬃciency  
   
  In this section, we provide experimental analyses of the proposed security claims for both the Prince cipher from Sect. 3.1 and the Midori64 cipher from Sect. 3.2. The section is divided into three parts: a noiseless veriﬁcation is done using the PROLEAD tool from M¨ uller et al. [19] in Sect. 4.1, a more realistic noisy result is obtained from FPGA experiments in Sect. 4.2, and an eﬃciency analysis is done in Sect. 4.3. 4.1  
   
  PROLEAD Experiments  
   
  PROLEAD is a recently developed automated tool which allows to analyse the statistical independence of simulated intermediates probed by a robust-probing adversary following the deﬁnition by Faust et al. [12]. Among its beneﬁts are the abilities to handle full masked cipher implementations and to detect ﬂaws related to the occurrence of glitches. The tool does not require any power model as it processes only gate-level netlists.  
   
  Threshold Implementations with Non-uniform Inputs  
   
  111  
   
  We use PROLEAD to check the correctness of our assumptions about the threshold-probing adversary advantage claimed in Sects. 3.1 and 3.2 following the analysis detailed in Sect. 2.3. Since the tool does not consider noise during statistical evaluation, we omit the noise parameter λ used in the Theorem 1 (we set λ = 1). To evaluate any leakage, PROLEAD uses a likelihood ratio G-test of independence [24] as a statistical hypothesis test based on multinomial distributions. It tests the goodness of ﬁt of observed frequencies to their expected frequencies. The method is applied to contingency tables containing the distributions for all glitch-extended probing sets generated for the design. The sizes of those probing sets depend on the glitch-extended probing model which can reveal many bits under a single probe making the tables signiﬁcantly large. For example, the maximum number of probes per largest set is 32 and 392 for the Midori64 and Prince designs, respectively. PROLEAD operates in two modes, namely a normal mode and a compact mode. The diﬀerence lies in the contingency table calculation. In normal mode, the columns of the distribution tables are calculated using the concatenated values of all probed bits from the particular set that are called labels. Each label represents an entry of a contingency table where the frequency of the label’s appearance per group is stored. Thus, it is possible to acquire the tables with up to 2np columns, where np equals the number of wires within one probe in the glitch-extended probing set. In compact mode, the labels for the distribution tables are provided based on the Hamming weight of a probing set which allows for a more computation friendly veriﬁcation at the cost of accuracy. To summarise, the normal mode provides more accurate information on the observed leakage, whereas it is possible to conduct more experiments for larger inputs in the compact mode. We begin the evaluation process with checking our implementations in a compact mode to verify their security against ﬁrst-order glitch-extended probing adversaries when uniform randomness is used. We perform up to 100M simulations. The results for those and the following experiments are shown in Table 1. We ﬁnd that the implementations with uniform inputs do not leak during the simulations as would be expected. Excerpts with the results from the PROLEAD reports can be found in Appendix C. Then, we use PROLEAD to evaluate leakage for the same designs with nonuniform inputs. Since the tool is only designed to create uniform maskings of the input, we add an intermediate module to our design, that is excluded from the probing list, to reuse generated randomness in composition of the non-uniform shares. For the cases with “insecure” inputs, the tool shows immediate3 leakage signiﬁcantly exceeding the threshold for the statistical tests in both modes which  
   
  2  
   
  3  
   
  The size of the largest set may include control logic wires of the algorithm (like counter, start, and select). These bits do not contribute to the advantage of the noisy threshold-probing adversary. Simulations in PROLEAD are split into several iterations with a chosen step. Here, we set a step of 1M and 128k traces for the compact and normal modes, respectively.  
   
  112  
   
  S. Dhooghe and A. Ovchinnikov  
   
  is in line with the theoretical analyses in Sect. 3. Moreover, the leakage is observed only in some particular rounds of the ciphers (see Table 1). Finally, we test the “secure” non-uniform designs. Again, we observe growing leakage during the simulations. However, this time the threshold is achieved only after signiﬁcantly more experiments, and the leakage is detected during the later rounds in accordance with our trails from Sect. 3. We pay closer attention to the tests in normal mode, since those are more related to a threshold-probing adversary model, and see the leakage again. This time the number of traces needed to conclude the insecurity of a design is closely related to the bounds we proposed considering the absence of noise (λ = 1). Table 1. Results of PROLEAD experiments of the Prince masking detailed in Sect. 3.1 and of the Midori64 cipher from Sect. 3.2. We detail after how many experiments PROLEAD ﬁnds leakage and in which clock cycles the leakage occurs (including two cycles to initialise the cipher) and in which round of the cipher the leakage occurs. Cipher  
   
  Case  
   
  Mode  
   
  Prince  
   
  Uniform  
   
  compact ✓  
   
  Passed #Traces #Cycle  
   
  “Insecure” Non-Uniform compact ✗  
   
  NA  
   
  4, ..., 10 1, 2, 3  
   
  48M  
   
  10  
   
  3  
   
  ✗  
   
  3.8M  
   
  10  
   
  3  
   
  compact ✓  
   
  100M  
   
  NA  
   
  NA  
   
  1M  
   
  5,6,7  
   
  2,3  
   
  128k  
   
  5,6,7  
   
  2,3  
   
  normal  
   
  ✗  
   
  compact ✗ normal  
   
  4.2  
   
  4, ..., 10 1, 2, 3  
   
  128k  
   
  “Insecure” Non-Uniform compact ✗ “Secure” Non-Uniform  
   
  1M  
   
  ✗  
   
  normal Midori64 Uniform  
   
  NA  
   
  compact ✗  
   
  normal “Secure” Non-Uniform  
   
  #Round  
   
  100M  
   
  ✗  
   
  2M  
   
  7  
   
  3  
   
  6.4M  
   
  8  
   
  3  
   
  FPGA Experiments  
   
  For the practical experiments, we used a Xilinx Spartan-6 FPGA on a SAKURAG evaluation board [14] and supplied the device with a stable 6.144 MHz clock. For the randomness generation, we focus on providing comprehensive results whether randomness can be re-used when masking the input of the cipher. As a result, we use a heavy cryptographic random number generator, namely one based on AES-128, to ensure the limited randomness given to the threshold implementation is of good quality such that it does not bias our results.  
   
  Threshold Implementations with Non-uniform Inputs  
   
  113  
   
  We collect power consumption traces by monitoring the voltage drop over a 1Ω shunt resistor placed on the VDD path of the target FPGA and using a digital oscilloscope at a sampling rate of 500 MS/s. For the measurement’s analysis, we follow TVLA requirements [2] to conduct ﬁrst-order ﬁxed vs. random plaintext t-tests. The encryption is performed up to 50M times receiving either masked ﬁxed or masked random plaintexts. This technique allows to detect the ﬁrst-order side-channel leakage without implementing an actual key-recovery attack. We perform an analysis for each example introduced in Sect. 3. To do the ﬁrst-order t-test, we collect 4000 and 3000 sample points (the number of points that comprise a complete waveform record, determined by the amount of data that can be captured by an oscilloscope.) for the Prince and Midori64 implementations, respectively. The corresponding results are shown in Figs. 7 and 8 including the absolute t-value changes through the number of traces sampled. From Fig. 7, we observe that the threshold implementation of Prince with a uniform input is secure (as expected). As a sanity check, we also evaluated the implementation with the random number generator turned oﬀ to ﬁnd that the implementation leaks (again, as expected). The “insecure” non-uniform input leaks almost immediately and we see a spike of leakage each cycle of the implementation. However, the “secure” non-uniform input does not leak even at 50M traces. Together with the theoretical analysis and the PROLEAD analysis, we can conclude that it is viable to use the threshold implementation of Prince with such a non-uniform input. From Fig. 8, we observe similar results for the threshold implementation of the Midori64 cipher. It is interesting to note that the “insecure” non-uniform input only leaks in the second round of the cipher, but then becomes secure. This can be explained from the strong cryptanalytic properties of its diﬀusion layer and its masked S-box. As the computation of the cipher continues, the non-uniform randomness is processed more-and-more via a good cryptographic function making it less-and-less distinguishable from uniform random. We note that Prince’s behaviour with the “insecure” non-uniform input should be the same, but that due to the multiplexers in the architecture the same leakage from the ﬁrst round is repeated each cycle. Again, the “secure” non-uniform input shows no leakage in the t-test showing that it is possible to work with the threshold implementation without giving a full entropy input. 4.3  
   
  Eﬃciency Comparison  
   
  We quickly provide the eﬃciency measures of the threshold implementations of the Prince and Midori64 ciphers from Sects. 3.1 and 3.2 though we note that we worked with tried-and-tested threshold implementations and that their eﬃciency was not the goal of the work. Table 2 provides the cost in terms of area, latency, and total random bits all designs use. We note that while we report on the randomness cost in bits, we have not investigated the related cost of the RNG which generates this randomness.  
   
  114  
   
  S. Dhooghe and A. Ovchinnikov  
   
  Fig. 7. Masked Prince implementation with 50M traces where we test the implementation with uniform randomness and the non-uniform case studies from Sect. 3.1. We show sample traces with masks oﬀ and masks on and we show the ﬁnal t-test together with the maximum absolute t-test value evolution over the number of traces.  
   
  For the Midori64 case, we ﬁnd that the non-uniform input allows a four times reduction of the total randomness cost without requiring a trade-oﬀ in area or latency. However, the Prince masking requires an extra register layer in order to have a suﬃcient theoretical bound. We have performed leakage tests with a non-uniform input without this extra layer and have seen secure results. Thus, we believe the cheaper Prince masking with the non-uniform input can still be used in practice and that the discrepancy comes from the theoretical framework’s overestimation on glitches, similar to the observations from [6] on glitches in a masked AES S-box.  
   
  Threshold Implementations with Non-uniform Inputs  
   
  115  
   
  Fig. 8. Masked Midori64 implementation with 50M traces where we test the implementation with uniform randomness and the case studies from Sect. 3.2. We show sample traces with masks oﬀ and masks on and we show the ﬁnal t-test together with the maximum absolute t-test value evolution over the number of traces. Table 2. The eﬃciency measures in area, latency, and total randomness cost of both the Prince and Midori64 threshold implementations using uniform or non-uniform randomness with the NANGATE 45nm Open Cell Library [20].  
   
  5  
   
  Cipher  
   
  #Shares Area (GE) Latency (Cycles) Rand. (Bits)  
   
  Prince  
   
  3  
   
  8353 11050  
   
  36 48  
   
  128 32  
   
  Midori64 3  
   
  7324 7324  
   
  32 32  
   
  128 32  
   
  Conclusion  
   
  In this paper, we have shown that using a non-uniform masked input for a threshold implementation can remain ﬁrst-order secure in face of a practical evaluation. We have also shown that the non-uniform masking itself should be chosen carefully and diﬀerently for each symmetric primitive following the prin-  
   
  116  
   
  S. Dhooghe and A. Ovchinnikov  
   
  ciples from this paper’s security framework based on linear cryptanalysis and the noisy probing model. We presented secure and insecure examples of non-uniform initial maskings for established threshold implementations of the Prince and Midori64 ciphers. We veriﬁed the examples using the PROLEAD tool and in practice by implementing them on FPGA. The vulnerabilities of the insecure examples were quickly detected and its leakage coincided with the provided bounds from the theoretical analyses. The secure examples did not show any leakage up to 50 million traces which shows that we can securely reduce the entropy of the initial masking four times over. While the scope of this paper was limited to ﬁrst-order secure implementations, we pose the open problem of continuing the investigation of the security of higher-order threshold implementations with a non-uniform masked input. Moreover, we pose the important problem of creating tools to automate this paper’s security analysis such that more complex examples, like using LFSRs to generate the initial masking, can be investigated. Acknowledgements. We thank Tim Beyne for the interesting discussions. This work was supported by CyberSecurity Research Flanders with reference number VR20192203.  
   
  A  
   
  Prince S-Box Masking  
   
  The Prince S-box has its lookup table [B, F, 3, 2, A, C, 9, 1, 6, 7, 8, 0, E, 5, D, 4] and is decomposed as follows S = A1 ◦ Q294 ◦ A2 ◦ Q294 ◦ A3 ◦ Q294 ◦ A4 . The inverse S-box has its lookup table [B, 7, 3, 2, F, D, 8, 9, A, 6, 4, 0, 5, E, C, 1] and is decomposed as S −1 = A5 ◦ Q294 ◦ A2 ◦ Q294 ◦ A3 ◦ Q294 ◦ A6 . The inputs of the following 4-bit functions are designated with a nibble (x, y, z, w), where x is the most signiﬁcant bit and w s the least signiﬁcant bit. A1 = [1 + x + z; 1 + y; z + w; z] A2 = [x + z + w; 1 + x + z; 1 + y + z + w; x + y + z + w] A3 = [w; z; y; x] A4 = [1 + x + y + z + w; x; 1 + x + z + w; w] A5 = [1 + y; x + z; 1 + y + w; 1 + z + w] A6 = [1 + x + w; y; w; 1 + z],  
   
  Threshold Implementations with Non-uniform Inputs  
   
  117  
   
  with A1 = [C, E, 7, 5, 8, A, 3, 1, 4, 6, F, D, 0, 2, B, 9] A2 = [6, D, 9, 2, 5, E, A, 1, B, 0, 4, F, 8, 3, 7, C] A3 = [0, 8, 4, C, 2, A, 6, E, 1, 9, 5, D, 3, B, 7, F ] A4 = [A, 1, 0, B, 2, 9, 8, 3, 4, F, E, 5, C, 7, 6, D] A5 = [B, 8, E, D, 1, 2, 4, 7, F, C, A, 9, 5, 6, 0, 3] A6 = [9, 3, 8, 2, D, 7, C, 6, 1, B, 0, A, 5, F, 4, E]. The above aﬃne functions are masked share-by-share. The quadratic layer Q294 = [a, b, c, d] = [x, y, z + xy, w + xz] is masked as follows ai = xi bi = y i ci = z i + xi y i + xi y i+1 + xi+1 y i di = wi + xi z i + xi z i+1 + xi+1 z i , for the shares i ∈ {0, 1, 2}, where the convention is used that superscripts wrap around at two. The above masking is uniform and non-complete.  
   
  B  
   
  Midori64 S-Box Masking  
   
  The Midori64 S-box has its lookup table [C, A, D, 3, E, B, F, 7, 8, 9, 1, 5, 0, 2, 4, 6] and is decomposed as S = A1 ◦ Q299 ◦ A2 ◦ Q294 ◦ A3 . The inputs of the following 4-bit functions are designated with a nibble (x, y, z, w), where x is the most signiﬁcant bit and w s the least signiﬁcant bit. A1 = [1 + x + y + z; 1 + x + y + w; 1 + x + y + z + w; y + w] A2 = [w; x; y; z] A3 = [1 + y + w; 1 + y + z + w; w; x + z + w], with A1 = [E, 9, 4, 3, 1, 6, B, C, 0, 7, A, D, F, 8, 5, 2] A2 = [0, 8, 1, 9, 2, A, 3, B, 4, C, 5, D, 6, E, 7, F ] A3 = [C, 3, 9, 6, 0, F, 5, A, D, 2, 8, 7, 1, E, 4, B].  
   
  118  
   
  S. Dhooghe and A. Ovchinnikov  
   
  The above aﬃne functions are masked share-by-share. The quadratic layer Q294 = [a, b, c, d] = [x, y, z + xy, w + xz] is masked as follows ai−1 = xi bi−1 = y i ci−1 = z i + xi y i + xi y i+1 + xi+1 y i di−1 = wi + xi z i + xi z i+1 + xi+1 z i , and the quadratic layer Q299 = [a, b, c, d] = [x, y + xy + xz, z + xy + xz + xw, w + xy + xw] is masked as ai−1 = xi bi−1 = y i + (xi y i + xi y i+1 + xi+1 y i ) + (xi z i + xi z i+1 + xi+1 z i ) ci−1 = z i + (xi y i + xi y i+1 + xi+1 y i ) + (xi z i + xi z i+1 + xi+1 z i ) + (xi wi + xi wi+1 + xi+1 wi ) di−1 = wi + (xi y i + xi y i+1 + xi+1 y i ) + (xi wi + xi wi+1 + xi+1 wi ). for the shares i ∈ {0, 1, 2}, where the convention is used that superscripts wrap around at two. The above maskings are uniform and non-complete.  
   
  C  
   
  PROLEAD Experiments  
   
  The PROLEAD tool outputs data in two formats: tables with an overview of the current status of the evaluation process and reports for each simulation step. We further provide shortened versions of the tables for our experiments and add them with the overview of some probing sets from the reports. In the following tables, we will write probe sets (such as 00480 [35] (7)). Such sets include several register values from the implementation together with the cycle of the computation (in this example seven). C.1  
   
  Designs with Uniform Randomness  
   
  (See Tables 3, 4 and 5). Table 3. Evaluation info: Uniform randomness. Cipher  
   
  #Standard Probes  
   
  #Extended Security Probes Order  
   
  #Probing Sets  
   
  Maximum #Probes per Set  
   
  Prince  
   
  17600  
   
  20880  
   
  1  
   
  9080  
   
  101  
   
  Midori64 50880  
   
  29120  
   
  1  
   
  14480  
   
  32  
   
  Threshold Implementations with Non-uniform Inputs  
   
  119  
   
  Table 4. Evaluation results: Prince, uniform case. Elapsed Time (s)  
   
  Required RAM (GB)  
   
  Processed Simulations  
   
  101 202 304 406  
   
  26.23 26.23 26.23 26.23  
   
  1000000 2000000 3000000 4000000  
   
  9655 9751  
   
  26.23 26.23  
   
  99000000 100000000  
   
  Probe Set with −log10(p) Highest Information Leakage 03951 04211 04081 04211  
   
  [61] [21] [32] [39]  
   
  Status  
   
  (18) (22) (6) (27)  
   
  4.887129 7.141349 5.383576 3.671223  
   
  OKAY LEAKAGE LEAKAGE OKAY  
   
  03951 [53] (40) 04081 [19] (1)  
   
  4.154447 4.118873  
   
  OKAY OKAY  
   
  Table 5. Evaluation results: Midori64, uniform case. Elapsed Time (s)  
   
  Required RAM (GB)  
   
  Processed Probe Set with Simulations Highest Information Leakage  
   
  89 179 269  
   
  33.32 33.32 33.32  
   
  1000000 2000000 3000000  
   
  8560 8646  
   
  33.32 33.32  
   
  99000000 100000000  
   
  C.2  
   
  Sbox[15].register.in[5] (20) ciphertext 1[63] (9) Sbox[7].register.in[1] (4) 00226 [19] (1) 00226 [19] (1)  
   
  −log10(p) Status 4.004151 2.891629 3.397437  
   
  OKAY OKAY OKAY  
   
  4.151201 4.091332  
   
  OKAY OKAY  
   
  Midori64, Non-uniform Randomness  
   
  (See Tables 6, 7 and 8). Table 6. Evaluation info: Midori64, non-uniform cases. Case  
   
  Mode  
   
  “Insecure” compact normal “Secure” compact normal  
   
  #Stand. #Extend. #Probe Probes Probes Sets  
   
  Maximum #Probes per Set  
   
  43200 10800 43200 10800  
   
  32  
   
  29100 7280 29120 7280  
   
  14480 3620 14480 3620  
   
  Table 7. Evaluation results: Midori64, “insecure”, normal mode. Elapsed Time (s)  
   
  Required Processed Probe Set with −log10(p) Status RAM (GB) Simulations Highest Information Leakage  
   
  34  
   
  8.24  
   
  128000/ 438057  
   
  00208 [63] (5)  
   
  inf  
   
  LEAKAGE  
   
  457  
   
  45.70  
   
  1280000/ 1346575  
   
  00208 [63] (5)  
   
  inf  
   
  LEAKAGE  
   
  120  
   
  S. Dhooghe and A. Ovchinnikov Table 8. Evaluation results: Midori64, “secure”, normal mode.  
   
  Elapsed Time (s)  
   
  Required Processed Probe Set with −log10(p) Status RAM (GB) Simulations Highest Information Leakage  
   
  33  
   
  9.21  
   
  73  
   
  13.74  
   
  4318  
   
  273.38  
   
  4473  
   
  282.42  
   
  C.3  
   
  128000/ 438083 256000/ 615365  
   
  Sbox[7].register.in[5] 3.912678 (6) Sbox[14].register.in[5] 3.354810 (10)  
   
  OKAY  
   
  6272000/ 2767077 6400000/ 2790153  
   
  Sbox[9].register.in[4] 4.880834 (8) Sbox[9].register.in[4] 5.094253 (8)  
   
  OKAY  
   
  OKAY  
   
  LEAKAGE  
   
  Prince, Non-uniform Randomness  
   
  (See Tables 9, 10 and 11) Table 9. Evaluation info: Prince, non-uniform cases. Case  
   
  Mode  
   
  “Insecure” compact normal “Secure” compact normal  
   
  #Stand. #Extend. Probes Probes  
   
  #Probe Sets  
   
  Maximum #Probes per Set  
   
  16500 7425 16500 7425  
   
  8380 3771 8380 3771  
   
  39  
   
  14300 6435 14300 6435  
   
  Table 10. Evaluation results: Prince, “insecure”, normal mode. Elapsed Time (s)  
   
  Required Processed Probe Set with −log10(p) Status RAM (GB) Simulations Highest Information Leakage  
   
  46  
   
  10.26  
   
  128000/ 293647  
   
  beta reg 3.in[22](4)  
   
  inf  
   
  LEAKAGE  
   
  276  
   
  45.70  
   
  768000/ 316035  
   
  beta reg 3.in[22](4)  
   
  inf  
   
  LEAKAGE  
   
  Threshold Implementations with Non-uniform Inputs  
   
  121  
   
  Table 11. Evaluation results: Prince, “secure”, normal mode. Elapsed Time (s)  
   
  Required Processed Probe Set with −log10(p) Status RAM (GB) Simulations Highest Information Leakage  
   
  48  
   
  11.05  
   
  95  
   
  11.79  
   
  1385  
   
  11.99  
   
  3968000/ 316035  
   
  1431  
   
  11.99  
   
  4096000/ 316035  
   
  C.4  
   
  128000/ 293811 256000/ 313129  
   
  05111 (4)  
   
  3.485201  
   
  OKAY  
   
  3.070294  
   
  OKAY  
   
  04193 [16] (10)  
   
  5.983958  
   
  LEAKAGE  
   
  04193 [17] (10)  
   
  5.668399  
   
  LEAKAGE  
   
  beta reg 2.in[53] (6)  
   
  Midori64, “Secure” Non-uniform Case, Column-Wise  
   
  (See Tables 12, 13 and 14) Table 12. Evaluation info: Midori64, column-wise. Case  
   
  Mode  
   
  #Stand. Probes  
   
  Column- compact 43200 wise normal 6480  
   
  #Extend. Probes  
   
  #Probe Sets  
   
  Maximum #Probes per Set  
   
  29120 4368  
   
  14480 2172  
   
  32  
   
  Table 13. Evaluation results: Midori64, column-wise, compact mode. −log10(p) Status  
   
  Elapsed Time (s)  
   
  Required RAM (GB)  
   
  Processed Simulations  
   
  Probe Set with Highest Information Leakage  
   
  94  
   
  33.34  
   
  1000000  
   
  ciphertext 1[35] (27)  
   
  4.696523  
   
  OKAY  
   
  187  
   
  33.34  
   
  2000000  
   
  Sbox[3].register.in[1] (20)  
   
  3.945218  
   
  OKAY  
   
  9222  
   
  33.34  
   
  101000000  
   
  Sbox[7].register.in[9] (8)  
   
  4.789482  
   
  OKAY  
   
  9312  
   
  33.34  
   
  102000000  
   
  Sbox[7].register.in[9] (8)  
   
  5.164452  
   
  LEAKAGE  
   
  13539  
   
  33.34  
   
  150000000  
   
  8.223624  
   
  LEAKAGE  
   
  00480 [35] (7)  
   
  122  
   
  S. Dhooghe and A. Ovchinnikov Table 14. Evaluation results: Midori64, column-wise, normal mode.  
   
  Elapsed Time (s)  
   
  Required Processed Probe Set with RAM (GB) Simulations Highest Information Leakage  
   
  21  
   
  7.71  
   
  46  
   
  10.60  
   
  6551  
   
  238.73  
   
  6712  
   
  239.51  
   
  128000/ 438075 256000/ 615407 9984000/ 3316723 10112000/ 3332201  
   
  −log10(p) Status  
   
  Sbox[9].register.in[1] (8) 2.371509  
   
  OKAY  
   
  00480 [14] (3)  
   
  2.493538  
   
  OKAY  
   
  00220 [61] (3)  
   
  3.992771  
   
  OKAY  
   
  00220 [61] (3)  
   
  3.959946  
   
  OKAY  
   
  References 1. Banik, S., et al.: Midori: a block cipher for low energy. In: Iwata, T., Cheon, J.H. (eds.) ASIACRYPT 2015. LNCS, vol. 9453, pp. 411–436. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48800-3 17 2. Becker, G.T., et al.: Test vector leakage assessment (TVLA) methodology in practice (2013) 3. Beierle, C., et al.: The SKINNY family of block ciphers and its low-latency variant MANTIS. In: Robshaw, M., Katz, J. (eds.) CRYPTO 2016. LNCS, vol. 9815, pp. 123–153. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-5300855 4. Beierle, C., Leander, G., Moradi, A., Rasoolzadeh, S.: CRAFT: lightweight tweakable block cipher with eﬃcient protection against DFA attacks. IACR Trans. Symmetric Cryptol. 2019(1), 5–45 (2019) 5. Beyne, T., Dhooghe, S., Moradi, A., Shahmirzadi, A.R.: Cryptanalysis of eﬃcient masked ciphers: applications to low latency. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(1), 679–721 (2022) ˇ ci´c, D.: A low-randomness second-order 6. Beyne, T., Dhooghe, S., Ranea, A., Sijaˇ masked AES. In: AlTawy, R., H¨ ulsing, A. (eds.) SAC 2021. LNCS, vol. 13203, pp. 87–110. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-99277-4 5 7. Beyne, T., Dhooghe, S., Zhang, Z.: Cryptanalysis of masked ciphers: a not so random idea. In: Moriai, S., Wang, H. (eds.) ASIACRYPT 2020. LNCS, vol. 12491, pp. 817–850. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-64837-4 27 8. Bilgin, B., Nikova, S., Nikov, V., Rijmen, V., St¨ utz, G.: Threshold implementations of all 3 × 3 and 4 × 4 S-boxes. In: Prouﬀ, E., Schaumont, P. (eds.) CHES 2012. LNCS, vol. 7428, pp. 76–91. Springer, Heidelberg (2012). https://doi.org/10.1007/ 978-3-642-33027-8 5 9. Bilgin, B., Nikova, S., Nikov, V., Rijmen, V., St¨ utz, G.: Threshold implementations of all 3 × 3 and 4 × 4 s-boxes. IACR Cryptology ePrint Archive, p. 300 (2012). https://eprint.iacr.org/2012/300 10. Borghoﬀ, J., et al.: PRINCE – a low-latency block cipher for pervasive computing applications. In: Wang, X., Sako, K. (eds.) ASIACRYPT 2012. LNCS, vol. 7658, pp. 208–225. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-349614 14  
   
  Threshold Implementations with Non-uniform Inputs  
   
  123  
   
  11. Bozilov, D., Knezevic, M., Nikov, V.: Optimized threshold implementations: securing cryptographic accelerators for low-energy and low-latency applications. J. Cryptogr. Eng. 12(1), 15–51 (2022). https://doi.org/10.1007/s13389-021-002765 12. Faust, S., Grosso, V., Pozo, S.M.D., Paglialonga, C., Standaert, F.: Composable masking schemes in the presence of physical defaults and the robust probing model. IACR Cryptology ePrint Archive, p. 711 (2017). https://eprint.iacr.org/2017/711 13. Faust, S., Grosso, V., Pozo, S.M.D., Paglialonga, C., Standaert, F.: Composable masking schemes in the presence of physical defaults & the robust probing model. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2018(3), 89–120 (2018) 14. Guntur, H., Ishii, J., Satoh, A.: Side-channel attack user reference architecture board SAKURA-G. In: IEEE 3rd Global Conference on Consumer Electronics, GCCE 2014, Tokyo, Japan, 7–10 October 2014, pp. 271–274. IEEE (2014). https:// doi.org/10.1109/GCCE.2014.7031104 15. Ishai, Y., Sahai, A., Wagner, D.: Private circuits: securing hardware against probing attacks. In: Boneh, D. (ed.) CRYPTO 2003. LNCS, vol. 2729, pp. 463–481. Springer, Heidelberg (2003). https://doi.org/10.1007/978-3-540-45146-4 27 16. Kocher, P., Jaﬀe, J., Jun, B.: Diﬀerential power analysis. In: Wiener, M. (ed.) CRYPTO 1999. LNCS, vol. 1666, pp. 388–397. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48405-1 25 17. Matsui, M.: Linear cryptanalysis method for DES cipher. In: Helleseth, T. (ed.) EUROCRYPT 1993. LNCS, vol. 765, pp. 386–397. Springer, Heidelberg (1994). https://doi.org/10.1007/3-540-48285-7 33 18. Moradi, A., Schneider, T.: Side-channel analysis protection and low-latency in action. In: Cheon, J.H., Takagi, T. (eds.) ASIACRYPT 2016. LNCS, vol. 10031, pp. 517–547. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-538876 19 19. M¨ uller, N., Moradi, A.: PROLEAD: a probing-based hardware leakage detection tool. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(4), 311–348 (2022). https://doi.org/10.46586/tches.v2022.i4.311-348 20. NANGATE: The NanGate 45 nm Open Cell Library, version: PDKv1.3 v2010 12.Apache.CCL. https://github.com/The-OpenROAD-Project/OpenROAD-ﬂowscripts/tree/master/ﬂow/platforms/nangate45 21. Nikova, S., Rechberger, C., Rijmen, V.: Threshold implementations against sidechannel attacks and glitches. In: Ning, P., Qing, S., Li, N. (eds.) ICICS 2006. LNCS, vol. 4307, pp. 529–545. Springer, Heidelberg (2006). https://doi.org/10. 1007/11935308 38 22. Shahmirzadi, A.R., Moradi, A.: Re-consolidating ﬁrst-order masking schemes nullifying fresh randomness. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2021(1), 305–342 (2021). https://doi.org/10.46586/tches.v2021.i1.305-342 23. Shahmirzadi, A.R., Moradi, A.: Second-order SCA security with almost no fresh randomness. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2021(3), 708–755 (2021). https://doi.org/10.46586/tches.v2021.i3.708-755 24. Sokal, R., Rohlf, F.: Biometry: The Principles and Practice of Statistics in Biological Research. W. H. Freeman (1981). https://books.google.be/books?id=COTQgAACAAJ 25. Tardy-Corfdir, A., Gilbert, H.: A known plaintext attack of FEAL-4 and FEAL-6. In: Feigenbaum, J. (ed.) CRYPTO 1991. LNCS, vol. 576, pp. 172–182. Springer, Heidelberg (1992). https://doi.org/10.1007/3-540-46766-1 12  
   
  Post-Quantum Constructions  
   
  SMAUG: Pushing Lattice-Based Key Encapsulation Mechanisms to the Limits Jung Hee Cheon1,2 , Hyeongmin Choe1(B) , Dongyeon Hong2 , and MinJune Yi1 1  
   
  Seoul National University, Seoul, South Korea {jhcheon,sixtail528,yiminjune}@snu.ac.kr 2 CryptoLab Inc., Seoul, South Korea  
   
  Abstract. Recently, NIST has announced Kyber, a lattice-based key encapsulation mechanism (KEM), as a post-quantum standard. However, it is not the most eﬃcient scheme among the NIST’s KEM ﬁnalists. Saber enjoys more compact sizes and faster performance, and Mera et al. (TCHES ’21) further pushed its eﬃciency, proposing a shorter KEM, Sable. As KEM are frequently used on the Internet, such as in TLS protocols, it is essential to achieve high eﬃciency while maintaining suﬃcient security. In this paper, we further push the eﬃciency limit of lattice-based KEMs by proposing SMAUG, a new post-quantum KEM scheme whose IND-CCA2 security is based on the combination of MLWE and MLWR problems. We adopt several recent developments in lattice-based cryptography, targeting the smallest and the fastest KEM while maintaining high enough security against various attacks, with a full-ﬂedged use of sparse secrets. Our design choices allow SMAUG to balance the decryption failure probability and ciphertext sizes without utilizing error correction codes, whose side-channel resistance remains open. With a constant-time C reference implementation, SMAUG achieves ciphertext sizes up to 12% and 9% smaller than Kyber and Saber, with much faster running time, up to 103% and 58%, respectively. Compared to Sable, SMAUG has the same ciphertext sizes but a larger public key, which gives a trade-oﬀ between the public key size versus performance; SMAUG has 39%–55% faster encapsulation and decapsulation speed in the parameter sets having comparable security. Keywords: Key Encapsulation Mechanism · Public Key Encryption · Post Quantum Cryptography · Module Learning With Errors · Module Learning With Roundings  
   
  1  
   
  Introduction  
   
  Recent advances in quantum computers raise the demand for quantum-resistant cryptographic protocols, i.e., the Post-Quantum Cryptographic (PQC) schemes. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 127–146, 2024. https://doi.org/10.1007/978-3-031-53368-6_7  
   
  128  
   
  J. H. Cheon et al.  
   
  As a consequence, the American National Institute of Standards and Technology (NIST) has established a standardization process focusing on Public Key Encryption (PKE), digital signature, and Key Encapsulation Mechanism (KEM). In particular, KEM is one of the most widely used algorithms over the Internet, such as in Transport Layer Security (TLS) protocols; however, the KEM currently used in the protocol is considered vulnerable to quantum attacks. Various lattice-based KEMs [4,10,14,15,19,24,43,50] have been proposed and submitted to NIST PQC standardization to secure the Internet in the quantum world. During the standardization process, diverse techniques improving eﬃciency or security were introduced. In 2020, NIST selected Kyber [14], Saber [50], and NTRU [19] as the lattice-based KEM ﬁnalists, having enough eﬃciency and quantum security based on the Module Learning With Errors (MLWE), Module Learning With Roundings (MLWR), and NTRU problems, respectively. Recently Kyber was selected as a future standard among the candidates. As of independent interest to NIST’s standardization, the KEM’s eﬃciency is crucial since it is executed and transmitted frequently on the Internet. In particular, the TLS protocols are also necessary for embedded devices, so the eﬃciency requirement has become even more pressing with the proliferation of the Internet of Things (IoT). To this end, some variants of Saber focusing on eﬃciency, Scabbard [46], have been recently proposed by Mera et al. Scabbard consists of three schemes based on the Ring Learning With Roundings (RLWR) or MLWR problems, Floreta, Espada, and Sable, each targeting HW/SW-eﬃcient, parallelizable, and shorter KEM than Saber. In particular, Sable achieves the smallest public key and ciphertext sizes among the KEM schemes targetting the NIST’s security level 1 and low enough Decryption Failure Probability (DFP). 1.1  
   
  Our Results  
   
  In this work, we ask: Can we further push the eﬃciency of the lattice-based KEMs to the limits? Speciﬁcally, we propose a new lattice-based KEM, SMAUG, constructed based on both MLWE and MLWR problems. By bringing the MLWE-based public key generation and the sparse secret to Sable, SMAUG can enhance eﬃciency further. We aimed to achieve the shortest ciphertext among the LWE/LWR-based KEM schemes while maintaining the security level with even better performance. The SMAUG. The design rationale of SMAUG aims to achieve small ciphertext and public key with low computational cost while maintaining security against various attacks. In more detail, we target the following practicality and security requirements considering real applications: Practicality • Both the public key and ciphertext must be short to minimize communication costs. We especially focus on the ciphertext size as it is more frequently transmitted.  
   
  SMAUG  
   
  129  
   
  • The key exchange protocol is frequently required on various personal devices, so a KEM algorithm with low computational costs is desirable. • A compact secret key is beneﬁcial in restricted settings such as embedded or IoT devices. Maintaining a secure zone is imperative to thwart physical attacks on the storage storing secret keys. Security • The shared key should have a large enough entropy, at least ≥ 256 bits, to prevent Grover’s search [35]. • Security should be concretely guaranteed concerning the attacks on the underlying assumptions. • The low enough DFP is essential to avoid the attacks boosting the failure and exploiting the decryption failures [26,40]. • As KEMs are widely used in various devices and systems, countermeasures against implementation-speciﬁc attacks should also be considered. Especially combined with DFP, using Error Correction Codes (ECC) on the message to reduce decryption failures should be avoided since masking ECC against side-channel attacks remains a challenging problem (Table 1). Table 1. Parameter sets of SMAUG for NIST’s security levels 1, 3, and 5. Security (Sec.) is given in classical core-SVP hardness. One core of Intel Core i7-10700k is used for cycle counts. Scheme  
   
  Sizes (bytes) sk pk ct  
   
  SMAUG-128 176 672  
   
  672  
   
  Security Cycle Lvl. Sec. DFP KeyGen Encap Decap 1  
   
  120 2−120 77k −136  
   
  77k  
   
  92k  
   
  SMAUG-192 236 1088 1024 3  
   
  181 2  
   
  153k  
   
  136k  
   
  163k  
   
  SMAUG-256 218 1792 1472 5  
   
  264 2−167 266k  
   
  270k  
   
  305k  
   
  To achieve this goal, we exploit the possible combination of the known techniques in lattice-based cryptography, such as underlying lattice assumptions, ciphertext compression, Fujisaki-Okamoto (FO) transforms, and the secret and error distribution. Among the possibilities on the choice of lattice assumptions, we conclude to use LWE-based key generation and LWR-based encapsulation with sparse secrets in module lattices. This choice allows SMAUG to enjoy the conservative secret key security from the hardness of the un-rounded MLWE problem and explore eﬃciency on encapsulation and decapsulation from the MLWR-based approach. Sparse secret allows SMAUG to enjoy fast polynomial multiplications and small secret keys. The sparse secret is widely used in homomorphic encryption (HE) schemes to speed up the expensive homomorphic operations and to reduce the noise [22,36], whose ability is attractive for eﬃcient KEMs. By using the SampleInBall algorithm of Dilithium [29], we can eﬃciently sample the sparse  
   
  130  
   
  J. H. Cheon et al.  
   
  ternary secrets. Regarding security, the hardness reductions for sparse LWE and LWR problems [23,24] from LWE problem exists; however, the concrete security should be treated carefully1 . The MLWE-based public key of SMAUG uses the discrete Gaussian noise, which is scaled and approximated. While this approximation may result in a security loss compared to the ideal case, we can eﬃciently upper-bound with a small scaling factor due to R´enyi divergence. By utilizing the logic minimization technique SMAUG uses a boolean discrete Gaussian sampler. We take the recent approaches in FO transform for key exchange in the Quantum Random Oracle Model (QROM) [39] and apply it to our IND-CPA PKE, SMAUG.PKE. We use the FO transform with implicit rejection and no ciphertext contributions (FO⊥ m ). We delicately choose three parameter sets for SMAUG regarding NIST’s security levels 1, 3, and 5 (classical core-SVP hardness of 120, 180, and 260, respectively) and having DFP at least smaller than Saber. Comparison to Other KEMs. We compare SMAUG with Kyber, Saber, and its variant Sable in Table 2. Table 2. Comparison of recent KEM schemes. Security is given in the classical coreSVP hardness with DFP. The cycle counts are given relative to that of SMAUG’s. The C implementations without AVX optimizations are used. Schemes  
   
  Sizes (bytes) sk pk ct  
   
  Security Cycle (ratio) Classic. DFP KeyGen Encap Decap  
   
  NIST’s security level 1 (120) Kyber512 [14] LightSaber [50] LightSable [46] SMAUG-128  
   
  1632 832 800 176  
   
  800 672 608 672  
   
  768 736 672 672  
   
  118 118 114 120  
   
  2−139 2−120 2−139 2−120  
   
  1.70 1.21 1.10 1  
   
  2.10 1.58 1.48 1  
   
  2.03 1.44 1.39 1  
   
  1088 1088 1024 1024  
   
  183 189 185 181  
   
  2−164 2−136 2−143 2−136  
   
  1.38 1.21 1.10 1  
   
  1.84 1.64 1.48 1  
   
  1.75 1.47 1.39 1  
   
  1568 1472 1376 1472  
   
  256 260 223 264  
   
  2−174 2−165 2−208 2−167  
   
  1.25 1.21 1.03 1  
   
  1.38 1.58 1.25 1  
   
  1.36 1.44 1.22 1  
   
  NIST’s security level 3 (180) Kyber768 [14] Saber [50] Sable [46] SMAUG-192  
   
  2400 1248 1152 236  
   
  1184 992 896 1088  
   
  NIST’s security level 5 (260) Kyber1024 [14] FireSaber [50] FireSable [46] SMAUG-256 1  
   
  3168 1664 1632 218  
   
  1568 1312 1312 1792  
   
  We used the lattice-estimator [2], from https://github.com/malb/lattice-estimator (commit 9687562), while additionally considering other attacks targeting the sparsity.  
   
  SMAUG  
   
  131  
   
  Compared to Kyber-512 [14], the NIST-selected standard having security level 1, SMAUG-128 has 16% and 12% smaller public key and ciphertext, respectively. The secret key size of SMAUG is tiny and ready to use, which enable eﬃcient management of secure zone in restricted IoT devices. With high enough security and low enough DFP, SMAUG further achieves 110% and 103% speed up in encapsulation and decapsulation. Compared to LightSaber [50], one of the round 3 ﬁnalists with the security level 1, SMAUG-128, has 9% smaller ciphertext and the same public key size. Again, the secret key is signiﬁcantly smaller than LightSaber, with a 58% and 44% speed up in encapsulation and decapsulation, respectively. When compared to LightSable [46], SMAUG-128 has the same ciphertext size but a larger public key size. It can be seen as a trade-oﬀ as SMAUG achieves 48% and 39% faster encapsulation and decapsulation. In NIST’s security levels 3 and 5, SMAUG similarly outperforms Kyber and provides a trade-oﬀ with Saber and Sable. We refer to Sect. 6.2 for detailed comparisons. Paper Organization. The rest of the paper is organized as follows. Section 2 deﬁnes the notations and summarizes the formal deﬁnitions of key encapsulation mechanisms with the relevant works. In Sect. 3, we introduce the design choices of SMAUG. In Sect. 4, we introduce SMAUG and its security proofs. We provide concrete security analysis and the recommended parameter sets in Sect. 5. Lastly, we give the performance result with comparisons to recent KEM schemes and the implementation details in Sect. 6.  
   
  2 2.1  
   
  Preliminaries Notation  
   
  We denote matrices with bold and upper case letters (e.g., A) and vectors with bold type and lower case letters (e.g., b). Unless otherwise stated, the vector is a column vector. We deﬁne a polynomial ring R = Z[x]/(xn + 1) where n is a power of 2 integers and denote a quotient ring by Rq = Z[x]/(q, xn + 1) = Zq [x]/(xn + 1) for a positive integer q. For an integer η, we denote the set of polynomials of degree less than n with coeﬃcients in [−η, η] ∩ Z as Sη . Let S˜η be a set of polynomials of degree less than n with coeﬃcients in [−η, η) ∩ Z. We denote a discrete Gaussian distribution with standard deviation σ as DZ,σ . 2.2  
   
  Public Key Encryption and Key Encapsulation Mechanism  
   
  We refer to [21] for the formalism of Public Key Encryption (PKE) and Key Encapsulation Mechanism (KEM). Note that we only focus on the adaptive IND-CCA attacks, i.e., IND-CCA2 attacks.  
   
  132  
   
  2.3  
   
  J. H. Cheon et al.  
   
  Fujisaki-Okamoto Transform  
   
  Fujiskai and Okamoto proposed a novel generic transform [33,34] that turns a weakly secure PKE scheme into a strongly secure PKE scheme in the Random Oracle Model (ROM), and various variants have been proposed to deal with tightness, non-correct PKEs, and in the quantum setting, i.e., QROM. Here, we recall the FO transformation for KEM as introduced by Dent [28] and revisited by Hofheinz et al. [38], Bindel et al. [12], and H¨ ovelmanns et al. [39]. We recap the QROM proof of Bindel et al. [12] allowing the KEMs constructed over non-perfect PKEs to have IND-CCA security. Theorem 1 ([12], Theorem 1 & 2). Let G and H be quantum-accessible random oracles, and the deterministic PKE is -injective. Then the advantage of IND-CCA attacker A with at most QDec decryption queries and QG and QH hash queries at depth at most dG and dH , respectively, is    (A) ≤ 2 (dG + 2) AdvIND-CPA (B1 ) + 8(QG + 1)/|M| AdvIND-CCA KEM PKE  +AdvDF PKE (B2 ) + 4 dH QH /|M| + ,  
   
  where B1 is an IND-CPA adversary on PKE and B2 is an adversary against ﬁnding a decryption failing ciphertext, returning at most QDec ciphertexts.  
   
  3  
   
  Design Choices  
   
  In this section, we explain the design choices for SMAUG. 3.1  
   
  MLWE Public Key and MLWR Ciphertext  
   
  One of the core designs of SMAUG uses the MLWE hardness for its secret key security and MLWR hardness for its message security. This choice is adapted from Lizard and RLizard, which use LWE/LWR and RLWE/RLWR, respectively. Using both LWE and LWR variant problems makes the conceptual security distinction between the secret key and the ephemeral sharing key: a more conservative secret key with more eﬃcient en/decapsulations. Combined with the sparse secret, bringing the LWE-based key generation to the LWR-based scheme enables balancing the speed and the DFP. Public Key. Public key of SMAUG consists of a vector b over a polynomial ring Rq and a matrix A, which can be viewed as an MLWE sample, × Rkq , (A, b = −A s + e) ∈ Rk×k q where s is a ternary secret polynomial with hamming weight hs , and e is an error sampled from discrete Gaussian distribution with standard deviation σ. The matrix A is sampled uniformly, using a seed and an eXtendable Output Function (XOF).  
   
  SMAUG  
   
  133  
   
  Ciphertext. The ciphertext of SMAUG is a tuple of a vector c1 ∈ Rkp and a polynomial c2 ∈ Rp . The ciphertext is generated by multiplying a random vector r to the public key with scaling and rounding:    
   
     p p A 0 c · · , · r + c= 1 = μ c2 b q t Along with the public key, it can be seen as an MLWR sample added by a scaled message, as (A , p/q · A · r) + (0, μ ), where A is a concatenated matrix of A and b . The public key and ciphertext can be further compressed by scaling but introduces a larger error. In SMAUG, we only compress the ciphertext which gives less signiﬁcant errors. 3.2  
   
  Sparse Secret  
   
  The sparse secret is widely used in homomorphic encryption to reduce the noise propagation and running time during the homomorphic operations [22,36]. As the lattice-based KEM schemes have inherent decryption error from LWE or LWR noise, the sparse secret can also improve the lattice-based KEMs. Concretely, the decryption error can be expressed as e, r + e1 , s + e2 , are noises added in the public key and a where e ← χkpk and (e1 , e2 ) ← χk+1 ct ciphertext. As the vectors r and s are binary (ternary, resp.), each coeﬃcient of the decryption error is an addition (signed addition, resp.) of hr variables from χpk and hs + 1 variables from χct . So, the magnitude of the decryption error depends greatly on the Hamming weights hr and hs ; thus, we can take advantage of the sparse secrets. Other major advantages of sparse secrets include reducing the secret key size and enabling fast polynomial multiplication. As the coeﬃcients of the secret key are sparse with a ﬁxed hamming weight, we can store only the information of the nonzero coeﬃcients. We can further use this structure for the polynomial multiplications, which we will describe in Sect. 3.4. As the sparse secret reduces the secret key entropy, the hardness of the lattice problem may be decreased. For the security of LWE problem using sparse secret, a series of works have been done, including [23] for asymptotic security, and [11,31,49] for concrete security. Independent of the secret distribution, the module variant (MLWE) is regarded as hard as LWE problem with appropriate parameters, including a smaller modulus. Also, a reduction from ordinary MLWE to MLWE using sparse secret or small errors is studied in [17]. The MLWR problem also has a simple reduction from MLWE independent of the secret distribution, and its concrete security is heuristically discussed in [27]. For the speciﬁc parameters of SMAUG, we exploit the lattice-estimator [2]. We also consider some attacks not in the estimator. Using a smaller modulus, SMAUG can maintain high security. To sample such ﬁxed Hamming weight secrets, we adapted the SampleInBall algorithm in Dilithium [29]. As a result, HWTh samples a ternary polynomial  
   
  134  
   
  J. H. Cheon et al.  
   
  vector having a hamming weight of h in a secret-independent running time. A detailed algorithm is given in the full version of this paper [21]. 3.3  
   
  Discrete Gaussian Noise  
   
  For the public key noise, we use an approximated discrete Gaussian distribution to a narrow range. As this approximated noise is used only once for each secret key, the security loss of using the approximated Gaussian can be eﬃciently bounded. We construct dGaussian, a constant-time approximate discrete Gaussian noise sampler, upon a Cumulative Distribution Table (CDT) but is not used during sampling, as it is expressed with bit operations. We ﬁrst scale the discrete Gaussian distribution and make a CDT approximating the discrete Gaussian distribution. We choose an appropriate scaling factor based on the analysis in [15,42] using R´enyi divergence. We then deploy the Quine-McCluskey method2 and apply logic minimization technique on the CDT. As a result, even though our dGaussian is constructed upon CDT, it is expressed with bit operations and is constant-time. The algorithms are easily parallelizable and suitable for IoT devices as their memory requirement is low. The algorithms and the detailed analysis for the approximation can be found in the full version of this paper [21]. 3.4  
   
  Polynomial Multiplication Using Sparsity  
   
  SMAUG uses the power-of-two moduli to ease the correct scaling and roundings. However, this makes the polynomial multiplications hard to beneﬁt from Number Theoretic Transform (NTT). To address this issue, we propose a new polynomial multiplication that takes advantage of sparsity, which we adapt from [1,43]. Our new multiplication, given in Fig. 1, is constant-time and is faster than the previous approaches. 3.5  
   
  FO Transform, FO⊥ m  
   
  We construct SMAUG upon the FO transform with implicit rejection and without ciphertext contribution to the sharing key generation, say FO⊥ m . This choice makes the encapsulation and decapsulation algorithm eﬃcient since the sharing key can be directly generated from a message. The public key is additionally fed into the hash function with the message to avoid multi-target decryption failure attacks.  
   
  The SMAUG  
   
  4 4.1  
   
  Specification of SMAUG.PKE  
   
  We now describe the public key encryption scheme SMAUG.PKE in Fig. 2: 2  
   
  We use the python package, from https://github.com/dreylago/logicmin.  
   
  SMAUG  
   
  135  
   
  Fig. 1. Polynomial multiplication using sparsity.  
   
  We now give the completeness of SMAUG.PKE. Please see the full version of this paper [21] for the detailed algorithm of the uniform random matrix sampler, expandA, which is adapted from the gen algorithm in Saber [50]. Theorem 2 (Completeness of SMAUG.PKE). Let A, b, s, e, and r are deﬁned as in Fig. 2. Let the moduli t, p, p , and q satisfy t | p | q and t | p | q. Let e1 ∈ RkQ and e2 ∈ RQ be the rounding errors introduced from the scalings and roundings of A · r and bT · r. That is, e1 = pq ( pq · A · r mod p) −   
   
  (A · r mod q) and e2 = pq ( pq · b, r  mod p ) − (b, r mod q). Let δ =  
   
  q Pr e, r + e1 , s + e2 ∞ > 2t , where the probability is taken over the randomness of the encryption. Then SMAUG.PKE in Fig. 2 is (1 − δ)-correct.  
   
  4.2  
   
  Specification of SMAUGKEM  
   
  The key encapsulation mechanism SMAUG.KEM is given in Fig. 3. SMAUG.KEM is designed following the Fujisaki-Okamoto transform with implicit rejection using the non-perfectly correct public key encryption SMAUG.PKE. The Fujisaki-Okamoto transform used in Fig. 3 defers from the FO⊥ m transform in [39] in encapsulation and decapsulation. When generating the sharing key and randomness, SMAUG’s Encap utilizes the hashed public key, which prevents certain multi-target attacks. As for Decap, if ct = ct holds, an alternative sharing key should be re-generated so as not to leak failure information against Side-Channel Attacks (SCA). We also remark that the randomly chosen message μ should also be hashed in the environments using a non-cryptographic Random Number Generator (RNG) system. Using a True Random Number Generator (TRNG) is recommended to sample the message μ in such devices. We now give the completeness of SMAUG.KEM based on the completeness of the underlying public key encryption scheme, SMAUG.PKE.  
   
  136  
   
  J. H. Cheon et al.  
   
  Fig. 2. Description of SMAUG.PKE  
   
  Theorem 3 (Completeness of SMAUG.KEM). We borrow the notations and assumptions from Theorem 2 and Fig. 3. Then SMAUG.KEM in Fig. 3 is also (1 − δ)-correct.  
   
  4.3  
   
  Security Proof  
   
  When proving the security of the KEMs constructed using FO transform in the (Q)ROM, on typically relies on the generic reductions from one-wayness or IND-CPA security of the underlying PKE. In the ROM, SMAUG.KEM has a tight reduction from the IND-CPA security of the underlying PKE, SMAUG.PKE. However, like other lattice-based constructions, the underlying PKE has a chance of decryption failures, which makes the generic reduction unapplicable [47] or non-tight [12,38,39] in the QROM. Therefore, we prove the IND-CCA security of SMAUG.KEM based on the non-tight QROM reduction of [12] as explained in Sect. 2 by proving the IND-CPA security of SMAUG.PKE. Theorem 4 (IND-CPA security of SMAUG.PKE). Assuming pseudorandomness of the underlying sampling algorithms, the IND-CPA security of SMAUG. PKE can be tightly reduced to the decisional MLWE and MLWR problems. Specifically, for any IND-CPA-adversary A of SMAUG.PKE, there exist adversaries B0 , B1 , B2 , and B3 attacking the pseudorandomness of XOF, and the pseudorandomness of sampling algorithms, the hardness of MLWE, and the hardness of MLWR,  
   
  SMAUG  
   
  137  
   
  Fig. 3. Description of SMAUG.KEM  
   
  respectively, such that, PR PR AdvIND-CPA SMAUG.PKE (A) ≤ AdvXOF (B0 ) + AdvexpandA,HWT,dGaussian (B1 ) MLWR + AdvMLWE n,q,k,k (B2 ) + Advn,p,q,k+1,k (B3 ).  
   
  The secret distribution terms omitted in the last two advantages (of B1 and B2 ) are uniform over ternary polynomials with Hamming weights hs and hr , respectively. The error distribution term omitted in the advantage of B2 is a pseudorandom distribution following the corresponding CDT. In the classical random random oracle model, the classical IND-CCA security of SMAUG.KEM is obtained directly from FO transforms [38]. Theorem 1 implies the quantum IND-CCA security of SMAUG.KEM in the quantum random oracle model. Due to space limitations, we omit the proofs. Please see the full version [21].  
   
  5  
   
  Parameter Selection and Concrete Security  
   
  In this section, we give concrete security analysis and recommended parameters. 5.1  
   
  Concrete Security Estimation  
   
  We exploit the best-known lattice attacks to estimate the concrete security of SMAUG.  
   
  138  
   
  J. H. Cheon et al.  
   
  Core-SVP Methodology. Most of the known attacks are essentially ﬁnding a nonzero short vector in Euclidean lattices, using the Block-Korkine-Zolotarev (BKZ) lattice reduction algorithm [20,37,48]. BKZ has been used in various lattice-based schemes [3,14,29,32,50]. The security of the schemes is generally estimated as the time complexity of BKZ in core-SVP hardness introduced in [4]. It depends on the block size β of BKZ reporting the best performance. According to Becker et al. [8] and Chailloux et al. [18], the β-BKZ algorithm takes approximately 20.292β+o(β) and 20.257β+o(β) time in the classical and quantum setting, respectively. We ignore the polynomial factors and o(β) terms in the exponent to ease the analysis. We use the lattice estimator [2] to estimate the concrete security of SMAUG in core-SVP hardness. Beyond Core-SVP Methodology. In addition to lattice reduction attacks, we also take into consideration the cost of other types of attacks, e.g., algebraic attacks like the Arora-Ge attack or Coded-BKW attacks and their variants. In general, these attacks have considerably higher costs and memory requirements compared to previously introduced attacks. We use the lattice estimator for estimating such attacks. We also focus on the attacks targeting sparse secrets, such as the MeetLWE [45] attack. This attack is inspired by Odlyzko’s Meet-in-the-Middle approach and involves using representations of ternary secrets in additive shares. We use a Python script to estimate the cost of the Meet-LWE attack, following the original description in [45]. MLWE Hardness. We estimated the cost of the best-known attacks for MLWE, including primal attack, dual attack, and their hybrid variations, in the core-SVP hardness. We remark that any MLWEn,q,k,,η instance can be viewed as an LWEq,nk,n,η instance. Although the MLWE problem has an additional algebraic structure compared to the LWE problem, no attacks currently take advantage of this structure. Therefore, we assess the hardness of the MLWE problem based on the hardness of the corresponding LWE problem. We also consider the distributions of secret and noise when estimating the concrete security of SMAUG. We have also analyzed the costs of recent attacks that aim to target the MLWE problem with sparse secrets. MLWR Hardness. To measure the hardness of the MLWR problem, we treat it as an MLWE problem since no known attack utilizes the deterministic error term in the MLWR structure. Banerjee et al. [7] provided the reduction from the MLWE problem to the MLWR problem, which was subsequently improved in [5,6,13]. Basically, for given an MLWR sample (A, p/q · A · s mod p) with uniformly chosen A ← Rkq and s ← Rp , it can be expressed as (A, p/q · (A · s mod q) + e mod p). The MLWR sample can be converted to an MLWE sample over Rq by multiplying q/p as (A, b = A · s + q/p · e mod q). Assuming that the error term in the resulting MLWE sample is a random variable, uniformly  
   
  SMAUG  
   
  139  
   
  distributed within the interval (−q/2p, q/2p], we can estimate the hardness of the MLWR problem as the hardness of the corresponding MLWE problem. 5.2  
   
  Parameter Sets  
   
  The SMAUG is parameterized by various integers such as n, k, q, p, p , t, hs and hr , as well as a standard deviation σ > 0 for the discrete Gaussian noise. Our main focus when selecting these parameters is to minimize the ciphertext size while maintaining security. We set SMAUG parameters to make SMAUG at least as safe as Saber. We ﬁrst set our ring dimension to n = 256 and plaintext modulus to t = 2 to have a 256-bit message space (or sharing key space). Then, we search for parameters with enough security to select the smallest ciphertext size. The compression factor p can be set to a small integer if the DFP is low enough. Otherwise, we can keep p = 256 as in the level-3 parameter and not compress the ciphertext. Table 3 shows the three parameter sets of SMAUG, corresponding to NIST’s security levels 1, 3, and 5. For security levels 3 and 5, we can not ﬁnd the parameters for q = 1024, so we use q = 2048. Especially, the standard deviation σ = 1.0625 is too low for security level 3, so we move to σ = 1.453713. For the level-5 parameters set, we use k = 5 since k = 4 is too small for enough security. Table 3. The NIST security level, selected parameters, classical and quantum coreSVP hardness and security beyond core-SVP (see Sect. 5.1), DFP (in log2 ), and sizes (in bytes) of SMAUG. Parameters sets  
   
  SMAUG-128  
   
  SMAUG-192  
   
  SMAUG-256  
   
  Security level  
   
  1  
   
  3  
   
  5  
   
  n k (q, p, p , t) (hs , hr ) σ  
   
  256 2 (1024, 256, 32, 2) (140, 132) 1.0625  
   
  256 3 (2048, 256, 256, 2) (198, 151) 1.453713  
   
  256 5 (2048, 256, 64, 2) (176, 160) 1.0625  
   
  181.7 160.9 202.0  
   
  264.5 245.2 274.6  
   
  Classical core-SVP 120.0 Quantum core-SVP 105.6 144.7 Beyond core-SVP DFP  
   
  –119.6  
   
  –136.1  
   
  –167.2  
   
  Secret key Public key Ciphertext  
   
  176 672 672  
   
  236 1088 1024  
   
  218 1792 1472  
   
  140  
   
  J. H. Cheon et al.  
   
  The core-SVP hardness is estimated3 via the lattice estimator [2] using the cost model “ADPS16” introduced in [4] and “MATZOV” [44]. In the table, the smaller cost is reported. We assumed that the number of 1s is equal to the number of –1 s for simplicity, which conservatively underestimates security. The security beyond core-SVP is estimated via the lattice estimator [2] and the Python script implementing the Meet-LWE attack cost estimation. It shows the lowest attack costs among coded-BKW, Arora-Ge, and Meet-LWE attack and their variants. We note that these attacks require a minimum memory size of 2130 to 2260 . 5.3  
   
  Decryption Failure Probability  
   
  As our primary goal is to push the eﬃciency of the lattice-based KEMs toward the limit while keeping roughly the same level of security, we follow the frameworks given in the NIST ﬁnalist Saber. In particular, we set the DFP to be similar to or lower than that of Saber’s. The impact of DFP on the security of KEM is still being investigated. However, we can justify our decision to follow Saber’s choice and why it is suﬃcient for real-world scenarios. We can deduce that the number of observable decryption failures can be upper bounded by 264+33+8+8 · 2−20 = 293 base on the following assumptions: 1. Each key pair has a limit of Qlimit = 264 decryption queries, as speciﬁed in NIST’s proposal call. 2. There are approximately 233 people worldwide, each with hundreds of devices. Each device has hundreds of usable public keys broadcasted for KEM. 3. We introduce an observable probability and assume it is far less than 2−20 . Even though the decryption failure occurs, it can only be used for an attack when it is observed. However, it is hard to observe all of the failures due to physical constraints. The best-known (multi-target) attacks for Saber [25, Figure 6 (a), 7(a)] shows that the quantum cost for ﬁnding a single failing ciphertext of SMAUG security level 3 (Resp. 5) is much higher than 2160 (Resp. 2245 ), as desired. Regardless of the attack cost estimated above, the scenario of checking the failures in more than 240 diﬀerent devices is already way too far from the real-world attack scenario.  
   
  6  
   
  Implementation  
   
  In this section, we give the implementation performance for each parameter set. We compare the sizes and the reported performance with prior works such as Kyber, Saber, and Sable. The constant-time reference implementation of SMAUG, along with the supporting scripts, can be found in our website: www. kpqc.cryptolab.co.kr/smaug. 3  
   
  There are some suspicions on the unsubstantiated dual-sieve attacks assuming the ﬂawed heuristic [30]. However, we hereby estimate the security of SMAUG following the methods in Kyber, Saber, and Sable for a fair comparison.  
   
  SMAUG  
   
  6.1  
   
  141  
   
  Performance  
   
  We instantiate the hash functions G, H and the extendable output function XOF with the following symmetric primitives: G is instantiated with SHAKE256, H is instantiated with SHA3-256, and XOF is instantiated with SHAKE128. Table 4 presents the performance results of SMAUG. For a fair comparison, we also performed measurements on the same system with identical settings of the reference implementation of Kyber, Saber, and Sable4 . Table 4. Median cycle counts of 1000 executions for Kyber, Saber, Sable, and SMAUG (and their ratios). Cycle counts are obtained on one core of an Intel Core i7-10700k, with TurboBoost and hyperthreading disabled, using the C implementations without AVX optimizations. Schemes  
   
  6.2  
   
  Cycles KeyGen Encap  
   
  Decap  
   
  Cycles (ratio) KeyGen Encap Decap  
   
  Kyber512 LightSaber LightSable SMAUG-128  
   
  131560 93752 85274 77220  
   
  162472 122176 114822 77370  
   
  18930 133764 128990 92916  
   
  1.7 1.21 1.1 1  
   
  2.1 1.58 1.48 1  
   
  2.03 1.44 1.39 1  
   
  Kyber768 Saber Sable SMAUG-192  
   
  214160 18722 170400 154862  
   
  251308 224686 211290 136616  
   
  285378 239590 23724 163354  
   
  1.38 1.21 1.1 1  
   
  1.84 1.64 1.55 1  
   
  1.75 1.47 1.45 1  
   
  Kyber1024 FireSaber FireSable SMAUG-256  
   
  332470 289278 275156 266704  
   
  371854 347900 337322 270123  
   
  415498 382326 371486 305452  
   
  1.25 1.08 1.03 1  
   
  1.38 1.29 1.25 1  
   
  1.36 1.25 1.22 1  
   
  Comparison to Prior/Related Work  
   
  In this section, we compare the sizes, security, and performance of SMAUG to the recent lattice-based KEM schemes. Kyber and SMAUG . Tables 4 and 5 demonstrate that SMAUG outperforms Kyber in almost every aspect, except for the DFP and public key size in level 5. Compared to Kyber-512 [14], SMAUG-128 has a 16% and 12% smaller public key and ciphertext, respectively. Additionally, the secret key size of SMAUG is signiﬁcantly smaller than that of Kyber. This makes it easy to manage secure zones in restricted IoT devices, as it is tiny and ready to use. Note that most of 4  
   
  From https://github.com/pq-crystals/kyber (518de24), https://github.com/KUL euven-COSIC/SABER (f7f39e4), and https://github.com/josebmera/scabbard (4b2b5de), respectively.  
   
  142  
   
  J. H. Cheon et al.  
   
  Table 5. Comparison of Kyber, Saber, Sable, and SMAUG. Sizes are given in bytes, and the ratios are given relative to the sizes of SMAUG. Security is provided in the classical core-SVP hardness with DFP (in logarithm base two). Schemes  
   
  Sizes (bytes) sk pk ct  
   
  Sizes (ratio) sk pk ct  
   
  Security Classic. DFP  
   
  Kyber512 LightSaber LightSable SMAUG-128  
   
  1,632 832 800 176  
   
  800 672 608 672  
   
  768 736 672 672  
   
  9.4 4.8 4.6 1  
   
  1.2 1 0.9 1  
   
  1.1 1.1 1 1  
   
  118 118 114 120  
   
  –139 –120 –139 –120  
   
  Kyber768 Saber Sable SMAUG-192  
   
  2,400 1,248 1,152 236  
   
  1,184 992 896 1,088  
   
  1,088 1,088 1,024 1,024  
   
  10.4 5.4 5 1  
   
  1.1 0.9 0.8 1  
   
  1.1 1.1 1 1  
   
  183 189 185 181  
   
  –164 –136 –143 –136  
   
  Kyber1024 FireSaber FireSable SMAUG-256  
   
  3,168 1,664 1,632 218  
   
  1,568 1,312 1,312 1,792  
   
  1,568 1,472 1,376 1,472  
   
  15.2 8 7.8 1  
   
  0.9 0.7 0.7 1  
   
  1.1 1 0.9 1  
   
  256 260 223 264  
   
  –174 –165 –208 –167  
   
  the KEMs can store a secret key as a seed, having 32 bytes, making them more vulnerable to side-channel attacks. Furthermore, SMAUG-128 achieves a 110% and 103% speed-up in encapsulation and decapsulation, respectively, while maintaining high security and low DFP. Similar results are presented for security levels 3 and 5, except that the public key size of Kyber is shorter than SMAUG’s in level 5. We note that the speed-ups decrease in higher security parameters. Saber, Sable, and SMAUG. When compared to Saber, one of NIST’s round 3 ﬁnalists, SMAUG-128 has a 9% smaller ciphertext and the same public key size as LightSaber. The secret key is signiﬁcantly smaller, with a 58% and 44% speed up in encapsulation and decapsulation, respectively. Compared to Sable, an eﬃcient variant of Saber, SMAUG-128, has the same ciphertext size but a larger public key size. This can be seen as a trade-oﬀ between smaller public keys versus faster running time. SMAUG-128 achieves 48% and 39% faster encapsulation and decapsulation, a smaller secret key, and a bit higher security at the cost of a larger public key. This trade-oﬀ is also observed in security levels 3 and 5, between the public key size versus the secret key size and running time. In level 3, the encapsulation and decapsulation speed of SMAUG outperforms by 44% to 64% with a much smaller, ready-to-use secret key. In level 5, FireSable has a classical core-SVP hardness of 223, much lower than 260. It achieves a smaller public key and ciphertext than SMAUG-256 but still with slower speeds.  
   
  SMAUG  
   
  6.3  
   
  143  
   
  Security Against Physical Attacks  
   
  As the full masked implementation is not the target of this paper, we left it as a future work. However, how easy a masking is can also be one of the eﬃciency measures. Therefore, we justify how we can make SMAUG secure against physical attacks based on the proﬁled Diﬀerential Power Analysis (DPA). As Kyber and Saber share many design aspects with SMAUG, we can follow recent works on masking Kyber and Saber [9,16] to add SCA countermeasures. Our new sampler, dGaussian, is expressed with bit operations, so adding SCA countermeasures like boolean masking is easy. While Krausz et al. [41] have recently proposed masking methods for the ﬁxed hamming weight sampler, their eﬃciency is lacking, so we see it as future work. The new multiplication method may be vulnerable to memory access patterns, but we can eﬃciently mask it using coeﬃcient-wise shuﬄing. Acknowledgments. This work was submitted to the ‘Korean Post-Quantum Cryptography Competition’ (www.kpqc.or.kr). Part of this work was done while MinJune Yi was in CryptoLab Inc.  
   
  References 1. Akleylek, S., Alkım, E., Tok, Z.Y.: Sparse polynomial multiplication for latticebased cryptography with small complexity. J. Supercomput. 72, 438–450 (2016) 2. Albrecht, M.R., Player, R., Scott, S.: On the concrete hardness of learning with errors. J. Math. Cryptol. 9(3), 169–203 (2015) 3. Alkim, E., Barreto, P.S.L.M., Bindel, N., Kramer, J., Longa, P., Ricardini, J.E.: The lattice-based digital signature scheme qtesla. Cryptology ePrint Archive, Paper 2019/085 (2019). https://eprint.iacr.org/2019/085 4. Alkim, E., Ducas, L., P¨ oppelmann, T., Schwabe, P.: Post-quantum key exchange - a new hope. In: Holz, T., Savage, S. (eds.) USENIX Security 2016, pp. 327–343. USENIX Association, August 2016 5. Alperin-Sheriﬀ, J., Apon, D.: Dimension-preserving reductions from LWE to LWR. Cryptology ePrint Archive, Paper 2016/589 (2016). https://eprint.iacr.org/2016/ 589 6. Alwen, J., Krenn, S., Pietrzak, K., Wichs, D.: Learning with rounding, revisited. In: Canetti, R., Garay, J.A. (eds.) Advances in Cryptology - CRYPTO 2013. CRYPTO 2013. LNCS, vol. 8042, pp. 57–74. Springer, Berlin, Heidelberg (2013). https://doi. org/10.1007/978-3-642-40041-4 4 7. Banerjee, A., Peikert, C., Rosen, A.: Pseudorandom functions and lattices. In: Pointcheval, D., Johansson, T. (eds.) Advances in Cryptology - EUROCRYPT 2012. EUROCRYPT 2012. LNCS, vol. 7237, pp. 719–737. Springer, Berlin, Heidelberg (2012). https://doi.org/10.1007/978-3-642-29011-4 42 8. Becker, A., Ducas, L., Gama, N., Laarhoven, T.: New directions in nearest neighbor searching with applications to lattice sieving, pp. 10–24. Society for Industrial and Applied Mathematics (2016). https://doi.org/10.1137/1.9781611974331.ch2 9. Beirendonck, M.V., D’anvers, J.P., Karmakar, A., Balasch, J., Verbauwhede, I.: A side-channel-resistant implementation of saber. J. Emerg. Technol. Comput. Syst. 17(2) (2021). https://doi.org/10.1145/3429983  
   
  144  
   
  J. H. Cheon et al.  
   
  10. Bernstein, D.J., Chuengsatiansup, C., Lange, T., Van Vredendaal, C.: Ntru prime. IACR Cryptol. ePrint Arch. 2016, 461 (2016) 11. Bi, L., Lu, X., Luo, J., Wang, K.: Hybrid dual and meet-LWE attack. In: Nguyen, K., Yang, G., Guo, F., Susilo, W. (eds.) Information Security and Privacy. ACISP 2022. LNCS, vol. 13494, pp. 168–188. Springer, Heidelberg (2022). https://doi.org/ 10.1007/978-3-031-22301-3 9 12. Bindel, N., Hamburg, M., H¨ ovelmanns, K., H¨ ulsing, A., Persichetti, E.: Tighter proofs of CCA security in the quantum random oracle model. In: Hofheinz, D., Rosen, A. (eds.) Theory of Cryptography. TCC 2019, Part II. LNCS, vol. 11892, pp. 61–90. Springer, Heidelberg (2019). https://doi.org/10.1007/978-3-030-3603373 13. Bogdanov, A., Guo, S., Masny, D., Richelson, S., Rosen, A.: On the hardness of learning with rounding over small modulus. In: Kushilevitz, E., Malkin, T. (eds.) Theory of Cryptography. TCC 2016. LNCS, vol. 9562, pp. 209–224. Springer, Berlin, Heidelberg (2016). https://doi.org/10.1007/978-3-662-49096-9 9 14. Bos, J., et al.: Crystals-kyber: a CCA-secure module-lattice-based KEM. In: 2018 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 353–367. IEEE (2018) 15. Bos, J.W., et al.: Frodo: take oﬀ the ring! Practical, quantum-secure key exchange from LWE. In: Weippl, E.R., Katzenbeisser, S., Kruegel, C., Myers, A.C., Halevi, S. (eds.) ACM CCS 2016, pp. 1006–1018. ACM Press, October 2016. https://doi. org/10.1145/2976749.2978425 16. Bos, J.W., Gourjon, M., Renes, J., Schneider, T., van Vredendaal, C.: Masking kyber: ﬁrst- and higher-order implementations. IACR TCHES 2021(4), 173– 214 (2021). https://doi.org/10.46586/tches.v2021.i4.173-214, https://tches.iacr. org/index.php/TCHES/article/view/9064 17. Boudgoust, K., Jeudy, C., Roux-Langlois, A., Wen, W.: On the hardness of module learning with errors with short distributions. J. Cryptol. 36(1), 1 (2023). https:// doi.org/10.1007/s00145-022-09441-3 18. Chailloux, A., Loyer, J.: Lattice sieving via quantum random walks. In: Tibouchi, M., Wang, H. (eds.) Advances in Cryptology - ASIACRYPT 2021. ASIACRYPT 2021. LNCS, vol. 13093, pp. 63–91. Springer, Cham (2021). https://doi.org/10. 1007/978-3-030-92068-5 3 19. Chen, C., et al.: Ntru: algorithm speciﬁcations and supporting documentation (2020). nIST PQC Round 3 Submision 20. Chen, Y., Nguyen, P.Q.: BKZ 2.0: better lattice security estimates. In: Lee, D.H., Wang, X. (eds.) Advances in Cryptology – ASIACRYPT 2011. ASIACRYPT 2011. LNCS, vol. 7073, pp. 1–20. Springer, Berlin, Heidelberg (2011). https://doi.org/ 10.1007/978-3-642-25385-0 1 21. Cheon, J.H., Choe, H., Hong, D., Yi, M.: Smaug: pushing lattice-based key encapsulation mechanisms to the limits. Cryptology ePrint Archive, Paper 2023/739 (2023). https://eprint.iacr.org/2023/739 22. Cheon, J.H., Han, K., Kim, A., Kim, M., Song, Y.: Bootstrapping for approximate homomorphic encryption. In: Nielsen, J.B., Rijmen, V. (eds.) Advances in Cryptology - EUROCRYPT 2018. EUROCRYPT 2018, Part I. LNCS, vol. 10820, pp. 360–384. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-78381-9 14 23. Cheon, J.H., Han, K., Kim, J., Lee, C., Son, Y.: A practical post-quantum publickey cryptosystem based on spLWE. In: Hong, S., Park, J.H. (eds.) Information Security and Cryptology - ICISC 2016. ICISC 2016. LNCS, vol. 10157, pp. 51–74. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-53177-9 3  
   
  SMAUG  
   
  145  
   
  24. Cheon, J.H., Kim, D., Lee, J., Song, Y.: Lizard: cut oﬀ the tail! A practical post-quantum public-key encryption from LWE and LWR. In: Catalano, D., De Prisco, R. (eds.) Security and Cryptography for Networks. SCN 2018. LNCS, vol. 11035, pp. 160–177. Springer, Heidelberg (2018). https://doi.org/10.1007/978-3319-98113-0 9 25. D’Anvers, J.P., Batsleer, S.: Multitarget decryption failure attacks and their application to saber and Kyber. In: Hanaoka, G., Shikata, J., Watanabe, Y. (eds.) Public-Key Cryptography - PKC 2022. PKC 2022, Part I. LNCS, vol. 13177, pp. 3–33. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-97121-2 1 26. D’Anvers, J.P., Guo, Q., Johansson, T., Nilsson, A., Vercauteren, F., Verbauwhede, I.: Decryption failure attacks on IND-CCA secure lattice-based schemes. In: Lin, D., Sako, K. (eds.) Public-Key Cryptography – PKC 2019. PKC 2019, vol. 11443, pp. 565–598. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-172596 19 27. D’Anvers, J.P., Karmakar, A., Roy, S.S., Vercauteren, F.: Saber: module-LWR based key exchange, CPA-secure encryption and CCA-secure KEM. In: Joux, A., Nitaj, A., Rachidi, T. (eds.) Progress in Cryptology – AFRICACRYPT 2018. AFRICACRYPT 2018. LNCS, vol. 10831, pp. 282–305. Springer, Heidelberg (2018). https://doi.org/10.1007/978-3-319-89339-6 16 28. Dent, A.W.: A designer’s guide to KEMs. In: Paterson, K.G. (eds.) Cryptography and Coding. Cryptography and Coding 2003. LNCS, vol. 2898, pp. 133–151. Springer, Berlin, Heidelberg (2003). https://doi.org/10.1007/978-3-54040974-8 12 29. Ducas, L., et al.: CRYSTALS-Dilithium: a lattice-based digital signature scheme. IACR TCHES 2018(1), 238–268 (2018). https://doi.org/10.13154/tches.v2018.i1. 238-268, https://tches.iacr.org/index.php/TCHES/article/view/839 30. Ducas, L., Pulles, L.: Does the dual-sieve attack on learning with errors even work? Cryptology ePrint Archive, Paper 2023/302 (2023). https://eprint.iacr.org/2023/ 302 31. Espitau, T., Joux, A., Kharchenko, N.: On a dual/hybrid approach to small secret LWE - a dual/enumeration technique for learning with errors and application to security estimates of FHE schemes. In: Bhargavan, K., Oswald, E., Prabhakaran, M. (eds.) Progress in Cryptology - INDOCRYPT 2020. INDOCRYPT 2020. LNCS, vol. 12578, pp. 440–462. Springer, Cham (2020). https://doi.org/10.1007/978-3030-65277-7 20 32. Fouque, P.A., et al.: Falcon: fast-fourier lattice-based compact signatures over NTRU. Submiss. NIST’s Post-quantum Cryptogr. Stand. Process 36(5) (2018) 33. Fujisaki, E., Okamoto, T.: Secure integration of asymmetric and symmetric encryption schemes. In: Wiener, M.J. (ed.) Advances in Cryptology – CRYPTO’99. CRYPTO 1999. LNCS, vol. 1666, pp. 537–554. Springer, Berlin, Heidelberg (1999). https://doi.org/10.1007/3-540-48405-1 34 34. Fujisaki, E., Okamoto, T.: Secure integration of asymmetric and symmetric encryption schemes. J. Cryptol. 26(1), 80–101 (2013). https://doi.org/10.1007/s00145011-9114-1 35. Grover, L.K.: A fast quantum mechanical algorithm for database search. In: Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, pp. 212–219 (1996) 36. Halevi, S., Shoup, V.: Bootstrapping for HElib. In: Oswald, E., Fischlin, M. (eds.) EUROCRYPT 2015, Part I. LNCS, vol. 9056, pp. 641–670. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-46800-5 25  
   
  146  
   
  J. H. Cheon et al.  
   
  37. Hanrot, G., Pujol, X., Stehl´e, D.: Algorithms for the shortest and closest lattice vector problems. In: Chee, Y.M., et al. (eds.) Coding and Cryptology. IWCC 2011. LNCS, vol. 6639, pp. 159–190. Springer, Berlin, Heidelberg (2011). https://doi. org/10.1007/978-3-642-20901-7 10 38. Hofheinz, D., H¨ ovelmanns, K., Kiltz, E.: A modular analysis of the FujisakiOkamoto transformation. In: Kalai, Y., Reyzin, L. (eds.) Theory of Cryptography. TCC 2017, Part I. LNCS, vol. 10677, pp. 341–371. Springer, Cham (2017). https:// doi.org/10.1007/978-3-319-70500-2 12 39. H¨ ovelmanns, K., Kiltz, E., Sch¨ age, S., Unruh, D.: Generic authenticated key exchange in the quantum random oracle model. In: Kiayias, A., Kohlweiss, M., Wallden, P., Zikas, V. (eds.) Public-Key Cryptography - PKC 2020. PKC 2020, Part II. LNCS, vol. 12111, pp. 389–422. Springer, Heidelberg (2020). https://doi. org/10.1007/978-3-030-45388-6 14 40. Howgrave-Graham, N., et al.: The impact of decryption failures on the security of NTRU encryption. In: Boneh, D. (ed.) Advances in Cryptology – CRYPTO 2003. CRYPTO 2003. vol. 2729, pp. 226–246. Springer, Berlin, Heidelberg (2003). https://doi.org/10.1007/978-3-540-45146-4 14 41. Krausz, M., Land, G., Richter-Brockmann, J., G¨ uneysu, T.: A holistic approach towards side-channel secure ﬁxed-weight polynomial sampling. In: Boldyreva, A., Kolesnikov, V. (eds.) Public-Key Cryptography – PKC 2023. PKC 2023, Part II. LNCS, vol. 13941, pp. 94–124. Springer, Cham (2023). https://doi.org/10.1007/ 978-3-031-31371-4 4 42. Langlois, A., Stehl´e, D., Steinfeld, R.: GGHLite: more eﬃcient multilinear maps from ideal lattices. In: Nguyen, P.Q., Oswald, E. (eds.) Advances in Cryptology – EUROCRYPT 2014. LNCS, vol. 8441, pp. 239–256. Springer, Berlin, Heidelberg (2014). https://doi.org/10.1007/978-3-642-55220-5 14 43. Lee, J., Kim, D., Lee, H., Lee, Y., Cheon, J.H.: RLizard: post-quantum key encapsulation mechanism for IoT devices. IEEE Access 7, 2080–2091 (2018) 44. MATZOV: Report on the Security of LWE: Improved Dual Lattice Attack, April 2022. https://doi.org/10.5281/zenodo.6493704 45. May, A.: How to meet ternary LWE keys. In: Malkin, T., Peikert, C. (eds.) Advances in Cryptology – CRYPTO 2021, Part II. LNCS, vol. 12826, pp. 701– 731. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-84245-1 24 46. Mera, J.M.B., Karmakar, A., Kundu, S., Verbauwhede, I.: Scabbard: a suite of eﬃcient learning with rounding key-encapsulation mechanisms. IACR TCHES 2021(4), 474–509 (2021). https://doi.org/10.46586/tches.v2021.i4.474509, https://tches.iacr.org/index.php/TCHES/article/view/9073 47. Saito, T., Xagawa, K., Yamakawa, T.: Tightly-secure key-encapsulation mechanism in the quantum random oracle model. In: Nielsen, J.B., Rijmen, V. (eds.) Advances in Cryptology - EUROCRYPT 2018. EUROCRYPT 2018, Part III. LNCS, vol. 10822, pp. 520–551. Springer, Cham (2018). https://doi.org/10.1007/978-3-31978372-7 17 48. Schnorr, C.P., Euchner, M.: Lattice basis reduction: improved practical algorithms and solving subset sum problems. Math. Program. 66(1), 181–199 (1994) 49. Son, Y., Cheon, J.H.: Revisiting the hybrid attack on sparse secret LWE and application to he parameters. In: Proceedings of the 7th ACM Workshop on Encrypted Computing & Applied Homomorphic Cryptography, pp. 11–20. WAHC’19, Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/ 10.1145/3338469.3358941 50. Vercauteren, I.F., Sinha Roy, S., D’Anvers, J.P., Karmakar, A.: Saber: mod-LWR based KEM, nIST PQC Round 3 Submision  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies Andrea Basso1,2(B) 1  
   
  University of Birmingham, Birmingham, UK [email protected]  2 University of Bristol, Bristol, UK  
   
  Abstract. An oblivious pseudorandom function, or OPRF, is an important primitive that is used to build many advanced cryptographic protocols. Despite its relevance, very few post-quantum solutions exist. In this work, we propose a novel OPRF protocol that is post-quantum, veriﬁable, round-optimal, and moderately compact. Our protocol is based on a previous SIDH-based construction by Boneh, Kogan, and Woo, which was later shown to be insecure due to an attack on its one-more unpredictability. We ﬁrst propose an eﬃcient countermeasure against this attack by redeﬁning the PRF function to use irrational isogenies. This prevents a malicious user from independently evaluating the PRF. The SIDH-based construction by Boneh, Kogan, and Woo is also vulnerable to the recent attacks on SIDH. We thus demonstrate how to eﬃciently incorporate the countermeasures against such attacks to obtain a secure OPRF protocol. To achieve this, we also propose the ﬁrst proof of isogeny knowledge that is compatible with masked torsion points, which may be of independent interest. Lastly, we design a novel non-interactive proof of knowledge of parallel isogenies, which reduces the number of communication rounds of the OPRF to the theoretically-optimal two. Putting everything together, we obtain the most compact postquantum veriﬁable OPRF protocol. Keywords: Oblivious Pseudorandom Functions  
   
  1  
   
  · Isogenies · SIDH  
   
  Introduction  
   
  An oblivious pseudorandom function (OPRF) is a two-party protocol between a user and a server. The two parties obliviously evaluate a PRF on a user-controlled input with a secret key held by the server. After engaging in the protocol, the user learns only the output of the PRF on their chosen input, while the server does not learn anything, neither the user’s input nor the output of the function. Oblivious PRFs can also satisfy a stronger property called verifiability: in a veriﬁable OPRF (vOPRF), the server initially commits to its secret key, and during the execution of the protocol it provides a proof that it has behaved honestly and it has used the previously committed secret key. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 147–168, 2024. https://doi.org/10.1007/978-3-031-53368-6_8  
   
  148  
   
  A. Basso  
   
  It is possible to build an OPRF using generic multi-party computation techniques, but such solutions can be ineﬃcient, and they require more rounds of communication than what an ad-hoc construction can achieve. Indeed, highlyeﬃcient and round-optimal (i.e., with two rounds) constructions exist based on Diﬃe-Hellman [15] or RSA blind signatures [9]. All such constructions are vulnerable to quantum attacks, and very few quantum-resistant OPRFs are reported in the literature. The ﬁrst quantum-secure veriﬁable OPRF was proposed by Albrecht, Davidson, Deo and Smart [1]. The protocol is based on the learningwith-errors problem and the short-integer-solution problem in one dimension, and it only requires two rounds of communication. However, the construction can be characterized as a feasibility result, as a single OPRF execution requires communicating hundreds of gigabytes of data. The only other post-quantum solutions were proposed by Boneh, Kogan, and Woo [5]. The authors proposed two moderately-compact OPRFs based on isogenies, one relying on SIDH and one on CSIDH. The OPRF based on SIDH is veriﬁable, but requires an even higher number of communication rounds, since the veriﬁability proof is highly interactive. A later work by Basso, Kutas, Merz, Petit and Sanso [4] cryptanalyzed the SIDH-based OPRF by demonstrating two attacks against the one-more unpredictability of the protocol, i.e. it showed that a malicious user can recover suﬃcient information to independently evaluate the PRF on any input. The ﬁrst attack is polynomial-time, but it can be easily prevented with a simple countermeasure; the second attack is subexponential but still practical, and the authors argue that there is no simple countermeasure against it. More recently, a series of works [7,19,22] developed an eﬃcient attack on SIDH that also extends to the SIDH-based OPRF. Contributions. In this work, we propose an OPRF protocol that is postquantum secure, veriﬁable, round-optimal, and moderately compact (≈9 MB per execution), with a security proof in the UC framework [6] in the randomoracle model. To do so, we follow the same high-level approach as the SIDH-based OPRF by Boneh, Kogan, Woo [5], but with the following changes: – We propose an eﬃcient countermeasure against the one-more unpredictability attack by Basso, Kutas, Merz, Petit, and Sanso [4]. We modify the PRF definition, and in particular we use irrational isogenies to map the user’s input to an elliptic curve. In this way, the information that allowed an attacker to independently evaluate the PRF is no longer deﬁned over a ﬁeld of small extension. A malicious user may still attempt to carry out the attack from [4], but this would now require the attacker to work with points with exponentially many coordinates over the base ﬁeld, which makes the attack infeasible. Besides preventing the attack, this change results in a smaller prime and a more compact protocol. – We discuss how to integrate M-SIDH, a recently-proposed countermeasure [12] against the SIDH attacks that relies on masked torsion, into the OPRF protocol. This requires using longer isogenies and a larger prime, but a series of optimizations allow us to maintain a reasonable communication  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  149  
   
  cost. To integrate M-SIDH, we also propose the ﬁrst zero-knowledge proof of knowledge that can guarantee the correctness of an M-SIDH public key, which may be of independent interest. The proof relies on splitting the masking value into three multiplicative shares: this enables the prover to build a commutative SIDH square and reveal a single edge, together with the torsion point images, without leaking any information about the witness. Repeating the process multiple times yields a proof with negligible soundness error. – We propose a novel proof of knowledge that can guarantee that two isogenies are parallel, i.e. they are computed by applying the same secret key to two starting curves and torsion points. The protocol is obtained by evaluating two proofs of isogeny knowledge in parallel with correlated randomness. The proof can be proved zero-knowledge under a new yet mild assumption. Such a proof can be used by the server to show that it has used a previously committed secret key, which is the key ingredient to make the OPRF veriﬁable. Since the proof is a proof of knowledge, it can be made non-interactive through standard transformations; this makes the proposed OPRF the ﬁrst isogenybased OPRF to be round-optimal.  
   
  2  
   
  Preliminaries  
   
  The Existing OPRF Construction. Boneh, Kogan, and Woo [5] introduced a veriﬁable OPRF protocol based on SIDH (for further preliminaries on SIDH, we refer to the full version of this paper [2]), which uses a prime p of the form p = NM NB NK N1 N2 f −1, where the values Ni are coprime smooth integers and f is a small cofactor. Initially, the server commits to its key k by publishing the curve ˜ with kernel P˜ + EC obtained as the codomain of the NK -isogeny starting from E ˜ ˜ ˜ ˜ [k]Q, where the values E, P , Q are protocol parameters. The commitment also includes a zero-knowledge proof πC of the correctness of the computation. Then, to evaluate the PRF on input m ∈ M, where M deﬁnes the input space, the user computes an isogeny φm of degree NM by hashing the input with a function H and computing φm : E0 → Em := E0 /P +[H(m)]Q, where the curve E0 and the points P, Q are also protocol parameters. Then, the user blinds the curve Em by computing a second isogeny φb : Em → Emb of degree NB . The user sends the curve Emb and the torsion images RK = φb ◦ φm (PK ), SK = φb ◦ φm (QK ) to the server, where the points PK , QK are also protocol parameters of order NK . The user also provides a non-interactive zero-knowledge proof that torsion information was honestly computed. The server validates the proof, computes the isogeny φk : Emb → Embk := Emb /RK + [k]SK  based on its secret key k, and sends the curve Emrk , the image φk (Emb [NB ]), and a non-interactive zero-knowledge proof of correctness to the user. Then, the server and the user engage in an interactive protocol where the server proves that the isogeny φk has used the same key k as the committed value. If the user is convinced, they use the provided torsion information to undo the blinding isogeny, i.e. to compute the translation of the dual of the blinding isogeny, to obtain the curve Emk .  The output of the OPRF is then the hash H m, j(Emk ), (EC , πC ) . The main exchange, without the commitments and the proofs, is represented in Fig. 1.  
   
  150  
   
  A. Basso  
   
  Fig. 1. The OPRF construction by Boneh, Kogan, and Woo. The protocol, without the relevant zero-knowledge proofs, is represented by the solid lines: the isogenies −→ (φm , φb , φˆb ) are computed by the client, while the isogeny −→ (φk ) is computed by the server. The isogenies  represent the PRF evaluation, and the isogenies  are relevant to the attack presented in [4]. The ﬁgure is based on Fig. 1 of [4].  
   
  The BKMPS Attacks. Basso, Kutas, Merz, Petit, and Sanso [4] proposed two attacks against the one-more unpredictability of the OPRF protocol by Boneh, Kogan, Woo [5] OPRF. In the ﬁrst attack, an attacker who acts as a malicious user engages in the OPRF with a message isogeny φm with kernel generator a point M , of order e . The attacker repeats the process with message isogenies with kernel generators []M, [2 ]M, . . . , [e ]M . The outputs of the PRF are the curves that lie on the isogeny path of φm : Ek → Emk (see Fig. 1), which allows the attacker to compute a generator of the kernel of such isogeny. The recomputed generator is a scalar multiple φk (M ), where φk is the isogeny parallel to the server’s secret isogeny, i.e. its kernel is generated by Pk + [k]Qk . By repeating this process three times with points M1 , M2 and M3 := M1 + M2 (where M1 and M2 are linearly independent), the attacker obtains the points R := [α]φk (M0 ), S := [β]φk (M1 ), T := [γ]φk (M3 ) = [γ/α]R + [γ/β]S, for some unknown values α, β, γ. By expressing T in terms of R and S, the attacker obtains the values γ/α and γ/β and the points R := [γ/α]R = [γ]φk (M0 ) and S  := [γ/β]S = [γ]φk (M1 ). Finally, the attacker can evaluate the PRF on any input m by computing EK /R + [H(m)]S  . The attack runs in polynomial time, but it crucially relies on using message isogenies φm of varying degree. The attack can be thwarted by server checking the order of the isogeny φm , which is possible because of the proof of knowledge provided by user. The authors of [4] also propose a second attack that cannot be easily prevented. The attack proceeds in a similar way to the previous one, but the malicious user uses only isogenies of full degree. To obtain the curves on the path of φm , the attacker needs to ﬁnd the middle curve between two PRF outputs. This introduces a trade-oﬀ between the complexity of the attack and the number of queries. Minimizing both yields a subexponential yet practical attack on the one-more unpredictability of the protocol.  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  3  
   
  151  
   
  Oblivious Pseudorandom Functions  
   
  The security properties of an OPRF can be hard to deﬁne. Oblivious pseudorandom functions were originally proposed by Naor and Reingold [21], who deﬁned an OPRF via an ideal functionality. Subsequent work [13,18] deﬁned OPRFs in terms of the two-party computation (k, x) → (⊥, f (k, x)), but such a definition has several drawbacks. On one side, it is hard to build protocols that satisfy such a deﬁnition, because the security proof would require extracting the user’s input, while at the same the deﬁnition is not secure enough, because it does not guarantee any security under composability. Since OPRFs are mainly used as building blocks in larger protocols, such a security guarantee is highly needed. For these reasons, Jarecki et al. [15,17] proposed new deﬁnitions in the UC framework [6]. To avoid extracting the user’s input, the ideal functionality introduces a ticketing system that increases a counter when the PRF is evaluated and decreases the counter when the user receives the PRF output. This captures the idea that a malicious user should learn only the PRF output for one input for each interaction. This results in the deﬁnition of Fig. 2, which is based on the deﬁnitions by Jarecki et al. [15–17].  
   
  Fig. 2. The FvOPRF functionality.  
   
  152  
   
  3.1  
   
  A. Basso  
   
  Security Assumptions  
   
  To prove that the OPRF protocol we propose implements the functionality of Fig. 2, we will make use of the properties listed in this section. Since our protocol and security proof follows the same high-level structure as that of the OPRF protocol by Boneh, Kogan, and Woo [5], these properties are also based on those of the augmentable commitment framework proposed in [5]. Unlike [5], we avoid the abstraction of augmentable commitments due to its restrictiveness (the counteremasures of Sect. 4 would not be possible within that framework), and we prefer an explicit description throughout this work. Correctness. Firstly, we require the OPRF to be correct, i.e. the output of the protocol is the output of function that deterministically depends only on the user’s input and the server’s secret key. In other words, we want that the blinding process that guarantees the obliviousness of the user’s input does not aﬀect the ﬁnal output. In the context of our protocol, we want that the unblinding isogeny undoes the eﬀect of the blinding isogeny. This is contained in the following lemma, whose proof follows from the correctness of the SIDH protocol [14]. Lemma 1 (Correctness). Let p be a prime of the form p = NB NK f − 1, where NB , NK , f are smooth coprime integers. Let E0 be a supersingular elliptic curve defined over Fp2 and let PB , QB and PK , QK be respectively a basis of E0 [NB ] and E0 [NK ]. Let also b and k be two values in ZNB and ZNK . Then, consider the isogenies φB : E0 → EB := E0 /PB + [b]QB , φK : E0 → EK := E0 /PK + [k]QK ,  φk : EB → EBK := EB /φB (PK ) + [k]φB (QK ). If RB , SB is a basis of EB [NB ] and the values b0 , b1 satisfy ker φˆB = [b0 ]RB + [b1 ]SB , then we have j (EBK /[b0 ]φk (RB ) + [b1 ]φk (SB )) = j(EK ). Input Hiding. To ensure that the OPRF is oblivious, we want that the server does not learn the user’s input. That holds in the strongest sense, i.e. the server should not learn the user’s input even when the input is randomly chosen between two inputs chosen by the server. In other words, the user must apply a blinding step that fully hides the chosen input. In the context of isogenies, we want the following problem to be hard. Problem 2. Let p be a prime of the form p = NB NK f − 1, where NB NK , f are smooth coprime integers. Let E0 and E1 be two supersingular elliptic curves deﬁned over Fp2 and chosen by the adversary, and let P0 , Q0 and P1 , Q1 be a basis of E0 [NK ] and E1 [NK ], respectively, such that eNK (P0 , Q0 ) = eNK (P1 , Q1 ). $  
   
  − {0, 1}, and B a random point in Ei [NB ], and Let i be a random bit, i.e. i ←  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  153  
   
  write φ : Ei → E  := Ei /B. Output i given E  and faux (φ(Pi ), φ(Qi )), where the latter is some auxiliary torsion information. The hardness of the problem clearly depends on the function faux ; if the torsion images were directly revealed, Problem 2 would be easy due to the SIDH attacks. We thus delay describing the function faux until Sect. 5, where we discuss the SIDH countermeasures and choose faux to reveal the values φ(Pi ), φ(Qi ), both scaled by the same unknown value. In the section, we also state the variant of the Decisional Isogeny problem that Problem 2 reduces to. One-More Unpredictability. A key property of an OPRF is that the user learns the output of the PRF only on its input of choice. That means that a malicious user should not learn the output on more inputs than the number of OPRF executions. The BKMPS attack [4] on the OPRF by Boneh, Kogan, and Woo [5] targets the one-more unpredictability, since it shows that a malicious user can extract enough information to independently evaluate the OPRF on any input of their choice. We propose an eﬃcient countermeasure against the one-more unpredictability attack in the next section; we thus delay until then a formalization of the isogeny-related assumption (see Problem 5) we need to guarantee the one-more unpredictability of the OPRF protocol. Commitment Binding. At the beginning of the OPRF protocol, the server commits to a secret key k, so that during each OPRF execution it can prove that the same key was used. To guarantee veriﬁability, we want a commitment scheme with an associated proof of input reuse. We propose to commit to a ˜ with a basis P˜ , Q ˜ of E[N ˜ K ] and revealing key k by ﬁxing a special curve E ˜ ˜ ˜ j(E/P + [k]Q). The proof of input reuse, which in the context of isogenies becomes a proof of parallel isogenies, is presented in Sect. 5.1. To guarantee that the commitment is binding, we want that the following problem to be hard. Problem 3 (Collision finding problem). Let E0 be a supersingular elliptic curve of unknown endomorphism ring. Find two distinct isogenies φ0 : E0 → E and φ1 : E0 → E  such that j(E0 ) = j(E1 ). Problem 3 has been studied in the context of the CGL hash function [8], and it has been shown to be heuristically equivalent to the following problem, which underpins every isogeny-based protocol [10]. Problem 4 (Endomorphism Ring problem). Let E be a supersingular elliptic curve. Find its endomorphism ring End(E).  
   
  4  
   
  Countermeasures Against the Unpredictability Attacks  
   
  The original protocol by Boneh, Kogan and Woo starts by mapping an input m to an isogeny φm . If we denote with NM the torsion space dedicated to the  
   
  154  
   
  A. Basso  
   
  message, the protocol ﬁxes a basis P, Q of E0 [NM ] and computes the isogeny φm given by (1) φm : E0 → E0 /P + [H(m)]Q =: Em , where H(·) maps the message m onto an element of ZNM . The subexponential attack [4] recovers the image Pk , Qk of the torsion basis P, Q, up to scalar multiplication, under the secret isogeny φk : E0 → Ek . With such information, the attacker can evaluate the PRF on any input of their choice. The output curve of the PRF is the curve computed as Ek /Pk + [H(m)]k . The attack is subexponential, and it is possible to obtain λ bits of security if the 2 isogeny φm has degree 2λ (this can be reduced if we limit the number of queries the attacker can make). This would require using very long isogenies (the degree would be 216,384 for λ = 128) and very large primes. Instead, in this section we propose a novel and eﬃcient countermeasure that sidesteps these issues. Our main idea is to accept that an attacker may recover the curve Ek and points Pk , Qk on it, but to prevent those points from being suﬃcient to evaluate the desired isogeny. To do so, we require that the isogeny φm has an irrational kernel, i.e. its kernel is deﬁned over a suﬃciently-large extension ﬁeld. Such an isogeny can be eﬃciently computed as a composition of rational isogenies. More formally, assume that NM = e , and e is the highest power of  that divides p + 1. Then, given an input m ∈ M, we compute the isogeny φm in the following way: 1. We ﬁrst map the message m to two elements in Ze through two hash functions H0 , H1 that are collision resistant. We thus have m0 = H0 (m) and m1 = H1 (m). 2. Given the starting curve E0 and two points P0 , Q0 spanning E0 [e ], we compute the isogeny φ0 : E0 → E1 := E0 /P0 + [m0 ]Q0 . 3. We determine a canonical basis P1 , Q1 of E1 [e ] and compute the isogeny φ1 : E1 → Em := E1 /P1 + [m1 ]Q1 , 4. The isogeny φm : E0 → Em is the composition φ1 ◦ φ0 . An attacker may still try to apply the one-more unpredictability attack. In the original case, the attacker recovers three isogenies from Ek to Emk and they combine their kernel generators to obtain the image points Pk , Qk . In the proposed construction, the attacker can still recover three (or more) isogenies from Ek to Emk . However, the kernel generators of these isogenies are points of order 2e , and thus they are deﬁned only over the extension ﬁeld Fp2e . This is an exponentially large ﬁeld, and even just representing such a point—let alone doing any computation—would be exponential in the security parameter. To guarantee security, it is important that the degree of φm is a prime power. If the degree were a product of prime powers, it is possible to represent a large extension by working over several smaller extensions because of the Chinese Remainder Theorem. This can reduce the complexity of working over a large extension and thus reduce the security of the proposed countermeasures. The attacker can work with the kernel generators of only the ﬁrst half of the isogenies and obtain a basis Pk , Qk of order e (see Fig. 3). This allows them  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  155  
   
  Fig. 3. Summary of the proposed countermeasure (this does not depict the blinding/unblinding phase). Isogenies in red are known or can be computed by the attacker, isogenies in black are unknown to the attacker, and the dotted isogeny represents the missing isogeny that the attacker needs to compute to succeed in the attack. (Color ﬁgure online)  
   
  to evaluate the ﬁrst isogeny φm0 to obtain the curve Em0 k for any message m. However, the attacker has no way of computing the remaining isogeny φm1 . To do so, the attacker would need to map the canonical basis on E1 to Em0 k , which does not seem to be possible without knowing the server secret key. Alternatively, the attacker could map the points P, Q and Pk , Qk under the isogenies φ0 and φk . At least one of the image points on each curve has full order, and the point of full order on Em0 k is the image of the point of full order on E1 . This suggest such an approach could be used to ﬁnd a basis, but the second point on each curve is always a scalar multiple of the ﬁrst point.1 Hence, guessing the remaining point has exponential complexity e . Lastly, the attacker cannot use a similar strategy as the one-more unpredictability attack to recover a basis on Em0 k because the curve Em0 k depends on the message m. It thus changes at every interaction, and it is hard for an attacker to ﬁnd two messages that have the same ﬁrst curve E1 and Em0 k since we assume that the hash function H0 is collision-resistant. Note that we require H0 and H1 to be collision-resistant, but we conjecture that only H0 needs to be. Overall, the knowledge of Em0 k does not help the attacker learn any information on the curve Emk , which successfully prevents the one-more unpredictability attack. Optimizations. We can extend this approach to obtain a more compact protocol. Rather than limiting ourselves to two isogenies, we can extend this to an arbitrary number I. We obtain the optimal case when I is maximal, i.e. when deg φm = I . Let Hi be I distinct random oracles for every i ∈ {1, . . . , I}. Then, given an input m and a starting curve E0 , the isogeny φm and the curve Em by computing an isogeny φi determined by Hi (m), generating a canonical basis on the codomain curve, and repeating the process I times (see Algorithm 1). We refer to this hashing as HI (x), and in the rest of the paper, we write (φm , Em ) = HI (x) to refer to the function in Algorithm 1; we also write [P0 , P1 , . . . , PI−1 ]E,N to denote a list points of order N where the point P0 belongs to E, and the point 1  
   
  If ker φ = P + αQ, it follows that φ(P ) = −αφ(Q).  
   
  156  
   
  A. Basso  
   
  Pi belongs to Ei := Ei1 /Pi−1 . We refer to this as a sequence, whose associated isogeny is the composition of the isogenies Ei → Ei /Pi . Algorithm 1 Function HI mapping the input m to the curve Em 1: for i ← 0 to I − 1 do 2: Set mi = Hi (m) and Pi , Qi = BM (E); 3: Compute φi : Ei → Ei+1 := Ei /Pi + [mi ]Qi ; 4: Set φm = φI−1 ◦ . . . ◦ φ0 and Em = EI−1 ; 5: return φm , Em ;  
   
  This technique to compute message isogenies results in a more compact OPRF protocol because only the shorter isogenies φi need to be deﬁned over Fp2 ; thus, using more isogenies can result in a smaller prime p while maintaining the same degree of the isogeny φm . However, this approach has also a security advantage: an attacker can use the BKMPS attack to recover the image of basis on Ek , which could potentially be used to recover the isogeny between E0 and Ek using the SIDH attacks. While this could be avoided by picking a suﬃciently long isogeny φk , choosing I = e, i.e. setting parameters such that only isogenies of degree  have a kernel deﬁned over Fp2 , ensures that an attacker obtains only a basis of very small order, which prevents the attack altogether. A New Assumption. We proposed a modiﬁed protocol that prevents the existing one-more unpredictability attacks. As in the original construction, the one-more unpredictability of the resulting protocol relies on the hardness of a novel problem, which is the following. Problem 5 (One-more unpredictability). Let p be a prime of the form p = NM NK f − 1, where NM and NK are smooth coprime integers, and f a cofactor. Let HI be a function as in Algorithm 1. Let E0 be a supersingular curve deﬁned over Fp2 , and let K be a point on E0 of order NK . Write φK for the isogeny φK : E0 → EK := E0 /K. Given the curves E0 , EK and an oracle that responds to the following queries: – challenge: returns a random sequence [M0 , . . . , MI−1 ]E0 ,NM , – solve([V0 , . . . , VI−1 ]E0 ,NM ): returns j(EV /φV (K)), where φV is the isogeny associated to the input sequence, – decide(i, j): returns true if j is equal to the output of a solve query with input the response of the i-th challenge query, and false otherwise, For any value n, produce n pairs (i, j) such that decide(i, j) = true with less than n solve queries. The problem is based on Game 12 of [5], but compared to it, this game involves multiple points during the challenge and solve query to abstract the behavior described in the previous section. Moreover, the problem includes the countermeasures against the polynomial time attack of [4], i.e. the attacker can only query points of the correct order. This can be replicated in the OPRF setting by checking the order of the isogenies in the proof of isogeny knowledge.  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  157  
   
  We included these countermeasures to prevent possible attacks since they are inexpensive. However, we conjecture that the problem remains hard even if the adversary is allowed to submit solve queries with points of arbitrary order. Furthermore, the problem remains hard after the SIDH attacks since it does not involve exchanging any torsion points. Countermeasure Costs. We brieﬂy discuss the impact of the proposed countermeasures on the performance of the OPRF protocol. Firstly, we need to determine the parameters , e, and I. Keeping in mind the possible SIDH attacks based on the recovered torsion on Ek , we choose I to be maximal. We require that the degree of the message isogeny is about ≈ 25/2λ to prevent the attack proposed in [20]. Hence, we set e = 1 and I = log (25/2λ ). The message component NM can then be chosen to be , to ensure that isogenies of degree  have kernel in Fp2 , or one, if torsion points of order  are deﬁned over a small extension ﬁeld. In the latter case, the prime p does not need to change to allow computations of the message isogeny. In both cases, not only do the proposed countermeasures protect against existing attacks, but also they reduce the prime size leading to a more compact and eﬃcient protocol.  
   
  5  
   
  Countermeasures Against the SIDH Attacks  
   
  The recent series of attacks by Castryck and Decru [7], Maino, Martindale, Panny, Pope, and Wesolowski [19], and Robert [22] exploits torsion-point information to break SIDH. These attacks trivially translate to the OPRF, where any third party can recover both the user’s hashed input (which breaks obliviousness) and the server’s secret key. Thus, to guarantee the security of the SIDH-based OPRF we need to rely on the masked-torsion countermeasures, as in masked SIDH (M-SIDH) [12]. We can now formulate the following problem, on whose hardness the input hiding property of the OPRF is based. Problem 6 (Decisional M-SIDH Isogeny problem). Let E0 be a supersingular elliptic curve, with a basis P, Q be of E0 [n]. Distinguish between the following distributions: – (E1 , R, S), where E1 is the codomain of a d-isogeny φ : E0 → E1 , where d is coprime with n, and the points R, S are the masked images of P, Q, i.e. $ − Z∗n ; R = [α]φ(P ) and S = [α]φ(Q) for some α ← – (E1 , R, S), where E1 is a random supersingular elliptic curve and the points 2 R, S are a random basis of E1 [n] such that e(R, S) = e(P, Q)α d , for some value α. The hardness of the problem clearly depends on the choices of n and d; the problem (conjecturally) requires O(2λ ) operations to solve when n > fM-SIDH √(λ, d), i.e. the product of the λ largest prime powers dividing n is smaller than d.  
   
  158  
   
  A. Basso  
   
  Concrete Cost. We have shown it is possible to protect the OPRF protocol from the SIDH attacks. Unfortunately, the proposed countermeasure do come at a signiﬁcant cost. The degrees of the blinding isogeny and the server’s isogeny are the same as in SIDH with the same countermeasures. At security level λ = 128, that corresponds to isogenies of degree ≈ 22956 . More generally, we see experimentally that the degree of the isogenies scales log-linearly in the security parameter with a constant of ≈ 6.7. We thus have that the degree of the blinding isogeny and the server’s isogeny must be ≈ 26.7λ log λ to guarantee the security of the protocol. 5.1  
   
  Adapting the Proof of Isogeny Knowledge  
   
  In this section, we propose a zero-knowledge proof, based on that in [5], of isogeny knowledge that can guarantee the correctness of torsion points up to a scalar, i.e. a proof for the following relation: ⎧  ⎫  φ : E0 → E1 is a cyclic d-isogeny, ⎬ ⎨  P1 = [α]φ(P0 ), Riso = ((E0 , P0 , Q0 , E1 , P1 , Q1 ), (φ, α))  . ⎩ ⎭  Q1 = [α]φ(Q0 ). The main idea is that the masking constant α can be split into three shares α = α1 α2 α3 . The prover can mask the torsion points with αi when computing the i-th side of the SIDH square, so that the composition of the three isogenies, together with their masking values, forms a commutative diagram with the isogeny φ with masking value α. The proof remains zero-knowledge because each single value αi is independent of α. More formally, let E0 and E1 be supersingular elliptic curves with points P0 , Q0 ∈ E0 [n] and P1 , Q1 ∈ E1 [n]. The prover wants to prove knowledge of a d-isogeny φ : E0 → E1 and a value α ∈ Zn such that P1 = [α]φ(P0 ) and Q1 = [α]φ(Q0 ). Let us assume n = fM-SIDH (λ, d), so that the isogeny φ is hard to extract from public information. The prover generates a random isogeny ψ : E0 → E2 of degree s, where s ≈ n is a smooth number coprime with both n and d, and generates the SIDH square (E0 , E1 , E2 , E3 ) with edges (φ, ψ, φ , ψ  ). To guarantee soundness, the prover needs to show that ψ and ψ  are parallel: the prover thus generates a s-basis R2 , S2 on E2 , maps it to E3 to obtain R3 , S3 , and expresses the kernels of ψˆ and ψˆ in terms of R2 , S2 and R3 , S3 with the same linear coeﬃcients. The prover also splits α in three shares α = α1 α2 α3 and maps the points P0 , Q0 through ψ and φ with masking values α1 and α2 to obtain the points P2 = [α1 ]ψ(P0 ), Q2 = [α1 ]ψ(Q0 ), and P3 = [α2 ]φ (P2 ), Q3 = [α2 ]φ (Q2 ), which implies that P3 and Q3 also satisfy the relation [α3 ]P3 = ψ  (P1 ), [α3 ]Q3 = ψ  (Q1 ). Hence, the SIDH square commutes with respect to the points Pi , Qi , i.e. if we restrict ourselves to the n-torsion, we have [α][s]φ = [α3 ]ψˆ ◦ [α2 ]φ ◦ [α1 ]ψ. Thus, the witness can be split into three components, and hence we obtain a proof with ternary challenges. The prover initially commits to the curves E2 , E3 and the relevant points on them with a commitment scheme C(·). Then, depending on the challenge, the prover responds with one edge of the SIDH square, the  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  159  
   
  relevant curves and points, and the corresponding commitment openings. The proof is described in details in the full version [2]. Since each iteration has soundness error 2/3, the proof must be repeated −λ log2/3 (2) ≈ 1.71 times to achieve a soundness error of 2−λ . Remark 7. If the kernel of the isogeny φ is not deﬁned over a small extension ﬁeld, as in the case of the message isogeny, the proof can be computed by gluing together multiple SIDH squares, as shown in [3]. We now sketch the proofs of correctness, three-special soundness and zeroknowledge. Given the similarity of the zero-knowledge proof with those in [5], the proofs also follow a similar approach. Correctness. A honest prover always generates proofs that are accepted by the veriﬁer. The veriﬁer recomputes the same operations as the prover and checks that the outputs match. The only diﬀerence is in the chall = ±1 cases, where the veriﬁer computes the dual of ψ and ψ  , which then introduces a factor s in the point equality check. Three-special Soundness. The protocol is three-special sound because there exists an extractor that extracts the witness given three accepting transcripts with the same commitments and diﬀerent challenges. The isogeny φ can be computed by mapping the kernel of φ (from chall = 0) under the isogeny ψˆ (from chall = −1). Since the isogenies ψ and ψ  are parallel (from all the challenges combined), this guarantees that φ is a d-isogeny from E0 to E1 . The masking value α can be recomputed as the product of α1 , α2 , and α3 . Zero-knowledge. We sketch a simulator that given a statement (E0 , P0 , Q0 , E1 , P1 , Q1 ) and a challenge chall can simulate a valid transcript without knowledge of the witness. For the case chall = −1, the simulator behaves like an honest prover. For chall = +1, the situation is similar: the simulator can compute a disogeny ψ  , pick a random basis R3 , S3 of E3 [d] and a random value α3 ∈ Z∗n , and compute the values a, b and points P3 , Q3 that pass veriﬁcation. Note that the points R3 , S3 are uniformly random among the bases of E3 [d], and the value α3 is uniformly random and independent of α; the simulated values are thus distributed as the honestly-generated ones. The case of chall = 0 is more complicated: the simulator can sample a random curve E2 , generate a random basis 2 P2 , Q2 of E2 [n] that satisﬁes e(P2 , Q2 ) = e(P0 , Q0 )x s for some random x, pick a random d-isogeny φ : E2 → E3 , and compute the image points on E3 . In this case, the indistinguishability of the simulator’s output is only computational. It is thus based on the conjectured hardness of the following problem, which is a modiﬁed version of the Decisional Supersingular Product (DSSP) problem introduced in [14]. Problem 8 (DSSP with Torsion (DSSPwT) problem). Given an isogeny φ : E0 → E1 and points P0 , Q0 ∈ E0 [n], where n = fM-SIDH (λ, d), distinguish between the following distributions:  
   
  160  
   
  A. Basso  
   
  – D0 = {(E2 , P2 , Q2 , E3 , φ )}, where E2 is the codomain of an s-isogeny ψ : E0 → E2 , the points P2 , Q2 satisfy P2 = [α]ψ(P0 ), Q2 = [α]ψ(Q0 ) for some α ∈ Z∗n , and φ : E2 → E3 is a d-isogeny with kernel ker φ = ψ(ker φ). – D1 = {(E2 , P2 , Q2 , E3 , φ )}, where E2 is a random supersingular curve with the same cardinality as E0 , P2 and Q2 are two random points of order n such that e(P2 , Q2 ) = e(P0 , Q0 )s , and the isogeny φ is a d-isogeny between E2 and E3 . Note that [5] argues that a similar proof can only reveal one torsion point (either Pi or Qi ) at a time to prevent a distinguishing attack on the simulator. The attack they present relies on computing the Weil pairing between two points of coprime order, and thus their pairing is always one. The attack thus does not apply, and the simulated transcript remains indistinguishable under Weil pairing checks because the sampled points P2 , Q2 are guaranteed to have the same pairing as the honestly-generated points. By revealing both points Pi and Qi we obtain a signiﬁcantly more eﬃcient proof, since it has 1/3 soundness rather than 1/6. We discuss potential optimizations and the concrete cost of such a proof in the full version of this paper [2].  
   
  6  
   
  Verifiability  
   
  Oblivious PRFs can satisfy a stronger security property called verifiability. Informally, this guarantees that the server behaves honestly and always uses the same long-term static key. This is needed to guarantee the privacy of the user in those instances where the user may later reveal the output of the OPRF. A malicious server may behave “honestly” while also using diﬀerent secret keys on diﬀerent interactions. After learning the OPRF output of the user, the server can then test which secret key was used to produce that speciﬁc output and thus link the user to a speciﬁc user-server interaction. The OPRF protocol by Boneh, Kogan, and Woo achieves veriﬁability by relying on many SIDH exchanges: not only is this broken by the attacks on SIDH [7,19,22], this requires ﬁve rounds of interaction. We avoid such issues by introducing a novel public-coin proof protocol of parallel isogeny. Since the proof does not rely on private randomness, we obtain a proof of knowledge that can be made non-interactive via the Fiat-Shamir transform [11] or the Unruh transform [23]. In the OPRF setting, we will rely on the latter to achieve the online-extractability without rewinding needed to get a proof in the UC model. Our main approach relies on executing two proofs of isogeny knowledge in parallel with correlated randomness. Since part of the randomness used is shared, we can obtain a proof of parallelness without needing additional computations. Firstly, we formalize the notion of parallelness. We say that two d-isogenies φ : ˜0 → E ˜1 are parallel with respect to the bases T0 , V0 ∈ E0 [d] E0 → E1 and φ˜ : E and T˜0 , V˜0 ∈ E0 [d] if there exists coeﬃcients a, b ∈ Zd such that ker φ = [a]T0 + [b]V0  and ker φ˜ = [a]T˜0 + [b]V˜0 . This suggests that the parallelness relation  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  that we are proving is the following: ˜0 , T˜0 , V˜0 , E ˜1 ), (k0 , k1 )) Rpar = ((E0 , T0 , V0 , E1 , E  
   
  161  
   
    E0 /[k0 ]T0 + [k1 ]V0  ∼ = E1 ,   E ˜0 /[k0 ]T˜0 + [k1 ]V˜0  ∼ ˜1 =E  
   
  However, as discussed before, we are combining several proofs together to obtain a larger proof that simultaneously proves knowledge of two isogenies and guarantees the two isogenies are parallel. We thus obtain a proof for the following relation, where we consider the case of a secret key with two coeﬃcients for completeness. For practical reasons, the OPRF will ﬁx k0 = 1 without any loss of security.  
   
  R∗par  
   
   ⎧ ⎫  ker φ = [k0 ]T0 + [k1 ]V0 , ⎪ ⎪ ⎪ ⎪ ⎨((E0 , T0 , V0 , P0 , Q0 , E1 , P1 , Q1 ,  ⎬  ˜0 + [k1 ]V˜0 , ker φ = [k ] T 0  ˜0 , T˜0 , V˜0 , P˜0 , Q ˜0, E ˜1 , P˜1 , Q ˜ 1 ), = E  (E0 , P0 , Q0 , E1 , P1 , Q1 ), (φ, α) ∈ Riso , ⎪ ⎪  ⎪ ⎪ ⎩ (k0 , k1 , α, α )) (E ˜0 , P˜0 , Q ˜0, E ˜1 , P˜1 , Q ˜ 1 ), (φ , α ) ∈ Riso⎭  
   
  ˜0 with a d-basis T˜0 , V˜0 be ﬁxed protocol parameters. Now, let the curve E Using the same notation as before, assume that server has committed to its key (k0 , k1 ) by publishing the codomain of the d-isogeny φ˜ that has kernel [k0 ]T˜0 + [k1 ]V˜0 . The server may also reveal some torsion information in its commitment, but as we will discuss later, this is not strictly needed. During the OPRF execution, the server receives a curve E0 with a d-basis T0 , V0 on it, and it computes φ : E0 → E1 := E0 /[k0 ]T0 + [k1 ]V0 . The server then wants to prove that it knows the isogenies φ and φ˜ and that they are parallel. If the server simply ran two instances of the PoIK Riso in parallel, there would be no way to convince the prover that the isogenies are indeed parallel. If the proofs share the same challenges, i.e. the veriﬁer sends the same challenges to both proofs, the server would respond with both φ and φ˜ when chall = 0. However, the isogenies φ and φ˜ are parallel with respect to the bases ψ(T0 ), ψ(V0 ) ˜ V˜0 ) (where ψ is the vertical isogeny used in the proof of knowledge), ˜ T˜0 ), ψ( and ψ( which are not revealed in the proof. If we were to reveal them, the proof would not be zero-knowledge, because when chall = 0, the veriﬁer could recompute the secret isogeny ψ and ψ˜ through the SIDH attacks. Instead, we want to modify ˜2 [d] such that φ the proof to reveal diﬀerent bases T2 , V2 ∈ E2 [d] and T˜2 , V˜2 ∈ E  ˜ and φ are parallel with regards to them, but also such that they do not reveal ˜ We thus propose that the prover generates much information about ψ and ψ. four random coeﬃcients w, x, y, z ∈ Zd such that wz − xy = 0 mod d, and computes T2 and V2 as the solution of ψ(T0 ) = [w]T2 + [x]S2 , ψ(V0 ) = [y]T2 + [z]V2 , and similarly for T˜2 and V˜2 . This is then secure, because the basis T2 , V2 is uniformly random. Thus, for a single proof, this change only does not aﬀect the security of the proof since no additional information is revealed. The rest of the proof needs to be modiﬁed to ensure that the process is followed correctly, i.e. we want the prover to reveal the values w, x, y, z together with ψ so that the veriﬁer can verify the correctness of T2 and V2 . The modiﬁed proof is denoted ∗ , and it is represented explicitly in the full version of this paper [2]. by Piso  
   
  162  
   
  A. Basso  
   
  Now, if the prover executes the modiﬁed proof of isogeny knowledge for φ and φ˜ in parallel, with the same challenges, and with the same values x, w, y, z, the isogenies φ , φ˜ revealed when chall = 0 are parallel when the isogenies φ, φ˜ are also parallel, as shown in the following lemma. Lemma 9. Let notation be as above. The isogenies φ, φ˜ are parallel if and only if the isogenies φ , φ˜ are also parallel. Proof. Assume the isogeny φ has kernel [k0 ]T0 + [k1 ]V0  and the isogeny φ˜ has kernel [k˜0 ]T˜0 + [k˜1 ]V˜0 . The kernel of φ is the image of the kernel of φ under ψ, i.e. ker φ = ψ(ker φ). Since ker φ = [k0 ]T0 + [k1 ]V0 , it follows that ker φ = [k0 ]ψ(T0 ) + [k1 ]ψ(V0 ) = [wk0 + yk1 ]T2 + [xk0 + zk1 ]V2 . Similarly, we obtain ker φ˜ = [wk˜0 + y k˜1 ]T˜2 + [xk˜0 + z k˜1 ]V˜2 . Since the coeﬃcients w, x, y, z were chosen such that the matrix ( wy xz ) is invertible, we obtain that k0 = k˜0 and k1 = k˜1 holds if and only if wk0 + yk1 =   wk˜0 + y k˜1 ) and xk0 + zk1 = xk˜0 + z k˜1 holds. ∗ We can now use the proof Piso to construct our proof of parallel isogeny knowledge. The prover runs two such proofs in parallel, with the same randomness (w, x, y, z), and responds to the veriﬁer’s challenges with the responses of the individual proofs. The resulting proof is represented explicitly in the full version of this paper [2]. The security proofs follow closely those of the PoIK Piso in Sect. 5.1: correctness of Piso implies correctness of Ppar , while the soundness of Ppar follows from the soundness of Piso and Lemma 9. The argument for zero-knowledge is also similar, but it is based on the hardness of the following problem, which takes into consideration that the two parallel instance partially share the same randomness.  
   
  Problem 10 (Double DSSP with Torsion (DDSSPwT) problem). Let D0 and D1 be as in Problem 8. Given: ˜0 → E ˜1 , 1. two d-isogenies φ : E0 → E1 , φ˜ : E ˜ ˜ ˜ 2. the points T0 , V0 ∈ E0 [d] and T0 , V0 ∈ E0 [d], ˜0 ∈ E ˜0 [n], where n = fM-SIDH (λ, d), 3. the points P0 , Q0 ∈ E0 [n] and P˜0 , Q distinguish between the following distributions: (E2 , T2 , V2 , P2 , Q2 , E3 , φ ), – D0∗ = ˜2 , T˜2 , V˜2 , P˜2 , Q ˜2, E ˜3 , φ˜ ), , where the curves and the n-torsion (E points follow the D0 -distribution, i.e. we have (E2 , P2 , Q2 , E3 , φ ) ← D0 , and ˜2, E ˜3 , φ˜ ) ← D0 , and moreover ˜2 , P˜2 , Q (E  
   
     
   
    
   
   ˜ T˜0 ) T˜2 T2 ψ(T0 ) ψ( , and =B = B ˜ V˜0 ) , V2 ψ(V0 ) V˜2 ψ( for some B ∈ GL2 (Zn ), and ψ and ψ˜ being respectively the s-isogenies ˜0 and E ˜2 that are guaranteed to exist because between E0 and E2 and E of the D0 distribution;  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  163  

  (E2 , T2 , V2 , P2 , Q2 , E3 , φ ), – = ˜2 , T˜2 , V˜2 , P˜2 , Q ˜2, E ˜3 , φ˜ ), , where the curves and the n-torsion (E points follow the D1 -distribution, i.e. we have (E2 , P2 , Q2 , E3 , φ ) ← D1 , and ˜2, E ˜3 , φ˜ ) ← D1 , and moreover the points T2 , V2 and T˜2 , V˜2 form a ˜2 , P˜2 , Q (E ˜2 [d], respectively. random basis of E2 [d] and E D1∗  
   
  The proof Ppar is a proof of knowledge, and it can be made non-interactive with standards transformations, such as the Fiat-Shamir [11] or the Unruh [23] transform. This is the ﬁrst non-interactive proof of parallelness. We discuss potential optimizations and the concrete cost of such a proof in the full version of this paper [2].  
   
  7  
   
  A New OPRF Protocol  
   
  In this section, we combine the countermeasures presented in Sect. 4, the SIDH countermeasures and the novel proof of isogeny knowledge discussed in Sect. 5, and the non-interactive proof of parallel isogeny introduced in Sect. 6 to obtain a veriﬁable OPRF protocol that is post-quantum secure, round-optimal, and moderately compact. The OPRF protocol is a two-party protocol between a user U and a server S. Let NM , NB , NK be coprime numbers representing the degrees of the message isogeny, the blinding isogeny, and the server’s isogeny, respectively. Let p be a ˜ be prime of the form p = NM NB NK f − 1, for some cofactor f , and let E0 , E two supersingular elliptic curves deﬁned over Fp2 . Moreover, let P, Q be a ﬁxed ˜ be a ﬁxed basis of E[N ˜ K ]. The ﬁrst curve is used basis of E0 [NM ] and let P˜ , Q to compute the PRF, while the second is used within the server’s commitment. At a high-level, to evaluate the OPRF on an input x, the user maps the input to a curve Em according to Algorithm 1 and computes a blinding isogeny φb : Em → Emb . The user then sends the codomain curve, together with torsion images and a proof of their correctness, to the server, which computes a second isogeny φk : Emb → Embk . The torsion information is appropriately masked to avoid the SIDH attacks. The server then responds with the curve Embk , some torsion information, a proof of their correctness, and a proof that it has used the previously-committed secret key. The user then concludes by using the torsion information provided by the server to undo the blinding isogeny and compute the curve Emk . Its j-invariant is then hashed together with the input and the server’s public key to form the PRF output. The protocol is described in Fig. 4, and it realizes the OPRF ideal functionality of Fig. 2, which allows us to state the following theorem. Theorem 11. The protocol described in Fig. 4 realizes the ideal functionality FvOPRF of Fig. 2 in the random oracle model. We sketch a proof in the full version of this paper [2].  
   
  164  
   
  A. Basso  
   
  Fig. 4. The veriﬁable OPRF protocol.  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  165  
   
  Parameter Selection. Firstly, we discuss how to select the starting curves ˜ As mentioned in Sect. 5, the cryptanalysis on masked-torsion SIDH E0 and E. with a starting curve with small endomorphism [12, Section 4.2] does not apply here, since the message isogeny removes this property from the starting curve of the blinding isogeny. Hence, the curve E0 does not need to have unknown ˜ as observed in [4], endomorphism ring. However, the situation is diﬀerent for E: ˜ allows to ﬁnd collisions in the server’s commitment. Thus, knowledge of End E ˜ would allow the server to break veriﬁability, since it could prove knowing End E ˜ is parallelness to two distinct isogenies. It is thus necessary that the curve E generated by a trusted party or through a multiparty trusted setup ceremony, such as the one presented in [3]. The main parameter of the OPRF protocol is the prime p. Firstly, if the message isogeny is the composition of many isogenies whose kernel is deﬁned over Fp4 , the value p + 1 does not need have a dedicated factor. Then, for the main exchange, i.e. the blinding, server’s isogeny, unblinding part, we need to smooth coprime integers NB and NK that are highly composite to prevent the SIDH attacks. Following the analysis of Sect. 5, we have NB ≈ NK ≈ 22956 . Lastly, the proofs of knowledge Piso and Ppar require a third cofactor NS that is coprime with both NB and NK . To guarantee the hardness of Problems 8 and 10, the integer NS needs to be of the same length as NB and NK . However, since torsion points of order NS do not need to be masked, the value NS can be a prime power. Putting this together, we obtain that the prime p needs to be of the form p = NB NS NK f − 1 and be at least 8868-bit long to guarantee λ = 128 bits of security. Note that the new computation of the message isogeny and the new proofs of knowledge has signiﬁcantly reduced the size of the prime; compared to the OPRF protocol by Boneh, Kogan, and Woo, we use a prime that is 5.8× larger, while relying on an SIDH protocol with isogenies that are 9.2× longer. Eﬃciency. We now estimate the communication cost of the OPRF protocol. The largest components are the non-interactive proofs of knowledge: given the analysis of the previous sections, they are less than 1.7λ(35 log p + 51λ)bit long. Since log p ≈ 10λ log λ, we obtain that one OPRF execution requires 1.7λ2 (350 log λ + 51) bits of communication. For λ = 128, this corresponds to a transcript of 8.7 MB. We remark that the size of the proofs is particularly large due to the Unruh transform needed to prove security in the UC framework. If the proofs were made non-interactive via the Fiat-Shamir transform, a single execution of the veriﬁable OPRF with λ = 128 would require 1.9 MB of communication on average and 3.8 MB in the worst case. Such an OPRF may be used in instances where security in the UC framework is not necessary. A direct comparison with the protocol by Boneh, Kogan, and Woo [5] is not simple since their bandwidth estimate does not appear to include the Unruh transform overhead. We estimate that one execution of the OPRF from [5] requires at least 10.9 MB. Our protocol is thus more compact than that in [5], despite being round-optimal and secure against both the one-more unpredictability attack and the SIDH attacks. This is made possible by the fact that the sigma  
   
  166  
   
  A. Basso  
   
  protocols are highly optimized and have ternary challenges, which signiﬁcantly reduces the overhead introduced in the Unruh transform. Indeed, if we compare a version of the two protocols with the Fiat-Shamir transform, our OPRF uses 31% more bandwidth than the one in [5]. We summarize the state of post-quantum OPRF protocols in Table 1. Table 1. Post-quantum OPRF protocols secure against malicious clients.  
   
  8  
   
  Protocol  
   
  Rounds Bandwidth (avg.) Veriﬁable Secure  
   
  [1] (LWE) [5] (CSIDH) [5] (SIDH)FO [5] (SIDH)Unruh [This work]FO [This work]Unruh  
   
  2 3 6 6 2 2  
   
  >128 GB 424 kB 1.4 MB >10.9 MB 1.9 MB 8.7 MB  
   
  ✓ ✗ ✓ ✓ ✓ ✓  
   
  ✓ ✓ ✗ ✗ ✓ ✓  
   
  Conclusion  
   
  In this work, we presented a post-quantum veriﬁable OPRF protocol that is moderately compact and round-optimal. The protocol is the ﬁrst round-optimal OPRF based on isogenies, and it is several orders of magnitude more compact than the existing round-optimal protocol. To obtain this protocol, we started from an insecure protocol by Boneh, Kogan, and Woo, and we proposed an eﬃcient countermeasure against the one-more unpredictability attack, integrated the existing SIDH countermeasures, developed a new zero-knowledge proof of isogeny that works with the SIDH countermeasures, and proposed a new noninteractive proof of parallel isogeny that reduced the number of rounds to two. The protocol is an important stepping stone towards fully practical postquantum OPRFs, but its performance is hindered by the ineﬃciency of the SIDH countermeasures. In future work, we aim at developing more eﬃcient solutions: a moderate reduction in the degree of the isogenies would signiﬁcantly improve the eﬃciency of the protocol. It is also interesting to improve the proof of parallel isogeny by avoiding validating the commitment isogeny at every interaction. Acknowledgements. The author thanks Christophe Petit and Luca de Feo for various suggestions, and Tako Boris Fouotsa, Christophe Petit, Chloe Martindale, and the anonymous reviewers of Crypto and the PQCifris workshop for feedback on earlier versions of this work. The author would also like to thank Luca de Feo, Antonin Leroux, and Benjamin Wesolowski for fruitful discussions on isogeny-based zero-knowledge proofs at the Banﬀ International Research Station workshop “Supersingular Isogeny Graphs in Cryptography”. This work has been supported in part by EPSRC via grant EP/R012288/1, under the RISE (http://www.ukrise.org) programme.  
   
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   
  167  
   
  References 1. Albrecht, M.R., Davidson, A., Deo, A., Smart, N.P.: Round-optimal veriﬁable oblivious pseudorandom functions from ideal lattices. In: Garay, J. (ed.) PKC 2021, Part II. LNCS, vol. 12711, pp. 261–289. Springer, Heidelberg (2021). https://doi. org/10.1007/978-3-030-75248-4 10 2. Basso, A.: A post-quantum round-optimal oblivious PRF from isogenies. Cryptology ePrint Archive, Report 2023/225 (2023). https://eprint.iacr.org/2023/225 3. Basso, A., et al.: Supersingular curves you can trust. In: Hazay, C., Stam, M. (eds.) EUROCRYPT 2023, Part II. LNCS, vol. 14005, pp. 405–437. Springer, Heidelberg (2023). https://doi.org/10.1007/978-3-031-30617-4 14 4. Basso, A., Kutas, P., Merz, S.P., Petit, C., Sanso, A.: Cryptanalysis of an oblivious PRF from supersingular isogenies. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021, Part I. LNCS, vol. 13090, pp. 160–184. Springer, Heidelberg (2021). https:// doi.org/10.1007/978-3-030-92062-3 6 5. Boneh, D., Kogan, D., Woo, K.: Oblivious pseudorandom functions from isogenies. In: Moriai, S., Wang, H. (eds.) ASIACRYPT 2020, Part II. LNCS, vol. 12492, pp. 520–550. Springer, Heidelberg (2020). https://doi.org/10.1007/978-3-030-648343 18 6. Canetti, R.: Universally composable security: a new paradigm for cryptographic protocols. In: 42nd FOCS, pp. 136–145. IEEE Computer Society Press, October 2001. https://doi.org/10.1109/SFCS.2001.959888 7. Castryck, W., Decru, T.: An eﬃcient key recovery attack on SIDH. In: Hazay, C., Stam, M. (eds.) Advances in Cryptology – EUROCRYPT 2023. EUROCRYPT 2023, Part V. LNCS, vol. 14008, pp. 423–447. Springer, Heidelberg (2023). https:// doi.org/10.1007/978-3-031-30589-4 15 8. Charles, D.X., Lauter, K.E., Goren, E.Z.: Cryptographic hash functions from expander graphs. J. Cryptol. 22(1), 93–113 (2009). https://doi.org/10.1007/ s00145-007-9002-x 9. Chaum, D.: Blind signatures for untraceable payments. In: Chaum, D., Rivest, R.L., Sherman, A.T. (eds.) CRYPTO’82, pp. 199–203. Plenum Press, New York, USA (1982) 10. Eisentr¨ ager, K., Hallgren, S., Lauter, K.E., Morrison, T., Petit, C.: Supersingular isogeny graphs and endomorphism rings: reductions and solutions. In: Nielsen, J.B., Rijmen, V. (eds.) EUROCRYPT 2018, Part III. LNCS, vol. 10822, pp. 329–368. Springer, Heidelberg (2018). https://doi.org/10.1007/978-3-319-78372-7 11 11. Fiat, A., Shamir, A.: How to prove yourself: Practical solutions to identiﬁcation and signature problems. In: Odlyzko, A.M. (ed.) CRYPTO’86. LNCS, vol. 263, pp. 186–194. Springer, Heidelberg (1987). https://doi.org/10.1007/3-540-47721-7 12 12. Fouotsa, T.B., Moriya, T., Petit, C.: M-SIDH and MD-SIDH: countering SIDH attacks by masking information. In: Hazay, C., Stam, M. (eds.) EUROCRYPT 2023, Part V. LNCS, vol. 14008, pp. 282–309. Springer, Heidelberg (2023). https:// doi.org/10.1007/978-3-031-30589-4 10 13. Freedman, M.J., Ishai, Y., Pinkas, B., Reingold, O.: Keyword search and oblivious pseudorandom functions. In: Kilian, J. (ed.) TCC 2005. LNCS, vol. 3378, pp. 303– 324. Springer, Heidelberg (2005). https://doi.org/10.1007/978-3-540-30576-7 17 14. Jao, D., De Feo, L.: Towards quantum-resistant cryptosystems from supersingular elliptic curve isogenies. In: Yang, B.Y. (ed.) Post-Quantum Cryptography - 4th International Workshop, PQCrypto 2011, pp. 19–34. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-25405-5 2  
   
  168  
   
  A. Basso  
   
  15. Jarecki, S., Kiayias, A., Krawczyk, H.: Round-optimal password-protected secret sharing and T-PAKE in the password-only model. In: Sarkar, P., Iwata, T. (eds.) ASIACRYPT 2014, Part II. LNCS, vol. 8874, pp. 233–253. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-45608-8 13 16. Jarecki, S., Kiayias, A., Krawczyk, H., Xu, J.: TOPPSS: cost-minimal passwordprotected secret sharing based on threshold OPRF. In: Gollmann, D., Miyaji, A., Kikuchi, H. (eds.) ACNS 17. LNCS, vol. 10355, pp. 39–58. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-319-61204-1 3 17. Jarecki, S., Krawczyk, H., Xu, J.: OPAQUE: an asymmetric PAKE protocol secure against pre-computation attacks. In: Nielsen, J.B., Rijmen, V. (eds.) EUROCRYPT 2018, Part III. LNCS, vol. 10822, pp. 456–486. Springer, Heidelberg (2018). https://doi.org/10.1007/978-3-319-78372-7 15 18. Jarecki, S., Liu, X.: Eﬃcient oblivious pseudorandom function with applications to adaptive OT and secure computation of set intersection. In: Reingold, O. (ed.) TCC 2009. LNCS, vol. 5444, pp. 577–594. Springer, Heidelberg (2009). https:// doi.org/10.1007/978-3-642-00457-5 34 19. Maino, L., Martindale, C., Panny, L., Pope, G., Wesolowski, B.: A direct key recovery attack on SIDH. In: Hazay, C., Stam, M. (eds.) EUROCRYPT 2023, Part V. LNCS, vol. 14008, pp. 448–471. Springer, Heidelberg (2023). https://doi.org/ 10.1007/978-3-031-30589-4 16 20. Merz, S.P., Minko, R., Petit, C.: Another look at some isogeny hardness assumptions. In: Topics in Cryptology - CT-RSA 2020 - the Cryptographers’ Track at the RSA Conference 2020, San Francisco, CA, USA, 24–28 February 2020, Proceedings, pp. 496–511 (2020) 21. Naor, M., Reingold, O.: Number-theoretic constructions of eﬃcient pseudo-random functions. In: 38th FOCS, pp. 458–467. IEEE Computer Society Press, October 1997. https://doi.org/10.1109/SFCS.1997.646134 22. Robert, D.: Breaking SIDH in polynomial time. In: Hazay, C., Stam, M. (eds.) EUROCRYPT 2023, Part V. LNCS, vol. 14008, pp. 472–503. Springer, Heidelberg (2023). https://doi.org/10.1007/978-3-031-30589-4 17 23. Unruh, D.: Non-interactive zero-knowledge proofs in the quantum random oracle model. In: Oswald, E., Fischlin, M. (eds.) EUROCRYPT 2015, Part II. LNCS, vol. 9057, pp. 755–784. Springer, Heidelberg (2015). https://doi.org/10.1007/9783-662-46803-6 25  
   
  Traceable Ring Signatures from Group Actions: Logarithmic, Flexible, and Quantum Resistant Wei Wei , Min Luo(B) , Zijian Bao , Cong Peng , and Debiao He(B) Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China {weiwei_only,mluo,cpeng}@whu.edu.cn, [email protected]  , [email protected]   
   
  Abstract. Traceable ring signature (TRS) is a variation of ring signature, allowing to expose the user’s identity whenever he signs two diﬀerent messages under the same tag. The accountable anonymity of TRS makes it widely used in many restrained anonymous applications, e.g., e-voting system, oﬄine coupon service. Traditional TRS schemes are built on mathematical problems, which are believed to be easy to solve by quantum computers. While numerous post-quantum (traceable) ring signature schemes have been proposed so far, there has been no TRS scheme based on isogenies proposed. We construct two TRS schemes from group actions that can be instantiated with isogenies and lattices. The critical technique is to generate multiple tags for the message and design an OR sigma protocol to generate proofs for multiple tag sets, which provides traceability for the TRS scheme. The signature size can be expressed as O(log N ), where N represents the ring size. Based on different instantiation parameters, our proposed scheme enables ring members to negotiate the signature size and signing time according to their speciﬁc requirements. Moreover, we prove the security of our scheme under the standard random oracle model. Keywords: Traceable ring signature · Post-Quantum cryptography · Isogeny-based cryptography · Lattice-based cryptography · OR sigma protocol  
   
  1  
   
  Introduction  
   
  Ring Signature (RS) [30] allows the signer to sign a message on behalf of the group without revealing the signer’s identity. Traceable ring signature (TRS) is a variation of the RS, if a signer produces two signatures for diﬀerent messages under the same tag, then the identity of the signer can be extracted by the ring This work was supported by the Key Research and Development of Shandong Province under Grant 2021CXG010107; in part by the National Natural Science Foundation of China under Grant U21A20466, Grant 62172307, Grant 61972294 and Grant 61932016. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 169–188, 2024. https://doi.org/10.1007/978-3-031-53368-6_9  
   
  170  
   
  W. Wei et al.  
   
  members, if signatures are for the same message, everyone can know that the two signatures were generated by the same signer. TRS limits the indubitable anonymity of ring signature, the tag in TRS consists of a group of members and a topic, the topic string refers to a social issue or voting. In many anonymous information systems such as e-voting [9] and oﬄine coupon services [21], users are not expected to sign messages twice under the same tag, e.g., double-spending, multiple voting. The TRS scheme mitigates this dishonest behavior, and further protects the privacy of members, therefore, it becomes a powerful cryptographic tool in such systems. The concept of TRS was proposed by Fujisaki and Suzuki [22] in 2007. Since then, several variant schemes [2,21] have been proposed to improve security or performance. However, these proposals are built on number theory, which can be solved by a large-scale quantum computer running Shor’s algorithm [33]. Consequently, a quantum-resistant ring signature and related variant schemes have drawn much attention over the past ten years. Lattice-based cryptography is one of the most promising candidates in post-quantum cryptography. In addition to resisting quantum attacks, it has the advantage of better performance. Isogeny-based cryptography was ﬁrst proposed by [11,31], it is an extension and thorough study of classical elliptic curve cryptography. Compared with other post-quantum cryptography candidates, isogeny-based cryptography stands out for its comparatively shorter key sizes [24]. Recently, various schemes based on isogeny assumption have been proposed: signature schemes [5,19], ring signature scheme [4], revocable ring signature scheme [24] and accountable ring signature scheme [10]. Although these constructions and many post-quantum ring signature schemes (including variants) from lattices, code and symmetric cryptographic primitives have been proposed successively, an eﬃcient isogeny-based TRS scheme has yet to be reported in the literature. To ﬁll this gap, we propose a general TRS scheme from group actions and instantiate the group action with isogenies and lattices. To the best of our knowledge, the isogeny-based instantiation is the ﬁrst isogeny-based TRS scheme. It provides a smaller signature size than lattice-based instantiation. However, its signiﬁcant overhead is signing time which is caused by the complex operation of isogeny. Under diﬀerent instantiation parameters, users can ﬂexibly customize the signature size and signing time according to their requirements with diﬀerent instantiation parameters. Note that the isogeny-based instantiation in this paper is from CSIDH [8]. The latest attack on isogenies proposed by Castryck and Decru [7] leads to key leakage in SIDH. This method has no impact on the security of primitives such as CSIDH and SQISign [19]. The eﬃciency of our TRS scheme is discussed in detail in Sect. 5.1. 1.1  
   
  Related Work on Post Quantum Ring Signature  
   
  Lattice-based schemes. The ﬁrst lattice-based ring signature was proposed by Libert et al. [26], which is non-linkable. Lu et al. [27] developed a general lattice-based (linkable) ring signature scheme from the short integer solution  
   
  Traceable Ring Signatures from Group Actions  
   
  171  
   
  (SIS) and NTRU assumptions. Esgin et al. [15] extended discrete logarithm proof techniques to the lattice setting in the one-out-of-many proofs, and designed a short ring signature scheme. They further optimized one out of many proofs, resulting in a smaller size ring signature scheme [14]. Then, they introduced a zero-knowledge proof and extractable commitment scheme from lattices, and designed an eﬃcient RingCT protocol [16]. Feng et al. [17] constructed an eﬃcient TRS scheme and instantiated the scheme with lattice-based building blocks: non-interactive zero-knowledge proof, collision-resistant hash function, and pseudorandom function. Nguyen et al. [28] proposed a unique ring signature (URS) scheme from lattices, which exploited a Merkle tree based accumulator as the building block. Isogeny-Based Schemes. Beullens et al. [4] constructed an eﬃcient (linkable) ring signature scheme and gave two concrete instances from isogenies and lattices. The signature size of their scheme scales with the number of ring members. Then, they constructed an accountable ring signature based on isogeny and lattice assumptions [3]. Through adding a valid ciphertext proof to their OR protocol and building an online extractable non-interactive zero-knowledge proof system, the signature size grows in O(log N ). Chung et al. [10] proposed a group signature and accountable ring signature scheme based on the decisional CSIDH assumptions (D-CSIDH) and proved the security of scheme under the quantum random oracle model (QROM), the signature size grows in O(N 2 ). Lai and Dobson [24] introduced the ﬁrst revocable ring signature (RRS) scheme from isogenies, which is proved secure under the QROM, the signature size grows in O(N log(N )). Other Post-quantum Schemes. Branco and Mateus [6] built a post-quantum resistant TRS scheme based on the syndrome decoding problem. Their scheme was built on the Fiat-Shamir heuristic [20], they gave the security proof under the classic random oracle. Derler et al. [13] proposed the ﬁrst sub-linear ring signature scheme from symmetric primitives. Scafuro and Zhang [32] introduced a one-time TRS scheme based on hash-function symmetric-key primitive. Overall, the TRS schemes based on lattices, code and symmetric primitives have better performance, but the signature size of these schemes is large. Especially, the post-quantum TRS schemes that can be instantiated by isogenies and lattices are still in their infancy. This work proposes a general OR sigma protocol construction and constructs two TRS schemes from isogeny-based and latticebased group action primitives. Both instantiations of the TRS schemes have a logarithmic communication complexity. Compared with other post-quantum schemes, the isogeny-based instantiation has the advantage of a smaller signature size, the lattice-based instantiation has a shorter signing time. Finally, we prove the security of our TRS scheme under the random oracle model. 1.2  
   
  Contribution  
   
  The major contribution of this work is the construction of a TRS from restricted group action in the random oracle model (ROM). As far as we know, this is the ﬁrst TRS scheme that can be instantiated with isogenies.  
   
  172  
   
  W. Wei et al.  
   
  – We propose a general TRS scheme based on restricted pair of group actions, OR sigma protocol and collision-resistant hash function. Furthermore, we instantiate the group action from isogenies and lattices to construct two TRS schemes. – We design a special OR protocol for the TRS scheme. The core of our technique is to provide traceability by generating tag sets based on messages and user identities. Traceability will be possible by checking whether each tag/vector in the two tag/vector sets is equal. Further, we add OR proof for multiple tag sets to ensure validity. – The scheme has logarithmic communication complexity. In order to reduce the signature size, we generate two Merkle trees and set the response as two paths in the tree in case challenge bit chall = 0, when challenge bit chall = 1, we send a seed as the response. Compared with other post-quantum TRS schemes, the signature size of our proposed TRS extends well with the ring size N , and our multiplicative factor on log N is much lower since the signatures mainly consist of two paths in a Merkle tree of depth log N . – The time and size of the signature can be ﬂexibly customized from diﬀerent instantiation parameters of the OR sigma protocol. The isogeny-based instantiation has a smaller signature size and lattice-based instantiation has better performance. 1.3  
   
  Overview of Results  
   
  In this paper, we will construct a general TRS scheme with generic security in terms of tag-linkability, anonymity and exculpability. With isogeny and lattice instantiation, it is resistant to attacks by quantum adversaries. There is a (linkable) ring signature framework [4] that has been proposed, this general construction utilizes the “admissible group action” primitive and is built upon a OR sigma protocol. However, for the dishonest users who signs the same message or two diﬀerent messages twice, it does not have the ability to track the identity of dishonest users. By adding multiple tags to the OR proof, we prove that the tag was generated by the signer while tracing the identity of the signer by comparing each tag in the tag set. The security of TRS scheme from isogenies in Sect. 3 relies on the group action inverse problem and its equivalent hard problems [34], the security of lattice-based instantiation relies on the module short integer solution problem and module learning with errors problem [25], which are believed to be resistant to attacks by quantum adversaries. According to the experimental results, the smaller the value of Q is, the less time it takes for signature generation and veriﬁcation. The minimum signature size is obtained when K = 36, where K and Q − K are the number of 0 and the number of 1 in the challenge space. Speciﬁcally, we have the 64/4096 bytes public key size under two diﬀerent instantiations. The secret key of a user is 16 bytes. The signature size of our TRS scheme relies on the proof size of the OR protocol, which is logarithmic. It is approximately 2 log N + 2.45/2 log N + 55.37 KB under the speciﬁc instantiation parameters and outperforms the post-quantum traceable signature size of [6,17,32].  
   
  Traceable Ring Signatures from Group Actions  
   
  173  
   
  Among the post-quantum signature schemes we investigated, these schemes support either linkability or traceability. In the case of supporting two properties, the signature size scales linearly with the ring size. Compared with lattice-based TRS schemes, our signature size under lattice-based instantiation is acceptable. Compared with isogeny-based linkable ring signature (without traceability), our scheme provides traceability with a smaller signature size. The details of the performance of TRS will be presented in Sect. 5.1.  
   
  2 2.1  
   
  Preliminaries Traceable Ring Signature  
   
  In this section, we review the TRS scheme proposed by Fujisaki and Suzuki [22]. Assuming that N is the number of users in the ring, PK = (pk1 , . . . , pkN ) is ring member’s public keys set, issue is a string representing the speciﬁc event of the signature and L = (issue, PK) is the tag of the signature. A TRS scheme consists of ﬁve algorithms TRS = (Setup, KeyGen, Sign, Verify, Trace) described as follows: – pp ← Setup(1λ ): The algorithm run by the trusted authority, which takes as input security parameter λ ∈ N and outputs public parameter pp. – (pk, sk) ← KeyGen(pp): The algorithm run by the ring member, which takes as input public parameter pp and returns public key pk and secret key sk. – σ ← Sign(skπ , L, M ): The algorithm run by the ring member, which takes as input the secret key skπ , a tag L and a message M ∈ {0, 1}∗ , and returns a signature σ. – {accept, reject} ← Verify(L, M, σ): The algorithm run by the signature receiver, which takes as input the tag L, message M and signature σ, and returns either accept or reject. – {indep, linked, pk} ← Trace(L, M, σ, M  , σ  ): The algorithm run by the ring member or trusted authority, which takes as input two traceable ring signatures σ on message M and σ  on message M  with the same tag L, and returns a string that is either indep, linked or an element pk ∈ PK. If σ = Sign(skπ , L, M ) and σ  = Sign(skπ , L, M  ), it holds that : ⎧ ⎪ ⎨indep   Trace(L, M, σ, M , σ ) = linked ⎪ ⎩ pki  
   
  2.2  
   
  if π = π  , else if M = M  , otherwise (π = π  ∧ M = M  ).  
   
  Security Model  
   
  A secure TRS scheme should satisfy the following properties: correctness and security. We use the security model in [21]. The security requirement for a TRS  
   
  174  
   
  W. Wei et al.  
   
  scheme has three: tag-linkability, anonymity and exculpability. The unforgeability can be derived from tag-linkability and exculpability [21]. Tag-linkability. Given N pairs of public and secret keys and N pairs of messagesignature under tag L, the adversary can output N + 1 valid pairs of messagetag−link (λ) ≤ nelg(λ), then signature. If for all PPT adversaries A, we have AdvA,Game we say that TRS scheme is tag-linkable. Anonymity. It is infeasible for the adversary to know who signed the message. anon If for all PPT adversaries A, we have AdvA,Game (λ) ≤ nelg(λ), then we say that TRS scheme is anonymous. Exculpability. This ensures that the adversary cannot construct two valid pairs of message-signature under tag L without knowing the secret key of the user. Consider the exculpability game Gameexcu A , if for all PPT adversaries A, we have excu (λ) ≤ nelg(λ), then we say that TRS scheme is exculpable. AdvA,Game 2.3  
   
  Restricted Pair of Group Actions  
   
  The restricted eﬀective group actions (REGA) can be endowed with the properties: one-wayness (OW), weak unpredictability (wU), and weak pseudorandomness (wPR) [1]. The special restricted pair of group actions used in this paper is called “admissible pair of group actions”, which is proposed by Beullens et al. [4]. Definition 1. Given a ﬁnite commutative group G, G 1 and G 2 are two subsets of G. Let S and T be two ﬁnite sets, DS and DT are distributions over two group actions  : G × S → S, G × T → T . For (S0 , T0 ) ∈ S × T , we say that ResPGA = (G, S, T , G 1 , G 2 , DS , DT ) is a ξ-restricted pair of group actions if the following holds: 1. Eﬃcient Group Action: For any g ∈ G 1 ∪ G 2 and (S, T ) ∈ S × T , it is eﬃcient to compute g  S and g  T , and uniquely represent the element of set G, S and T . 2. Eﬃcient Rejection Sampling:  For all g ∈ G 1 , the intersection of all sets G 2 + g is large enough. Let G 3 = g∈G 1 G 2 + g, then |G 3 | = ξ|G 2 |. 3. Eﬃcient Membership Testing: It is eﬃcient to verify that an element z ∈ G 1 , or z ∈ G 2 , or z ∈ G 3 . 4. Given (g  S0 , g  T0 ) for any element g sampled from G 1 uniformly, it is indistinguishable from the elements (S, T ) sampled from S × T uniformly. 5. It is diﬃcult to ﬁnd two elements g, g  ∈ G 2 + G 3 , that satisfy g  S0 = g   S0 and g  T0 = g   T0 . 6. For the element g sampled from set G 1 uniformly, given S = g  S0 and T = g  T0 , it is diﬃcult to ﬁnd g  ∈ G 2 + G 3 such that T = g   T0 .  
   
  Traceable Ring Signatures from Group Actions  
   
  2.4  
   
  175  
   
  Collision-Resistant Hash Function  
   
  In this paper, the cryptographic primitives used in the TRS scheme, such as pseudo-random number generators (PRG) and commitment schemes, are instantiated by the hash function. Speciﬁcally, we deﬁne ﬁve collision-resistant hash functions: H1 , H2 , H3 , H4 and H5 , where: H1 : {0, 1}∗ → G 1 , H2 : {0, 1}∗ → {0, 1}2λ , Q H3 : {0, 1}∗ → CK ,  
   
  H4 : {0, 1}∗ → G †1 , H5 : {0, 1}∗ → G †† 1 . For the Fiat-Shamir transform, we deﬁne a hash function H3 to produce an Q , which is a  set of string in {0, 1}Q , such that K unbalanced challenge space CK Q ≥ 2λ1 . bits are 0. The integers Q, K satisfying K 2.5  
   
  Sigma Protocol  
   
  A sigma protocol is a three-move public coin interactive protocol between the prover and veriﬁer for the relation R ⊆ X×W , where X is the space of statements and W is the space of witnesses. The sigma protocol under the random oracle includes the following three properties: correctness, special honest-veriﬁer zeroknowledge and special soundness [12]. Definition 2. A sigma protocol ΠΣ for the relation R ⊆ X × W consists of four PPT algorithms (P = (P1 , P2 ), V = (V1 , V2 )), where V2 is deterministic, P1 and P2 share the same information. Under the random oracle, the ΠΣ protocol has the three-move ﬂow as follows: – P1 (X, W ) → com. The prover runs P1 (X, W ) on input (X, W ) ∈ R to generate a commitment com, and sends it to the veriﬁer. – V1 (com) → chall. The veriﬁer runs V1 (com) on input com to generate a random challenge bit chall, and sends it to the prover. – P2 (X, W, chall) → rsp. The prover, after receiving chall, runs P2 (X, W, chall) to obtain the response rsp and sends it to the veriﬁer. In the case of P2 termination, the prover sets rsp with symbol ⊥ and sends it to the veriﬁer. → {accept, reject}. The veriﬁer – V2 (X, com, chall, rsp) runs V2 (X, com, chall, rsp) to check whether X is valid under the transcript (com, chall, rsp), and outputs accept or reject.  
   
  1  
   
  Under the and lattice-based instantiations, G †1 and G †† 1 are two diﬀerent subsets of G, the speciﬁc sets are shown is Sect. 5.  
   
  176  
   
  3  
   
  W. Wei et al.  
   
  General Construction of Traceable Ring Signature  
   
  In this section, we will present a general construction of the TRS scheme from restricted pair of group actions. We ﬁrst design a sigma protocol for the OR relation, then obtain a TRS scheme by applying the Fiat-Shamir transformation to the OR sigma protocol. 3.1  
   
  Our Special or Sigma Protocol for Traceable Ring Signature  
   
  Our construction is based on a special OR sigma protocol, a variant of OR sigma protocol presented in [4] by adding the tag set. The essential technique of the protocol is to generate proofs for multiple tags and multiple public-secret key pairs, and the proof size of our sigma protocol grows logarithmically in N . Let the relation R ⊂ S N +1 ×T N +1 ×(G 1 , ZN ), where R = {(S0 , S1 , . . . , SN ), (T0 , T1 , . . . , TN ),(g, π), |g ∈ G 1 , Si ∈ S, Ti ∈ T , Sπ = g  S0 , Tπ = g  T0 }. We deﬁne a relation R slightly wider than the relation R, and (R, R ) satisﬁes R ⊆ R , in addition to the relation R, R contains two pairs of hash-preimage, and the extractor in special-soundness only extracts the witness of relation R . Under the relation (R, R ), the OR sigma protocol is still useful as long as the relation (R, R ) is suﬃciently diﬃcult. ⎧ ⎫ Si ∈ S, Ti ∈ T and ⎪ ⎪ ⎪ ⎪ ⎨ w = (g, π) : g ∈ G 2 + G 3 , Sπ = g  S0 , ⎬  R = (S0 , . . . , SN ) , (T0 , . . . , TN ) , w Tπ = g  T0 or ⎪ ⎪ ⎪ ⎪ ⎩ w = (x, x ) : x = x , H2 (x) = H2 (x ) ⎭ The diﬃculty of applying the accumulator to our construction is that each instance in the relation (R, R ) is a pair of elements (Si , Ti ) rather than a single element. We solve this problem by hashing the commitments after applying the accumulator scheme to get the ﬁnal commitments. Based on the relation (R, R ), the OR sigma protocol proves that: 1) the prover owns a secret g and there exists i ∈ N , such that g  S0 = Si , g  T0 = Ti , without revealing the secret g and speciﬁc index i. Otherwise, 2) the prover owns a pair of collisions for H2 . Given (S0 , S1 , . . . , SN ), (T0 , T1 , . . . , TN ), we propose a sigma protocol (P, V ) = ((P1 , P2 ), (V1 , V2 )) under binary challenge space, for proving the ring member Pπ possesses the secret key sk that satisﬁes relation (R, R ). A simple OR sigma protocol is shown in Fig. 1 and the speciﬁc OR sigma construction as Fig. 2. Through repeating the basic OR sigma protocol under binary space, we construct a main OR sigma protocol under large challenge space and optimize it using three optimizations: unbalanced challenge space, Seed tree, and adding salt [4].  
   
  Traceable Ring Signatures from Group Actions  
   
  177  
   
  Fig. 1. The base OR sigma protocol, which proves that secret (skπ , π) satisﬁes skπ  S0 = Sπ and skπ  T0 = Tπ . If chall = 0, then the commitments Cπ and Cπ will be revealed, otherwise all commitments will be revealed.  
   
  3.2  
   
  Traceable Ring Signature from or Sigma Protocol  
   
  We now present two concrete TRS schemes based on isogenies and lattices. The instantiation of both schemes is built on the aforementioned main OR sigma protocol, mainly by combining the design principles of Fujisaki and Suzuki [22] with restricted pair of group actions. Given the security parameters λ, the main OR sigma protocol (Pmain , Vmain ), and the collision-resistant hash function H1 , H2 , H3 , H4 and H5 , we construct two secure TRS schemes ΠISO and ΠLAT by applying FS transform to main OR sigma protocol. Figure 3 illustrates two instantiations of the TRS scheme under lattice and isogeny. The general construction of the Setup and KeyGen in both schemes is as follows. – Setup(1λ ): takes security parameter λ as input, selects S0 ← S, and outputs public parameter rpp = S0 , ResPGA = (G, S, T , G 1 , G 2 , DS , DT ). – KeyGen(rpp): takes public parameter as input, selects g ← G 1 , S = g  S0 , and outputs public key pki = S and secret key ski = g. To ensure that the secret key is embedded in the tag, and that the components in the signature do not disclose any information about the secret key and the identity of the member, we apply group action operations in the calculation of the tags and auxiliary parameters. Concretely, we set T0 = H1 (L)  S0 and the auxiliary parameter T = (skπ − H1 (a, π))  T0 in the isogeny-based instantiation. The lattice-based instantiation is slightly diﬀerent since the secret key is not sampled in the addition group, but in the polynomial ring, which supports  
   
  178  
   
  W. Wei et al.  
   
  Fig. 2. The details of binary challenge space OR sigma protocol (P, V ) = = ((P1 , P2 ), (V1 , V2 )), under a restricted pair of group actions ResPGA (G, S, T , G 1 , G 2 , DS , DT ) and (S0 , T0 ) ∈ S × T , PRG and hash function H2 is an instantiation of random oracle.  
   
  Traceable Ring Signatures from Group Actions  
   
  179  
   
  Fig. 3. The isogeny-based TRS scheme (left column) and lattice-based TRS scheme (right column) from group actions.  
   
  180  
   
  W. Wei et al.  
   
  multiplication and addition, we set T0 = H4 (L), Tπ = skπ  T0 and auxiliary parameter aux = (Tππ−a) . As a result, the cost of the Trace algorithm diﬀers between the two instantiations, with the isogeny-based instantiation requiring 2N group actions and the lattice-based instantiation requiring only 2N polynomial multiplications and additions. Let (S0 , S1 , . . . , SN ) be the public parameter, each member Pi possesses a pair of public and secret keys: ski = g, pki = g  S0 . Moreover, each member will generate N diﬀerent tags (T0 , T1 , . . . , TN ) to link or trace signatures. In order to generate the ring signature σ for the message M ∈ {0, 1}∗ under the tag L = (issue, rpk), the ring member Pπ invokes RSign{ISO,LAT} (skπ , L, M ). The receivers verify signature σ on (L, M ) by running RVer{ISO,LAT} (L, M, σ). To trace the relation between two valid signatures σ on M and σ  on M  with the same tag L, the ring members invoke RTrace{ISO,LAT} (L, M, σ, M  , σ  ) which outputs linked, indep or pki .  
   
  4  
   
  Analysis of Our Traceable Ring Signature Scheme  
   
  In this section, we analyzed the correctness and security of TRS scheme under isogeny-based instantiation. A detailed proof of the lattice-based TRS scheme is presented in full version of paper. 4.1  
   
  Correctness  
   
  The correctness of our TRS scheme ΠISO is composed of completeness and traceability. The completeness can be deduced from the correctness of the main OR sigma protocol. The detailed proof is presented in full version of paper. 4.2  
   
  Security  
   
  Theorem 1. If the OR sigma protocol is soundness and zero-knowledge, the hash function H1 , H2 are collision-resistant, the ResPGA is a restricted pair of group actions, then our TRS scheme ΠISO satisﬁes tag-linkability, anonymity and exculpability. Proof. Tag-Linkability. Conversely, assuming there exists an adversary A that makes at most B random oracle queries, the probability of A winning the game is not negligible. Then we demonstrate how to construct an algorithm B using A, B breaks the Item 5 of ResPGA and collision resistance of H2 . The simulation of B under random oracle is as follows: – Sim1B : The output of A in winning the tag-linkability game is the input of B. Let {(L, (M1 , σ1 )), . . . , (L, (MN +1 , σN +1 ))} be the output of A, σi = (T i , comi , challi , rspi ), challi is the output of H3 on input (Mi , rpki , TagSeti , comi ), A records these transcripts into list List = {i, T i , (comi , challi , rspi ), Mi }i∈[N +1] .  
   
  Traceable Ring Signatures from Group Actions  
   
  181  
   
  – Sim2B : B re-invokes A until A wins the tag-linkability game. Diﬀerent from Sim1B , B controls the randomness used in the underlying main OR sigma protocol to generate signatures in each query. Speciﬁcally, for responding to the j-th signing query, the randomness used by the underlying main OR sigma protocol is the same as Sim1B before qi -th (qi ∈ [B]) random oracle, after that, B uses fresh randomness to interact with A. Assuming that the output form of  A is σ  = (T j , (comj , challj , rspj ), Mj , L)j∈[N +1] . If the signature σj does not appear in the qj -th random oracle query, the simulation of B starts again from  Sim2B , otherwise, B updates list List = List ∪ {j, T j , (comj , challj , rspj ), Mj }, challj is the result of the qj -th random oracle query. Since we ﬁx the randomness before qj -th for both simulations, for all the entries in the list, we have  (Mj , T j , comj ) = (Mj , T j , comj ). – Sim3B : B extracts two entries from List that satisfy the above requirements, which one generated in Sim1B : (T j , (comj , challj , rspj ), Mj ) and the other generated in Sim2B : (T j , (comj , challj , rspj ), Mj ). If challj = challj , B aborts the simulation, otherwise, B invokes the underlying main OR sigma protocol extraction algorithm, on input the statement (rpkj , TageSetj ) and two accepted transcripts (comj , challj , rspj ), (comj , challj , rspj ), where TagSetj is generated by the public input T j , Mj and L, it outputs a witness wj . – Sim4B : For j, j  ∈ [N + 1], if there exists wj = (skj , π), wj = (skj , π), then B outputs (skj , skj ), if wj forms a collision of H2 , B outputs wj , otherwise, B aborts the simulation. We can see that the witness (wj )j∈[N +1] has the form: wj = (skj , πj ) such that Sπj = skj S0 , Tπj = skj T0 or a collision of H2 . If no collision occurs, then there must have two indexes j  , j ∈ [N +1] such that wj = (skj , π), wj = (skj , π), since the pigeonhole principle and the conditions for winning the tag-linkability game, which indicate that skj  S0 = skj  S0 but skj  T0 = skj  T0 , this violates the Item 5 of the restricted pair of group actions. Otherwise wj is a collision of H2 .   
   
  Anonymity. We demonstrated the anonymity of the scheme by building a sequence of games. The ﬁrst game is the same as the original anonymity game, where c = 0. Similarly, the last game is exactly like the original anonymity game, where c = 1. We will prove that for any PPT adversary A, the probability that anon denotes he distinguishes between any two games is negligible. Let AdvA,Game i the advantage of adversary A in Gamei . where c = 0, the adver– Game1 : This is an actual anonymity game Gameanon A sary A is allowed to query RSign(sk0 , ·), RSign(sk1 , ·) and RSign(skc , ·). Challenger C invokes the actual signing algorithm with the secret key to generate the signature, and outputs it as the result of a signing query. – Game2 : In the second game, challenger C invokes the underlying main OR sigma protocol zero-knowledge simulation protocol Sim to respond to the signing query of A instead of running the real main OR sigma protocol.  
   
  182  
   
  W. Wei et al.  
   
  From the zero-knowledge property of the underlying main OR sigma protocol, the output distribution of Game1 and Game2 is indistinguishable, we have: anon anon (λ) ≈ AdvA,Game (λ). AdvA,Game 1 2 – Game3 : In the third game, the challenger C simulates N public-secret key pairs {(ski , pki )}i∈[N ] instead of running the Setup algorithm, then C guesses that the pair of public keys {pkj∗ , pkk∗ } sent by the adversary happens to be the j-th and k-th public keys, and generates two tag sets:     
   
   TagSet0 = T0 , Tj = skj∗  T0 , Ti = skj∗ + H1 (a, i) − H1 (a, j)  T0 i∈[N ]\j   TagSet1 = T0 , Tk = skk∗  T0 , (Ti = (skk∗ + H1 (a, i) − H1 (a, k))  T0 )i∈[N ]\k where a = H1 (L, M ), T0 = H1 (L)  S0 . If the guess is incorrect, the challenger randomly samples a bit as the output of A and terminates the game. Otherwise, it responds to the signing queries using a pre-computed TagSet0 , TagSet1 and T 0 = (skj∗ − H1 (a, j))  T0 , T 1 = (skk∗ − H1 (a, k))  T0 at the beginning of Game2 . Since the probability of the challenger correctly guessing the two pubanon anon (λ) ≈ N12 AdvA,Game (λ). lic keys is at most 1/N 2 , thus we have: AdvA,Game 3 2 – Game4 : Diﬀerent from Game3 , the challenger C samples {i0 , i1 } ← [N ] and simulates N − 2 public-secret key pairs {(ski , pki )}i∈[N ]\{i0 ,i1 } , then samples (E0 , E1 ) uniformly from T and computes:    T 0 = (−H1 (a, i0 ))  E0 , TagSet0 = T0 , Ti = H1 (a, i)  T 0 i∈[N ]    T 1 = (−H1 (a, i1 ))  E1 , TagSet1 = T0 , Ti = H1 (a, i)  T 1 i∈[N ] where T0 = H1 (L)  S0 , the rest is the same as Game3 . Using the weakpseudrandom of the restricted pair of group actions, we have: (sk  T0 : sk ← G 1 ) ≈ (E : E ← T ). Thus Game4 is computationally indistinguishable from anon anon (λ) ≈ AdvA,Game (λ). Now, the secret key is no longer used Game3 : AdvA,Game 1 3 to generate the signature, i.e., the output of signing query does not reveal any information about the bit c in Game4 . – Game5 : This is the same as an actual anonymous game, where c = 1. It can be deduced from the above game sequence, there is no such adversary A that can distinguish any two games with a non-negligible probability, thus the probability of the adversary winning the real anonymity game is negligible.  with non-negligible Exculpability. If there exists an adversary A wins Gameexcu A probability, then we show how to construct an algorithm B from A that breaks the property Item 6 of restricted pair of group actions and collision resistance of H1 , H2 . First, we simulate a game Game1 , it is indistinguishable from the real game Gameexcu A . In Game1 , the challenger invokes the OR sigma protocol zero-knowledge simulation protocol Sim to simulate the signature, and generates N public-secret key pairs {(ski , pki )}i∈[N ] , let T i = (ski − H1 (a, i))  T0 , then challengercomputes N tag sets TagSeti = T0,i = H1 (L)  S0 , (Tj,i = H1 (a, j)  T i )j∈[N ] i∈[N ]  
   
  Traceable Ring Signatures from Group Actions  
   
  183  
   
  before the game starts. If the signing query made by the adversary contains index i, then the challenger uses N public-secret key pairs, precomputed N tag sets TagSeti and N elements T i to generate the response. From the zero-knowledge of OR sigma protocol, indistinguishable of tag sets and collision resistance of H1 , excu excu ≈ AdvA,Game we have AdvA,Game excu . We show that when A wins Game1 , the sim1 ulation of B on the input (S, T ) as follows: – Sim1B : B randomly samples index j ← [N ], sets pkj = S, T j = (−H1 (a,j))T , and computes TagSetj = T0 = H1 (L)  S0 , (Ti = H1 (a, i)  T j )i∈[N ] , then generates the remaining N − 1 public-secret key pairs {(pki , ski )}i∈[N ]\j . – Sim2B : B simulates the view of Game1 , since Game1 does not contain any information of secret key. After interacting with B, A outputs a forgery (M, rpk∗ , σ ∗ = (T ∗ , com∗ , chall∗ , rsp∗ )). To make sure the signature σ ∗ wins the Game1 , B must have responded to the signing query (i, M, rpk) with signature σ = (T  , com , chall , rsp ). If i = j, B terminates the simulation, otherwise, we have T  = T and T ∗ = T , then B can extract witness w from the signature σ ∗ by rerunning A. It is the same as what we have shown in the proof for tag-linkability. – Sim3B : If w does not constitute a collision of H2 , then we have w = (sk, π) such that sk  T0 = Tπ , B outputs w = (sk, π), which violates the Item 6 of the underlying restricted pair of group actions, otherwise B outputs a pair of   
   
  collisions of H1 .  
   
  5  
   
  Instantiations  
   
  Isogeny-based Instantiation. Theoretically, our TRS scheme can be instantiated with any CSIDH parameter set, e.g., CSIDH-512, CSIDH-1024 and CSIDH1792 [4,18]. Nevertheless, taking into account that eﬃciency plays a vital role in the implementation, we implemented our TRS with the ﬁrst group action parameter set proposed by Beullens et al. [5], which relies on the CSIDH group action proposed by [8], cSHAKE proposed by [23]. Let the ideal class group C(O) be a cyclic group, and the order of generator g is N . Then the group action  := C(O) × E(O, π) → E(O, π) can be instantiated (a, E) := ga  E. We set G = G 1 = G 2 = C(O) = ZN , ξ = 1, T = E(O, π), S = E(O, π), and S0 = E0 , T0 = H1 (L)  E0 , where E0 is the elliptic curve y 2 = x3 + x over Fp . Lattice-Based Instantiation. Let q = 5 mod 4, and let k, m be integers, n be a power of 2, B1 and B2 are integers such that B1 < B2 < q. Then the group action  := (s, e)  t → As + e + t. We set (G, S, T ) = (Rqk×m × Rqk × Rqm , Rqm , Rqm ), G 1 = {(s, e1 ) ∈ G | s∞ , e1 ∞ ≤ B1 }, G 2 = {(s, e2 ) ∈ G | s∞ , e2 ∞ ≤ B2 }, where Rq = Z[X]/(q, X n +1). For the collision-resistant hash function, let H4 : {0, 1}∗ → Rqk×m , H5 : {0, 1}∗ → Rqm .  
   
  184  
   
  5.1  
   
  W. Wei et al.  
   
  Implementation and Performance  
   
  Table 1 presents a detailed performance of our scheme, including signature size and time. Note that the addition of traceability necessitates additional group action computations, which impact the eﬃciency of our TRS scheme, especially for groups with large numbers of members. This explains why our scheme may be less eﬃcient than the original linkable ring signature scheme proposed by Beullens et al. [4]. Table 1. Performance of proposed TRS scheme under diﬀerent instantiations. N TRS_ISO  
   
  Time KeyGen(ms) Sign(s) Verify(s) Size Public Key(Byte) Secret Key(Byte) Signature(KB)  
   
  TRS_LAT Time KeyGen(ms) Sign(ms) Verify(ms) (NIST 2) Size Public Key(Byte) Secret Key(Byte) Signature(KB)  
   
  21  
   
  22  
   
  39 3.37 × 101 3.20 × 101 64 16 4.45  
   
  39 39 39 39 39 6.63 × 101 1.31 × 102 2.64 × 102 5.23 × 102 1.07 × 103 6.02 × 101 1.16 × 102 2.31 × 102 4.64 × 102 9.22 × 102  
   
  23  
   
  24  
   
  25  
   
  26  
   
  6.43  
   
  8.25  
   
  10.09  
   
  12.06  
   
  13.87  
   
  0.2 68.5 27.4 4096 16 56.37  
   
  0.2 101.3 34.9  
   
  0.2 131.4 50.3  
   
  0.2 230.8 81.1  
   
  0.2 390.3 144.0  
   
  0.2 764.0 265.4  
   
  57.37  
   
  58.37  
   
  59.37  
   
  60.37  
   
  61.37  
   
  Table 2. Comparison of public key size, secret key size and signature size of our TRS scheme with post-quantum (traceable) ring signature schemes. Schemes  
   
  Public key (KB)  
   
  Secret key (KB)  
   
  Signature size (KB) 21 23 26 210  
   
  Calamari [4]  
   
  64 (Byte)  
   
  16 (Byte)  
   
  3.5  
   
  5.4  
   
  8.2  
   
  10  
   
  *  
   
  Beullens_ISO [3]  
   
  64 (Byte)  
   
  16 (Byte)  
   
  3.6  
   
  –  
   
  6.6  
   
  9.0  
   
  *  
   
  Raptor [27]  
   
  0.9  
   
  9.1  
   
  2.6  
   
  11  
   
  82  
   
  1331.2  
   
  100bits  
   
  Beullens_LAT [3]  
   
  5120 (Byte)  
   
  16 (Byte)  
   
  124  
   
  –  
   
  126  
   
  129  
   
  NIST 2  
   
  Falaﬂ [4]  
   
  5120 (Byte)  
   
  16 (Byte)  
   
  49  
   
  50  
   
  52  
   
  55  
   
  NIST 2  
   
  Branco [6]  
   
  1577  
   
  0.5  
   
  –  
   
  1920 1536 245(MB) NIST 5  
   
  Alessandra [32]  
   
  6  
   
  4  
   
  4  
   
  16  
   
  Feng H [17]  
   
  –  
   
  –  
   
  135.1 136.3 138.2 140.7  
   
  NIST 5  
   
  Esign [15]  
   
  ≤ 8.33  
   
  ≤ 0.83  
   
  –  
   
  NIST 5  
   
  this work  
   
  ∗  
   
  –  
   
  131 774  
   
  1024 1021  
   
  ISO 64 (Byte) 16 (Byte) 4.5 8.3 13.9 22.2 LAT 4096 (Byte) 16 (Byte) 56.3 58.3 61.3 65.3 LAT 6144 (Byte) 16 (Byte) 74.3 76.3 79.3 83.3  
   
  Security Level  
   
  NIST 5  
   
  * NIST 2 NIST 5  
   
  : 128bits classical security and 60bits quantum security [29]  
   
  We compare our TRS with existing post-quantum (traceable) ring signature schemes. The results are shown in Table 2. The signature size of our latticebased TRS scheme outperforms the size of [3,6,15,17]. When the ring members are small, the signature size of [32] and [27] is advantageous. However, once the  
   
  Traceable Ring Signatures from Group Actions  
   
  185  
   
  ring members exceed 26 , the signature size of [32] and [27] becomes signiﬁcantly larger than in our lattice-based TRS scheme. Compared with the original scheme [4] and [3], Our isogeny-based instantiation has a larger signature size. With isogeny-based instantiation, it can be concluded from Fig. 4 (right) that the smaller the value of Q is, the less time it takes for signature generation and veriﬁcation. We conducted experiments with N = 2 to analyze the relationship between signature size and the value of K. The results are presented on the left side of Fig. 4. It can be seen that the minimum signature size is obtained when K = 36.  
   
  Fig. 4. The signature size of TRS (left) and number of group actions (right) under diﬀerent (Q, K).  
   
  With the same security level, we set constant rounds for OR sigma protocol to observe the eﬀect of diﬀerent values of K on the signature size and signing time. The results are shown on the right in Fig. 5, the value of K increases, the time required for signature veriﬁcation decreases while the signature size increases. Users can customize the value of K according to their speciﬁc requirements. In addition, we provide three optimal (Q, K) pairs under diﬀerent ring sizes in Fig. 5 (left), which result in smaller signature sizes.  
   
  Fig. 5. Three superior (Q, K) pairs under diﬀerent ring sizes (left) and the relationship among K, signature size and time spent on signature generation and veriﬁcation (right) under the same Q.  
   
  186  
   
  6  
   
  W. Wei et al.  
   
  Conclusion  
   
  This work presents a quantum-resistant TRS scheme from group action. First, we construct a special OR sigma protocol based on the restricted group action, which can be instantiated by isogenies and lattices. Then, using Fiat-Shamir transform to the OR sigma protocol, we derive two concrete TRS schemes. The core of our technique is to construct an OR proof for multiple tags and public key set. Under the random oracle model, we further prove the security of our TRS scheme in terms of tag-linkability, anonymity and exculpability. Finally, we give two TRS implementations from CSIDH-512/CSI-Fish, Dilithium and cSHAKE, the results show that our TRS is competitive in signature size and performance compared with other post-quantum (traceable) ring signature schemes.  
   
  References 1. Alamati, N., De Feo, L., Montgomery, H., Patranabis, S.: Cryptographic group actions and applications. In: Moriai, S., Wang, H. (eds.) Advances in Cryptology – ASIACRYPT 2020. ASIACRYPT 2020. LNCS, vol. 12492, pp. 411–439. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-64834-3_14 2. Au, M.H., Liu, J.K., Susilo, W., Yuen, T.H.: Secure ID-based linkable and revocable-iﬀ-linked ring signature with constant-size construction. Theor. Comput. Sci. 469, 1–14 (2013). https://doi.org/10.1016/j.tcs.2012.10.031 3. Beullens, W., Dobson, S., Katsumata, S., Lai, Y.F., Pintore, F.: Group signatures and more from isogenies and lattices: generic, simple, and eﬃcient. Des. Codes Cryptogr. 91(6), 2141–2200 (2023). https://doi.org/10.1007/s10623-023-01192-x 4. Beullens, W., Katsumata, S., Pintore, F.: Calamari and Falaﬂ: Logarithmic (linkable) ring signatures from isogenies and lattices. In: Moriai, S., Wang, H. (eds.) Advances in Cryptology –ASIACRYPT 2020. ASIACRYPT 2020. LNCS, vol. 12492, pp. 464–492. Springer, Cham (2020). https://doi.org/10.1007/978-3-03064834-3_16 5. Beullens, W., Kleinjung, T., Vercauteren, F.: CSI-FiSh: Eﬃcient isogeny based signatures through class group computations. In: Galbraith, S., Moriai, S. (eds.) Advances in Cryptology – ASIACRYPT 2019. ASIACRYPT 2019. LNCS, vol. 11921, pp. 227–247. Springer, Cham (2019). https://doi.org/10.1007/978-3-03034578-5_9 6. Branco, P., Mateus, P.: A traceable ring signature scheme based on coding theory. In: Ding, J., Steinwandt, R. (eds.) Post-Quantum Cryptography. PQCrypto 2019. LNCS, vol. 11505, pp. 387–403. Springer, Cham (2019). https://doi.org/10.1007/ 978-3-030-25510-7_21 7. Castryck, W., Decru, T.: An eﬃcient key recovery attack on SIDH. In: Hazay, C., Stam, M. (eds.) Advances in Cryptology – EUROCRYPT 2023. EUROCRYPT 2023. LNCS, vol. 14008, pp. 423–447. Springer, Cham (2023). https://doi.org/10. 1007/978-3-031-30589-4_15 8. Castryck, W., Lange, T., Martindale, C., Panny, L., Renes, J.: CSIDH: an eﬃcient post-quantum commutative group action. In: Peyrin, T., Galbraith, S. (eds) Advances in Cryptology – ASIACRYPT 2018. ASIACRYPT 2018. LNCS, vol. 11274, pp. 395–427. Springer, Cham (2018). https://doi.org/10.1007/978-3-03003332-3_15  
   
  Traceable Ring Signatures from Group Actions  
   
  187  
   
  9. Chow, S.S., Liu, J.K., Wong, D.S.: Robust receipt-free election system with ballot secrecy and veriﬁability. In: NDSS, vol. 8, pp. 81–94 (2008) 10. Chung, K.M., Hsieh, Y.C., Huang, M.Y., Huang, Y.H., Lange, T., Yang, B.Y.: Group signatures and accountable ring signatures from isogeny-based assumptions. arXiv e-prints pp. arXiv-2110 (2021) 11. Couveignes, J.M.: Hard homogeneous spaces. Cryptology ePrint Archive, Paper 2006/291 (2006). https://eprint.iacr.org/2006/291 12. Cramer, R., Damgård, I., Schoenmakers, B.: Proofs of partial knowledge and simpliﬁed design of witness hiding protocols. In: Desmedt, Y.G. (eds.) Advances in Cryptology – CRYPTO ’94. CRYPTO 1994. LNCS, vol. 839, pp. 174–187. Springer, Berlin, Heidelberg (1994). https://doi.org/10.1007/3-540-48658-5_19 13. Derler, D., Ramacher, S., Slamanig, D.: Post-quantum zero-knowledge proofs for accumulators with applications to ring signatures from symmetric-key primitives. In: Lange, T., Steinwandt, R. (eds.) Post-Quantum Cryptography. PQCrypto 2018. LNCS, vol. 10786, pp. 419–440. Springer, Cham (2018). https://doi.org/10.1007/ 978-3-319-79063-3_20 14. Esgin, M.F., Steinfeld, R., Liu, J.K., Liu, D.: Lattice-based zero-knowledge proofs: new techniques for shorter and faster constructions and applications. In: Boldyreva, A., Micciancio, D. (eds.) Advances in Cryptology – CRYPTO 2019. CRYPTO 2019. LNCS, vol. 11692, pp. 115–146. Springer, Cham (2019). https://doi.org/10.1007/ 978-3-030-26948-7_5 15. Esgin, M.F., Steinfeld, R., Sakzad, A., Liu, J.K., Liu, D.: Short lattice-based oneout-of-many proofs and applications to ring signatures. In: Deng, R., GauthierUmana, V., Ochoa, M., Yung, M. (eds.) Applied Cryptography and Network Security. ACNS 2019. LNCS, vol. 11464, pp. 67–88. Springer, Cham (2019). https:// doi.org/10.1007/978-3-030-21568-2_4 16. Esgin, M.F., Zhao, R.K., Steinfeld, R., Liu, J.K., Liu, D.: MatRiCT. In: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. ACM, November 2019. https://doi.org/10.1145/3319535.3354200 17. Feng, H., Liu, J., Wu, Q., Li, Y.N.: Traceable ring signatures with post-quantum security. In: Jarecki, S. (eds.) Topics in Cryptology - CT-RSA 2020. CT-RSA 2020. LNCS, vol. 12006, pp. 442–468. Springer, Cham (2020). https://doi.org/10.1007/ 978-3-030-40186-3_19 18. Feo, L.D., Galbraith, S.D.: SeaSign: compact isogeny signatures from class group actions. In: Ishai, Y., Rijmen, V. (eds.) Advances in Cryptology–EUROCRYPT 2019. EUROCRYPT 2019. LNCS, vol. 11478, pp. 759–789. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-17659-4_26 19. Feo, L.D., Kohel, D., Leroux, A., Petit, C., Wesolowski, B.: SQISign: compact post-quantum signatures from quaternions and isogenies. In: Moriai, S., Wang, H. (eds.) Advances in Cryptology – ASIACRYPT 2020. ASIACRYPT 2020. LNCS, vol. 12491, pp. 64–93. Springer, Cham (2020). https://doi.org/10.1007/978-3-03064837-4_3 20. Fiat, A., Shamir, A.: How to prove yourself: practical solutions to identiﬁcation and signature problems. In: Odlyzko, A.M. (eds.) Advances in Cryptology – CRYPTO’ 86. CRYPTO 1986. LNCS, vol. 263, pp. 186–194. Springer, Berlin, Heidelberg (1987). https://doi.org/10.1007/3-540-47721-7_12 21. Fujisaki, E.: Sub-linear size traceable ring signatures without random oracles. In: Kiayias, A. (eds.) Topics in Cryptology - CT-RSA 2011. CT-RSA 2011. LNCS, vol. 6558, pp. 393–415. Springer, Berlin, Heidelberg (2011). https://doi.org/10.1007/ 978-3-642-19074-2_25  
   
  188  
   
  W. Wei et al.  
   
  22. Fujisaki, E., Suzuki, K.: Traceable ring signature. In: Okamoto, T., Wang, X. (eds.) Public Key Cryptography - PKC 2007. PKC 2007. LNCS, vol. 4450, pp. 181–200. Springer, Berlin, Heidelberg (2007). https://doi.org/10.1007/978-3-54071677-8_13 23. Kelsey, J., Jen Change, S., Perlner, R.: SHA-3 derived functions: cSHAKE, KMAC, TupleHash and ParallelHash. Technical report (2016). https://doi.org/10.6028/ nist.sp.800-185 24. Lai, Y.F., Dobson, S.: Collusion resistant revocable ring signatures and group signatures from hard homogeneous spaces. Cryptology ePrint Archive, Paper 2021/1365 (2021). https://eprint.iacr.org/2021/1365 25. Langlois, A., Stehlé, D.: Worst-case to average-case reductions for module lattices. Des. Codes Cryptogr. 75(3), 565–599 (2014). https://doi.org/10.1007/s10623-0149938-4 26. Libert, B., Ling, S., Nguyen, K., Wang, H.: Zero-knowledge arguments for latticebased accumulators: logarithmic-size ring signatures and group signatures without trapdoors. J. Cryptol. 36(3) (2023). https://doi.org/10.1007/s00145-023-09470-6 27. Lu, X., Au, M.H., Zhang, Z.: Raptor: a practical lattice-based (linkable) ring signature. In: Deng, R., Gauthier-Umana, V., Ochoa, M., Yung, M. (eds.) Applied Cryptography and Network Security. ACNS 2019. LNCS, vol. 11464, pp. 110–130. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-21568-2_6 28. Nguyen, T.N., et al.: Eﬃcient unique ring signatures from lattices. In: Atluri, V., Di Pietro, R., Jensen, C.D., Meng, W. (eds.) Computer Security - ESORICS 2022. ESORICS 2022. LNCS, vol. 13555, pp. 447–466. Springer, Cham (2022). https:// doi.org/10.1007/978-3-031-17146-8_22 29. Peikert, C.: He gives c-sieves on the CSIDH. In: Canteaut, A., Ishai, Y. (eds.) Advances in Cryptology – EUROCRYPT 2020. EUROCRYPT 2020. LNCS, vol. 12106, pp. 463–492. Springer, Cham (2020). https://doi.org/10.1007/978-3-03045724-2_16 30. Rivest, R.L., Shamir, A., Tauman, Y.: How to leak a secret. In: Boyd, C. (eds.) Advances in Cryptology - ASIACRYPT 2001. ASIACRYPT 2001. LNCS, vol. 2248, pp. 552–565. Springer, Berlin, Heidelberg (2001). https://doi.org/10.1007/3-54045682-1_32 31. Rostovtsev, A., Stolbunov, A.: Public-key cryptosystem based on isogenies. Cryptology ePrint Archive, Paper 2006/145 (2006). https://eprint.iacr.org/2006/145 32. Scafuro, A., Zhang, B.: One-time traceable ring signatures. In: Bertino, E., Shulman, H., Waidner, M. (eds.) Computer Security - ESORICS 2021. ESORICS 2021. LNCS, vol. 12973, pp. 481–500. Springer, Cham (2021). https://doi.org/10.1007/ 978-3-030-88428-4_24 33. Shor, P.W.: Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM Rev. 41(2), 303–332 (1999). https://doi. org/10.1137/s0036144598347011 34. Stolbunov, A.: Cryptographic schemes based on isogenies (2012)  
   
  Symmetric Cryptography and Fault Attacks  
   
  The Random Fault Model Siemen Dhooghe(B)  
   
  and Svetla Nikova  
   
  COSIC, KU Leuven, Leuven, Belgium {Siemen.Dhooghe,Svetla.Nikova}@esat.kuleuven.be  
   
  Abstract. In this work, we introduce the random fault model - a more advanced fault model inspired by the random probing model, where the adversary can fault all values in the algorithm but the probability for each fault to occur is limited. The new adversary model is used to evaluate the security of side-channel and fault countermeasures such as Boolean masking, error detection techniques, error correction techniques, multiplicative tags, and shuﬄing methods. The results of the security analysis reveal new insights both in the novel random fault model as well as in the established random probing model including: shuﬄing masked implementations does not signiﬁcantly improve the random probing security over regular masking; error correction providing little security when faults target more bits (versus the signiﬁcant improvement when using error detection); and the order in which masking and duplication are applied providing a trade-oﬀ between random probing and fault security. Moreover, the results also explain the experimental results from CHES 2022 and ﬁnd weaknesses in the shuﬄing method from SAMOS 2021.  
   
  Keywords: Encoding Probing · Shuﬄing  
   
  1  
   
  · Masking · Physical Security · Random  
   
  Introduction  
   
  The ﬁeld of side-channel analysis, following Diﬀerential Power Analysis (DPA) by Kocher et al. [17], has made signiﬁcant progress over the years. Currently, we are capable of practically protecting hardware applications against side-channel attacks using masking. However, such progress did not come without diﬃculty. Often masking schemes were proposed, broken, and patched. This trial-and-error approach caused the need for theoretical proofs to guarantee the security of new masking methods. The main adversary in the academic literature is considered in the probing model proposed by Ishai et al. [16]. While this probing model already captures basic attacks and allows for easy proofs, it does not cover more advanced attacks in practice. Instead, the model proposed by Chari et al. [6], called the noisy leakage model, is much closer to practice but requires a complicated security analysis for countermeasures making the model less used in papers. In 2014, Duc et al. [12] made a reduction between the probing and noisy leakage models. This reduction introduced a new intermediate model called the random probing model which allows to capture attacks such as horizontal attacks introduced by Clavier c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 191–212, 2024. https://doi.org/10.1007/978-3-031-53368-6_10  
   
  192  
   
  S. Dhooghe and S. Nikova  
   
  et al. [8] and which has an easier security analysis compared to the noisy leakage model. A security analysis in the random probing model allows for better insight in the security provided by diﬀerent countermeasures, such as Boolean masking or shuﬄing, versus those provided in the standard probing model. Compared to side-channel analysis, the ﬁeld of fault attacks, following diﬀerential fault analysis by Biham and Shamir [4], is less studied and has not seen the same progress as side-channel analysis. As a result, standard fault security models are not yet accepted. The current most used academic model is an active variant of the probing model where the adversary can inject faults in a circuit up to a threshold number of wires or gates. However, experimental works such as the results by Bartkewitz et al. [3] note that a threshold fault model does not properly capture the practice where a single laser fault typically aﬀects multiple values. Other adversary models from this threshold model have not been used to investigate and compare countermeasures. As such, the question remains how eﬀective certain countermeasures are in a more realistic security model. Some popular countermeasures include: Boolean masking introduced by Patarin [14] and Chari et al. [6]; error detection methods such as in ParTI [20] and Impeccable Circuits [1]; error correction methods such as in Impeccable Circuits II [21]; multiplicative tags such as in CAPA [19] and M&M [9]; and shuﬄing such as Rocky [18]. However, none of the above countermeasures have been properly analyzed or compared to one another in a similar adversary model. Contributions. The work essentially provides contributions in two ﬁelds: on random probing security, and on random fault security. Considering random probing security, we investigate the security of the following countermeasures: Boolean masking, duplication, masked duplication, and shuﬄing. From the analysis, we made the following interesting observations. – There is a security diﬀerence between masking-then-duplicating a variable versus duplicating-then-masking it. – Both shuﬄing and shuﬄing with masking provides little improvement in random probing security no matter how the shuﬄing is performed. Considering fault security, inspired by the random probing model, we propose a new fault adversary, the random fault model, which is allowed to fault all values in an algorithm but where each fault has a limited probability to apply. We then use the random fault model to analyze countermeasures in two security models: correctness, where the adversary’s goal is to have an incorrect output, and privacy, where the adversary’s goal is to retrieve secret information from the abort state of the algorithm. We analyze Boolean masking, error detection methods, error correction methods, multiplicative tags, and shuﬄing. From the analysis, we made the following interesting observations. – There is a security diﬀerence between masking-then-duplicating a variable versus duplicating-then-masking it. – We give a theoretical foundation to the experiments of Bartkewitz et al. [3] on faulting encoded variables with a diﬀerent number of parity bits.  
   
  The Random Fault Model  
   
  193  
   
  – Triplication provides signiﬁcantly less security than duplication methods when the fault targets many bits. – We observe that the random fault model can explain the success rate of statistical ineﬀective faults targeting multiple values similar to the random probing model explaining horizontal attacks. – Multiplicative tags provide an exponential security gain in the ﬁeld size in the correctness model. – Shuﬄing exhibits several weaknesses when used to secure against fault attacks. As a result, we show that the work by Miteloudi et al. [18] has vulnerabilities.  
   
  2 2.1  
   
  Background Notation  
   
  We consider stochastic variables, denoted as capital letters, over a ﬁnite ﬁeld F2 . We denote the probability of a stochastic variable attaining a value x as Pr[X = x] and the probability of X conditioned on Y as Pr[X = x|Y = y]. We deﬁne random functions as stochastic variables over a set of functions. For example, a function uniform randomly drawn from a set of functions. 2.2  
   
  Algorithmic Representation  
   
  We represent algorithms as a string of elements or elementary operations over a ﬁnite ﬁeld F. In this work, we consider only the ﬁeld F2 for which the elementary operations are the ﬁeld addition (XOR) and multiplication (AND). We assume that algorithms can sample uniform random ﬁeld elements. Moreover, an algorithm is also able to abort the computation providing ⊥ as the output. We give an example of a binary algorithm (x, y, r ← $, z ← x + y, w ← zy, v ← y + r) . An algorithm can have an encoding phase, where its input is, for example, masked or encoded. Oppositely, an algorithm can have a decoding phase where, for example, masked variables can be revealed and encoded variables can be checked for errors. For clarity, an error check in a duplication countermeasure is not part of the decoding phase, only the veriﬁcation of the ﬁnal output is. These phases are important in the security models of Sect. 2.3 and Sect. 3 since the adversaries cannot target these parts of the algorithm. 2.3  
   
  Random Probing Model  
   
  In this section, we introduce the random probing model as originally introduced by Duc et al. [12]. More speciﬁcally, its adversary and its security model. Later in Sect. 3, we study the main contribution of the work, namely the random fault model, which can be seen as the fault counterpart of the random probing model.  
   
  194  
   
  S. Dhooghe and S. Nikova  
   
  Random Probing Adversary. Consider the set of two functions N = {f0 , f1 } with f0 : F2 → F2 ∪ {⊥} : x → x and f1 : F2 → F2 ∪ {⊥} : x →⊥. Namely, the function which maps a bit to itself and the function which returns nothing (⊥). We consider a Bernoulli distribution over N with mean 0 ≤ ε ≤ 1. Thus, from the set N we draw the function f0 with probability ε and the function f1 with probability 1 − ε. In words, when drawing a random function and evaluating a variable, the adversary has an ε probability to view that variable. We note for clarity that the adversary can precisely target the location of the probes, but the probability for each probe to provide a value is random. In this paper, we only consider random probes over bits. The model is easily generalized to work over larger ﬁelds. However, we note that in practice, the adversary never views the leakage of a large ﬁeld element but, rather, a function of the bit vector (such as its Hamming weight) which was processed. As a result, the link with practice is weaker when a direct generalization of the random probing model is made. Security Model. Consider an algorithm and denote the set of all its variables (excluding the encoding and decoding phases) by V, the set of ε-random probes Bern(ε)  
   
  on V is the set {F (v)|v ∈ V, F ← N } with V the values the variables V in the algorithm attained with its input and internal randomness, and where for each value an independent random probe is chosen. The random probing security model is the bounded query, left-right security game represented in Fig. 1. The game consists of a challenger picking a random bit b, the challenger then creates an oracle Ob from the algorithm C and provides this to the adversary A. This adversary is computationally unbounded, but it is bounded in the number of queries to the oracle. The adversary provides two secrets k0 , k1 (for a cipher, a secret is the plaintext and the key) and the set of variables V which it wants to probe. The oracle then picks the secret kb , generates its internal randomness, computes the values V on V, and provides the random probing leakage to the adversary. After q queries (for ease, in this work q = 1), the adversary guesses the bit b which was chosen by the challenger.  
   
  Fig. 1. The random probing leakage model.  
   
  The advantage of A is deﬁned as 0  
   
  1  
   
  Adv(A) = | Pr[AO = 1] − Pr[AO = 1] | .  
   
  The Random Fault Model  
   
  3  
   
  195  
   
  Random Fault Model  
   
  We propose a novel adversary model inspired by the previous explained random probing model where the adversary can fault the entire state but there is a limited probability for each fault to occur (following the mechanisms of statistical fault analysis [13]). This random fault security model is novel and is meant to improve over the standard threshold fault model where an adversary can fault a ﬁxed number of values. 3.1  
   
  Random Fault Adversary  
   
  Consider a function g : F2 → F2 which is the fault the adversary wants to inject. Denote the set of functions Fg = {g, id}, with id : F2 → F2 : x → x, and consider a Bernoulli distribution Bern(κ) on the set to take a random function F . For this random function F , we have that  g(x) with probability κ , F (x) = x with probability 1 − κ . Considering the possible fault injections g an adversary can make over bits, there are three possibilities. – bitflip: F2 → F2 : x → x + 1 . – set to zero: F2 → F2 : x → 0 . – set to one: F2 → F2 : x → 1 . In Sect. 5 and Sect. 6, we use the above function names to indicate which fault is injected. We note for clarity that the adversary can precisely target the faults (over bits), but the probability for the fault to occur is random. We only consider fault over bits and not over larger ﬁelds. Since our hardware and software works over bits and not over abstract algebraic structures, faults would hit separate gates or wires causing them to aﬀect bit-by-bit. We leave the generalization of using realistic distributions of fault attacks in the random fault model over words (larger sets of bits) as an open problem. 3.2  
   
  Security Model: Correctness  
   
  The random fault model has two security models, namely a correctness and a privacy model (after the models by Ishai et al. [16]). The goal of the adversary in the correctness game is for the algorithm to output a wrong value (diﬀerent than the value in case no faults were injected). In case the algorithm detected a fault and aborts, the adversary does not win the game. A fault attack covered by this model is for example diﬀerential fault analysis by Biham and Shamir [4]. However, the correctness model is more general as it does not look whether secret information can be gathered from an incorrect output.  
   
  196  
   
  S. Dhooghe and S. Nikova  
   
  Consider an algorithm and denote the set of the variables the adversary targets (excluding the encoding and decoding phases) by V. Denote the set of κ-random faults by a set of functions G = {gi | i ∈ V}. The correctness game of the random fault model consists of an adversary querying the oracle implementing the algorithm providing it with the input secret k and the set of faults G. The oracle then implements the algorithm with the secret k faulting its values with random functions FG following a Bern(κ) distribution. The oracle outputs 1 if the output was correct or abort ⊥, and 0 if the output was incorrect. This is depicted in Fig. 2. We require, for the algorithm to be useful, that it outputs a correct result in case no faults were present.  
   
  Fig. 2. The correctness game of the random fault model.  
   
  The advantage is the probability the oracle outputs zero after a single query. Adv(A) = Pr[O(k, FG ) = 0] 3.3  
   
  Security Model: Privacy  
   
  The goal of the adversary in the privacy game is to uncover internal information (the input) of the algorithm from the algorithm’s abort state after faulting it. In case the algorithm does not have an abort signal (such as with masking or error correction), the countermeasure is automatically secure in the privacy model and its protection is solely determined by the correctness model. Examples of fault attacks in the privacy model include Clavier’s ineﬀective faults [7] and statistical ineﬀective faults by Dobraunig et al. [11]. The privacy game of the random fault model is the bounded query, left-right security game represented in Fig. 3. The game consists of a challenger picking a random bit b, the challenger then creates an oracle Ob from the algorithm and provides this to the adversary A. This adversary is computationally unbounded, but it is bounded in the number of queries to the oracle. The adversary provides two secrets k0 , k1 together with a set of functions on the values (excluding encoding and decoding phases) G = {gi | i ∈ V}. The oracle then picks the secret kb , generates its internal randomness, computes the algorithm, and applies the random fault functions FG on the targeted variables V (following a Bern(κ) distribution). The oracle returns the state of the abort signal of the algorithm. After q queries (for ease, in this work q = 1), the adversary returns the bit b which was chosen by the challenger.  
   
  The Random Fault Model  
   
  197  
   
  Fig. 3. The privacy game of the random fault model.  
   
  The advantage of A is deﬁned as 0  
   
  1  
   
  Adv(A) = | Pr[AO = 1] − Pr[AO = 1] | .  
   
  4  
   
  Case Studies: Random Probing Model  
   
  In order to showcase the random fault model, we apply it to several popular countermeasures with the goal to ﬁnd general bounds over the parameters of the countermeasure. However, in order to properly provide the security of each countermeasure, we also evaluate them over the established random probing model (as introduced in Sect. 2.3). For example, to show that masking might not improve the bound over the random fault model, but that it does increase the security in the random probing model. Although the random probing model has already been established for some time, some of the results in this section are novel. For example, as far as we are aware, no concrete random probing bounds have been given for duplicateand-mask countermeasures. In addition, we show that shuﬄing methods do not signiﬁcantly improve the security over the random probing model. 4.1  
   
  Inﬂuence of Duplication  
   
  We investigate what happens if you view the same variable m times (for example if the value is duplicated to defend against fault attacks) or when the variable is an element of F2m . Consider a uniform random variable X ∈ F2 (Pr[X = x] = 1/2) and m independent random probes F0 , ..., Fm−1 taken from the set N following a Bern(ε) distribution. Then, the probability that at least one probe views a value is 1 − (1 − ε)m . Thus, the advantage of a random probing adversary is 0  
   
  1  
   
  Adv(A) = | Pr[AO = 1] − Pr[AO = 1] | = 1 − (1 − ε)m ≤ mε , where the last inequality is Bernoulli’s inequality. It is the above observed gain in advantage when viewing the same variable multiple times which causes horizontal attacks [8] to be eﬀective.  
   
  198  
   
  4.2  
   
  S. Dhooghe and S. Nikova  
   
  Inﬂuence of Masking  
   
  Masking was introduced by Goubin and Patarin [14] and Chari et al. [6] in 1999. The deﬁnition for Boolean masking is given as follows. Deﬁnition 1 (Boolean masking). The n-shared Boolean masking of a varin−1 able x ∈ F2 consists of a vector (x0 , . . . , xn−1 ) ∈ (F2 )n such that x = i=0 xi . In a countermeasure, a share vector is made using random bits. For example, to mask a secret x in two shares one can use a random bit r and create the vector (x + r, r) = (x0 , x1 ). That way, each share x0 or x1 is uniform random. We start by showing that masking indeed improves the protection against a random probing adversary. Given a uniform value X 0 and X 1 such that X 0 + X 1 = 0 and two independent random probing functions F0 and F1 with probability ε to observe the value, then Pr[F0 = fid , F1 = fid ] Pr[X 0 ⊕ X 1 = 0|F0 (X 0 ), F1 (X 1 )] = ε2 Pr[F0 = f⊥ , F1 = fid ] Pr[X 0 ⊕ X 1 = 0|F0 (X 0 ), F1 (X 1 )] = ε(1 − ε)/2 Pr[F0 = f⊥ , F1 = f⊥ ] Pr[X 0 ⊕ X 1 = 0|F0 (X 0 ), F1 (X 1 )] = (1 − ε)2 /2 , with fid : x → x and f⊥ : x →⊥. Similarly, Pr[F0 = fid , F1 = fid ] Pr[X 0 ⊕ X 1 = 1|F0 (X 0 ), F1 (X 1 )] = 0 Pr[F0 = f⊥ , F1 = fid ] Pr[X 0 ⊕ X 1 = 1|F0 (X 0 ), F1 (X 1 )] = ε(1 − ε)/2 Pr[F0 = f⊥ , F1 = f⊥ ] Pr[X 0 ⊕ X 1 = 1|F0 (X 0 ), F1 (X 1 )] = (1 − ε)2 /2 , As a result, the advantage of the adversary is ε2 (the absolute subtraction of the three corresponding equations). Similarly, for n shares, the adversary only guesses correctly when all random probes return a value which happens with probability εn (which is then the advantage). 4.3  
   
  Inﬂuence of Masked Duplication  
   
  Consider the case where a variable is both masked and duplicated. In practice, this happens when we require both fault protection and side-channel security. We distinguish two cases. – Mask-then-duplicate: A variable is ﬁrst masked and then duplicated. For two shares and two duplicates, this means a bit x ∈ F2 is encoded to (x00 , x10 ), (x01 , x11 ) where x00 + x10 = x and x00 = x01 , x10 = x11 . Examples of countermeasures which use this technique include [10,15]. – Duplicate-then-mask: A variable is ﬁrst duplicated and then masked. For two shares and two duplicates, this means a bit x ∈ F2 is encoded to (x00 , x11 , x02 , x13 ) where x00 + x11 = x02 + x13 = x. Examples of countermeasures which use this technique include [9,19,20].  
   
  The Random Fault Model  
   
  199  
   
  Mask-then-Duplicate. For the ﬁrst case with two shares (n = 2) and two duplicates (k = 2), we have that 0  
   
  1  
   
  Adv(A) = | Pr[AO = 1] − Pr[AO = 1] | = (1 − (1 − ε)2 )2 . Namely, the adversary only gets an advantage if it observes both shares, but since each share is duplicated, observing a single share happens with probability 1 − (1 − ε)2 . By combining the advantages for duplication and for masking, for n shares and k duplicates, we have an advantage of Adv(A) = (1 − (1 − ε)k )n ≤ (kε)n . Duplicate-then-Mask. For the second case with n, k = 2, we have that 0  
   
  1  
   
  Adv(A) = | Pr[AO = 1] − Pr[AO = 1] | = 1 − (1 − ε2 )2 . Namely, the adversary has a ε2 advantage to when observing a masking and the adversary has two chances to break it. For n shares and k duplicates, we have a random probing advantage of Adv(A) = 1 − (1 − εn )k ≤ kεn . We observe that the duplicate-then-mask method protects better against a random probing adversary compared to the mask-then-duplicate method. We depict the diﬀerences between the advantages in Fig. 4.  
   
  Fig. 4. The random probing advantage of the mask-then-duplicate method (in green) and the duplicate-then-mask method (in red). The full lines depict (n, k) = (2, 2), the dashed lines depict (n, k) = (2, 3), and the dotted lines depict (n, k) = (3, 2). (Color ﬁgure online)  
   
  4.4  
   
  Inﬂuence of Shuﬄing  
   
  We take a look at shuﬄing as a countermeasure and assess its security in the random probing model. We then move to the shuﬄing of masked values. Consider two values x, y ∈ F2 . With shuﬄing, the encoding phase of the algorithm randomly shifts the two values from place. Consider the security model  
   
  200  
   
  S. Dhooghe and S. Nikova  
   
  from Sect. 2.3. Since the adversary can choose the two secret inputs, we can take x = y (meaning, x = y = 0 or x = y = 1 for the two cases of inputs). The advantage of the adversary using random probes (denoted F0 and F1 and calling the ﬁrst operation O0 and the second O1 ) is Adv(A) = | Pr[F0 (O0 ) = 1 ∨ F1 (O1 ) = 1|X = 1] | = 1 − (1 − ε)2 ≤ 2ε . The above adversary randomly guesses the secret when both random probes return ⊥ and provides the answer to the probes when a value is returned. The above bound generalizes to nε for shuﬄing n bits. We observe that the bound of the shuﬄing method is the same as the bound of an unshuﬄed n-bit state. As a result, shuﬄing provides no additional security when the adversary chooses weak inputs. Shuﬄed Masking. We consider a state of k-bit n-shared values. For example, for k = 3 and n = 2, we have a state (x0 , x1 ), (y0 , y1 ), (z0 , z1 ) such that x = x0 + x1 , y = y0 + y1 , z = z0 + z1 for three bits x, y, z ∈ F2 . Consider then that the total of nk bits are randomly permuted, meaning that a permutation nk is uniform randomly drawn for the set of all Fnk 2 → F2 permutations and is applied to the nk bits. We note that practical implementations of shuﬄing often consider a weaker case where the permutation is drawn from a smaller set, such as cyclical shifts of the shares. The security mentioned in related works such as [2, Sect. 2.4] is that shuﬄing these masked values at least improves the sidechannel security by a factor k. We will show that in the random probing model, shuﬄing does not signiﬁcantly improve the security. We consider the advantage of random probing all the nk shares in the state. Since the order of the shares is shuﬄed (from the countermeasure), the values returned to the adversary are also randomized. As a result, the adversary does not know which value belongs to which variable. For example, the adversary can receive the transcript (1, 0, 0) for n = 2 and k = 3, meaning that the adversary receives three values out of six but does not know which three it received. We consider an adversary which takes, for the security model, the all-zero secret versus the all-one secret. Meaning that all k sharings are of either the secret zero (in the ﬁrst case) or the secret all-one (in the second case). The adversary then considers all possible n-sums of the received values. In case the majority of the sums are zero, the adversary decides it is in the ﬁrst case (with the secrets all equal to zero), otherwise it decides it is in the second case. We calculate the advantage of this adversary. The probability that the adversary receives exactly i bits from the random   i ε (1−ε)nk−i . Given that the adversary has i values, it can calculate probes is nk i     a total of ni n-sums out of the total of nk n n-sums. Finally, there are a total of k sums which sum to the secret (we call these “correct sums”). The advantage is thus given as follows.  
   
  The Random Fault Model  
   
  Adv(A) =  
   
   nk   nk i  
   
  i=0  
   
  εi (1 − ε)nk−i  
   
  k   
   
  201  
   
  C(i, j)K(i, j) ,  
   
  j=0  
   
    with C(i, j) the probability to have j correct sums over the total of ni sums, and K(i, j) the probability to win the game   minus the probability to lose the game given j correct sums over a total of ni sums. In more detail,    C(i, j) =  
   
  a j  
   
  b−a k−j  
   
  b  
   
  ,  
   
  k  
   
      since C(i, j) is the probability of j successes in the with a = ni and b = nk n hypergeometric distribution. The value K(i, j) =  
   
  a−j  = a+1 2 −j  
   
  1 2a−j  
   
    
   
  a−j   
   
    
   
  a 2 −j  
   
  −  
   
   =0  
   
  1  
   
    
   
  2a−j  
   
   a−j ,   
   
    with a = ni is the probability that the majority of the n-sums are equal to the secret (given that j sums are correct) minus the probability that the majority of the sums equal zero. It is clear that an upper bound for the above advantage is equal to the advantage of an k times n-sharing without shuﬄing, which is equal to 1 − (1 − εn )k . For n, k = 2, we also ﬁnd that the above advantage is equal to 2ε2 − 2ε3 + 3 4 2 4 8 ε , which is lower than the 2ε −ε advantage without shuﬄing. This shows that shuﬄing indeed increases the security of an implementation. However, we see that both advantages have a leading coeﬃcient 2ε2 . Meaning that the advantage of shuﬄing equals the advantage without shuﬄing plus terms in ε3 or higher. In words, shuﬄing only achieves a very insigniﬁcant (i.e. less than linear) increase in random probing security. We prove this “less than linear” security gain more formally. First, since C(i, j) = 0 for i < n,we can write Adv(A) = cεn + O(εn+1 ). Second, we ﬁnd k that c = k since c = nk n C(n, 1)K(n, 1) with C(n, 1) = (nk) and K(n, 1) = 1. n As a result, we ﬁnd that Adv(A) = kεn +O(εn+1 ) which can be compared to the advantage of the state without shuﬄing 1 − (1 − εn )k = kεn + O(εn+1 ). Thus, we have proven that shuﬄing can only improve the random probing security by a less-than-linear amount when the adversary chooses weak inputs. We mention that similar results were found by Bogdanov et al. [5] on using higher-order diﬀerential computational analysis on white-box implementations using masking and shuﬄing.  
   
  5  
   
  Case Studies: Random Fault Correctness  
   
  We investigate the random fault security of several popular countermeasures including duplication, error correcting codes, masking, multiplicative tags, and  
   
  202  
   
  S. Dhooghe and S. Nikova  
   
  shuﬄing. Recall from Sect. 3.2 that the random fault model has two security models, namely the correctness and the privacy model. In this section, we evaluate the countermeasures in the correctness model. To better understand the bounds given in this section, we establish a baseline. Namely, we investigate the correctness security of storing a single bit. The advantage of a random fault adversary changing this single bit value is κ. In case there are m variables (or m queries), then the advantage is 1 − (1 − κ)m ≤ mκ. 5.1  
   
  Inﬂuence of Masking  
   
  Consider masking from Sect. 4.2 where a variable x ∈ F2 is split in two parts x0 , x1 such that x0 +x1 = x. Then using a random bitﬂip fault F with probability κ only on x0 , the advantage of the adversary against an n-masking is still Adv(A) = Pr[F (X 0 ) + X 1 = X 0 + X 1 ] = κ . However, the adversary can bitﬂip both shares (denoted F0 and F1 ) to attain the advantage 2κ(1 − κ). For n shares, this advantage becomes  n 2  n 1 Adv(A) = κ2i+1 (1 − κ)n−2i−1 = (1 − (1 − 2κ)n ) ≤ nκ . 2i + 1 2 i=0 We observe that, if κ ≤ 1/2, masking increases the advantage of a faulting adversary in the correctness model over a non-masked alternative. 5.2  
   
  Inﬂuence of Duplication  
   
  We then investigate the eﬀect of duplicating the variable and error checking the duplicates at the end of the computation. Deﬁnition 2 (Duplication). The n-duplication of a variable x ∈ F2 consists of a vector (x0 , . . . , xn−1 ) ∈ (F2 )n such that x0 = ... = xn−1 . The above is combined with an error check which veriﬁes if xi = xj with i = j and aborts the computation if they are not equal. We calculate the advantage of a random fault adversary injecting a bitflip (see Sect. 3) in both duplicates for a two-duplication using random faults (F0 , F1 ) with probability κ to occur. We ﬁnd the following advantage Adv(A) = Pr[F0 (X0 ) = F1 (X1 ), F0 (X0 ) = X0 ] = κ2 . This is extended to k-duplication with an advantage of κk . As a result, duplication (or any linear code with nontrivial distance) exponentially decreases the advantage of the adversary in the correctness game. For m-bit k-duplicated variables, the advantage of the adversary is still κk if the adversary attacks only one pair of duplicates. In case the adversary attacks all duplicates, the advantage becomes  
   
  The Random Fault Model  
   
  Adv(A) =  
   
  m    m i=1  
   
  i  
   
  203  
   
  κik (1 − κ)k(m−i) = (κk + (1 − κ)k )m − (1 − κ)km ,  
   
  where the equality comes from the binomial theorem. The above advantage is higher compared to attacking one pair of duplicates in case κ is small. Speciﬁc Codes. We consider the advantage for encodings using diﬀerent linear codes from the repetition (duplication) code. Consider a value x ∈ F2m encoded as a codeword c ∈ C with C and [n, m, d] code. It is clear that if the adversary faults c to the nearest other codeword, the advantage is κd . For a diﬀerent attack, the adversary bitﬂips each bit of c where the advantage is the probability they form a codeword. In particular, if κ = 0.5, one gets a m random m-bit fault for which the advantage is 2 2n−1 . This result is, for example, given by Schneider et al. [20] where it is called the “fault coverage” of the code. For more accurate results when κ = 0.5, the speciﬁc advantage of the adversary depends on the actual code that is used. We provide some examples. Consider the [m + 1, m, 2] parity code (i.e. (x[0], ..., x[m − 1], c) with c = m−1 i=0 x[i]). The advantage of a random fault adversary is  m+1 2   
   
  Adv(A) =  
   
   i=1  
   
   m + 1 2i κ (1 − κ)m−2i+1 2i  
   
  1 = (1 + (1 − 2κ)m+1 ) − (1 − κ)m+1 . 2 For other examples, we need the weight distribution of the codes we are investigating. – For the [7, 4, 3] Hamming code, the weight distribution, ranging from zero to seven, of the codewords is [1, 0, 0, 7, 7, 0, 0, 1]. As a result, the advantage is 7κ3 (1 − κ)4 + 7κ4 (1 − κ)3 + κ7 . – Similarly, the weight distribution of the [8, 4, 4] extended Hamming code, ranging from zero to eight, is [1, 0, 0, 0, 14, 0, 0, 0, 1]. Thus, the advantage is 14κ4 (1 − κ)4 + κ8 . The diﬀerence in advantages between the codes is shown in the ﬁrst graph of Fig. 5. From this ﬁgure, we ﬁnd that the number of parity bits have a signiﬁcant eﬀect on the advantage of a random fault adversary and that not only the minimal distance of the code matters. Note that the “kink” in the graphs is given by the diﬀerence of advantages of diﬀerent attacks. In the work by Bartkewitz et al. [3], experiments were performed by faulting only the message bits in an implementation (leaving the parity bits unaltered). Assuming that a κ-random fault in the message bits was injected (and that indeed the parity bits were unaﬀected), the result of the advantage for the different codes investigated by Bartkewitz et al. is given in the second graph of Fig. 5. The diﬀerence between the codes becomes more signiﬁcant when faulting only the message bits versus faulting all bits in the codeword.  
   
  204  
   
  S. Dhooghe and S. Nikova  
   
  Fig. 5. The advantage of a random fault adversary against encoded values on the left and on the right when only the message bits are attacked. Blue depicts the [5, 4, 2] code, green [8, 4, 2], yellow [7, 4, 3], and red [8, 4, 4]. For the right ﬁgure, the [8, 4, 2] and [8, 4, 4] codes have advantage zero. (Color ﬁgure online)  
   
  5.3  
   
  Inﬂuence of Triplication  
   
  Consider the duplication method from before, but with a minimum of three duplicates (x0 , x1 , x2 ). Instead of using error detection where the algorithm can abort, we correct the errors using a majority voting. We consider an adversary which bitﬂips two out of three duplicates of a single bit. This adversary has the following advantage Adv(A) = Pr[F0 (X0 ) = F1 (X1 ), F0 (X0 ) = X0 ] = κ2 . For k duplicates, the advantage would be κk/2 . We extend the above analysis by considering m bits. When each variable is duplicated and an error detection method is used, the advantage of the adversary is (κ2 +(1−κ)2 )m −(1−κ)2m when attacking all variables, and κ2 when attacking one pair of duplicates. However, with error correction, the advantage against an m-bit three-duplicate correction method becomes Adv(A) = 1 − (1 − κ2 )m , as the adversary can re-try the attack with each variable and win when one of the m variables is error-corrected to the wrong output. The advantage is depicted in Fig. 6. We ﬁnd that triplication performs signiﬁcantly worse when faults can target a large state size compared to duplication. We note that the combination of Boolean masking with triplication would not improve the advantage. 5.4  
   
  Inﬂuence of Masked Duplication  
   
  Recall the two methods of both masking and duplicating variables from Sect. 4.3. Mask-then-Duplicate. We consider the ﬁrst case where each share is duplicated. If the adversary only bitﬂips one pair of duplicates, the advantage is κ2  
   
  The Random Fault Model  
   
  205  
   
  Fig. 6. The advantage against error correction (with three duplicates) is shown in red (for m = 2) and yellow (m = 16). The advantage against error detection (with two duplicates) is shown in green (for m = 2) and blue (m = 16). (Color ﬁgure online)  
   
  (or κk for k duplicates). In case the adversary bitﬂips all values (denoting the random faults F0 , F1 , F2 , F3 ), the advantage for n, k = 2 is Adv(A) = Pr[F0 (X00 ) = F1 (X10 ), F2 (X01 ) = F3 (X11 ), F0 (X00 ) + F2 (X01 ) = X] = 2κ2 (1 − κ)2 . Given that the probability to break the correctness of a k-duplication is κk and the probability to leave each duplicate unchanged is (1 − κ)k , the advantage with n shares and k duplicates becomes  n−1 2   
   
  Adv(A) =  
   
   i=0  
   
   n κk(2i+1) (1 − κ)k(n−2i−1) 2i + 1  
   
  1 = ((1 − κ)k + κk )n − ((1 − κ)k − κk )n . 2 Duplicate-then-Mask. Recall that for the second case, the variable is ﬁrst duplicated and then each duplicate is shared separately. When attacking only one share per duplicate, the advantage is κk . When attacking all variables (denoting the random faults F0 , F1 , F2 , F3 ), for n, k = 2, the advantage becomes Adv(A) = Pr[F0 (X00 ) + F1 (X01 ) = F2 (X20 ) + F3 (X31 ), F0 (X00 ) + F1 (X01 ) = X] = 4κ2 (1 − κ)2 . For n shares and k duplicates, the probability to change the correctness of an n-sharing is 12 (1 − (1 − 2κ)n ), so we have Adv(A) = 2−k (1 − (1 − 2κ)n )k . We observe that for small parameters κ, the mask-then-duplicate method provides more security as opposed to the duplicate-then-mask method. This is depicted for small variables n, k in Fig. 7.  
   
  206  
   
  S. Dhooghe and S. Nikova  
   
  Fig. 7. The random fault advantage in the correctness model of the mask-thenduplicate method (in green) and the duplicate-then-mask method (in red). The full lines depict (n, k) = (2, 2), the dashed lines depict (n, k) = (2, 3), and the dotted lines depict (n, k) = (3, 2). (Color ﬁgure online)  
   
  5.5  
   
  Inﬂuence of Multiplicative Tags  
   
  We can encode variables against fault attacks by multiplying the duplicate with a random value. We call this a multiplicative tag. Deﬁnition 3 (Multiplicative Tag). A multiplicative tag of x ∈ F2m is a value αx ∈ F2m with α ∈ F2m chosen uniformly random with each query. Consider the encoding of x ∈ F2m with a multiplicative tag (x, αx) ∈ (F2m )2 and α ∈ F2m chosen randomly with every query. Error detection is performed by taking the message x, multiplying it with the tag α, and verifying it against the duplicate αx. We investigate the security of this method in the correctness game with a random fault adversary. We consider a ﬁrst adversary which changes a single bit of x assuming α = 0. Consider F a random fault ﬂipping the ﬁrst bit of x. Then, the advantage is Adv(A) = Pr[F (X0 ) = X0 , α = 0] = 2−m κ . For a second adversary, we take x = 1 and fault both x and αx with set-tozero faults. We number the bits of x by (x[0], ..., x[m − 1]) and the bits of αx by (αx[0], ..., αx[m − 1]). Then, the advantage of the adversary applying random faults F0 , ..., Fm against an m-bit multiplicative tag is given as follows Adv(A) = Pr[F0 (X[0]) = 0, F1 (αX[0]) = 0, ..., Fm (αX[m − 1]) = 0]  m 1+κ . =κ 2 While, in this work, we are not able to provide bounds for general adversaries, we observe that multiplicative tags provide a promising countermeasure against faults compared to using linear codes from Sect. 5.2. 5.6  
   
  Inﬂuence of Shuﬄing  
   
  Consider the shuﬄing countermeasure from Sect. 4.4. Without duplication, the adversary can bitﬂip each value for the same advantage as in the non-shuﬄed  
   
  The Random Fault Model  
   
  207  
   
  case. Since there is no detection step, as long as one fault hits, the adversary wins. In case duplication is used on top of the shuﬄing, similar to the weak input attack in Sect. 4.4, there are weak inputs in the correctness model. Namely, for two two-duplicated bits (x0 , y0 ), (x1 , y1 ), pick the secret (0, 1). By applying a setto-zero fault on all values, only one variable can change in its value providing the same advantage as in the non-shuﬄed case where the adversary only targets one pair of duplicates. Moreover, recall from Sect. 5.2 that when k is small, the best attack of the adversary is to fault all duplicates. Such an attack has the same probability to break correctness when shuﬄing the duplicates. As a result, for small κ, shuﬄing does not improve security in the random fault model (independent of how the shuﬄing is done). Together with the weak inputs when attacking only one pair of duplicates, we conclude that we cannot ﬁnd a nontrivial upper bound on the security of shuﬄing in the correctness model.  
   
  6  
   
  Case Studies: Random Fault Privacy  
   
  In Sect. 5, we investigated the correctness security of several countermeasures. In this section, we investigate the privacy security (from Sect. 3.3). Recall that the privacy model is only relevant to countermeasures which can abort the computation. Therefore, we do not investigate countermeasures such as masking. 6.1  
   
  Inﬂuence of Duplication  
   
  Consider the duplication method from Sect. 5.2 (the advantage is similar when using multiplicative tags from Sect. 5.5). A privacy adversary (taking x = 0 and x = 1) faulting one variable to zero with probability κ has an advantage 0  
   
  1  
   
  Adv(A) = | Pr[AO =⊥] − Pr[AO =⊥] | = κ . This is because, when x = 0, the countermeasure can never abort or, when x = 1, it aborts with probability κ. The bound for m-bit variables (or viewing the variable m times) is 1 − (1 − κ)m ≤ mκ. In case the adversary faults all k duplicates to zero, the advantage becomes Adv(A) =  
   
  k−1  i=1  
   
  6.2  
   
   k i κ (1 − κ)k−i = 1 − κk − (1 − κ)k . i  
   
  Inﬂuence of Masked Duplication  
   
  Similar to Sect. 5.4, we consider a variable which is both masked and duplicated.  
   
  208  
   
  S. Dhooghe and S. Nikova  
   
  Mask-then-Duplicate. Consider a masked and encoded value (x00 , x10 , x01 , x11 ) where the shares are duplicated. When faulting both x00 and x10 to zero, we get 0  
   
  1  
   
  Adv(A) = | Pr[AO =⊥] − Pr[AO =⊥] | = κ − (1 − (1 − κ)2 )/2 = κ2 /2 . When considering n shares, the probability to change at least one share out of n (due to a set fault) when the sharing has secret zero κ0 or secret one κ1 is n  
   
  κ0 =  
   
  2  i=0  
   
  1−n  
   
  2  
   
    n (1 − (1 − κ)2i ), 2i  
   
    
   
   n−1 2   
   
  κ1 =  
   
    
   
  1−n  
   
  2  
   
  i=0  
   
   n (1 − (1 − κ)2i+1 ) . 2i + 1  
   
  The advantage of the above attack is | κ0 − κ1 | = 21−n κn . When faulting (x00 , x10 , x01 , x11 ) all to zero, the advantage becomes 2κ2 (1 − κ)2 . For small κ, the advantage is higher when faulting all duplicates and shares. For the bound when faulting all k duplicates and n shares is Adv(A) = 21−n (1 − (1 − κ)k − κk )n , since the advantage when faulting a k-duplication is 1 − (1 − κ)k − κk . Duplicate-then-Mask. Consider the masking (x00 , x11 , x02 , x13 ) such that x02 + x13 = x00 + x11 . When faulting x00 and x11 both to zero, we have an advantage 0  
   
  1  
   
  Adv(A) = | Pr[AO =⊥] − Pr[AO =⊥] | = κ2 . For n shares, this attack generalizes to the advantage κn . When faulting all bits (x00 , x11 , x02 , x13 ) to zero, the advantage is 2κ2 (1 − κ)2 . When investigating the advantage for n shares and k duplicates, when faulting all shares to zero, the probability to change the value of the secret when it is equal to zero is ⎞ ⎛     n i−1  2  n 2i ⎝ 21−n κ0 = κ2j+1 (1 − κ)2i−2j−1 ⎠ 2i 2j + 1 i=1 j=0 n  
   
  =  
   
  2  i=1  
   
  2−n  
   
    n (1 − (1 − 2κ)2i ) = 1/2(1 − (1 − κ)n − κn ) , 2i  
   
  where the equalities are derived from the binomial expansion theorem. Similarly, the probability to change the secret of a sharing of one is ⎞ ⎛      n−1 i  2   n 2i + 1 ⎝ 21−n κ2j+1 (1 − κ)2i−2j ⎠ κ1 = 2i + 1 2j + 1 i=0 j=0 = 1/2(1 − (1 − κ)n + κn ) .  
   
  The Random Fault Model  
   
  209  
   
  Then, when faulting all shares in the duplication, the probability to abort for secret zero is 1 − κk0 − (1 − κ0 )k . Similarly, for secret one the probability is 1 − κk1 − (1 − κ1 )k . Thus, the advantage against n shares and k duplicates is Adv(A) = | κk0 − κk1 + (1 − κ0 )k − (1 − κ1 )k | . We observe that the mask-then-duplicate method scales better for higher parameters n and the duplicate-then-mask method scales better for higher parameters k (with the mask-then-duplicate method performing better for equal parameters n, k) as depicted in Fig. 8.  
   
  Fig. 8. The privacy advantage of the mask-then-duplicate method (in green) and the duplicate-then-mask method (in red). The full lines depict (n, k) = (2, 2), the dashed lines depict (n, k) = (2, 3), and the dotted lines depict (n, k) = (3, 2). (Color ﬁgure online)  
   
  6.3  
   
  Inﬂuence of Shuﬄing  
   
  Consider the shuﬄing method from Sect. 4.4 but with duplicated values. Similar to Sect. 4.4 and Sect. 5.6, there are weak inputs in the privacy model. Namely, when shuﬄing (x, y) ∈ F22 and taking the two secrets x = y, shuﬄing becomes obsolete as the same values are shuﬄed. Moreover, the same attack described in Sect. 6.1 still applies. Namely, to attack all duplicates with a set-to-zero fault between the all-zero and all-one secrets. As a result, shuﬄing with duplication does not improve the random fault security in the privacy model. Moreover, when using a masking countermeasure, for small parameters κ the best attack is to fault all shares and duplicates with the same fault. The advantage of this attack would not change when shuﬄing the values. Together with the weaknesses found in the correctness model in Sect. 5.6, we conclude that shuﬄing against fault attacks exhibits several weaknesses. The vulnerabilities found in this work directly apply to the Rocky countermeasure [18] which we show is weak in both the correctness and privacy models of the random fault model for certain parameters κ and for certain weak inputs. In addition with the weaknesses found for shuﬄing in the random probing model from Sect. 4.4, we do not believe shuﬄing can provide a signiﬁcant improvement for security against either random probing or random fault adversaries.  
   
  210  
   
  7  
   
  S. Dhooghe and S. Nikova  
   
  Conclusion  
   
  In this work, we proposed a new fault adversary model called the random fault model which is a model inspired from the known random probing model. The goal of the work was to investigate and compare diﬀerent countermeasures and observe which can promise good security against fault attacks. To make this comparison properly, we also investigated the countermeasures in the random probing model. Most results in the random probing model are intuitive. Masking provides exponential protection in the number of shares and duplication linearly decreases security in the number of duplicates. However, we did observe a diﬀerence between the security of ﬁrst duplicating a variable and masking each duplicate versus masking a variable and duplicating each share which, to the best of our knowledge, had not been investigated before. One surprising result was that both shuﬄing or shuﬄing masked algorithms does not signiﬁcantly increase the security in the random probing model. This result is quite signiﬁcant since it holds no matter how the shuﬄing is performed. In the random fault model, we found that encoding techniques such as duplication exponentially improves the security in the minimal distance of the code. But we also observed that the number of parity bits has an inﬂuence to the countermeasure’s security which provides a theoretical explanation for the practical results by Bartkewitz et al. [3]. Interestingly enough, we also ﬁnd that triplication methods (error correction) are signiﬁcantly less secure than duplication methods (error detection) when the adversary faults several bits. Since there is a lot of work on error correction methods, it remains a question whether these works can provide any good security in a formal security model and we are left with the open question of investigating their security in practice. Moving to masking, we showed that masking reduces the security of a countermeasure against fault attacks and, similar to results in the random probing model, that there is a trade-oﬀ in security between duplicate-then-masking a variable versus masking-then-duplicating one. Finally, similar to the results in the random probing model, we show that shuﬄing does not improve a countermeasure’s security against fault attacks which implies that the countermeasure Rocky [18] does not currently provide a theoretical foundation for its security. Acknowledgement. A special thanks to Vincent Rijmen for the helpful discussions. This work was supported by CyberSecurity Research Flanders with reference number VR20192203.  
   
  References 1. Aghaie, A., Moradi, A., Rasoolzadeh, S., Shahmirzadi, A.R., Schellenberg, F., Schneider, T.: Impeccable circuits. IEEE Trans. Comput. 69(3), 361–376 (2020). https://doi.org/10.1109/TC.2019.2948617 2. Azouaoui, M., Bronchain, O., Grosso, V., Papagiannopoulos, K., Standaert, F.: Bitslice masking and improved shuﬄing: how and when to mix them in software? IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(2), 140–165 (2022)  
   
  The Random Fault Model  
   
  211  
   
  3. Bartkewitz, T., Bettendorf, S., Moos, T., Moradi, A., Schellenberg, F.: Beware of insuﬃcient redundancy an experimental evaluation of code-based FI countermeasures. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(3), 438–462 (2022) 4. Biham, E., Shamir, A.: Diﬀerential fault analysis of secret key cryptosystems. In: Kaliski, B.S., Jr. (ed.) CRYPTO 1997. LNCS, vol. 1294, pp. 513–525. Springer, Heidelberg (1997). https://doi.org/10.1007/BFb0052259 5. Bogdanov, A., Rivain, M., Vejre, P.S., Wang, J.: Higher-order DCA against standard side-channel countermeasures. In: Polian, I., St¨ ottinger, M. (eds.) COSADE 2019. LNCS, vol. 11421, pp. 118–141. Springer, Cham (2019). https://doi.org/10. 1007/978-3-030-16350-1 8 6. Chari, S., Jutla, C.S., Rao, J.R., Rohatgi, P.: Towards sound approaches to counteract power-analysis attacks. In: Wiener, M. (ed.) CRYPTO 1999. LNCS, vol. 1666, pp. 398–412. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-484051 26 7. Clavier, C.: Secret external encodings do not prevent transient fault analysis. In: Paillier, P., Verbauwhede, I. (eds.) CHES 2007. LNCS, vol. 4727, pp. 181–194. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-74735-2 13 8. Clavier, C., Feix, B., Gagnerot, G., Roussellet, M., Verneuil, V.: Horizontal correlation analysis on exponentiation. In: Soriano, M., Qing, S., L´ opez, J. (eds.) ICICS 2010. LNCS, vol. 6476, pp. 46–61. Springer, Heidelberg (2010). https://doi.org/10. 1007/978-3-642-17650-0 5 9. De Meyer, L., Arribas, V., Nikova, S., Nikov, V., Rijmen, V.: M&M: masks and macs against physical attacks. IACR Trans. Cryptographic Hardw. Embed. Syst. 2019(1), 25–50 (2018). https://doi.org/10.13154/tches.v2019.i1.25-50, https:// tches.iacr.org/index.php/TCHES/article/view/7333 10. Dhooghe, S., Nikova, S.: My gadget just cares for me - how NINA can prove security against combined attacks. In: Jarecki, S. (ed.) CT-RSA 2020. LNCS, vol. 12006, pp. 35–55. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-40186-3 3 11. Dobraunig, C., Eichlseder, M., Korak, T., Mangard, S., Mendel, F., Primas, R.: SIFA: exploiting ineﬀective fault inductions on symmetric cryptography. IACR Trans. Cryptographic Hardw. Embed. Syst. 2018(3), 547–572 (2018). https://doi.org/10.13154/tches.v2018.i3.547-572, https://tches.iacr.org/ index.php/TCHES/article/view/7286 12. Duc, A., Dziembowski, S., Faust, S.: Unifying leakage models: from probing attacks to noisy leakage. In: Nguyen, P.Q., Oswald, E. (eds.) EUROCRYPT 2014. LNCS, vol. 8441, pp. 423–440. Springer, Heidelberg (2014). https://doi.org/10.1007/9783-642-55220-5 24 ´ Lomn´e, V., Thillard, A.: Fault attacks on AES with faulty 13. Fuhr, T., Jaulmes, E., ciphertexts only. In: Fischer, W., Schmidt, J. (eds.) 2013 Workshop on Fault Diagnosis and Tolerance in Cryptography, Los Alamitos, CA, USA, 20 August 2013, pp. 108–118. IEEE Computer Society (2013). https://doi.org/10.1109/FDTC.2013.18 14. Goubin, L., Patarin, J.: DES and diﬀerential power analysis the “Duplication” method. In: Ko¸c, C ¸ .K., Paar, C. (eds.) CHES 1999. LNCS, vol. 1717, pp. 158–172. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48059-5 15 15. Ishai, Y., Prabhakaran, M., Sahai, A., Wagner, D.: Private circuits II: keeping secrets in tamperable circuits. In: Vaudenay, S. (ed.) EUROCRYPT 2006. LNCS, vol. 4004, pp. 308–327. Springer, Heidelberg (2006). https://doi.org/10. 1007/11761679 19 16. Ishai, Y., Sahai, A., Wagner, D.: Private circuits: securing hardware against probing attacks. In: Boneh, D. (ed.) CRYPTO 2003. LNCS, vol. 2729, pp. 463–481. Springer, Heidelberg (2003). https://doi.org/10.1007/978-3-540-45146-4 27  
   
  212  
   
  S. Dhooghe and S. Nikova  
   
  17. Kocher, P., Jaﬀe, J., Jun, B.: Diﬀerential power analysis. In: Wiener, M. (ed.) CRYPTO 1999. LNCS, vol. 1666, pp. 388–397. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48405-1 25 18. Miteloudi, K., Batina, L., Daemen, J., Mentens, N.: ROCKY: rotation countermeasure for the protection of keys and other sensitive data. In: Orailoglu, A., Jung, M., Reichenbach, M. (eds.) SAMOS 2021. LNCS, vol. 13227, pp. 288–299. Springer, Cham (2021). https://doi.org/10.1007/978-3-031-04580-6 19 19. Reparaz, O., et al.: CAPA: the spirit of beaver against physical attacks. In: Shacham, H., Boldyreva, A. (eds.) CRYPTO 2018. LNCS, vol. 10991, pp. 121– 151. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-96884-1 5 20. Schneider, T., Moradi, A., G¨ uneysu, T.: ParTI – towards combined hardware countermeasures against side-channel and fault-injection attacks. In: Robshaw, M., Katz, J. (eds.) CRYPTO 2016. LNCS, vol. 9815, pp. 302–332. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-53008-5 11 21. Shahmirzadi, A.R., Rasoolzadeh, S., Moradi, A.: Impeccable circuits II. In: 57th ACM/IEEE Design Automation Conference, DAC 2020, San Francisco, 20–24 July 2020, pp. 1–6. IEEE (2020). https://doi.org/10.1109/DAC18072.2020.9218615  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis Muzhou Li1,4 , Nicky Mouha3 , Ling Sun1,2,4,5 , and Meiqin Wang1,2,4(B) 1  
   
  School of Cyber Science and Technology, Shandong University, Qingdao, China [email protected]  , {lingsun,mqwang}@sdu.edu.cn 2 Quan Cheng Shandong Laboratory, Jinan, China 3 Strativia, Largo, MD, USA [email protected]  4 Key Laboratory of Cryptologic Technology and Information Security, Ministry of Education, Shandong University, Jinan, China 5 State Key Laboratory of Cryptology, P.O. Box 5159, Beijing 100878, China  
   
  Abstract. The related-key statistical saturation (RKSS) attack is a cryptanalysis method proposed by Li et al. at FSE 2019. It can be seen as the extension of previous statistical saturation attacks under the relatedkey setting. The attack takes advantage of a set of plaintexts with some bits ﬁxed, while the other bits take all possible values, and considers the relation between the value distributions of a part of the ciphertext bits generated under related keys. Usually, RKSS distinguishers exploit the property that the value distribution stays invariant under the modiﬁcation of the key. However, this property can only be deterministically veriﬁed if the plaintexts cover all possible values of a selection of bits. In this paper, we propose the probabilistic RKSS cryptanalysis which avoids iterating over all non-ﬁxed plaintext bits by applying a statistical method on top of the original RKSS distinguisher. Compared to the RKSS attack, this newly proposed attack has a signiﬁcantly lower data complexity and has the potential of attacking more rounds. As an illustration, for reduced-round Piccolo, we obtain the best key recovery attacks (considering both pre- and post-whitening keys) on both versions in terms of the number of rounds. Note that these attacks do not threaten the full-round security of Piccolo. Keywords: Related-Key Statistical Saturation  
   
  1  
   
  · Piccolo · Statistic  
   
  Introduction  
   
  Integral cryptanalysis is a cryptanalytic method for symmetric-key ciphers. First proposed by Daemen et al. as a dedicated attack on the Square cipher [9], the technique was later generalized by Knudsen and Wagner as the integral attack [18]. The integral distinguisher used in such an attack exploits the propagation of well-chosen sets of plaintexts through the cipher. In practice, a part of the plaintext bits is often ﬁxed to some constant while all possible values c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 213–232, 2024. https://doi.org/10.1007/978-3-031-53368-6_11  
   
  214  
   
  M. Li et al.  
   
  are taken for the other bits, and the evolution of the variable bits in the cipher state is tracked. To reduce its data complexity, the statistical integral attack [33] was proposed by Wang et al. at FSE 2016. It avoids iterating over all non-ﬁxed plaintext bits by applying a statistical technique on top of the original integral attack. In [13], Dobraunig et al. introduced a related-tweak square attack on KIASU-BC that extends the single-key attack by one round. The statistical saturation attack [7] was proposed by Collard and Standaert. It uses the same set of plaintexts as integral distinguishers, however, it tracks the evolution of a non-uniform value distribution of the ciphertext. At FSE 2019, Li et al. introduced the related-key statistical saturation (RKSS) attack [19] for key-alternating ciphers [10]. It also takes advantage of a set of plaintexts with some bits ﬁxed while the others take all possible values, however, it considers the relation between the value distributions of a part of the ciphertext bits generated under related keys. RKSS distinguishers exploit the property that a part of the ciphertexts keeps their value distribution invariant under the modiﬁcation of the key. However, this property can only be deterministically veriﬁed if the plaintexts cover all possible values of a selection of bits. In this paper, we revisit the RKSS cryptanalysis and propose a new method that can address such limitations with the help of a statistical model. This new method is referred to as probabilistic RKSS cryptanalysis. Compared to the original method, the data complexity here can be much smaller with only a small decrease in success probability. An intuitive comparison of these two methods is shown by their applications on Piccolo [29]. We now provide a detailed overview of the contributions of this paper. Probabilistic RKSS Cryptanalysis. In Sect. 3, we will introduce the probabilistic RKSS cryptanalysis method, which avoids iterating over all non-ﬁxed plaintext bits. In this way, we require less data than the original RKSS method, but the same value distribution property of the original RKSS will not strictly hold. However, we can still distinguish between a right key guess and a wrong key guess by choosing an appropriate statistic that considers the diﬀerent distributions in these two cases. First, we recall the value distribution property that the original RKSS method relies on. Let s be the number of plaintext bits that take all possible values while the other bits are ﬁxed. For all these 2s plaintexts, we encrypt them under related-key pairs and obtain two sets of ciphertexts. Denote t as the number of ciphertext bits whose value distribution is considered here. For any t-bit value of this part, we have the same number of occurrences in these two sets of ciphertexts. When less than 2s plaintexts are available, the occurrences of each t-bit value may not be the same anymore, but their differences may be small if enough plaintexts are given. Hence, the statistic is constructed by summing all 2t squared diﬀerences of the number of occurrences counted under these two related keys. With the help of Stuart-Maxwell tests for marginal homogeneity [24,30], we can prove that such a statistic follows a χ2 -distribution with diﬀerent parameters for right and wrong key guesses. The validity of this statistical model is also conﬁrmed experimentally on a toy cipher.  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  215  
   
  With this statistical model, the data complexity of the RKSS attack can be reduced from 2s to (2t −1) qα1 s s N = 2 − (2 − 1) (2t −1) , q1−α0 (2t −1)  
   
  (2t −1)  
   
  and q1−α0 represent the quantiles of the central χ2 -distribution where qα1 with each having a degree of freedom equal to 2t − 1. Meanwhile, α0 (resp. α1 ) is the probability of rejecting the right key (resp. of accepting a wrong key). This new attack has a success probability of Prs = 1 − α0 . Note that the trade-oﬀ between the success probability Prs and the data complexity N allows the attack to cover more rounds than the original RKSS method. Improved Key Recovery Attacks on Round-Reduced Piccolo with both Whitenings. Piccolo [29] is a 64-bit ultra-lightweight key-alternating block cipher designed by Shibutani et al. at CHES 2011. It is suitable for constrained environments such as RFID tags and sensor nodes. The cipher supports 80-bit and 128-bit keys, denoted as Piccolo-80 and Piccolo-128, respectively. Since its proposal, many key recovery attacks have been introduced such as (conditional) linear attacks [2], (multidimensional) zero-correlation linear attacks [1,15], meet-in-the-middle attacks [16,21,22,32], and (related-key) impossible diﬀerential attacks [4,25,31]. In addition, there are some other results such as biclique attacks [17,34]. However, there is a consensus in the literature that biclique attacks are not a threat to a cipher, as they require an exhaustive search over a reduced number of rounds of the cipher. From all these attacks, we ﬁnd that the security resistance of Piccolo is different depending on whether the pre/post-whitening key layers are included or not. Speciﬁcally, when both whitenings are considered, the best-known attack on Piccolo-80 is on 8 rounds [2], not including biclique attacks. Meanwhile, the best result on Piccolo-128 with both whitenings is a biclique attack [17]. When including none or only one of these two whitening key layers, the best key recovery attack can cover 14 rounds for Piccolo-80 [21,32] and 18 rounds for Piccolo-128 [21]. This conﬁrms that key whitening may strengthen the security of Piccolo. Thus, we are motivated to investigate its real impact on security, and try to narrow the gap between the cryptanalytic results in the above two cases. In Sect. 4, we mount several key recovery attacks on both variants of Piccolo using the probabilistic RKSS method. To show the eﬀectiveness of this new method, we also propose attacks using the RKSS method in Sect. 4. All these results are presented in Table 1. Compared to previous results, they are the best key recovery attacks containing both pre- and post-whitening keys on Piccolo. From Table 1, for 16-round Piccolo-128, we can see that the probabilistic RKSS method needs only 3.44% of the number of plaintexts required in the RKSS attack with only a little decrease in its success probability from 100% to 99%. Moreover, the probabilistic RKSS method can cover one more round than the RKSS method. As for Piccolo-80, the data complexity used in the new method is only 10% of that required in the RKSS method where its success probability is 99%.  
   
  216  
   
  M. Li et al.  
   
  Table 1. Comparison of attacks on Piccolo containing both pre- and post-whitening key layers. Time complexities are evaluated in encryption units, while memory costs are evaluated in bits, and #k denotes the number of diﬀerent keys used. Cipher  
   
  2  
   
  Attacks  
   
  Rounds Data  
   
  Time  
   
  Memory #k Ref.  
   
  Piccolo-128 RKSS 16 Prob. RKSS 16 Prob. RKSS 17  
   
  249 2114.19 238 244.14 2114.18 238 260.14 2115.44 267.14  
   
  2 2 2  
   
  Sect. 4.2 Sect. 4.2 Sect. 4.3  
   
  Piccolo-80  
   
  254 254 241 274.49 37.68 2 274.48  
   
  1 2 2  
   
  [2] Sect. 4.1 Sect. 4.1  
   
  Cond. Linear 8 RKSS 10 Prob. RKSS 10  
   
  N.A. 233.81 233.81  
   
  Preliminaries  
   
  Key-alternating ciphers form a signiﬁcant subset of modern block ciphers, which was introduced by Daemen and Rijmen in [10]. Many block ciphers, including almost all Substitution-Permutation Networks (SPNs) and some Feistel ciphers, belong to this subset [11]. Definition 1 (Key-Alternating Block Cipher [10]). Given an r-round iterative block cipher E, let ki represent its i-th round key with 1 ≤ i ≤ r. If ki is XORed into the state at the end of the i-th round and there exists a subkey k0 introduced by XORing with the plaintext before the ﬁrst round, the block cipher E is a key-alternating block cipher. The related-key statistical saturation (RKSS) attack [19] is a new cryptanalytic method for key-alternating ciphers proposed by Li et al. at FSE 2019. This method can be regarded as an extension of statistical saturation attack [7] in the related-key setting. As pointed out in [19], this method is also applicable for tweak/tweakey-alternating ciphers, where related-tweak/tweakey are taken into consideration, since tweak/tweakey can be seen as a kind of key. For simplicity, all of these are referred to as RKSS attacks in this paper. The main idea of the RKSS attack is that we ﬁx a part of the plaintext bits and take all possible values for the other bits, and then consider the relation between the value distributions of a part of the ciphertext bits under related-key pairs (z, z  = z ⊕ Δz), where Δz is a ﬁxed value for all possible values of the key z. To obtain such RKSS distinguishers, Li et al. [19] introduced a conditional equivalent property between the KDIB distinguisher [6] and the RKSS distinguisher. The KDIB technique [6] is another method proposed for key-alternating ciphers, which can be seen as an extension of linear cryptanalysis [23]. Linear cryptanalysis typically uses a linear trail. Denote θ = (θ0 , θ1 , · · · , θr ) as an r-round linear trail, where θi−1 is the input mask of round i (1 ≤ i ≤ r) and θi is the output mask. Its bias εθ is related to the unknown key z. For key-alternating ciphers, only the sign of εθ is aﬀected by z. A linear hull (u, w) consists of all trails satisfying u = θ0 and w = θr [27], whose bias is evaluated by summing  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  217  
   
  all biases of these trails under the same key z. Hence, the bias of a linear hull can be invariant if it is evaluated under related-key pairs (z, z  ) fulﬁlling some speciﬁc key diﬀerence Δz. This is the fact that the KDIB distinguisher exploits. To explain the conditional equivalent property between KDIB and RKSS distinguishers, we adopt the same notation used in [19]. Denote Fn2 as the space of n-dimensional binary vectors over F2 = {0, 1}. Let H : Fn2 × Fk2 → Fn2 be the target block cipher with block size n and key size k. The n-bit input of H is split into two parts (x, y), where x is the part ﬁxed and y is the part taking all possible values. Note that these two parts can be composed of arbitrary input bits. Similarly, the output of H is also divided into two parts (H1 (x, y, z), H2 (x, y, z)) and only the value distribution of H1 (x, y, z) is considered. Thus, we have H : Fr2 × Fs2 × Fk2 → Ft2 × Fu2 , H(x, y, z) = (H1 (x, y, z), H2 (x, y, z)). Fixing x to a constant value I and only focusing on the H1 part of the output, we can obtain the function TI : Fs2 × Fk2 → Ft2 , TI (y, z) = H1 (I, y, z). In an RKSS distinguisher, we will consider the relation between the value distributions of TI (y, z) and TI (y, z  ) after encrypting all possible values of y. Given the above notation, the conditional equivalent property between the KDIB and the RKSS distinguishers can be described in Theorem 1. Once the KDIB distinguisher is found, an RKSS distinguisher covering the same rounds can also be obtained using Theorem 1. Theorem 1 (Conditional Equivalent Property [19]). Let (Γ, Λ) be the linear hull of the target block cipher with Γ = (Γin , 0) and Λ = (Λout , 0), where Γin ∈ Fr2 and Λout ∈ Ft2 \{0}. Given a ﬁxed Δz, if for all possible mask pairs (Γin , Λout ), the bias is invariant under related-key pairs (z, z  = z ⊕Δz), TI (y, z) will have the same value distribution as TI (y, z  ) when y takes all possible values and vice versa. In other words, for any c ∈ Ft2 , we have #{y ∈ Fs2 | TI (y, z) = c} = #{y ∈ Fs2 | TI (y, z  ) = c}. Note that this holds for any I ∈ Fr2 . Note that in Theorem 1, the restriction to masks of the form (Γin , 0) and (Λout , 0), where the last bits are ﬁxed to zeros, is solely for the simplicity of notation. As pointed out in [19], the positions of the zero bits do not aﬀect the applicability of this property. From Theorem 1, we can see that the RKSS distinguisher exploits the property that the value distribution of some ciphertext bits stays invariant under the modiﬁcation of the key. When mounting the RKSS key recovery attack, we have to traverse all possible values of y under a ﬁxed value of x, and ask for ciphertexts under z and z  . Thus, we can observe whether TI (y, z) has the same value distribution with TI (y, z  ) after guessing the corresponding key bits. If so, the guessed key bits will be taken as the right key bits. Otherwise, they will be discarded. According to Theorem 1, for a right key guess, TI (y, z) always has the same value distribution with TI (y, z  ). Hence, the probability of rejecting the right key α0 is zero. As for the probability of accepting a wrong key α1 , t they proved that log2 (α1 ) is no more than (2t − 1 − t) 2s+1 − 2s(2 −1)/2 , which is extremely small. For instance, when Li et al. [19] attacked 10-round QARMA-64 [3]  
   
  218  
   
  M. Li et al.  
   
  with s = 56 and t = 4, it was found that log2 (α1 ) ≤ −2.7 × 10126 , which implies that α1 ≈ 0.  
   
  3 3.1  
   
  Probabilistic Related-Key Statistical Saturation Attack Introducing a Statistical Model into RKSS Cryptanalysis  
   
  In this subsection, we adopt the notation introduced in Sect. 2. Let qj (resp. qj ) denote the probability that TI (y, z) = j (resp. TI (y, z  ) = j) when iterating over 2t −1 2t −1 all possible values of y ∈ Fs2 . Thus, j=0 qj = 1 and j=0 qj = 1. Note that in the RKSS attack, qj and qj can take various values for diﬀerent wrong key candidates z and z  , while qj = qj holds for any j for a right key guess. Let χ2 (l, λ) represent the noncentral χ2 -distribution with degree of freedom l and noncentrality parameter λ. For an RKSS distinguisher, we can obtain Lemma 1 for both wrong and right key guesses, according to Stuart-Maxwell [24,30] tests for marginal homogeneity. Due to the limit of paper length, proof of Lemma 1 is shown in the full version [20]. Lemma 1. When 2s is suﬃciently large, for a wrong key guess, the statistic 2t −1 (2s qj −2s qj )2 approximately follows χ2 (2t − 1, 0). For the right key γ = j=0 2s qj +2s qj guess, the statistic γ = 0. The only way to reduce the data complexity of an RKSS attack is to reduce the number of y that are chosen. However, the same value distribution property under a right key guess will not hold if we choose some random values for y. The advantage is that we can distinguish a right key guess from a wrong one by constructing a statistic with the information of similar frequencies of each possible output under related-key pairs (z, z  ), if a considerable number of distinct values of plaintexts are reachable. This new kind of RKSS attack with reduced data complexity will be referred to as a probabilistic RKSS attack hereafter. Assume that we have obtained two independent randomly chosen distinct plaintext sets S and S  with the same size N . All plaintexts share the same ﬁxed I. For each y ∈ S (resp. y  ∈ S  ), we can get a t-bit value TI (y, z) (resp. TI (y  , z  )) that is computed under z (resp. z  ). Then we respectively add one to the counter V [j1 ] and V  [j2 ], where j1 = TI (y, z) and j2 = TI (y  , z  ). After traversing all these N values of y and N values of y  , we can construct an eﬃcient distinguisher by investigating the distribution of the following statistic C=  
   
  t 2 −1  
   
  j=0  
   
  (V [j] − V  [j])2 , 2N · 2−t  
   
  where V [j] = #{y ∈ S | TI (y, z) = j} and V  [j] = #{y  ∈ S  | TI (y  , z  ) = j}. This statistic C considers diﬀerent distributions determined by whether we are dealing with an actual cipher (right key guess) or a random permutation (wrong key guess). These two distributions of C are derived under Hypothesis 1. The validity of this hypothesis has been veriﬁed experimentally in [20].  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  219  
   
  Hypothesis 1. For any 0 ≤ i ≤ 2t − 1, 0 ≤ j ≤ 2t − 1, we assume that qi qj ≈ (2−t )2 , qi qj ≈ (2−t )2 , and qi + qj ≈ 2 · 2−t hold when 2s is suﬃciently large1 . Proposition 1. Denote Crandom as the statistic C for a wrong key guess and Ccipher as the statistic C for the right key guess. Under Hypothesis 1, for suﬃciently large N , the statistic   2s − 1 Ccipher ∼ χ2 2t − 1, 0 , s 2 −N while the statistic  
   
    Crandom ∼ χ2 2t − 1, 0 .  
   
  To prove this proposition, we have to recall the following lemma. Lemma 2. (See [12]) Let X = (X1 , X2 , · · · , Xd )T be a d-dimensional statistic vector that follows the multivariate normal distribution with expectation μ and 2 covariance matrix Σ, where Σ is a symmetric  matrix of rank r ≤ d. If Σ = Σ T 2 T and Σμ = μ, we have X X ∼ χ r, μ μ . With Hypothesis 1 and Lemmas 1 and 2, we can prove Proposition 1 as follows. Due to the limit of paper length, we only show the sketch of our proof. Details are shown in the full version [20]. Proof. (Sketch) Recall that when mounting probabilistic RKSS attacks, the counters V [TI (y, z)] and V  [TI (y  , z  )] are generated by encrypting two independently chosen values y and y  under z and z  . Therefore, these two counters are independent of each other. Since we choose distinct values of y (sampling without replacement), the statistic vector (V [0], V [1], · · · , V [2t − 1]) follows a multivariate hypergeometric distribution with parameters (K, 2s , N ) where K = (N q0 , N q1 , · · · , N q2t −1 ). Similarly, the vector (V  [0], V  [1], · · · , V  [2t − 1]) also follows a multivariate hypergeometric distribution however the parameters are (K  , 2s , N ) where K  = (N q0 , N q1 , · · · , N q2 t −1 ). When N is suﬃciently large, both hypergeometric distributions can be approximated into multivariate normal ones. j = V [j] − V  [j]. Then we have that For any 0 ≤ j ≤ 2t − 1, deﬁne X  = (X 0 , X 1 , · · · , X 2t −1 ) also follows a multivariate normal distribution. Since X  expectation of Xj is E(V [j] − V  [j]) = E(V [j]) − E(V  [j]) = N qj − N qj , the  can be obtained. The covariance between X i and X j can be expectation of X     computed by Cov( Xi , Xj ) = Cov(V [i], V [j]) + Cov(V  [i], V [j]). s 2s −N −t   Let Xj = Xj / 2N 2 2s −1 . Then X = X/ 2N 2−t 22s−N −1 also follows a multivariate normal distribution with expectation μ = (μ0 , μ1 , · · · , μ2t −1 ) where 2s − N 2s − N  −t  = (N qj − N qj )/ 2N 2−t s , μj = E(Xj )/ 2N 2 s 2 −1 2 −1 1  
   
  In our experimental veriﬁcation, s = 12 and it is enough to ensure the validity of this hypothesis, as well as other assumptions used in this paper.  
   
  220  
   
  M. Li et al.  
   
  and covariance matrix Σ where Σi,i =  
   
  −qi qj − qi qj qi (1 − qi ) + qi (1 − qi ) , Σ = . i,j 2 · 2−t 2 · 2−t  
   
  Due to Hypothesis 1, Σi,i ≈ 1−2−t and Σi,j ≈ −2−t . Notice that Σ is symmetric and its rank is 2t − 1. It is easy to verify that Σ 2 = Σ and Σμ = μ. According to Lemma 2, we can conclude that 2 2t −1 2t −1  2 2s − 1  (V [j] − V  [j]) 2s − 1  N qj − N qj 2 t ∼ χ (2 −1, λ) with λ = s . 2s − N j=0 2N 2−t 2 − N j=0 2N 2−t Under Hypothesis 1, γ in Lemma 1 can be approximated as t 2 −1  
   
  γ≈  
   
    
   
  j=0  
   
  and then λ ≈ words,  
   
  s  
   
  2 −1 N 2s −N 2s γ.  
   
  2s qj − 2s qj 2 · 2s · 2−t  
   
  2  
   
  Thus, for a right key guess, λ = 0 since γ = 0. In other   2s − 1 Ccipher ∼ χ2 2t − 1, 0 . s 2 −N s  
   
  s  
   
  2 t While for a wrong key guess, 2N 22s−N −1 λ ∼ χ (2 − 1, 0) according to Lemma 1. Thus, the distribution of Crandom can be obtained with the characteristic func  tions of χ2 -distributions.  
   
  To decide whether the obtained statistic C is computed from the cipher (a right key guess) or the random permutation (a wrong key guess), we have to perform a statistic test. In this test, we compare C to a threshold value τ . If C ≤ τ , we conclude that C is obtained from the cipher; otherwise, it is from a random permutation. The data complexity needed to perform the statistic test and the threshold value τ can be computed as follows, given error probabilities. Due to the limit of paper length, proof of Corollary 1 is shown in the full version [20]. Corollary 1. Denote α0 as the probability of rejecting the right key and α1 as the probability of accepting a wrong key. Under the assumption of Proposition 1, the number of distinct plaintexts encrypted under a single key is (2t −1)  
   
  N = 2 − (2 − 1) s  
   
  (2t −1)  
   
  s  
   
  s  
   
  qα1  
   
  (2t −1)  
   
  q1−α0  
   
  ,  
   
  (2t −1)  
   
  (2t −1)  
   
  and the threshold value is τ = qα1 = 22s−N −1 q1−α0 , where qα1 2 t are the respective quantiles of χ (2 − 1, 0).  
   
  (2t −1)  
   
  and q1−α0  
   
  According to Corollary 1, we can see that the data encrypted under a single key in the probabilistic RKSS attack is less than 2s , which is the data collected  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  221  
   
  under a single key of the original RKSS attack. In other words, our newly proposed method needs less data than the original one. Meanwhile, the success probability of this attack is Prs = 1 − α0 . Note that such a trade-oﬀ between Prs and N can make it possible to mount attacks that cover more rounds than the original RKSS method. Further comparisons between these two methods are shown in the full version [20]. 3.2  
   
  Experimental Verification of the Statistical Model  
   
  To verify the theoretical model, we implement a distinguishing attack on a mini version of an SPN cipher denoted as SmallSPN (a variant of Mini-AES [28]).2 SmallSPN is a 20-round key-alternating cipher with a block size of 16 bits. Its round function contains four operations, i.e., SB, SR, M C, and AK. Additionally, there is another AK operation before the ﬁrst round.  
   
  The 16-bit plaintext P0 P1 P = P0 ||P1 ||P2 ||P3 is arranged into a 2 × 2 matrix and SB uses 4-bit P2 P3 S-box in QARMA-64  
   
  [3]. SR is the operation interchanging P2 and P3 . The matrix 01 used in M C is . Denote rk i as the round key in the i-th round, 0 ≤ i ≤ 20, 11 and rkji is the j-th nibble of rk i where 0 ≤ j ≤ 3. Each subkey rk i will be XORed with the nibbles in AK operations, all of which are chosen uniformly at random.  
   
  Fig. 1. Experimental results related to the statistical model using SmallSPN.  
   
  The 20-round RKSS distinguisher used here can be described as follows: when we ﬁx P3 and iterate over all 212 possible values of P0 ||P1 ||P2 , the value 2  
   
  SmallSPN has a structure that is similar to Mini-AES, but they have a diﬀerent number of rounds, S-box, linear matrix, and key schedule.  
   
  222  
   
  M. Li et al.  
   
  distributions of C3 obtained under K and C3 obtained under K  will be the same. K  and K only have non-zero diﬀerences on rk00 , rk10 , rk20 , rk11 , rk21 , rk31 , rk12 , rk32 , rk33 , rk118 , rk019 , rk318 , rk020 , rk120 , and rk220 . Now we mount the probabilistic RKSS attack using the statistical model described in Proposition 1 where s = 12 and t = 4. Setting α0 = 0.2 and choosing diﬀerent values for N , we can obtain α1 and τ according to Corollary 1. In each experiment, we independently and randomly collect two plaintext sets with size N , where all plaintexts share the same ﬁxed I, and query their ciphertexts generated with SmallSPN. After computing the statistic C and comparing it with τ , we can decide whether we are facing the real cipher. By launching this experiment 1000 times, we can obtain the empirical error probability αˆ0 . Similarly, if we generate these ciphertexts with random permutations, we can obtain the empirical error probability αˆ1 following the same procedure. Thereafter, we can compare these error probabilities with theoretical ones α0 and α1 , which is illustrated in Fig. 1. From Fig. 1, we can see that the test results for error probabilities are in good accordance with those for the theoretical model. Thus, our statistical model is accurately constructed.  
   
  4  
   
  Improved Key Recovery Attacks on Piccolo Considering Pre- And Post-whitening  
   
  At CHES 2011, Piccolo was proposed by Shibutani et al. [29] as a lightweight block cipher with a 64-bit block size. The key size can be either 80 or 128 bits, and we will denote these variants as Piccolo-80 and Piccolo-128, respectively. In this section, we provide the best key recovery attacks on Piccolo (containing both pre- and post-whitening key layers) in terms of the number of rounds, compared to previous results. When no whitening keys or only either pre- or post-whitening is considered, the best attacks on Piccolo are meet-in-the-middle (MITM) attacks [21,22]. However, according to [14,29], whitening keys are essential to construct ciphers that are resistant to MITM attacks. Thus, to check the resistance of Piccolo against MITM attacks when both whitening keys are included, we had a private communication with the authors of [21,22]. We both agree that MITM cannot attack 10-round Piccolo-80 and 16-round Piccolo-128 in this case since almost all key bits have to be guessed. Hence, to the best of our knowledge, our key recovery attacks are the best-known attacks on Piccolo. 4.1  
   
  Probabilistic RKSS Attack on 10-Round Piccolo-80  
   
  The ﬁrst step to mount attacks is to ﬁnd an RKSS distinguisher. As explained in Sect. 2, Li et al. [19] constructed a search algorithm for KDIB distinguishers, and then RKSS distinguishers covering the same rounds can be obtained using Theorem 1. To make our paper self-contained, we brieﬂy recall the principle of this automatic search algorithm. For more details, we refer to [19]. Their search algorithm is based on STP,3 which is a Boolean Satisﬁability Problem (SAT) [8]/Satisﬁability Modulo Theories (SMT) problem [5] solver. The 3  
   
  http://stp.github.io/.  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  223  
   
  application of STP as an automatic search tool for diﬀerential cryptanalysis was ﬁrst suggested by Mouha and Preneel in [26]. It takes a set of equations as input and decides whether or not they have a valid solution. Therefore, when using STP to ﬁnd KDIB distinguishers, we have to build some equations that describe the propagation properties of each operation. More speciﬁcally, for operations in the round function, the word-level mask propagation properties should be described; while for each operation in the key schedule, we have to describe its bit-level diﬀerence propagation property. Moreover, there are also some equations required to describe the relation between the masks and the key diﬀerence. Inserting all these equations into STP, we can obtain a KDIB distinguisher for a ﬁxed number of rounds or conclude that no KDIB distinguishers exist. Like other related-key attacks, the starting round of the distinguisher has an impact on the length of the distinguisher. Using this automatic search algorithm, we found an 8-round KDIB distinguisher with the pre-whitening key layer starting from the third round. Detailed distinguisher is shown in the full version [20]. The key diﬀerence of this distinguisher is Δk4 [1] = β which can be any non-zero value in F42 . Denote the 16-bit value X as X = X[0]||X[1]||X[2]||X[3] with X[i] ∈ F42 , and let X[i, j] represents X[i]||X[j]. Combining the 8-round KDIB distinguisher with Theorem 1 leads to the following RKSS distinguisher. Corollary 2. With the notation of Fig. 2, for the 8-round Piccolo-80 including pre-whitening key layer, when we take all 240 plaintexts with P0 [0, 2, 3]||P2 [0, 2, 3] ﬁxed, the value distribution of the 12-bit value W1 [0, 2, 3]⊕k2 [0, 2, 3] stays invariant under (K, K  ), where K and K  only diﬀer at k4 [1].  
   
  Fig. 2. Probabilistic RKSS attack on 10-round Piccolo-80 with full whitening, where • are active nibbles and × are nibbles that we need to know in the key recovery procedure.  
   
  Using this distinguisher, a probabilistic key recovery attack on 10-round Piccolo-80 can be carried out by adding two rounds and the post-whitening key layer at the end. Algorithm 1 and Fig. 2 show the details of this attack. As  
   
  224  
   
  M. Li et al.  
   
  usual, we collect N plaintexts P with P0 [0, 2, 3] and P2 [0, 2, 3] ﬁxed. For each plaintext, we can query its corresponding ciphertext. Since wk2 and wk3 have been guessed, we can compute x1 and increase V1 [x1 ] by one. With a similar procedure, another counter V1 can be obtained from another N plaintexts P  where P0 [0, 2, 3] = P0 [0, 2, 3] and P2 [0, 2, 3] = P2 [0, 2, 3]. With another guess of k0R and k1L , we can obtain the counters V2 and V2 from V1 and V1 , respectively. Using the statistical model proposed in Sect. 3, we can get the right key after checking its validity with two new plaintext-ciphertext pairs.  
   
  Algorithm 1: Key recovery attack procedure of 10-round Piccolo-80 containing both pre- and post-whitening keys. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  
   
  for 216 wk2 and 216 wk3 do Allocate and initialize two arrays V1 [x1 ] and V1 [x1 ] with |x1 | = 28 = |x1 |; wk2 = wk2 ⊕ 0x0β00 and wk3 = wk3 ; for N plaintexts P with P0 [0, 2, 3] and P2 [0, 2, 3] fixed do Query the ciphertexts C under K; Decrypt C0 , C2 to get Y0 [2, 3], Z0 [0], Y1 [0, 1] and Z2 [2, 3]; Let x1 ← Z0 [0]||(Y0 [2, 3] ⊕ C1 [2, 3])||Z2 [2, 3]||(Y1 [0, 1] ⊕ C3 [0, 1]) and V1 [x1 ] ← V1 [x1 ] + 1; for N plaintexts P  with P0 [0, 2, 3] = P0 [0, 2, 3] and P2 [0, 2, 3] = P2 [0, 2, 3] do Query the ciphertexts C  under K  ; Decrypt C0 , C2 to get Y0 [2, 3], Z0 [0], Y1 [0, 1] and Z2 [2, 3]; Let x1 ← Z0 [0]||(Y0 [2, 3] ⊕ C1 [2, 3])||Z2 [2, 3]||(Y1 [0, 1] ⊕ C3 [0, 1]) and V1 [x1 ] ← V1 [x1 ] + 1; for 28 k0R and 28 k1L do Allocate V2 [x2 ] and V2 [x2 ] with |x2 | = 12 = |x2 |, and initialize them to zeros; (k0 )R = k0R and (k1 )L = k1L ; for 228 x1 and x1 do Decrypt half-round for x1 and x1 to get W1 [0, 2, 3] ⊕ k2 [0, 2, 3] and W1 [0, 2, 3] ⊕ k2 [0, 2, 3]; Let x2 ← W1 [0, 2, 3] ⊕ k2 [0, 2, 3] and V2 [x2 ] ← V2 [x2 ] + V1 [x1 ]; Let x2 ← W1 [0, 2, 3] ⊕ k2 [0, 2, 3] and V2 [x2 ] ← V2 [x2 ] + V1 [x1 ]; C = 0; for 212 x do   12  C ← C + 2x=0−1 (V2 [x] − V2 [x])2 /(2N · 2−12 ) ; if C ≤ τ then The guessed key bits are possibly right; for 216 k2 , 28 k0L and 28 k1R do Use two plaintext-ciphertext pairs to check if they are right;  
   
  Suppose that one memory access to an array of size 228 costs less than one encryption of 10-round Piccolo-80. Then, the time complexity of this key recovery attack is at most T = 232 N · 2 · (1 + 1) + 232 · 216 · 228 · 2 · (1/2) · (1/10) + 2 · 280 α1 ,  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  225  
   
  where N can be computed using Corollary 1 after choosing the values of α0 and α1 . Here, we set α0 = 0.01 and α1 = 2−7.16 . In this way, N ≈ 236.68 , τ ≈ 211.92 . Hence, the data complexity is D = 2N ≈ 237.68 chosen plaintextciphertext pairs, while the time complexity is T = 274.48 10-round encryptions. The memory requirements are M = 2 · 228 · 28 ≈ 233.81 bits needed for arrays. To show the advantages of our newly proposed method, we also give the complexity of the original RKSS attack using the same distinguisher. Since we have to iterate over all possible values of P0 [1]||P1 ||P2 [1]||P3 in the original RKSS ˜ = 241 chosen plaintext-ciphertext pairs. attack, the data complexity will be D The time complexity can be computed as before except that it is 232 rather than 280 α1 and N = 240 , which is T˜ = 274.49 times a 10-round encryption. ˜ = M . As we can see, D < D. ˜ More precisely, The memory requirement is M ˜ D = 10% × D. 4.2  
   
  Probabilistic RKSS Attack on 16-Round Piccolo-128  
   
  In this subsection, we provide a probabilistic RKSS key recovery attack on 16round Piccolo-128 containing both pre- and post-whitening layers. This attack is based on the 11-round RKSS distinguisher starting from the 14-th round described in Corollary 3. Corollary 3. With the notation of Fig. 3, for the 11-round Piccolo-128, when we take all 248 input values of 14-th round with X0 [0, 1]||X2 [2, 3] ﬁxed, the value distribution of the 16-bit value W3 ⊕ k3 stays invariant under (K, K  ), where K and K  only diﬀer at k0 [2, 3] = β ∈ F82 \{0}. The probabilistic RKSS attack on 16-round Piccolo-128 can be mounted by adding the pre-whitening key layer before the distinguisher and ﬁve rounds, as well as the post-whitening key layer at the end. The detailed key recovery procedure is illustrated in Fig. 3 and described in Algorithm 2. One thing we should mention here is that to get the same value distribution property, we have to encrypt two independent data sets with X0 [0, 1] and X2 [2, 3] ﬁxed under related keys. Since wk1 [2, 3] = k0 [2, 3] has a non-zero known diﬀerence β, we can obtain the same ﬁxed X2 [2, 3] by setting P  [2, 3] = P [2, 3] ⊕ β. Suppose that one memory access to an array of size 232 costs less than one encryption of 16-round Piccolo-128. Then, the time complexity of this key recovery attack can be computed as T = 264 N · 2 · (1 + 4/16 + 1) + 264 · 216 · 232 · 2 · (1/2) · (1/16) + 2 · 2128 α1 . By setting α0 = 0.01 and α1 = 2−14.89 , we can obtain N ≈ 243.14 with τ ≈ 215.97 according to Corollary 1. Thus, the data complexity is D ≈ 244.14 chosen plaintext-ciphertext pairs, while the time complexity is T ≈ 2114.18 16-round encryptions. The memory requirements are M = 2 · 232 · 32 = 238 bits needed for these arrays. Compared to the RKSS key recovery attack using the same distinguisher, ˜ = 249 chosen plaintext-ciphertext pairs and T˜ ≈ 2114.19 16which needs D round encryptions, the probabilistic RKSS method performs much better than ˜ the original one. Speciﬁcally, D = 3.44% × D.  
   
  226  
   
  M. Li et al.  
   
  Fig. 3. Probabilistic RKSS attack on 16-round Piccolo-128 with full whitening, where • are active nibbles and × are nibbles that we need to know in the key recovery procedure.  
   
  4.3  
   
  Probabilistic RKSS Attack on 17-Round Piccolo-128  
   
  Using the same distinguisher introduced in Corollary 3, we can mount a 17round key recovery attack on Piccolo-128 by adding an extra round before it. This key recovery attack is the best one on Piccolo-128 considering both preand post-whitening keys in terms of the number of rounds, compared to previous known results. Due to Corollary 3, to guarantee that W3 ⊕k3 has the same value distribution with W3 ⊕ k3 , we need to iterate over all possible values of the input of 14-th round X = X0 ||X1 ||X2 ||X3 with X0 [0, 1]||X2 [2, 3] ﬁxed, which is equivalent to all possible values of U = U0 ||U1 ||U2 ||U3 with U1 ﬁxed (See Fig. 4). In other words, s = 48 and t = 16 here. Under α0 = 0.01 and α1 = 2−14.89 , we need  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  227  
   
  Algorithm 2: Key recovery attack procedure of 16-round Piccolo-128 with both pre- and post-whitening keys. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  
   
  for 216 k4 , 216 k7 , 216 k0 and 216 k1 do wk2 = k4L ||k7R and wk3 = k7L ||k4R ; wk2 = wk2 , wk3 = wk3 , k4 = k4 , k7 = k7 , k0 = k0 ⊕ 0x00β and k1 = k1 ; Allocate and initialize two arrays V1 [x1 ] and V1 [x1 ] with |x1 | = 32 = |x1 |; for N plaintexts P with P0 [0, 1] and P2 [2, 3] fixed do Query the ciphertext C for P under K; Decrypt C to get Z0 [2, 3], Z1 [0, 1], Z2 [0, 1], Z3 [2, 3], Y0 [0, 1] and Y1 [2, 3]; Let x1 ← Z0 [2, 3]||(Z1 [0, 1] ⊕ Y0 [0, 1])||Z2 [0, 1]||(Z3 [2, 3] ⊕ Y1 [2, 3]) and V1 [x1 ] ← V1 [x1 ] + 1; for N plaintexts P  with P0 [0, 1] = P0 [0, 1] and P2 [2, 3] = P2 [2, 3] ⊕ β do Decrypt C  to get Z0 [2, 3], Z1 [0, 1], Z2 [0, 1], Z3 [2, 3], Y0 [0, 1] and Y1 [2, 3]; Let x1 ← Z0 [2, 3]||(Z1 [0, 1] ⊕ Y0 [0, 1])||Z2 [0, 1]||(Z3 [2, 3] ⊕ Y1 [2, 3]) and V1 [x1 ] ← V1 [x1 ] + 1; for 28 k2L and 28 k5R do (k2 )L = k2L and (k5 )R = k5R ; Allocate and initialize two arrays V2 [x2 ] and V2 [x2 ] with |x2 | = 16 = |x2 |; for 232 x1 and x1 do Decrypt half-round for x1 and x1 to get W3 ⊕ k3 and W3 ⊕ k3 ; Let x2 ← W3 ⊕ k3 , and V2 [x2 ] ← V2 [x2 ] + V1 [x1 ]; Let x2 ← W3 ⊕ k3 , and V2 [x2 ] ← V2 [x2 ] + V1 [x1 ]; C = 0; for 216 x do   16  C ← C + 2x=0−1 (V2 [x] − V2 [x])2 /(2N · 2−16 ) ; if C ≤ τ then The guessed key bits are possibly right; for 28 k2R , 216 k3 , 28 k5L and 216 k6 do Use two plaintext-ciphertext pairs to check if they are right;  
   
  N ≈ 243.14 U with the same U1 and the threshold value τ ≈ 215.97 . To generate these N values of U , we traverse all possible values of P0 and P2 , randomly choose 211.14 values for P3 , and set P2 = F (P0 ⊕ wk0 ) after guessing wk0 . U  can be obtained similarly. All key bits can then be recovered following Algorithm 3. Suppose that one memory access to an array of size 232 or of size 259.14 costs less than one encryption of 17-round Piccolo-128. Then, the time complexity of this attack is T = 259.14 · 2 + 259.14 · 4 + 264 · 243.14 · 4 + 264 · 243.14 · 2 · (4/17 + 1) + 264 · 216 · 232 · 2 · (1/2) · (1/17) + 2 · 2128 α1 ≈ 2115.44 17-round encryptions. The data complexity is D = 2 · 216 N ≈ 260.14 chosen plaintext-ciphertext pairs. The dominant memory requirements are to store these plaintext-ciphertext pairs, about M = 4 · 259.14 · 64 = 267.14 bits are needed for these arrays. To show the advantage of this new method, we also try to mount an RKSS ˜ = 2·216 ·248 = attack based on the same distinguisher. However, we have to use D 65 2 chosen plaintext-ciphertext pairs in such an attack. In other words, the full  
   
  228  
   
  M. Li et al.  
   
  Algorithm 3: Key recovery attack procedure of 17-round Piccolo-128 with both pre- and post-whitening keys. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  
   
  Allocate and initialize four arrays VP [], VP [], VC [] and VC [] with size 259.14 ; Take 211.14 distinct random values of P3 and store them in a set S; Choose another 211.14 distinct random values of P3 and store them in a set S  ; a ← 0; for 216 P0 , 216 P1 , and 216 P2 do for 211.14 P3 in set S do Query the ciphertexts C for P under K; VP [a] = P , VC [a] = C, and increase a by one; a ← 0; for 216 P0 , 216 P1 , and 216 P2 do for 211.14 P3 in set S  do Query the ciphertexts C  for P  under K  ; VP [a] = P  , VC [a] = C  , and increase a by one; 16 for 2 k4 , 216 k7 , 216 k0 , and 216 k1 do wk0 = k0L ||k1R , wk2 = k4L ||k7R , and wk3 = k7L ||k4R ,wk0 = wk0 , wk2 = wk2 , wk3 = wk3 , k4 = k4 , k7 = k7 , k0 = k0 ⊕ 0x00β1 β2 , and k1 = k1 ; Allocate and initialize two arrays V1 [x1 ] and V1 [x1 ] with |x1 | = 32 = |x1 |; for 216 P0 , 216 P2 , and 211.14 P3 in set S do Compute P1 = F (P0 ⊕ wk0 ); // We have 243.14 U with the same U1 Access VP [] with P0 ||P1 ||P2 ||P3 and get the index a, then access VC [a] to get the corresponding ciphertexts C; Decrypt C to get Z0 [2, 3], Z1 [0, 1], Z2 [0, 1], Z3 [2, 3], Y0 [0, 1] and Y1 [2, 3]; Let x1 ← Z0 [2, 3]||(Z1 [0, 1] ⊕ Y0 [0, 1])||Z2 [0, 1]||(Z3 [2, 3] ⊕ Y1 [2, 3]) and V1 [x1 ] ← V1 [x1 ] + 1; for 216 P0 , 216 P2 , and 211.14 P3 in set S  do Compute P1 = F (P0 ⊕ wk0 );// We have 243.14 U  with U1 = U1 Access VP [] with P0 ||P1 ||P2 ||P3 and get the index a, then access VC [a] to get the corresponding ciphertexts C  ; Decrypt C  to get Z0 [2, 3], Z1 [0, 1], Z2 [0, 1], Z3 [2, 3], Y0 [0, 1], Y1 [2, 3]; Let x1 ← Z0 [2, 3]||(Z1 [0, 1] ⊕ Y0 [0, 1])||Z2 [0, 1]||(Z3 [2, 3] ⊕ Y1 [2, 3]) and V1 [x1 ] ← V1 [x1 ] + 1; for 28 k2L and 28 k5R do (k2 )L = k2L and (k5 )R = k5R ; Allocate and initialize two arrays V2 [x2 ] and V2 [x2 ] with |x2 | = 16 = |x2 |; for 232 x1 and x1 do Decrypt half-round for x1 and x1 to get W3 ⊕ k3 and W3 ⊕ k3 ; Let x2 ← W3 ⊕ k3 , and V2 [x2 ] ← V2 [x2 ] + V1 [x1 ]; Let x2 ← W3 ⊕ k3 , and V2 [x2 ] ← V2 [x2 ] + V1 [x1 ]; C = 0; for 216 x do   16  C ← C + 2x=0−1 (V2 [x] − V2 [x])2 /(2N · 2−16 ) ; if C ≤ τ then The guessed key bits are possibly right; for 28 k2R , 216 k3 , 28 k5L and 216 k6 do Use two plaintext-ciphertext pairs to check if they are right;  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  229  
   
  codebook is used, and the attack would not be valid. Therefore, the probabilistic RKSS method can make it possible to cover one more round than the original RKSS method.  
   
  Fig. 4. One round added before the distinguisher when attacking 17-round Piccolo-128, where • are active nibbles and × are nibbles that we need to know in the key recovery procedure.  
   
  5  
   
  Conclusion and Future Work  
   
  In this paper, we revisited the RKSS cryptanalysis technique and proposed a new method called probabilistic RKSS cryptanalysis, which requires a lower data complexity and has the potential of attacking more rounds than the original RKSS method. This new method was proposed by adopting an appropriate statistic that considers diﬀerent χ2 -distributions under right and wrong key guesses. The statistic is constructed as the squared Euclidean distance between the partial-value distributions of two ciphertext sets obtained from encrypting two independently chosen plaintext sets under related keys. The distributions of this statistic have been proved rigorously under several reasonable assumptions and conﬁrmed experimentally using a toy cipher. To show the eﬀectiveness of this new method, we have applied it to the reduced-round Piccolo. As a result, we obtained the best key recovery attacks containing both pre- and post-whitening keys on 10-round Piccolo-80 and 17round Piccolo-128. Note that we only use 10% of the number of plaintexts required for RKSS attacks on the 10-round Piccolo-80 and the success probability only decreases by 1%. Meanwhile, the data complexity needed in the new method on 16-round Piccolo-128 is only 3.44% of that required in the RKSS method. Moreover, we can cover one additional round on Piccolo-128 using the new method. To make a more clear comparison between the probabilistic RKSS method and the original RKSS method, some theoretical discussions, as well as key recovery attacks on reduced-round SKINNY-128-256 and full-round LiCi-2, are given in the full version due to space constraints.  
   
  230  
   
  M. Li et al.  
   
  The probabilistic RKSS method has shown its advantage compared to the original RKSS by new cryptanalysis results on Piccolo, SKINNY-128-256 and LiCi-2. The applications of this new method on other primitives are an interesting topic to explore in future work. Acknowledgments. The authors would like to thank the anonymous reviewers whose comments greatly improved this paper. Also thanks to Giovanni Uchoa de Assis for editorial improvements. This work was supported by Qingdao Innovation project (Grant No. QDBSH20230101008), Quan Cheng Laboratory (Grant No. QCLZD202306), the National Key Research and Development Program of China (Grant No. 2018YFA0704702), the Major Basic Research Project of Natural Science Foundation of Shandong Province, China (Grant No. ZR202010220025), the National Natural Science Foundation of China (Grant No. 62032014), and the Program of Qilu Young Scholars (Grant No. 61580082063088) of Shandong University.  
   
  References 1. Ahangarkolaei, M.Z., Najarkolaei, S.R.H., Ahmadi, S., Aref, M.R.: Zero correlation linear attack on reduced round Piccolo-80. In: ISCISC 2016, pp. 66–71. IEEE (2016). https://doi.org/10.1109/ISCISC.2016.7736453 2. Ashur, T., Dunkelman, O., Masalha, N.: Linear cryptanalysis reduced round of Piccolo-80. In: Dolev, S., Hendler, D., Lodha, S., Yung, M. (eds.) CSCML 2019. LNCS, vol. 11527, pp. 16–32. Springer, Cham (2019). https://doi.org/10.1007/9783-030-20951-3 2 3. Avanzi, R.: The QARMA block cipher family. Almost MDS matrices over rings with zero divisors, nearly symmetric Even-Mansour constructions with non-involutory central rounds, and search heuristics for low-latency S-boxes. IACR Trans. Symmetric Cryptol. 2017(1), 4–44 (2017). https://doi.org/10.13154/tosc.v2017.i1.444 4. Azimi, S.A., Ahmadian, Z., Mohajeri, J., Aref, M.R.: Impossible diﬀerential cryptanalysis of Piccolo lightweight block cipher. In: ISCISC 2014, pp. 89–94. IEEE (2014). https://doi.org/10.1109/ISCISC.2014.6994028 5. Barrett, C.W., Sebastiani, R., Seshia, S.A., Tinelli, C.: Satisﬁability modulo theories. In: Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.) Handbook of Satisﬁability, Frontiers in Artiﬁcial Intelligence and Applications, vol. 185, pp. 825–885. IOS Press (2009). https://doi.org/10.3233/978-1-58603-929-5-825 6. Bogdanov, A., Boura, C., Rijmen, V., Wang, M., Wen, L., Zhao, J.: Key diﬀerence invariant bias in block ciphers. In: Sako, K., Sarkar, P. (eds.) ASIACRYPT 2013. LNCS, vol. 8269, pp. 357–376. Springer, Heidelberg (2013). https://doi.org/10. 1007/978-3-642-42033-7 19 7. Collard, B., Standaert, F.-X.: A statistical saturation attack against the block cipher PRESENT. In: Fischlin, M. (ed.) CT-RSA 2009. LNCS, vol. 5473, pp. 195– 210. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-00862-7 13 8. Cook, S.A.: The complexity of theorem-proving procedures. In: Harrison, M.A., Banerji, R.B., Ullman, J.D. (eds.) Proceedings of the 3rd Annual ACM Symposium on Theory of Computing, Shaker Heights, Ohio, USA, 3–5 May 1971, pp. 151–158. ACM (1971). https://doi.org/10.1145/800157.805047 9. Daemen, J., Knudsen, L., Rijmen, V.: The block cipher square. In: Biham, E. (ed.) FSE 1997. LNCS, vol. 1267, pp. 149–165. Springer, Heidelberg (1997). https://doi. org/10.1007/BFb0052343  
   
  Probabilistic Related-Key Statistical Saturation Cryptanalysis  
   
  231  
   
  10. Daemen, J., Rijmen, V.: The Design of Rijndael: AES - The Advanced Encryption Standard. Information Security and Cryptography. Springer, Cham (2002). https://doi.org/10.1007/978-3-662-04722-4 11. Daemen, J., Rijmen, V.: Probability distributions of correlation and diﬀerentials in block ciphers. J. Math. Cryptol. 1(3), 221–242 (2007). https://doi.org/10.1515/ JMC.2007.011 12. DasGupta, A.: Asymptotic Theory of Statistics and Probability. Springer, New York (2008). https://doi.org/10.1007/978-0-387-75971-5 13. Dobraunig, C., Eichlseder, M., Mendel, F.: Square attack on 7-round Kiasu-BC. In: Manulis, M., Sadeghi, A.-R., Schneider, S. (eds.) ACNS 2016. LNCS, vol. 9696, pp. 500–517. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39555-5 27 14. Fouque, P.-A., Karpman, P.: Security ampliﬁcation against meet-in-the-middle attacks using whitening. In: Stam, M. (ed.) IMACC 2013. LNCS, vol. 8308, pp. 252–269. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-452390 15 15. Fu, L., Jin, C., Li, X.: Multidimensional zero-correlation linear cryptanalysis of lightweight block cipher Piccolo-128. Secur. Commun. Netw. 9(17), 4520–4535 (2016). https://doi.org/10.1002/sec.1644 16. Isobe, T., Shibutani, K.: Security analysis of the lightweight block ciphers XTEA, LED and Piccolo. In: Susilo, W., Mu, Y., Seberry, J. (eds.) ACISP 2012. LNCS, vol. 7372, pp. 71–86. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3642-31448-3 6 17. Jeong, K., Kang, H., Lee, C., Sung, J., Hong, S.: Biclique cryptanalysis of lightweight block ciphers PRESENT, Piccolo and LED. Cryptology ePrint Archive, Paper 2012/621 (2012). https://eprint.iacr.org/2012/621 18. Knudsen, L., Wagner, D.: Integral cryptanalysis. In: Daemen, J., Rijmen, V. (eds.) FSE 2002. LNCS, vol. 2365, pp. 112–127. Springer, Heidelberg (2002). https://doi. org/10.1007/3-540-45661-9 9 19. Li, M., Hu, K., Wang, M.: Related-tweak statistical saturation cryptanalysis and its application on QARMA. IACR Trans. Symmetric Cryptol. 2019(1), 236–263 (2019). https://doi.org/10.13154/tosc.v2019.i1.236-263 20. Li, M., Mouha, N., Sun, L., Wang, M.: Probabilistic related-key statistical saturation cryptanalysis. IACR Cryptology ePrint Archive, p. 1245 (2023). https:// eprint.iacr.org/2023/1245 21. Liu, Y., Cheng, L., Liu, Z., Li, W., Wang, Q., Gu, D.: Improved meet-in-the-middle attacks on reduced-round Piccolo. Sci. China Inf. Sci. 61(3), 032108:1–032108:13 (2018). https://doi.org/10.1007/s11432-016-9157-y 22. Liu, Y., et al.: New analysis of reduced-version of Piccolo in the single-key scenario. KSII Trans. Internet Inf. Syst. 13(9), 4727–4741 (2019). https://doi.org/10.3837/ tiis.2019.09.022 23. Matsui, M.: Linear cryptanalysis method for DES cipher. In: Helleseth, T. (ed.) EUROCRYPT 1993. LNCS, vol. 765, pp. 386–397. Springer, Heidelberg (1994). https://doi.org/10.1007/3-540-48285-7 33 24. Maxwell, A.E.: Comparing the classiﬁcation of subjects by two independent judges. Br. J. Psychiatry 116, 651–655 (1970). https://doi.org/10.1192/bjp.116.535.651 25. Minier, M.: On the security of Piccolo lightweight block cipher against relatedkey impossible diﬀerentials. In: Paul, G., Vaudenay, S. (eds.) INDOCRYPT 2013. LNCS, vol. 8250, pp. 308–318. Springer, Cham (2013). https://doi.org/10.1007/ 978-3-319-03515-4 21  
   
  232  
   
  M. Li et al.  
   
  26. Mouha, N., Preneel, B.: Towards ﬁnding optimal diﬀerential characteristics for ARX: application to Salsa20. Cryptology ePrint Archive, Paper 2013/328 (2013). https://eprint.iacr.org/2013/328 27. Nyberg, K.: Linear approximation of block ciphers. In: De Santis, A. (ed.) EUROCRYPT 1994. LNCS, vol. 950, pp. 439–444. Springer, Heidelberg (1995). https:// doi.org/10.1007/BFb0053460 28. Phan, R.C.: Mini advanced encryption standard (Mini-AES): a testbed for cryptanalysis students. Cryptologia 26(4), 283–306 (2002). https://doi.org/10.1080/ 0161-110291890948 29. Shibutani, K., Isobe, T., Hiwatari, H., Mitsuda, A., Akishita, T., Shirai, T.: Piccolo: an ultra-lightweight blockcipher. In: Preneel, B., Takagi, T. (eds.) CHES 2011. LNCS, vol. 6917, pp. 342–357. Springer, Heidelberg (2011). https://doi.org/10. 1007/978-3-642-23951-9 23 30. Stuart, A.: A test for homogeneity of the marginal distribution of a two-way classiﬁcation. Biometrika 42, 412–416 (1955). https://doi.org/10.1093/biomet/42.3-4. 412 31. Todo, Y.: Impossible diﬀerential attack against 14-round Piccolo-80 without relying on full code book. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 99A(1), 154–157 (2016). https://doi.org/10.1587/transfun.E99.A.154 32. Tolba, M., Abdelkhalek, A., Youssef, A.M.: Meet-in-the-middle attacks on reduced round Piccolo. In: G¨ uneysu, T., Leander, G., Moradi, A. (eds.) LightSec 2015. LNCS, vol. 9542, pp. 3–20. Springer, Cham (2016). https://doi.org/10.1007/9783-319-29078-2 1 33. Wang, M., Cui, T., Chen, H., Sun, L., Wen, L., Bogdanov, A.: Integrals go statistical: cryptanalysis of full skipjack variants. In: Peyrin, T. (ed.) FSE 2016. LNCS, vol. 9783, pp. 399–415. Springer, Heidelberg (2016). https://doi.org/10.1007/9783-662-52993-5 20 34. Wang, Y., Wu, W., Yu, X.: Biclique cryptanalysis of reduced-round piccolo block cipher. In: Ryan, M.D., Smyth, B., Wang, G. (eds.) ISPEC 2012. LNCS, vol. 7232, pp. 337–352. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3642-29101-2 23  
   
  Compactly Committing Authenticated Encryption Using Encryptment and Tweakable Block Cipher Shoichi Hirose1(B)  
   
  and Kazuhiko Minematsu2,3  
   
  1  
   
  3  
   
  University of Fukui, Fukui, Japan hrs [email protected]  2 NEC, Kawasaki, Japan [email protected]  Yokohama National University, Yokohama, Japan  
   
  Abstract. Message franking is a feature of end-to-end encrypted messaging introduced by Facebook that enables users to report abusive contents in a veriﬁable manner. Grubbs et al. (CRYPTO 2017) formalized a symmetric-key primitive usable for message franking, called compactly committing authenticated encryption with associated data (ccAEAD), and presented schemes with provable security. Dodis et al. (CRYPTO 2018) proposed a core building block for ccAEAD, called encryptment, and presented a generic construction of ccAEAD combining encryptment and conventional AEAD. We show that ccAEAD can be built on encryptment and a tweakable block cipher (TBC), leading to simpler and more eﬃcient constructions of ccAEAD than Dodis et al.’s methods. Our construction, called EnCryptment-then-TBC (ECT), is secure under a new but feasible assumption on the ciphertext integrity of encryptment. We also formalize the notion of remotely keyed ccAEAD (RK ccAEAD) and show that our ECT works as RK ccAEAD. RK ccAEAD was ﬁrst considered by Dodis et al. as a useful variant of ccAEAD when it is implemented on a platform consisting of a trusted module and an untrusted (leaking) module. However, its feasibility was left open. Our work is the ﬁrst to show its feasibility with a concrete scheme. Keywords: Authenticated encryption · Commitment block cipher · Remotely keyed encryption  
   
  1  
   
  · Tweakable  
   
  Introduction  
   
  Background. End-to-end encrypted messaging systems are now widely deployed, such as Facebook Messenger [15], Signal [33], and Whatsapp Messenger [35]. Accordingly, new security issues arise in addition to those on the privacy and authenticity of messages. A signiﬁcant problem is preventing malicious senders from sending harassing messages and harmful/abusive contents. To achieve this goal, Facebook introduced message franking [16]. It is a cryptographic protocol allowing users to report the receipt of abusive messages in a veriﬁable manner to Facebook. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 233–252, 2024. https://doi.org/10.1007/978-3-031-53368-6_12  
   
  234  
   
  S. Hirose and K. Minematsu  
   
  At Crypto 2017, Grubbs et al. [18] initiated the formal study of message franking and presented a new variant of AEAD, called compactly committing AEAD (ccAEAD), as a symmetric-key primitive that is useful for message franking. For ccAEAD, a small part of the ciphertext works as a commitment to the message and the associated data. They also presented two generic constructions of ccAEAD. One is called CtE (Commit-then-Encrypt), which consists of commitment and AEAD. The other is called CEP (Committing Encrypt-and-PRF). It consists of a pseudorandom generator, a pseudorandom function (PRF), and a collision-resistant PRF. At Crypto 2018, Dodis et al. [13] further studied ccAEAD and proposed a new primitive, called encryptment, as a core component of ccAEAD. They show that, given encryptment, ccAEAD can be built from an additional common cryptographic primitive. Concretely, they presented two transformations. One transformation needs a call to (randomized) AEAD in addition to encryptment, and the other needs two calls to a related-key-secure PRF. The former is a randomized scheme, and the latter is nonce-based. In addition, they considered remotely keyed ccAEAD (RK ccAEAD) in the full version [14] of [13]. Here, RK ccAEAD is an extension of ccAEAD inheriting the property of remotely keyed encryption, which was proposed by Blaze [7] and extensively studied in the late 90’s [8,23,29,30]. Its goal was to enable secure encryption under a setting where one could use a resource-limited personal device storing secret keys and computing cryptographic functions. The problem was how to do bulk encryption and decryption by utilizing the power of a host and the security of the personal device. Dodis et al. [14] suggested RK ccAEAD as a useful variant of ccAEAD under such environments. However, they left open its feasibility and concrete constructions. Our Contributions. Focusing on the work of Dodis et al. [13,14], this paper makes two contributions. First, we present a new construction of ccAEAD based on encryptment, dubbed ECT (EnCryptment-then-TBC). While Dodis et al. [13,14] used AEAD as an additional component, we show that an additional single call of a tweakable block cipher (TBC) [27,28] is suﬃcient. Since the integrity mechanism provided by AEAD is not needed, our proposal allows simpler and more eﬃcient ccAEAD compared with Dodis et al.’s proposals. In more detail, when encryption, Dodis et al.’s method needs AEAD taking two elements, B (for associated data) and L (for plaintext) as an input in addition to a secret key, while ECT simply encrypt L with tweak B by a TBC. The latter is arguably much simpler. See Fig. 1 for their illustrations. Note that the encryption output C1 of AEAD in Dodis et al.’s method contains a random nonce and a tag in addition to the “raw” ciphertext of |L| bits as otherwise decryption is not possible. Hence, ECT is more eﬃcient in terms of bandwidth1 . The security requirements of ECT are reduced to those of the underlying encryptment and a TBC. In particular, the ciphertext integrity of ECT requires a new but feasible 1  
   
  The second method of Dodis et al. has also larger bandwidth than ours for the existence of tag. A concrete comparison is not possible as it is nonce-based.  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  235  
   
  type of ciphertext unforgeability for the encryptment. Actually, we show that HFC [13] – a hash-based eﬃcient encryptment scheme proposed by Dodis et al. – satisﬁes this new ciphertext unforgeability in the random oracle model. We note that HFC originally assumed the random oracle, so we do not introduce any new assumption. Second, we provide the ﬁrst formalization of remotely keyed (RK) ccAEAD, and show that ECT is secure RK ccAEAD. This answers the aforementioned open question posed by Dodis et al. positively. Our formalization is based on that of RK authenticated encryption by Dodis and An [12]. The conﬁdentiality of ECT as RK ccAEAD requires a new variant of conﬁdentiality for encryptment. It is also shown that HFC satisﬁes the new variant of conﬁdentiality in the random oracle model. ECT has a similar structure to the AEAD scheme named CONCRETE [6], which oﬀers ciphertext integrity in the presence of nonce misuse and leakage. As mentioned above, remotely keyed encryption [7] is practically relevant when composing a trusted (small) module with an untrusted/leaking module. We think this similarity exhibits an interesting relationship with RK ccAEAD and leakage-resilient AEAD, where the latter has been actively studied in recent years, e.g., [4,5,11,31,32]. Related Work. Authenticated encryption is one of the central topics in symmetric cryptography. Its formal treatments were initiated by Katz and Yung [24] and by Bellare and Namprempre [3]. A variation of message franking scheme that enables a receiver to report an abusive message by revealing only the abusive parts was investigated independently by Leontiadis and Vaudenay [26] and by Chen and Tang [10]. HugueninDumittan and Leontiadis formalized and instantiated a secure bidirectional channel with message franking [21]. Yamamuro et al. [36] proposed forward secure message franking and presented a scheme based on ccAEAD, a forward secure pseudorandom generator, and a forward secure MAC. Tyagi et al. [34] formalized asymmetric message franking and constructed a scheme from signatures of knowledge [20] for designated veriﬁer signatures [22]. Hirose [19] proposed a generic construction of nonce-based ccAEAD. The proposal is similar to the second method of Dodis et al. Since it simply replaces a PRF with a TBC, it needs two additional TBC calls, while ECT needs only one TBC call. In addition, his scheme is nonce-based while ours is randomized as originally proposed. Thus, ECT is less restrictive and more eﬃcient in computation. Dodis and An [12] proposed and investigated a cryptographic primitive called concealment. They formalized RK authenticated encryption as an application and provided a generic construction with concealment and authenticated encryption. Farshim et al. [17], Albertini et al. [1], Len et al. [25], Bellare and Hoang [2], and Chan and Rogaway [9] discussed so-called committing authenticated encryption. While their deﬁnitions and security goals are not identical, their primary goal was basically to decrease the risk of error or misuse by application design-  
   
  236  
   
  S. Hirose and K. Minematsu  
   
  Fig. 1. Encryption and decryption algorithms of ccAEAD. (Top) our proposal, ECT, (Bottom) Dodis et al.’s method using AEAD [13]. For both cases, enc and dec are encryptment and decryptment algorithms of the encryptment scheme. The ccAEAD decryption algorithms omit the case of veriﬁcation failures. In ECT, EK and DK denote the TBC’s encryption and decryption, where the thick line is tweak input.  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  237  
   
  ers, and message franking was out of scope for the lack of opening key needed by ccAEAD. Organization. Section 2 introduces notations and formalizes tweakable block ciphers, ccAEAD, and encryptment. Section 3 describes the generic construction of ccAEAD, called ECT, and conﬁrms its security. Section 4 formalizes RK ccAEAD. Section 5 conﬁrms the security of ECT as RK ccAEAD. Section 6 concludes the paper.  
   
  2  
   
  Preliminaries  
   
  Let Σ := {0, 1}. For l ≥ 0, let Σ l be the set of all Σ-sequences of  any integer ∗ i length l. Let Σ := i≥0 Σ . The length of x ∈ Σ ∗ is denoted by |x|. Concatenation of x1 , x2 ∈ Σ ∗ is denoted by x1 x2 . A uniform random choice of an element s from a set S is denoted by s ← ← S. 2.1  
   
  Tweakable Block Cipher  
   
  A TBC is formalized as a pair of encryption and decryption functions TBC := (E, D) such that E : Σ nk × Σ nt × Σ nb → Σ nb and D : Σ nk × Σ nt × Σ nb → Σ nb . Σ nk is a set of keys, Σ nt is a set of tweaks, and Σ nb is a set of plaintexts or ciphertexts. For every (K, T ) ∈ Σ nk × Σ nt , both E(K, T, ·) and D(K, T, ·) are permutations, and D(K, T, E(K, T, ·)) is the identity permutation over Σ nb . Let Pnt ,nb be the set of all tweakable permutations: For every p ∈ Pnt ,nb and T ∈ Σ nt , p(T, ·) is a permutation over Σ nb . Let p−1 ∈ Pnt ,nb be the inverse of p ∈ Pnt ,nb : p−1 (T, p(T, ·)) is the identity permutation for every T ∈ Σ nt . The security requirement of a TBC is formalized as indistinguishability from a uniform random tweakable permutation. Let A be an adversary with oracle access to a tweakable permutation (and its inverse) in Pnt ,nb . A can make adaptive queries to the oracle(s) and ﬁnally outputs 0 or 1. The advantage of A against TBC for a tweakable pseudorandom permutation (TPRP) is   EK  Advtprp = 1] − Pr[A = 1], TBC (A) := Pr[A where K ← ← Σ nk and  ← ← Pnt ,nb . Similarly, the advantage of A against TBC for a strong tweakable pseudorandom permutation (STPRP) is   −1 EK ,DK  = 1] − Pr[A, = 1]. Advstprp TBC (A) := Pr[A 2.2  
   
  ccAEAD  
   
  Syntax. ccAEAD [18] is formalized as a tuple of algorithms CAE := (Kg, Enc, Dec, Ver). It is involved with a key space K := Σ n , an associated-data space A ⊆ Σ ∗ , a message space M ⊆ Σ ∗ , a ciphertext space C ⊆ Σ ∗ , an openingkey space L ⊆ Σ  , and a binding-tag space T := Σ τ . The “cc” (compactly committing) property requires that τ = O(n) is small.  
   
  238  
   
  S. Hirose and K. Minematsu  
   
  – The key-generation algorithm Kg returns a secret key K ∈ K chosen uniformly at random. – The encryption algorithm Enc takes as input (K, A, M ) ∈ K × A × M and returns (C, B) ∈ C × T . – The decryption algorithm Dec takes as input (K, A, C, B) ∈ K × A × C × T and returns (M, L) ∈ M × L or ⊥ ∈ M × L. – The veriﬁcation algorithm Ver takes as input (A, M, L, B) ∈ A × M × L × T and returns b ∈ Σ. Kg and Enc are randomized algorithms, and Dec and Ver are deterministic algorithms. For every l ∈ N, Σ l ⊆ M or Σ l ∩ M = ∅. For (C, B) ← Enc(K, A, M ), |C| depends only on |M |, and there exists a function clen : N → N such that |C| = clen(|M |). CAE satisﬁes correctness. Namely, for any (K, A, M ) ∈ K×A×M, if (C, B) ← Enc(K, A, M ), then there exists some L ∈ L such that Dec(K, A, C, B) = (M, L) and Ver(A, M, L, B) = 1. Security Requirements. The security requirements of ccAEAD are conﬁdentiality, ciphertext integrity, and binding properties. Confidentiality. The games MO-REAL and MO-RAND shown in Fig. 2 are introduced to formalize the conﬁdentiality as real-or-random indistinguishability in the multi-opening setting. The advantage of an adversary A for conﬁdentiality is   A A   Advmo-ror CAE (A) := Pr[MO-REALCAE = 1] − Pr[MO-RANDCAE = 1] . A is allowed to make queries adaptively to the oracles Enc, Dec, and ChalEnc. In both of the games, Enc and Dec work in the same ways. For each query (A, C, B), Dec returns (M, L) ← Dec(K, A, C, B) only if the query is a previous reply from Enc. Ciphertext Integrity. The game MO-CTXT shown in Fig. 3 is introduced to formalize the ciphertext integrity as unforgeability in the multi-opening setting. The advantage of an adversary A for ciphertext integrity is (A) := Pr[MO-CTXTA Advmo-ctxt CAE = true]. CAE A is allowed to make queries adaptively to the oracles Enc, Dec, and ChalDec. The game outputs true if A asks a query (A, C, B) to ChalDec such that Dec(K, A, C, B) = ⊥ without obtaining it from Enc by a previous query. Binding Properties. Binding properties are deﬁned for a sender and a receiver. Receiver binding describes that a malicious receiver cannot report a non-abusive sender for sending an abusive message. The advantage of an adversary A for receiver binding is      Advr-bind CAE (A) := Pr[((A, M, L), (A , M , L ), B) ← A : (A, M ) = (A , M )  
   
  ∧ Ver(A, M, L, B) = Ver(A , M  , L , B) = 1].  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  239  
   
  Fig. 2. Games for conﬁdentiality of ccAEAD  
   
  Fig. 3. Game MO-CTXTA CAE for ciphertext integrity of ccAEAD  
   
  The advantage of A for strong receiver binding is Advsr-bind (A) := Pr[((A, M, L), (A , M  , L ), B) ← A : (A, M, L) = (A , M  , L ) CAE ∧ Ver(A, M, L, B) = Ver(A , M  , L , B) = 1]. sr-bind (A) for any CAE and A. It holds that Advr-bind CAE (A) ≤ AdvCAE Sender binding describes that a malicious sender of an abusive message cannot prevent the receiver from reporting it. The advantage of A for sender binding is  
   
  Advs-bind CAE (A) := Pr[(K, A, C, B) ← A : Dec(K, A, C, B) = ⊥ (M, L) ← Dec(K, A, C, B) ∧ Ver(A, M, L, B) = 0]. Message Franking Using ccAEAD. A service provider is assumed to relay all communication among users. Users encrypt their communication with ccAEAD. For a ciphertext from a sender, the service provider computes a tag with a MAC function for the binding tag in the ciphertext and transfers the ciphertext to the receiver together with the tag. Suppose that an abusive message is recovered  
   
  240  
   
  S. Hirose and K. Minematsu  
   
  from the ciphertext. Then, the receiver reports it to the service provider with the opening key, binding tag, and the tag attached by the service provider. The receiver binding prevents malicious receivers from blaming non-abusive senders. The sender binding prevents malicious senders from denying abusive reports by honest receivers. 2.3  
   
  Encryptment  
   
  Syntax. Encryptment [13] is roughly one-time ccAEAD. It is formalized as a tuple of algorithms EC = (kg, enc, dec, ver). It is involved with a key space Kec := Σ  , an associated-data space A ⊆ Σ ∗ , a message space M ⊆ Σ ∗ , a ciphertext space C ⊆ Σ ∗ , and a binding-tag space T := Σ τ . – The key-generation algorithm kg returns a secret key Kec ∈ Kec chosen uniformly at random. – The encryptment algorithm enc takes as input (Kec , A, M ) ∈ Kec × A × M and returns (C, B) ∈ C × T . – The decryptment algorithm dec takes as input (Kec , A, C, B) ∈ Kec ×A×C×T and returns M ∈ M or ⊥ ∈ M. – The veriﬁcation algorithm ver takes as input (A, M, Kec , B) ∈ A×M×Kec ×T and returns b ∈ Σ. kg is a randomized algorithm, and enc, dec and ver are deterministic algorithms. For (C, B) ← enc(Kec , A, M ), it is assumed that |C| depends only on |M |. EC satisﬁes correctness: For any (Kec , A, M ) ∈ Kec × A × M, if (C, B) ← enc(Kec , A, M ), then dec(Kec , A, C, B) = M and ver(A, M, Kec , B) = 1. A stronger notion of correctness called strong correctness is also introduced: For any (Kec , A, C, B) ∈ Kec ×A×C ×T , if M ← dec(Kec , A, C, B), then enc(Kec , A, M ) = (C, B). Security Requirements. The security requirements of encryptment are conﬁdentiality, second-ciphertext unforgeability, and binding properties. Confidentiality. Two games otREAL and otRAND shown in Fig. 4 are introduced to formalize the conﬁdentiality. In both of the games, an adversary A asks only a single query to the oracle enc. The advantage of A for conﬁdentiality is   A  (A) := Pr[otREALA Advot-ror EC = 1] − Pr[otRANDEC = 1] , EC where “ot-ror” stands for “one-time real-or-random.” Second-Ciphertext Unforgeability. An adversary A asks only a single query (A, M ) ∈ A × M to encKec and gets (C, B) and Kec , where Kec ← kg and (C, B) ← encKec (A, M ). Then, A outputs (A , C  ) ∈ A × C. The advantage of A for second-ciphertext unforgeability is     Advscu EC (A) := Pr[(A, C) = (A , C ) ∧ decKec (A , C , B) = ⊥].  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  241  
   
  Fig. 4. Games for conﬁdentiality of encryptment  
   
  Binding Properties. The advantage of A for receiver binding is  Advr-bind (A) := Pr[((Kec , A, M ), (Kec , A , M  ), B) ← A : (A, M ) = (A , M  ) EC  ∧ ver(A, M, Kec , B) = ver(A , M  , Kec , B) = 1].  
   
  The advantage of A for strong receiver binding is  Advsr-bind (A) := Pr[((Kec , A, M ), (Kec , A , M  ), B) ← A : EC   (Kec , A, M ) = (Kec , A , M  ) ∧ ver(A, M, Kec , B) = ver(A , M  , Kec , B) = 1].  
   
  The advantage of an adversary A for sender binding is Advs-bind (A) := Pr[(Kec , A, C, B) ← A, M ← dec(Kec , A, C, B) : EC M = ⊥ ∧ ver(A, M, Kec , B) = 0]. For strongly correct encryptment, Dodis et al. [13] reduced second-ciphertext unforgeability to sender binding and receiver binding. The following proposition shows that it can be reduced only to receiver binding. On the other hand, receiver binding cannot be reduced to second-ciphertext unforgeability. Suppose that EC is secure except that it has a weak key such that receiver binding is broken using the weak key. For second-ciphertext unforgeability, the probability that the weak key is chosen is negligible for a query made by an adversary. Proposition 1. Let EC be a strongly correct encryptment scheme. Then, for any adversary A against EC for second-ciphertext unforgeability, there exists an ˙ and the run time of A ˙ is at ˙ such that Advscu (A) ≤ Advr-bind (A) adversary A EC EC most about that of A. The proof is omitted due to the page limit.  
   
  3 3.1  
   
  ccAEAD Using Encryptment and TBC Scheme  
   
  New ccAEAD construction ECT (EnCryptment-then-TBC) ECT = (KG, ENC, DEC, VER) is proposed. It uses an encryptment scheme EC = (kg, enc, dec, ver) and a TBC TBC = (E, D). For ECT, let K := Σ n be its key space, A be its  
   
  242  
   
  S. Hirose and K. Minematsu  
   
  associated-data space, M be its message space, C be its ciphertext space, L := Σ  be its opening-key space, and T := Σ τ be its binding-tag space. Then, for EC, L is its key space, A is its associated-data space, M is its message space, C is its ciphertext space, and T is its binding-tag space. For TBC, its set of keys is K, its set of tweaks is T , and its set of plaintexts or ciphertexts is L. ENC and DEC are shown in Fig. 5. Also refer to Fig. 1 for illustration. They are also depicted in Fig. 1. KG selects a secret key K for TBC from Σ n . VER simply runs ver.  
   
  Fig. 5. The encryption and decryption algorithms of ECT  
   
  3.2  
   
  Security  
   
  ECT replaces AEAD of the Dodis et al. scheme with TBC. This change does not impact the conﬁdentiality or binding properties. However, it does aﬀect the ciphertext integrity. With ECT, a candidate for the opening key can always be obtained for a ciphertext. Thus, to ensure the ciphertext integrity, it must be intractable to create a new valid ciphertext for the binding tag of the original ciphertext and the opening key candidate. Confidentiality. The conﬁdentiality of ECT is reduced to the conﬁdentiality of EC and the TPRP property of TBC. The proof is omitted due to the page limit. Theorem 1 (Confidentiality). Let A be an adversary against ECT making at most qe , qd , and qc queries to Enc, Dec, and ChalEnc, respectively. Then, ˙ and D such that there exist adversaries A ot-ror ˙ 2 2  Advmo-ror (A) + 2 · Advtprp ECT (A) ≤ qc · AdvEC TBC (D) + (qe + (qe + qc ) )/2 .  
   
  ˙ and D is at most about that of MO-REALA . D makes at The run time of A ECT most (qe + qc ) queries to its oracle. Ciphertext Integrity. For the ciphertext integrity of ECT, a new notion is introduced to the ciphertext unforgeability of encryptment EC:  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  243  
   
  Definition 1 (Targeted Ciphertext Unforgeability). Let A := (A1 , A2 ) be an adversary acting in two phases. First, A1 takes no input and outputs (B, state), where B ∈ T and state is some state information. Then, A2 takes (B, state) and Kec as input, where Kec ← kg, and outputs (A, C) ∈ A × C. The advantage of A for targeted ciphertext unforgeability is Advtcu EC (A) := Pr[dec(Kec , A, C, B) = ⊥]. It is not diﬃcult to see that the HFC encryptment scheme [13] satisﬁes targeted ciphertext unforgeability in the random oracle model. The ciphertext integrity of ECT is reduced to the second-ciphertext unforgeability and the targeted ciphertext unforgeability of EC and the STPRP property of TBC: Theorem 2 (Ciphertext Integrity). Let A be an adversary against ECT making at most qe , qd , and qc queries to Enc, Dec, and ChalDec, respectively. ˙ A, ¨ and D such that Then, there exist adversaries A, stprp tcu ¨ ˙ Advmo-ctxt (A) ≤ qe · Advscu ECT EC (A) + (qd + qc ) · AdvEC (A) + AdvTBC (D)  
   
  + (qe + qd + qc )2 /2+1 . ˙ A, ¨ and D is at most about that of MO-CTXTA . D The run time of A, ECT makes at most qe + qd + qc queries to its oracle. Proof. The game MO-CTXTA ECT is shown in Fig. 6. Without loss of generality, it is assumed that A terminates right after win gets true. A The game MO-CTXT-GA 1 in Fig. 7 is diﬀerent from MO-CTXTECT in that the former records all the histories of EK and DK by “P[B, C1 ] ← L” and uses them to answer to queries to Dec and ChalDec. Thus, A Advmo-ctxt (A) = Pr[MO-CTXTA ECT = true] = Pr[MO-CTXTG1 = true]. ECT A The game MO-CTXT-GA 2 in Fig. 8 is diﬀerent from MO-CTXT-G1 in that the former uses a random tweakable permutation  instead of TBC. Let D be an adversary against TBC. D has either (EK , DK ) or (, −1 ) as an oracle and A simulates MO-CTXT-GA 1 or MO-CTXT-G2 with the use of its oracle. Thus,   A A   Advstprp TBC (D) = Pr[MO-CTXT-G1 = true] − Pr[MO-CTXT-G2 = true] .  
   
  D makes at most qe + qd + qc queries to its oracle, and its run time is at most about that of MO-CTXTA ECT . In the game MO-CTXT-GA 3 shown in Fig. 8, Dec and ChalDec select L uniformly at random from Σ  , while they call −1 in MO-CTXT-GA 2 . As long as no collision is found for L, the games are equivalent to each other. Thus,   A 2 +1  Pr[MO-CTXT-GA . 2 = true]−Pr[MO-CTXT-G3 = true] ≤ (qe +qd +qc ) /2 Now, Pr[MO-CTXT-GA 3 = true] is evaluated. Suppose that win is set true by a query (A∗ , C ∗ , B ∗ ) to ChalDec. Let Win1 , Win2 , and Win3 be the cases that  
   
  244  
   
  S. Hirose and K. Minematsu  
   
  1. P[B ∗ , C1∗ ] = ⊥ and P[B ∗ , C1∗ ] is already set by Enc, 2. P[B ∗ , C1∗ ] = ⊥ and P[B ∗ , C1∗ ] is already set by Dec or ChalDec, and 3. P[B ∗ , C1∗ ] = ⊥, respectively, where C1∗ is the least signiﬁcant  bits of C ∗ . Then, Pr[MO-CTXT-GA 3 = true] = Pr[Win1 ] + Pr[Win2 ] + Pr[Win3 ]. ˙ B∗) For Win1 , suppose that Enc sets P[B ∗ , C1∗ ] while computing a reply (C, ∗ ∗ ∗ ˙ C, ˙ B ) ∈ Y and (A∗ , C ∗ , ˙ M˙ ). Then, (A, ˙ C) ˙ = (A , C ) since (A, to a query (A, ∗ ˙ with the oracle enc ˙ against secondB ) ∈ Y. Thus, the following adversary A L ˙ ˙ ciphertext unforgeability is successful. A runs MO-CTXT-GA 3 except that A ∗ ˙ ˙ ˙ ˙ ˙ guesses (A, M ), asks it to encL˙ and gets (C, B ) and L. Finally, A outputs ˙ = Pr[Win1 ]/qe . ˙ A∗ , C ∗ , B ∗ ) = ⊥. Thus, Advscu (A) (A∗ , C ∗ ) satisfying dec(L, EC ¨ ¨ 1, A ¨ 2 ) against tarFor Win2 and Win3 , the following adversary A = (A ¨ geted ciphertext unforgeability is successful. First, A1 runs MO-CTXT-GA 3 and guesses (B ∗ , C1∗ ). It interrupts the execution of MO-CTXT-GA 3 right after it ¨ 2 takes (B ∗ , state ∗ ) and obtains (B ∗ , C1∗ ) and outputs (B ∗ , state ∗ ). Then, A  ¨ L ← ← Σ as input and resumes the execution of MO-CTXT-GA 3 by making ¨ 2 outputs (A∗ , C ∗ ) satisfying dec(L, ¨ A∗ , C ∗ , B ∗ ) = ⊥. use of state ∗ . Finally, A 0 0 ¨   Thus, Advtcu EC (A) = (Pr[Win2 ] + Pr[Win3 ])/(qd + qc ).  
   
  Fig. 6. Game MO-CTXTA ECT  
   
  Binding Properties. ECT inherits (strong) receiver binding from EC. ECT also inherits sender binding from EC. Suppose that (K, A, C, B) satisﬁes DEC(K, A, C, B) = ⊥ and VER(A, M, L, B) = 0, where (M, L) ← DEC(K, A, C, B). Then, L = DK (B, C1 ), dec(L, A, C0 , B) = M and M = ⊥, where C = C0 C1 . In addition, ver(A, M, L, B) = 0.  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  245  
   
  Fig. 7. MO-CTXT-GA 1 . All the entries of the table P are initialized by ⊥.  
   
  4  
   
  Remotely Keyed ccAEAD  
   
  RK ccAEAD is a particular type of ccAEAD. Their diﬀerence is that, for RK ccAEAD, some parts of encryption and decryption are done by a trusted device keeping the secret key. A user or a host performs encryption and/or decryption by making use of the trusted device. The amount of computation for the trusted device is required to be independent of the lengths of a message, associated data, and a ciphertext due to the common case that the computational power of the trusted device is limited. Dodis et al. [14] left it as an open problem to formalize and construct RK ccAEAD schemes. An answer will be given to the problem in this section. 4.1  
   
  Syntax  
   
  RK ccAEAD is formalized as a tuple of algorithms RKCAE = (RKKg, RKEnc, RKDec, RKVer). It is involved with a key space K := Σ n , an associated-data space A ⊆ Σ ∗ , a message space M ⊆ Σ ∗ , a ciphertext space C ⊆ Σ ∗ , an opening-key space L := Σ  , and a binding-tag space T := Σ τ . In the formalization below, for simplicity, it is assumed that the trusted device is called only once during encryption and decryption: – The key generation algorithm RKKg returns a secret key K ∈ K chosen uniformly at random.  
   
  246  
   
  S. Hirose and K. Minematsu  
   
  A Fig. 8. MO-CTXT-GA 2 and MO-CTXT-G3  
   
  – The encryption algorithm RKEnc takes as input (K, A, M ) ∈ K × A × M and returns (C, B) ∈ C × T . K is given to an algorithm TE, and it is run by a trusted device. The encryption proceeds in the following three steps: (Qe , Se ) ← Pre-TE(A, M ); Re ← TEK (Qe ); (C, B) ← Post-TE(Re , Se ), where Se is some state information. – The decryption algorithm RKDec takes as input (K, A, C, B) ∈ K × A × C × T and returns (M, L) ∈ M × L or ⊥ ∈ M × L. K is given to an algorithm TD, and it is run by a trusted device. The decryption proceeds in the following three steps: (Qd , Sd ) ← Pre-TD(A, C, B); Rd ← TDK (Qd ); (M, L)/⊥ ← Post-TD(Rd , Sd ), where Sd is some state information. – The veriﬁcation algorithm RKVer takes as input (A, M, L, B) ∈ A×M×L×T and returns b ∈ Σ. As well as CAE, RKCAE satisﬁes correctness. For every l ∈ N, Σ l ⊆ M or Σ l ∩ M = ∅. For any message M and the corresponding ciphertext C, |C| depends only on |M | and let |C| = clen(|M |). 4.2  
   
  Security Requirement  
   
  For RK ccAEAD, an adversary is allowed to have direct access to the trusted device. Thus, the adversary can run RKEnc and RKDec by using TEK and TDK as oracles, respectively.  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  247  
   
  Confidentiality. Conﬁdentiality of RK ccAEAD is deﬁned as real-or-random indistinguishability. The games RK-REAL and RK-RAND shown in Fig. 9 are introduced. An adversary A is given access to oracles E, D, and ChalEnc. A is not allowed to decrypt (A, C, B) obtained by asking (A, M ) to ChalEnc. The advantage of A for conﬁdentiality is   A A   Advrk-ror RKCAE (A) := Pr[RK-REALRKCAE = 1] − Pr[RK-RANDRKCAE = 1] .  
   
  Fig. 9. Games for conﬁdentiality of RK ccAEAD  
   
  Ciphertext Integrity. The game RK-CTXTA RKCAE , shown in Fig. 10, is introduced. An adversary A is given access to oracles E, D, and ChalDec. A is not allowed to repeat the same queries to ChalDec. The game outputs true if the number of valid ciphertexts produced by A is greater than the number of queries to E made by A. The advantage of A for ciphertext integrity is A Advrk-ctxt RKCAE (A) := Pr[RK-CTXTRKCAE = true].  
   
  sr-bind s-bind Binding Properties. Advr-bind RKCAE , AdvRKCAE , and AdvRKCAE are deﬁned as r-bind sr-bind s-bind AdvCAE , AdvCAE , and AdvCAE , respectively, simply by replacing Dec with RKDec and Ver with RKVer.  
   
  248  
   
  S. Hirose and K. Minematsu  
   
  Fig. 10. Game RK-CTXTA RKCAE for ciphertext integrity of RK ccAEAD  
   
  ECT as RK ccAEAD  
   
  5 5.1  
   
  Scheme  
   
  ECT functions as RK ccAEAD if E and D of TBC are used for TE and TD, respectively. For simplicity, ECT as RK ccAEAD is called RK ECT in the remaining parts. 5.2  
   
  Security  
   
  Confidentiality. The crucial diﬀerence between RK ECT and ordinary ECT is that, for a ciphertext (C, B), the former allows adversaries to check whether L ∈ L is the corresponding opening key or not only by asking (B, L ) to EK . It requires a new notion on the conﬁdentiality of encryptment for the conﬁdentiality of RK ECT:  and Definition 2 (Confidentiality with Attachment). Two games otREAL  shown in Fig. 11 are introduced to formalize confidentiality. In both of otRAND the games, an adversary A is allowed to ask only a single query to the oracle enc, while A is allowed to ask multiple queries adaptively to the oracle (, −1 ). The advantage of A for confidentiality is ot-ror  
   
   Adv EC  
   
  A A    EC = 1] − Pr[otRAND  EC = 1], (A) := Pr[otREAL  
   
  It is not diﬃcult to see that the HFC encryptment scheme [13] satisﬁes conﬁdentiality with attachment in the random oracle model. The conﬁdentiality of RK ECT is reduced to the conﬁdentiality of EC with attachment and the STPRP of TBC. The proof is omitted due to the page limit. Theorem 3 (Confidentiality). Let A be an adversary against RK ECT making at most qe , qd , and qc queries to E, D, and ChalEnc, respectively. Then, ˙ and D such that there exist adversaries A ot-ror  
   
   Advrk-ror ECT (A) ≤ qc · AdvEC  
   
  ˙ + 2 · Advstprp (D) + qc (qe + qd + qc )/2−1 . (A) TBC  
   
  ˙ makes at ˙ and D is at most about that of RK-REALA . A The run time of A ECT most qe +qd +qc queries to the uniform random tweakable permutation (, −1 ). D makes at most qe + qd + qc queries to its oracle.  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  249  
   
  Fig. 11. The games for conﬁdentiality of encryptment  
   
  Ciphertext Integrity. The ciphertext integrity of RK ECT is reduced to the receiver-binding and the targeted ciphertext unforgeability of EC and the STPRP property of TBC. The proof is omitted due to the page limit. Theorem 4 (Ciphertext Integrity). Suppose that the encryptment scheme used for RK ECT satisfies strong correctness. Let A be an adversary against RK ECT making at most qe , qd , and qc queries to E, D, and ChalDec, respectively. ˙ A, ¨ and D such that Then, there exist adversaries A, ˙ + (qd + qc ) · Advtcu (A) ¨ + Advstprp (D) Advrk-ctxt (A) ≤ Advr-bind (A) ECT EC EC TBC + (qe + qd + qc )2 /2 . ˙ A, ¨ and D is at most about that of RK-CTXTA . D makes The run time of A, ECT at most qe + qd + qc queries to its oracles. Binding Properties. To see ECT as RK ccAEAD does not aﬀect the binding properties. Thus, as discussed in Sect. 3.2, RK ECT inherits both (strong) receiver binding and sender binding from EC.  
   
  6  
   
  Conclusions  
   
  We have studied the problem of constructing compactly committing AEAD (ccAEAD) based on encryptment, originally proposed by Dodis et al. [13,14] in the context of end-to-end messaging. We proposed ECT, a conceptually simpliﬁed, more eﬃcient construction than those proposed by Dodis et al. by using a TBC instead of AEAD. We also present a formalization of remotely keyed variant of ccAEAD (RK ccAEAD) and show that our ECT is indeed RK ccAEAD, which addresses the open question posed by Dodis et al. [14] positively. This indicates that ECT is useful when ccAEAD is implemented on the platform consisting of (small, slow) trusted and untrusted (but cheap and fast) modules. Future work is to explore the relationship between remotely keyed ccAEAD and leakage-resilient AEAD. It is also interesting to see if other generic constructions such as CtE and CEP in [18] can be simpliﬁed. Acknowledgements. The authors thank Akiko Inoue for fruitful discussions. The ﬁrst author was supported by JSPS KAKENHI Grant Number 21K11885.  
   
  250  
   
  S. Hirose and K. Minematsu  
   
  References 1. Albertini, A., Duong, T., Gueron, S., K¨ olbl, S., Luykx, A., Schmieg, S.: How to abuse and ﬁx authenticated encryption without key commitment. In: Butler, K.R.B., Thomas, K. (eds.) 31st USENIX Security Symposium, USENIX Security 2022, pp. 3291–3308. USENIX Association (2022). https://www.usenix.org/ conference/usenixsecurity22/presentation/albertini 2. Bellare, M., Hoang, V.T.: Eﬃcient schemes for committing authenticated encryption. In: Dunkelman, O., Dziembowski, S. (eds.) EUROCRYPT 2022. LNCS, vol. 13276, pp. 845–875. Springer, Heidelberg (2022). https://doi.org/10.1007/978-3031-07085-3 29 3. Bellare, M., Namprempre, C.: Authenticated encryption: relations among notions and analysis of the generic composition paradigm. In: Okamoto, T. (ed.) ASIACRYPT 2000. LNCS, vol. 1976, pp. 531–545. Springer, Heidelberg (2000). https://doi.org/10.1007/3-540-44448-3 41 4. Bellizia, D., et al.: Mode-level vs. implementation-level physical security in symmetric cryptography. In: Micciancio, D., Ristenpart, T. (eds.) CRYPTO 2020. LNCS, vol. 12170, pp. 369–400. Springer, Cham (2020). https://doi.org/10.1007/978-3030-56784-2 13 5. Berti, F., Guo, C, Pereira, O., Peters, T., Standaert, F-X.,: TEDT, a leakageresistant AEAD mode for high physical security applications. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2020(1), 256–320 (2020). https://doi.org/10.13154/ tches.v2020.i1.256-320 6. Berti, F., Pereira, O., Standaert, F.-X.: Reducing the cost of authenticity with leakages: a CIML2−secureAE scheme with one call to a strongly protected tweakable block cipher. In: Buchmann, J., Nitaj, A., Rachidi, T. (eds.) AFRICACRYPT 2019. LNCS, vol. 11627, pp. 229–249. Springer, Cham (2019). https://doi.org/10.1007/ 978-3-030-23696-0 12 7. Blaze, M.: High-bandwidth encryption with low-bandwidth smartcards. In: Gollmann, D. (ed.) FSE 1996. LNCS, vol. 1039, pp. 33–40. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-60865-6 40 8. Blaze, M., Feigenbaum, J., Naor, M.: A formal treatment of remotely keyed encryption. In: Nyberg, K. (ed.) EUROCRYPT 1998. LNCS, vol. 1403, pp. 251–265. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0054131 9. Chan, J., Rogaway, P.: On committing authenticated-encryption. In: Atluri, V., Pietro, R.D., Jensen, C.D., Meng, W. (eds.) ESORICS 2022. LNCS, vol. 13555, pp. 275–294. Springer, Heidelberg (2022). https://doi.org/10.1007/978-3-031-171468 14 10. Chen, L., Tang, Q.: People who live in glass houses should not throw stones: targeted opening message franking schemes. Cryptology ePrint Archive, Report 2018/994 (2018). https://eprint.iacr.org/2018/994 11. Dobraunig, C., et al.: Isap v2.0. IACR Trans. Symm. Cryptol. 2020(S1), 390–416 (2020). https://doi.org/10.13154/tosc.v2020.iS1.390-416 12. Dodis, Y., An, J.H.: Concealment and its applications to authenticated encryption. In: Biham, E. (ed.) EUROCRYPT 2003. LNCS, vol. 2656, pp. 312–329. Springer, Heidelberg (2003). https://doi.org/10.1007/3-540-39200-9 19 13. Dodis, Y., Grubbs, P., Ristenpart, T., Woodage, J.: Fast message franking: from invisible salamanders to encryptment. In: Shacham, H., Boldyreva, A. (eds.) CRYPTO 2018. LNCS, vol. 10991, pp. 155–186. Springer, Cham (2018). https:// doi.org/10.1007/978-3-319-96884-1 6  
   
  Compactly Committing Authenticated Encryption Using Encryptment  
   
  251  
   
  14. Dodis, Y., Grubbs, P., Ristenpart, T., Woodage, J.: Fast message franking: from invisible salamanders to encryptment. Cryptology ePrint Archive, Paper 2019/016 (2019). https://eprint.iacr.org/2019/016 15. Facebook: Facebook messenger. https://www.messenger.com. Accessed 09 Oct 2022 16. Facebook: Messenger secret conversations. Technical Whitepaper (2016). https:// about.fb.com/wp-content/uploads/2016/07/messenger-secret-conversationstechnical-whitepaper.pdf 17. Farshim, P., Orlandi, C., Rosie, R.: Security of symmetric primitives under incorrect usage of keys. IACR Trans. Symm. Cryptol. 2017(1), 449–473 (2017). https:// doi.org/10.13154/tosc.v2017.i1.449-473 18. Grubbs, P., Lu, J., Ristenpart, T.: Message franking via committing authenticated encryption. In: Katz, J., Shacham, H. (eds.) CRYPTO 2017. LNCS, vol. 10403, pp. 66–97. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63697-9 3 19. Hirose, S.: Compactly committing authenticated encryption using tweakable block cipher. In: Kutylowski, M., Zhang, J., Chen, C. (eds.) NSS 2020. LNCS, vol. 12570, pp. 187–206. Springer, Heidelberg (2020). https://doi.org/10.1007/978-3030-65745-1 11 20. Huang, Q., Yang, G., Wong, D.S., Susilo, W.: Eﬃcient strong designated veriﬁer signature schemes without random oracle or with non-delegatability. Int. J. Inf. Secur. 10(6), 373–385 (2011). https://doi.org/10.1007/s10207-011-0146-1 21. Huguenin-Dumittan, L., Leontiadis, I.: A message franking channel. In: Yu, Yu., Yung, M. (eds.) Inscrypt 2021. LNCS, vol. 13007, pp. 111–128. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-88323-2 6 22. Jakobsson, M., Sako, K., Impagliazzo, R.: Designated veriﬁer proofs and their applications. In: Maurer, U. (ed.) EUROCRYPT 1996. LNCS, vol. 1070, pp. 143– 154. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-68339-9 13 23. Jakobsson, M., Stern, J.P., Yung, M.: Scramble all, encrypt small. In: Knudsen, L. (ed.) FSE 1999. LNCS, vol. 1636, pp. 95–111. Springer, Heidelberg (1999). https:// doi.org/10.1007/3-540-48519-8 8 24. Katz, J., Yung, M.: Complete characterization of security notions for probabilistic private-key encryption. In: Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, pp. 245–254 (2000) 25. Len, J., Grubbs, P., Ristenpart, T.: Partitioning oracle attacks. In: Bailey, M., Greenstadt, R. (eds.) 30th USENIX Security Symposium, USENIX Security 2021, pp. 195–212. USENIX Association (2021). https://www.usenix.org/conference/ usenixsecurity21/presentation/len 26. Leontiadis, I., Vaudenay, S.: Private message franking with after opening privacy. Cryptology ePrint Archive, Report 2018/938 (2018). https://eprint.iacr.org/2018/ 938 27. Liskov, M., Rivest, R.L., Wagner, D.: Tweakable block ciphers. In: Yung, M. (ed.) CRYPTO 2002. LNCS, vol. 2442, pp. 31–46. Springer, Heidelberg (2002). https:// doi.org/10.1007/3-540-45708-9 3 28. Liskov, M.D., Rivest, R.L., Wagner, D.A.: Tweakable block ciphers. J. Cryptol. 24(3), 588–613 (2011). https://doi.org/10.1007/s00145-010-9073-y 29. Lucks, S.: On the security of remotely keyed encryption. In: Biham, E. (ed.) FSE 1997. LNCS, vol. 1267, pp. 219–229. Springer, Heidelberg (1997). https://doi.org/ 10.1007/BFb0052349 30. Lucks, S.: Accelerated remotely keyed encryption. In: Knudsen, L. (ed.) FSE 1999. LNCS, vol. 1636, pp. 112–123. Springer, Heidelberg (1999). https://doi.org/10. 1007/3-540-48519-8 9  
   
  252  
   
  S. Hirose and K. Minematsu  
   
  31. Naito, Y., Sasaki, Y., Sugawara, T.: Secret can be public: low-memory AEAD mode for high-order masking. In: Dodis, Y., Shrimpton, T. (eds.) CRYPTO 2022. LNCS, vol. 13509, pp. 315–345. Springer, Heidelberg (2022). https://doi.org/10. 1007/978-3-031-15982-4 11 32. Shen, Y., Peters, T., Standaert, F., Cassiers, G., Verhamme, C.: Triplex: an eﬃcient and one-pass leakage-resistant mode of operation. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(4), 135–162 (2022). https://doi.org/10.46586/tches.v2022.i4. 135-162 33. Signal Foundation: Signal. https://signal.org/. Accessed 09 Oct 2022 34. Tyagi, N., Grubbs, P., Len, J., Miers, I., Ristenpart, T.: Asymmetric message franking: content moderation for metadata-private end-to-end encryption. In: Boldyreva, A., Micciancio, D. (eds.) CRYPTO 2019. LNCS, vol. 11694, pp. 222–250. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-26954-8 8 35. WhatsApp: WhatsApp Messenger. https://www.whatsapp.com. Accessed 09 Oct 2022 36. Yamamuro, H., Hara, K., Tezuka, M., Yoshida, Y., Tanaka, K.: Forward secure message franking. In: Park, J.H., Seo, S. (eds.) ICISC 2021. LNCS, vol. 13218, pp. 339–358. Springer, Heidelberg (2021). https://doi.org/10.1007/978-3-031-088964 18  
   
  Post-Quantum Analysis and Implementations  
   
  Bit Security Analysis of Lattice-Based KEMs Under Plaintext-Checking Attacks Ruiqi Mi1,2(B) , Haodong Jiang3 , and Zhenfeng Zhang1,2 1  
   
  2  
   
  University of Chinese Academy of Sciences, Beijing 100049, China Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing 100190, China {ruiqi2017,zhenfeng}@iscas.ac.cn 3 Henan Key Laboratory of Network Cryptography Technology, Zhengzhou, Henan, China  
   
  Abstract. Plaintext-checking attack (PCA) is a type of attack where an adversary recovers the secret key with the help of a plaintext-checking (PC) oracle that decides if a given ciphertext decrypts to a given plaintext. In particular, PCA exists in both the key misuse attacks for INDCPA-secure lattice-based KEMs and generic side-channel attacks for IND-CCA-secure lattice-based KEMs. The query number of PC-oracle is a vital criterion for evaluating a PCA attack. Recently, Qin et al. [ASIACRYPT 2021] gave a systematic approach to ﬁnding the theoretical lower bound of PC-oracle query numbers for NIST-PQC lattice-based KEMs. Most of the prior works consider the substantial Oracle queries needed to recover the entire key. However, the adversary often has inadequate access to PC Oracles to fully recover the secret key. The concrete bit security loss with arbitrary PC Oracle access is unknown. In this paper, we give a uniﬁed method to analyze the bit security loss with arbitrary PC Oracle access for lattice-based KEMs. First, we model the information leakage in the PC Oracle by PC-hint, and give a generic transformation from PC-hints to the perfect inner-product hint, which allows the adversary to integrate PC-hints progressively. Then, following the security analysis for LWE with the perfect inner-product hint given in Dachman-Soled et al. [CRYPTO 2020], we give a concrete relationship between the PC Oracle query number and the bit-security of the lattice-based KEM under PCA. Our proposed method is applicable to all CCA-secure NIST candidate lattice-based KEMs. Applying our methods to NIST-PQC lattice-based KEMs, we get the bit-security loss of the lattice-based KEM under PCA. Take Kyber768 (original 182bit-security) as an example, the bit security of Kyber768 is reduced to 128 after 444 PC-oracle queries and reduced to 64 after 998 PC-oracle queries, while in Qin et al. [ASIACRYPT 2021] 1774 queries are required to recover the whole secret key. Our analysis also demonstrates the possibility of reducing the Oracle queries needed in PCA. The adversary may stop querying plaintext-checking oracle and solves the remaining part of reused secret oﬄine with the help of lattice reduction algorithms when the cost of lattice reduction algorithms becomes acceptable. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 255–274, 2024. https://doi.org/10.1007/978-3-031-53368-6_13  
   
  256  
   
  R. Mi et al. Keywords: Lattice-based cryptography KEM · Learning With Errors · Kyber  
   
  1  
   
  · Plaintext-checking Attacks ·  
   
  Introduction  
   
  Current Diﬃe-Hellman key exchange and other widely used public key cryptography based on factoring or discrete logarithm problems will no longer be secure if large-scale quantum computers become available. According to the roadmap released by the US National Institute of Standards and Technology (NIST) and the Department of Homeland Security [13], the transition to post-quantum standards should be completed by 2030. NIST began the call for post-quantum cryptography algorithms from all over the world in February 2016 [12]. In the third round, there are 4 ﬁnalists and 5 alternative candidates for Public Key Encryption (PKE) or Key Encapsulation Mechanism (KEM). There are 3 lattice-based KEMs among the 4 ﬁnalists. After careful analysis, NIST has selected one ﬁnalist and four alternate candidates to move on to the fourth round. Crystals-Kyber [2] is the ﬁrst PKE/KEM candidate to be standardized, which is based on the lattice assumption [14]. The construction of CPA-secure PKE usually follows the design pattern given in [10]. Most of the lattice-based NIST candidate CPA-secure KEMs are designed in such a pattern (e.g. Crystals-Kyber [2], Saber [4], FrodoKEM [11], NewHope [16]), and their hardness comes from the Learning With Errors (LWE) problem [20]. All LWE-based KEMs in Rounds 2 and 3 of the NIST standardization use a Fujisaki-Okamoto (FO) transformation [6] to achieve IND-CCA security. The ongoing standardization process raises an important question: a plaintext-checking attack may happen when the public key is reused, thus there is no security guarantee on both IND-CPA and IND-CCA KEMs. For IND-CPA secure KEM, the plaintext-checking attack runs as follows. Suppose Alice reuses her public key pkA . The adversary A impersonates Bob and tries to recover each coeﬃcient of Alice’s reused secret key skA with the help of the plaintextchecking oracle (PC Oracle) O. A crafts ciphertext ct and shared secret K and sends ct, K to O. O determines if the two shared keys match or not. For each coeﬃcient skA [i] of skA , A determines the subset to which skA [i] belongs based on O’s reply. For IND-CCA secure KEMs, a plaintext-checking attack can also be launched with the help of side-channel information. According to [19], FO transformation can be bypassed by accessing physical decapsulation devices and collecting useful match or mismatch information. Practically, the adversary A often has limitations in gathering suﬃcient perfect side-channel information and constructing a PC Oracle. In plaintextchecking attacks, users may stop misusing their public key in a short time. Thus, the adversary has restricted time to query the PC Oracle and fully recover the secret key. For example, the adversary A can only access a PC Oracle that is constructed from a USB key for online banking service before the users report the loss. PC Oracle constructed by reusing the KEM’s public key cannot be accessed when users stop reusing the public key. Thus, an optimal plaintextchecking attack has the least number of plaintext-checking Oracle queries for  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  257  
   
  successful key recovery. There are numerous works on reducing oracle queries in the plaintext-checking attack [5,9,15,17,18]. All these attacks aim to fully recover the reused secret with as few queries as possible. Qin et al. [18] gave a systematic approach to ﬁnding the theoretical lower bound of PC-oracle query numbers for all NIST-PQC lattice-based IND-CPA/IND-CCA secure KEMs. The calculation of their lower bounds is essentially the computation of a certain Shannon entropy. Thus, one cannot ﬁnd a better attack with fewer queries on average for full key recovery. Their lower bounds are also conﬁrmed by experiments. Most of the prior works about plaintext-checking attacks to lattice-based KEMs focus on recovering the full reused secret key. However, the adversary may not have enough oracle access to construct a reliable plaintext-checking oracle for recovering the full secret. Thus, compared to recovering the full secret with substantial amounts of oracle queries, one may be interested in the following question: How to analyze the concrete bit security loss of PKE/KEM after a limited number of oracle queries in plaintext-checking attacks?. Some works already analyzed the eﬀects of information leakage on the LWE problem. For example, Dachman-Soled et al. [3] give a general framework to analyze the inﬂuence of side-channel information. They provide four types of side-channel information (“hint”) and analyze the concrete security loss for each type of hint. However, the side-channel information leaked in the plaintextchecking attack (“plaintext-checking hint”) has not been considered. Thus, it remains unclear how to analyze the inﬂuence of plaintext-checking attacks on the hardness of LWE information. Let S = {S0 , S1 , ..., Sn −1 } be the set of all possible values for one coeﬃcient block and its corresponding probabilities {P0 , P1 , ..., Pn−1 }. For a single coeﬃcient block skA [i], Pj = P r(skA [i] = Sj |skA [i] ← S) for j = 0, 1, ..., n − 1. Let H(S) the Shannon entropy for S, Typically, we have H(S|P Chint) ≤ H(S). In other words, each oracle query decreases the Shannon entropy of Alice’s reused secret skA . O returns a bit b depending on whether the plaintext matches or not. Intrinsically, for each coeﬃcient of reused secret key skA , the querying process can be described as a function f of reused secret key skA , plaintext pt, ciphertext ct, in which: f (skA , ct, pt) = b ∈ {0, 1} We deﬁne such type of side-channel information as a plaintext-checking hint. Suppose the adversary A tries to recover the i-th coeﬃcient of skA . Let v be a unit vector with v[i] = 1. Thus it is very natural to express f (skA , ct, pt) as: f (skA , ct, pt) := f (skA , v) f (skA , ct, pt) is a general description of plaintext-checking hint. We ﬁnd a solution to transform plaintext-checking hints to known hints for lattice-based KEMs. Contributions. The main contributions of this paper include:  
   
  258  
   
  R. Mi et al.  
   
  – We give a uniﬁed method to analyze the bit security loss even with very little PC Oracle access for all lattice-based NIST candidate KEMs. Our basic idea is to give a uniﬁed description of the information leakage in the PC-oracle (called PC-hint). PC-hint is described in the form of f (skA , ct, pt), which is suitable for analyzing a security loss when the adversary has limited Oracle access. Then, we give a generic transformation from the least number of PChints to the perfect inner-product hint for lattice-based KEMs in the form of skA , v = l. The least number of PC-hints needed in such a transformation is the lower bound of oracle queries needed to recover a single coeﬃcient block as analyzed in [18]. We show that PC-hints can be transformed into a perfect inner-product hint when a coeﬃcient block of skA is recovered, and the adversary can integrate PC-hints progressively. Finally, by following the security analysis for LWE with the perfect inner-product hint given in Dachman-Soled et al. [3], we establish a concrete relationship between the PC-oracle query number and the bit-security of the lattice-based KEM under PCA in Sect. 4. Our proposed method is applicable to all CPA-secure and CCA-secure NIST candidate lattice-based KEMs. – We analyze the bit security loss under the plaintext-checking attack for all lattice-based NIST candidate KEMs with the help of the toolbox given in [3]. We present the relationship between bit security and plaintext-checking oracle query times in Table 1. The number in parentheses is the Oracle query needed when classical bit security is 100 (original classical bit-security less than 128). Note that the classical bit security of Kyber512 is 118. The classical bit security of LightSaber is 118. The classical bit security of NewHope512 is 112. The bit security of Kyber768 is reduced to 128 after 444 PC-oracle queries and further reduced to 64 after 998 PC-oracle queries, whereas 1774 queries are required to recover the entire secret key, as indicated in [18]. We provide the concrete relationship between the number of oracle queries and classical/quantum bit security in Sect. 5. Such a result reminds us that the loss of security is non-negligible even when the adversary cannot fully recover the secret key. – Based on the analysis above, the plaintext-checking attack can be further enhanced by combining Qin’s plaintext-checking attack [18] with standard lattice reduction techniques. The adversary may stop querying the plaintextchecking oracle and solve the remaining part of the reused secret oﬄine with the help of lattice reduction algorithms when the cost of lattice reduction algorithms becomes acceptable. We present a detailed analysis of dimension, volume, and lattice basis after each PC-hint integration in Sect. 4.3. These results can be directly used as input to lattice reduction algorithms when the cost becomes acceptable. Organizations. We start with some preliminaries in Sect. 2. Section 3 models the secret leakage in plaintext-checking attack (plaintext-checking hint). Section 4 gives a concrete mathematical expression of plaintext-checking hint (Sect. 4.2), explains how to integrate plaintext-checking hint into the lattice  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  259  
   
  Table 1. Relationship between the number of queries and classical bit security of all lattice-based NIST KEMs, E(#Queries) denotes the theoretical lower bound for the number of queries given in [18]. The number in parentheses is the Oracle queries needed when classical bit security is 100 (original classical bit-security less than 128). Bit  
   
  Kyber512  
   
  Kyber768  
   
  Kyber1024  
   
  Security  
   
  (E(#Queries)=1312)  
   
  (E(#Queries)=1774)  
   
  (E(#Queries)=2365)  
   
  128(100)  
   
  (80)  
   
  444  
   
  950  
   
  64  
   
  533  
   
  998  
   
  1459  
   
  Bit  
   
  LightSaber  
   
  Saber  
   
  FireSaber  
   
  Security  
   
  (E(#Queries)=1460)  
   
  (E(#Queries)=2091)  
   
  (E(#Queries)=2642)  
   
  128(100)  
   
  (228)  
   
  612  
   
  1181  
   
  64  
   
  631  
   
  1230  
   
  1782  
   
  Bit  
   
  Frodo640  
   
  Frodo976  
   
  Frodo1344  
   
  Security (E(#Queries)=18329) (E(#Queries)=26000) (E(#Queries)=29353) 128  
   
  833  
   
  8177  
   
  14006  
   
  64  
   
  8005  
   
  15445  
   
  20234  
   
  Bit  
   
  NewHope512  
   
  NewHope1024  
   
  Security  
   
  (E(#Queries)=1660)  
   
  (E(#Queries)=3180)  
   
  128(100)  
   
  (137)  
   
  1406  
   
  64  
   
  571  
   
  2140  
   
  (Sect. 4.3). Section 5 gives experimental results. It shows the concrete relationship between bit security and the number of oracle queries for NIST second-round KEM candidates: Kyber, Saber, Frodo and NewHope. Independent and Concurrent Work. Very recently, Guo and Mårtensson [8] showed an improved plaintext-checking attack that recovers multiple secret coeﬃcients in a parallel way. The comparisons are summarized below: 1 Guo and Mårtensson showed how to recover partial information of multiple secret entries in each oracle call. The adversary split the two-dimensional plane for two secret coeﬃcients and decides from the mismatch oracle call which part the two coeﬃcients belong to. Compared to the lower bound given in [18], the attack given in [8] reduces the number of queries needed by 0.08%, 10.6%, 10.6% for Kyber512, Kyber768, Kyber1024, and 3.4%, 5.01%, 8.1% for LightSaber, Saber, FireSaber. 2 In the discussion part, they give a rough estimation of the query sample complexity for Kyber and Saber when post-processing is allowed. They employ the lattice estimator given in [1]. They did not give concrete relationship between the number of queries and the geometry of the lattice in theory.  
   
  2  
   
  Preliminaries  
   
  A lattice is a discrete additive subgroup of Rm , denoted as Λ. Lattice Λ is generated by a set of linearly independent basis {bj } ⊂ Rm , that is Λ :=  
   
  260  
   
  R. Mi et al.  
   
  {Σj zj bj : zj ∈ Z}. The i-th successive minimum of a lattice, λi (Λ), is the radius of the smallest ball centered at the origin containing at least i linearly independent lattice vectors. We denote the dimension of lattice Λ as m and the rank as n. If n = m, the lattice is full rank. Matrix B having all basis vectors as  rows can be called a basis. The volume of the lattice is deﬁned as V ol(Λ) := det (BB T ). The dual lattice of Λ in Rn is deﬁned as: Λ∗ := {y ∈ Span(B) | ∀x ∈ Λ, x, y ∈ Z}  
   
  (1)  
   
  Definition 1 (search-LWE problem with short secrets). Let n, m, q be positive integers, and let χ be a distribution over Z. The search LWE problem (with short secrets) for parameters (n, m, q, χ) is: , b = zAT + e ∈ Zm Given the pair (A ∈ Zm×n q q ) where: 1. A ∈ Zm×n is sampled uniformly at random. q 2. z ← χn , and e ← χm are sampled with independent and identically distributed coeﬃcients following the distribution χ. Find z. The complexity of solving (search-)LWE against primal attack consists of viewing the LWE as an instance of (Distorted-)Bounded Distance Decoding problem, reducing DBDD to uSVP(via Kannan’s Embedding, and ﬁnally applying lattice reduction algorithm to solve the uSVP instance. DBDD accounts for potential distortion in the distribution of the secret noise vector that is to be recovered, and the secret noise vector is found at a lower cost. Definition 2 (γ-uSVP). given a lattice Λ such that λ2 (Λ) > γλ1 (Λ), ﬁnd a shortest nonzero vector in Λ. Definition 3 (Distorted Bounded Distance Decoding Probbe a symmetric matrix lem, DBDD). Let Λ ⊂ Rd be a lattice, Σ ∈ Rd×d   and μ ∈ Span(Λ) ⊂ Rd such that Span(Σ)  Span Σ + μT μ = Span(Λ) The Distorted Bounded Distance Decoding Problem DBDDΛ,μ,Σ is: Given μ, Σ and a basis of Λ. Find the unique vector x ∈ Λ ∩ E(μ, Σ). Where E(μ, Σ) denotes the ellipsoid E(μ, Σ) := {x ∈ μ + Span(Σ) | (x − μ) · Σ ∼ · (x − μ)T ≤ rank(Σ)}  
   
  (2)  
   
  The triple I = (Λ, μ, Σ) will be referred to as the instance of the DBDDΛ,μ,Σ problem. Definition 4 (Primitive Vectors). A set of vector y1 , · · · , yk ∈ Λ is said primitive with respect to Λ if Λ∩Span(y1 , · · · , yk ) is equal to the lattice generated by y1 , · · · , yk . Equivalently, it is primitive if it can be extended to a basis of Λ. / Λ for any integer i ≥ 2. If k = 1, y1 , this is equivalent to y1 /i ∈  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  3 3.1  
   
  261  
   
  Side-Channel Information in Plaintext-Checking Attacks The Meta-Structure of IND-CPA Secure KEM  
   
  Suppose there exists six additive Abelian groups Ssk , SA , SB , St , SU , SV and four bilinear mappings(denoted as ×). The four bilinear mappings are SA ×Ssk → SB , SU × Ssk → SV , St × SA → SU , St × SB → SV . The multiplication satisﬁes associativity in the sense that (t × A) × sk = t × (A × sk) for all t ∈ St , A ∈ SA , and skA ∈ Ssk . The multiplication works as block 1 on Fig. 1. We list the meta-structure of CPA-secure KEM in Algorithm 1, in which:  
   
  Algorithm 1. The meta-structure of IND-CPA secure KEM 1: function setup(1λ ) 2: setup the algebra 3: deﬁne public parameter pp 4: return pp 1: function Gen(pp; coinA ) 2: 3: 4: 5: 6: 7: 8:  
   
  $  
   
  − SA A← $ skA ← − Ssk $ d← − sB randomness comes from coinA B ← A × skA + d pkA ← (A, B) return (skA , pkA )  
   
  1: function Enc(pp, pkA , pt; coinB ) 2: parse pkA = (A, B)  
   
  $  
   
  $  
   
  $  
   
  3: t← − St , e ← − SU , f ← − SV 4: randomness comes from coinB ¯ ←t×A+e 5: U 6: V¯ ← t × B + f + encode(pt) ¯) 7: U ← Compress(U 8: V ← Compress(V¯ ) 9: K ← H(ptct = (U, V )) 10: return K 1: function Dec(pp, skA , ct) 2: P arse ct = (U, V ) ¯ ← Decompress(U ) 3: U 4: V¯ ← Decompress(V ) ¯ × skA 5: W ← V¯ − U  6: pt ← decode(W ) 7: K  ← H(pt ct = (U, V )) 8: return K   
   
  – For t, d, f, e, sk, such sparse elements are chosen to be sampled from discrete Gaussian distribution or central binomial distribution Bη whose sample is η (ai −bi ), where ai , bi ← {0, 1} and mutually independent. A generated by Σi=1 sample is chosen according to Bη means every component is chosen randomly from Bη . – the encode : M → SV , decode : SV → M is not necessary but usually employed. The encode is an injective function. A typical code is D − v lattice code. Message bits are encoded by multiplication to L = (q − 1)/2 and represented v times in Y = encode(pt). NewHope [16] selects v = 2, thus Yi = Yi+256 = pti · (q − 1)/2. The decoding process of Y = encode(pt) is q−1 ﬁnding the value b that minimizes |Yi − b · q−1 2 | + |Yi+256 − b · 2 |. – The Compress/Decompress is usually used to decrease the communication cost. Typically, a ciphertext V¯ is replaced by V = Compress(V¯ , p) =  
   
  262  
   
  R. Mi et al.  
   
  p/q · V¯ mod p and the decompress operates in an opposite way V¯ = Compress(V, p) = q/p · V . Almost all NIST candidate lattice-based CPA-secure KEM are designed as Algorithm 1. We give two examples below: NewHope [16], Crystals-Kyber [2]. Example 1. Kyber deﬁnes Ssk = Rkq , SB = Rkq , SU = Rkq , St = Rkq , SV = Rq , . Kyber does not use encode/decode algorithm. Elements in Rq SA = Rk×k q are considered as polynomials in variable X modulo X n + 1. Elements in Rkq are considered as vector with components in Rq . Elements in Rk×k are conq sidered as matrix with components in Rq . In Kyber512-KEM, e, f, d is sampled sparsely from B3 . In Kyber768-KEM abd Kyber1024-KEM, t, d, f, e, sk are sampled from B2 . Other parameters for Kyber is q = 3329, n = 256. k = 2/3/4 for Kyber512/Kyber 768/Kyber 1024. Example 2. NewHope-CPA-PKE deﬁnes Ssk = SA = SB = St = SU = SV = Rq . Elements in Rq are considered as polynomials in variable X modulo X n + 1. For t, d, f, e, sk, sparse elements are sampled from centered binomial distribution B8 . For NewHope512/NewHope1024, the parameters are n = 512, n = 1024 and q = 12289. The encode/decode algorithm are described as above. 3.2  
   
  Model of Plaintext-Checking Attack  
   
  In a plaintext-checking attack, the adversary interacts with plaintext checking oracle O, which works as shown in Algorithm 2. O is a plaintext checking oracle which receives ct and pt, returning one bit showing if ct decrypts to pt. INDCPA secure public key encryption/key encapsulation mechanisms are vulnerable to plaintext-checking attack. The plaintext-checking oracle exists in many cases. In the client-server protocol where the ciphertext is the encryption of some symmetric key k. The adversary can construct faulty ciphertexts that may or may not decode to k and deliver them to the server. Then the adversary can see if secure messaging works and hence simulates a plaintext-checking oracle O. IND-CCA secure KEM may also suﬀer a plaintext-checking attack since the adversary can create oracle O by a test-based template approach as given in [7].  
   
  Algorithm 2. Plaintext-Checking Attack  1: skA ← AO (pkA )  = skA then 2: if skA 3: return 1 4: else 5: return 0  
   
  1: ORACLE O(ct = (U, V ), K) 2: K  ← KEM.Dec(ct) 3: if K = K  then 4: return 1 5: else 6: return 0  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  3.3  
   
  263  
   
  Secret Leakage Model in Plaintext-Checking Attack  
   
  Since many lattice-based IND-CPA secure KEMs use the same meta-structure, they may have similar plaintext-checking attack procedures. Suppose Alice reuses her public key pkA = (A, B). As described in Algorithm 2, the adversary A crafts diﬀerent plaintext and ciphertext ct, pt to recover Alice’s secret key skA with as fewer oracle access as possible. Each coeﬃcient of skA is sampled independently from S ⊂ Ssk . When the adversary A tries to recover the i-th coeﬃcient of skA , each oracle call leaks information about S. Without loss of generality, let S = {S0 , S1 , ..., Sn−1 } be the set of all possible values of the original sparse secret distribution. Let Pj be the probability that skA [i] = Sj where skA [i] is generated from the distribution S, that is, Pj = P r[skA [i] = Sj |skA [i] ← S] for j = 0, 1, ..., n − 1. Denote the new secret distribution after the oracle query as S  after querying plaintext checking oracle O. When the adversary gets a returned value from the Oracle, he can narrow the range of skA [i] from S to S  until the exact value of skA [i] is determined. The change of secret distribution is shown in Fig. 1. As described in Sect. 3.1, block 1 (in a dashed rectangle) represents the multiplication of Abelian groups SA , SB , St , SU , SV (the yellow blocks) before the adversary A queries the PC Oracle O. The blue block in block 1 represents the secret distribution S ⊂ Ssk before the adversary queries the PC Oracle O. The green block in block 2 represents the new secret distribution S  ⊂ Ssk after A queries the PC Oracle O. Other Abelian groups remain unchanged.  
   
  Fig. 1. Oracle query in Plaintext-Checking Attack and the change of secret distribution.  
   
  264  
   
  R. Mi et al.  
   
  In plaintext-checking attack, the adversary A tries to recover the reused secret by accessing oracle O as few as possible. In other words, each oracle query decreases the Shannon entropy of reused secret skA . A tries to reduce the entropy of S as much as possible. Intrinsically, for each coeﬃcient of Alice’s reused secret key skA , the querying process can be described as a function f of reused secret key skA , secret distribution S, ciphertext ct ∈ SU × SV , pt ∈ M. Formally, we can deﬁne the information leakage in plaintext-checking attack as: Definition 5 (Plaintext-Checking Hint). A plaintext-checking hint on the reused secret skA is the crafted plaintext pt, ciphertext ct, such that f (skA , ct, pt) = b ∈ {0, 1}. Let v be a unit vector with v[i] = 1. The expression of plaintext-checking hint f (skA , ct, pt) can be simpliﬁed as f (skA , v) = b ∈ {0, 1} since the adversary tries to recover the i-th coeﬃcient of skA . Most of the prior works consider the Oracle access times for recovering the full reused key. Since the adversary may be prohibited from gathering suﬃcient sidechannel information to build a plaintext-checking oracle, the adversary A does not have suﬃcient access to a plaintext-checking oracle to completely recover the reused secret. For example, PC Oracle constructed by reusing KEM’s public key cannot be accessed when users stop reusing the public key. Thus one may be interested in a more precise analysis of f (skA , ct, pt) to learn security loss after certain times of Oracle access. To investigate security loss after limited times of Oracle queries, one possible way is to express f (skA , ct, pt) in the form that can be integrated into the lattice.  
   
  4  
   
  Reducing PC-Hint to Perfect Inner-Product Hint  
   
  This section presents how to reduce PC-Hint to a perfect inner-product hint. First, we describe a practical plaintext-checking attack that reaches the theoretical lower bound of oracle queries in Sect. 4.1. We then analyze the message leakage in each PC Oracle query, which is described in the form of plaintextchecking hint in Sect. 4.2. Finally, we present a method for integrating plaintextchecking hints into the lattice in Sect. 4.3. We do this by transforming plaintextchecking hints into perfect inner-product hints and integrating perfect innerproduct hints into the lattice. Additionally, we analyze the changes in lattice volume and dimension, which directly aﬀect the bit security of KEMs. 4.1  
   
  Practical Plaintext-Checking Attack with Theoretical Lower Bound  
   
  We adopt the plaintext-checking attack given in [18]. They get their lower bounds for all lattice-based NIST KEM candidates by building the optimal Binary Recovery Tree (BRT), and they show that the calculation of these bounds becomes essentially the computation of a certain Shannon entropy, which means  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  265  
   
  that on average one cannot ﬁnd a better attack with fewer queries than their results in the full key recovery. The lower bound of oracle access for recovering a single coeﬃcient of reused secret skA has been analyzed in Theorem 1 in [18]. Let min E(S) represent the lower bound for the minimum average number of queries. Moreover, let H(S) represent the Shannon entropy for S. Then, we have H(S) ≤ min E(S) < H(S) + 1. In the following section, we give a brief explanation of their attack. Take plaintext-checking attack to IND-CPA secure Kyber512 as an example. The adversary selects proper ciphertext ct = (U, V ) as inputs to O. Then, the adversary is able to recover Alice’s reused secret skA from the oracle response sˆ. The recovery of each coeﬃcient is mutually independent. We give the approach to recover the ﬁrst coeﬃcient block skA [0] of sk, other coeﬃcient blocks can be recovered similarly. The attacker selects plaintext pt = (1, 0, ..., 0) and ct = (U, V ), where ¯ = ( q , 0, ..., 0), U = Compress(U ¯ , 2dU ) and V = (h, 0, ..., 0). dU = U 16 10, dV = 4 is the parameter selected by Kyber512. Then the attack query ¯ = the plaintext-checking oracle O with ct. The oracle O calculates U q h , 0, ..., 0). Decompress(U, 2dU ), V¯ = Decompress(V, 2dV ) = ( 16 Thus, the adversary constructs a relationship between pt [0] and skA [0] T ¯ )[0], 2) = 2 (V¯ [0] − (sk T · ·U after decryption as pt [0] = Compress((V¯ − skA A q ¯ )[0]) mod 2. U q T ¯ T ¯ [0] = skA [0] q , it holds that Since V¯ [0] = 16 h and (skA · U )[0] = skA [0]U 16 q q 2  T pt [0] = q ([ 16 h] − skA [0][ 16 ]) mod 2, where h is a parameter chosen by the attacker. Let h = 4, if skA [0] ∈ [0, 3], pt [0] = 0, then the oracle O will output 0. Otherwise, skA [0] ∈ [−3, −1], pt [0] = 1, the oracle O will output 1. The attacker could adaptively choose h to recover skA [0] based on the sequence sˆ from oracle O. If the attacker uses well-selected h, he could recover skA [0] with as few queries as possible. With the help of the optimal binary recovery tree, the adversary divides the range of the coeﬃcient block in half each time and tries to recover Si with the biggest probability as soon as possible. We list the selection of h in Table 2. In such a plaintext-checking attack, each query divides the possible range of skA [0] into (nearly) half. [18] gives the selection of h and the corresponding changes of states in Sect. 4.1. Table 2. The choice of h and the States for Kyber512  
   
  h  
   
  State1  
   
  State2  
   
  State3  
   
  State4  
   
  State5  
   
  State6  
   
  4  
   
  3  
   
  9  
   
  12  
   
  13  
   
  7  
   
  O → 0 State4 skA [0] = −1 skA [0] = −3 skA [0] = 0 skA [0] = 1 skA [0] = 3 O → 1 State2 State3  
   
  skA [0] = −2 State5  
   
  State6  
   
  skA [0] = 2  
   
  According to Theorem [18], The lower bound for Kyber512, Kyber768, and Kyber1024, in theory, is 1216, 1632, 2176. The expectation of queries needed to  
   
  266  
   
  R. Mi et al.  
   
  5 3 1 recover a single coeﬃcient in skA is 16 × 2 + 15 64 × (3 + 2) + 32 × (4 + 3) + 32 × 3 = 2.56. The average number of queries needed in a plaintext-checking attack for Kyber512, Kyber768, and Kyber1024, in theory, is 1312, 1774, and 2365. The gap is less than 9%. Besides, Qin et al. also did an experiment to verify their theory. The experiment result shows that the number of queries is 1311, 1777, 2368 separately.  
   
  4.2  
   
  Message Leakage in Each Query  
   
  Suppose Alice reuses her public key, the corresponding secret key is skA . The adversary tries to recover the ﬁrst coeﬃcient of skA . The analysis is similar to other coeﬃcients. In Sect. 3.3, we give a plaintext-checking hint f (skA , ct, pt) to describe the change of secret distribution after each oracle query. For Kyber512, A sets ct = q q T h] − skA [0][ 16 ]) mod 2 = 0, (U, V ). If skA [0] ∈ [0, 3], set h = 4, pt [0] = 2q ([ 16  then O → 0. Otherwise pt [0] = 1, O → 1. The plaintext-checking hint can be T ¯ )[0]) mod 2. ·U described as: f (skA , ct = (U, V ), pt) = 2q (V¯ [0] − (skA Let v be a unit vector with v[0] = 1. Since the adversary tries to recover a certain coeﬃcient in each oracle query, it is very natural to express f (skA , ct, pt) as f (skA , v). Speciﬁcally, we have: – h = 4, after the ﬁrst query, plaintext-checking hint P CHint1 : f (skA , v) = q q T · 4] − skA [0][ 16 ]) mod 2 = 0, and skA , v ∈ [0, 3]. 2q ([ 16 – h = 12, after the second query, plaintext-checking hint P CHint2 : f (skA , v) q q T · 12] − skA [0][ 16 ]) mod 2 = 1, and skA , v ∈ [1, 3]. = 2q ([ 16 – h = 13, after the third query, plaintext-checking hint P CHint3 : f (skA , v) = q q T · 13] − skA [0][ 16 ]) mod 2 = 1, and skA , v ∈ [2, 3]. 2q ([ 16 – h = 7, after the fourth query, plaintext-checking hint P CHint4 : f (skA , v) = q q T · 7] − skA [0][ 16 ]) mod 2 = 0, and skA , v = 3. 2q ([ 16 Now the adversary collect four plaintext-checking hints P CHint1 , P CHint2 , P CHint3 , P CHint4 . Then the adversary can transform these hints into “perfect hint” as described in [3]: skA , v = skA [0]. Let S = {S0 , S1 , ..., Sn−1 } be the set of all possible values of the original sparse secret distribution. Denote by Hi the number of plaintext-checking hints A needs to determine the coeﬃcient block when it is exactly Si , which is actually the oracle access need to determine the coeﬃcient block as analyzed at the beginning of Sect. 4. Let E(#P CHint) be the average number we needed to transform a plaintext-checking hint into a perfect hint. According to the analysis n−1 P i Hi . above, we have E(#P CHint) = Σi=0 Intrinsically, the average number we needed to transform plaintext-checking hints into a perfect hint is the average number of oracle queries A needed to recover a single coeﬃcient block. Thus the lower bound of E(#P CHint) can be derived from [18]. We list E(#P CHint) for all lattice-based NIST KEMs (both IND-CPA/IND-CCA) in Table 3.  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  267  
   
  Table 3. E(#P CHint) against lattice-based NIST KEMs Schemes  
   
  E(#P CHint) Schemes  
   
  E(#P CHint) Schemes  
   
  E(#P CHint)  
   
  Kyber512  
   
  2.77  
   
  Saber  
   
  2.73  
   
  Frodo1344  
   
  2.73  
   
  Kyber768  
   
  2.31  
   
  3.24  
   
  FireSaber 2.56  
   
  NewHope512  
   
  Kyber1024 2.31  
   
  Frodo640  
   
  3.59  
   
  NewHope1024 3.11  
   
  LightSaber 2.88  
   
  Frodo976  
   
  3.34  
   
  –  
   
  –  
   
  In the following parts, we describe how to predict security loss after collecting several PC hints and transformed these PC hints into a perfect hint. 4.3  
   
  Integrating Plaintext-Checking Hints into Lattice  
   
  The intuition behind estimating security loss (under primal attack) is to estimate the hardness of the underlying LWE problem (as deﬁned in Deﬁnition 1) after integrating plaintext-checking hints. The adversary A collects plain LWE samples as shown in line 5, 6 in function Enc of Algorithm 1. Then A transforms the LWE problem to DBDD problem (Deﬁnition 3) and constructs a lattice basis. Then A integrates the plaintext-checking hints into the DBDD problem. Finally, A transforms the DBDD problem to uSVP problem (Deﬁnition 2). The uSVP problem can be solved by lattice reduction algorithm. The solution of the LWE problem is (e, skA ). It can be extended to a short vector (e, skA , 1), which is an short vector of the lattice Λ = {(x, y, w) ∈ which is of full rank in Rd and has volume Zn+m+1 |x + yAT − bw} ⎡ = 0 mod q, ⎤ qIm 0 0 q m . The row vectors of ⎣ AT −In 0 ⎦ constitute a basis of Λ. For a single coefb 0 1 ﬁcient block skA [b], Pi = P r(skA [b] = Si |skA ← S) for i = 0, 1, ..., n − 1. We denote the average and variance of the LWE original secret distribution S as μ can be converted to a DBDDΛ,μ,Σ instance with and σ 2 . Such a LWE instance 2 σ Im+n 0 . μ = [μ, ..., μ, 1], Σ = 0 0 To integrate plaintext-checking hints into DBDD problem, the adversary A collect several plaintext-checking hints until they can be transformed to a perfect hint skA , v = skA [0] as shown in Sect. 4.2. Suppose A recovers the ¯ := (0; v; −l), where 0 is an allﬁrst coeﬃcient of skA . v can be extended to v zero vector of dimension m, v = (1, 0, · · · , 0) of dimension n, l = skA [0]. The adversary A can integrate v ¯ by modifying DBDDΛ,μ,Σ to DBDDΛ ,μ  ,Σ  . Integrating v ¯ into DBDDΛ,μ,Σ means ﬁnding an intersection between Λ and an hyperplane orthogonal to v ¯. We denote the new lattice as Λ . Intuitively, Λ has lower dimension than Λ, meaning that solving DBDDΛ ,μ  ,Σ  is easier than DBDDΛ,μ,Σ . The new mean μ and new covariance Σ  can be derived according to the equation given in equations (12) and (13) in [3]. Speciﬁcally, we have Σ  = T v ,μ Σ− (¯vv¯ΣΣ) v¯Tv¯Σ , and μ = μ− v¯¯ v ¯Σ. Since v ¯ is an all-zero vector with dimension Σv ¯T  
   
  268  
   
  R. Mi et al.  
   
  m + n + 1 except that the m + 1-th coeﬃcient is 1 and the m + n + 1-th coeﬃcient ¯Σ is an all-zero vector except that the m + 1-th is −skA [0]. Thus we have v coeﬃcient is σ 2 . Thus we have Σ  = Σ − σ12 M , where M is a m + n + 1-dimension diagonal matrix with Mm+1,m+1 = σ 4 , Mm+n+1,m+n+1 = 0 and all other diagonal elements 0. Thus Σ  is a m + n + 1-dimension diagonal matrix with Mm+1,m+1 = 0, Mm+n+1,m+n+1 = 0 and all other diagonal elements σ 2 . μ = [μ, ..., μ, 1] − (μ − skA [0]) σ12 v ¯Σ. Thus μ is an all-μ vector except that   μm+1 = skA [0], μm+n+1 = 1. The volume of Λ is analyzed in Theorem 1. T Theorem 1. Given the LWE sample (A ∈ Zm×n , b = skA A + e ∈ Zm q q ). Suppose the adversary A collects #P CHint plaintext-checking hints f1 , ..., f#P CHint when recovering skA [i]. The adversary A can transform f1 , ..., f#P CHint into a perfect hint v ¯ := (0; v; −l), where 0 is an all-zero vector of dimension m, v is a ¯ modiﬁes unit vector with v[i] = 1 and dimension n, l = skA [i]. Including hint v DBDDΛ,μ,Σ to DBDDΛ ,μ  ,Σ  with dimension dim(Λ ) and volume V ol(Λ ):  
   
  dim (Λ ) = dim(Λ) − 1  2 V ol (Λ ) = Vol(Λ) · 1 + skA [i] · det(ΠΛ )  
   
  (3)  
   
  When v ¯ is a primitive vector, we have  Vol (Λ ) = Vol(Λ) 1 + skA [i]2  
   
  (4)  
   
  Proof. When v ¯ is a primitive vector(¯ v can be extended to a basis of Λ), the volume of the lattice after integrating hint v ¯ is V ol (Λ) = ¯ v  · Vol(Λ) = Vol(Λ) ·  1 + skA [i]2 (see Lemma 12 of [3]). When v ¯ is not in the span of Λ, we can also apply orthogonal projection ¯·ΠΛ of ¯ v onto Λ. Replacing v ¯ by v ¯ is still valid. The orthogonal projection v ¯ = v √ ∼ √ ∼T matrix is ΠΛ = ΠΣ  = Σ  · Σ  · Σ √ , where Σ  = Σ + μT · μ is ∼ the covariance matrix after homogenization, Σ  is the restricted  inverse of √ Σ  (see deﬁnition 3 of [3]). Thus we have Vol (Λ ) = Vol(Λ) · 1 + skA [i]2 · det(ΠΛ )), where ΠΛ is the orthogonal projection onto Λ. It is predicted that the BKZ − β  can solve a uSV PΛ after E(#P CHint) √ 2β  −dim(Λ )−1 1/dim(Λ ) queries s.t. β  ≤ δβ  , where dim(Λ ), V ol(Λ ) are · V ol (Λ ) as described in Eq. (3, 4). The expectation of #P CHint (E(#P CHint)) and the transformation from plaintext-checking hints to perfect hints has been described in Sect. 4.2.  
   
  5 5.1  
   
  Experiment Results Kyber  
   
  Figure 2 gives the concrete relationship between the number of queries and bitsecurity for all parameter sets of Kyber in the NIST third-round submission.  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  269  
   
  In Table 4, we present the number of queries needed for Kyber512/ Kyber768/ Kyber1024 when the bit security of the underlying LWE reaches 128, 64, 48, 32, 24, 16 under primal and dual attacks. Taking Kyber512 as an example, the adversary A can query PC Oracle O for only 867 times instead of 1312 times. The classical bit security of the LWE problem is decreased to 32. The adversary may stop querying the PC Oracle and solve the remaining part of the secret key using the lattice reduction algorithm. For our experiment, we make use of the LWE estimator from [3]. Estimating the hardness needs the dimension of the lattice Λ and its volume only. According to Theorem 1, for Kyber512/Kyber768/ Kyber1024, every 2.77/2.31/2.31 queries reduces the dimension of the lattice by 1. After integrating short vectors into the lattice, we get the concrete dimension of the lattice Λ, which tells us the security of current LWE problem after certain times of queries. The number in parentheses is the Oracle queries needed when classical/quantum bit security is 100 (The original classical bit security of Kyber is 118).  
   
  Fig. 2. Relationship between query and security under primal attack for Kyber. Table 4. Classical&Quantum Query-Security For Kyber 512/768/1024. Bit Security  
   
  Kyber512 (classical/quantum)  
   
  Kyber768 (classical/quantum)  
   
  Kyber1024 (classical/quantum)  
   
  –  
   
  Primal  
   
  Dual  
   
  Primal  
   
  Dual  
   
  Primal  
   
  Dual  
   
  128(100)  
   
  (80)/(65)  
   
  (150)/(129)  
   
  444/333  
   
  562/543  
   
  950/848  
   
  1274/1205  
   
  64  
   
  533/464  
   
  657/624  
   
  998/938  
   
  1112/1096  
   
  1459/1404 1732/1673  
   
  48  
   
  699/646  
   
  761/733  
   
  1144/1098 1206/1176  
   
  1593/1550 1805/1773  
   
  32  
   
  867/831  
   
  865/838  
   
  1292/1259 1320/1302  
   
  1728/1699 1915/1893  
   
  24  
   
  953/925  
   
  922/906  
   
  1366/1343 1396/1361  
   
  1796/1775 1973/1959  
   
  16  
   
  1033/1016 981/962  
   
  1437/1423 1481/1476  
   
  1862/1849 2095/2029  
   
  270  
   
  5.2  
   
  R. Mi et al.  
   
  Saber  
   
  Figure 3 gives the relationship between the number of queries and security for all parameter sets of Saber in NIST third round submission. We list the number of queries needed for LightSaber/Saber/FireSaber when the bit security of the underlying LWE reaches 128, 64, 48, 32, 24, 16 in Table 5 under primal/dual attack. According to Theorem 1, for LightSaber/ Saber/ FireSaber, every 2.88/ 2.73/ 2.56 queries reduce the dimension of the lattice by 1. After integrating short vectors into the lattice, we get the concrete dimension of the lattice Λ, which tells us the security of current LWE problem after certain times of queries. 5.3  
   
  Frodo  
   
  Figure 4 gives the relationship between the number of queries and security for all parameter sets of Frodo in NIST second round submission. We list the number of queries needed for Frodo640/ Frodo976/ Frodo1344 when the bit security of  
   
  Fig. 3. Relationship between query and security under primal attack for Saber. Table 5. Classical&Quantum Query-Security For LightSaber/Saber/FireSaber. Bit Security  
   
  LightSaber (classical/quantum)  
   
  Saber (classical/quantum)  
   
  FireSaber (classical/quantum)  
   
  –  
   
  Primal  
   
  Primal  
   
  Dual  
   
  Primal  
   
  128(100)  
   
  (228)/(124) (407)/(352) 612/487  
   
  832/817  
   
  1181/1063 1395/1364  
   
  64  
   
  631/553  
   
  844/799  
   
  1230/1162 1446/1415  
   
  1782/1718 1963/1929  
   
  48  
   
  839/772  
   
  933/902  
   
  1390/1339 1550/1517  
   
  1941/1890 2067/2029  
   
  32  
   
  1075/1023  
   
  1029/1003  
   
  1554/1521 1656/1625  
   
  2100/2066 2179/2165  
   
  24  
   
  1204/1164  
   
  1096/1066  
   
  1638/1611 1714/1699  
   
  2179/2153 2257/2223  
   
  16  
   
  1340/1311  
   
  1155/1138  
   
  1717/1701 1774/1759  
   
  2258/2243 2326/2289  
   
  Dual  
   
  Dual  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  271  
   
  the underlying LWE reaches 128, 64, 48, 32, 24, 16 in Table 6 under primal/dual attack. The number in parentheses is the Oracle queries needed when classical/quantum bit security is 100 (The original quantum bit security of Frodo640 is 124). Table 6. Classical&Quantum Query-Security For Frodo 640/976/1344 Bit Security  
   
  Frodo640 (classical/quantum)  
   
  Frodo976 (classical/quantum)  
   
  Frodo1344 (classical/quantum)  
   
  –  
   
  Primal  
   
  Dual  
   
  Primal  
   
  Primal  
   
  128(100)  
   
  833/(2755)  
   
  2943/(4861) 8177/6734  
   
  64  
   
  8005/7230  
   
  10508/9961  
   
  48  
   
  9899/9296  
   
  12001/11474 17368/16754 21041/20792 21872/21370 24257/23812  
   
  32  
   
  11821/11419 13572/13230 19346/18918 22368/21994 23577/23205 25217/25124  
   
  24  
   
  12796/12509 14577/14235 20334/20014 23154/22773 24429/24145 25841/25581  
   
  16  
   
  13772/13571 15475/15162 21296/21109 23609/23527 25259/25084 26304/26174  
   
  Dual  
   
  Dual  
   
  12546/11986 14006/12761 17859/16758  
   
  15445/14670 19863/19384 20234/19556 23284/23009  
   
  Fig. 4. Relationship between query and security under primal attack for Frodo.  
   
  5.4  
   
  NewHope  
   
  Figure 5 gives the relationship between the number of queries and security for all parameter sets of NewHope in NIST second round submission. We list the number of queries needed for NewHope512/ NewHope1024 when the bit security of the underlying LWE reaches 128, 64, 48, 32, 24, 16 in Table 7 under primal/dual attack. The number in parentheses is the Oracle queries needed when classical/quantum bit security is 100 (The original classical bit security of NewHope512 is 112).  
   
  272  
   
  R. Mi et al.  
   
  Fig. 5. Relationship between query and security under primal attack for NewHope. Table 7. Classical&Quantum Query-Security For NewHope 512/1024  
   
  6  
   
  Bit Security  
   
  NewHope512 (classical/quantum)  
   
  NewHope1024 (classical/quantum)  
   
  –  
   
  Primal  
   
  Primal  
   
  128(100)  
   
  (137)/(20) (528)/(461) 1406/1260 1820/1756  
   
  64  
   
  571/490  
   
  915/866  
   
  2140/2062 2475/2438  
   
  48  
   
  772/710  
   
  1054/1011  
   
  2333/2274 2594/2555  
   
  32  
   
  979/934  
   
  1173/1148  
   
  2532/2488 2714/2678  
   
  24  
   
  1083/1050 1231/1224  
   
  2632/2600 2771/2738  
   
  16  
   
  1183/1164 1325/1309  
   
  2728/2709 2864/2837  
   
  Dual  
   
  Dual  
   
  Conclusions and Discussions  
   
  In this paper, we explicitly build the relationship between the number of Oracle queries and the security loss of the reused secrets for all NIST second-round lattice-based KEMs. Our analysis can be divided into three steps. First, we model the information leakage in the PC-oracle by PC-hint and give a generic transformation from PC-hints to the perfect inner-product hint, which allows the adversary to integrate PC-hints progressively. Then, we give a concrete relationship between the PC Oracle query number and the bit security of the lattice-based KEM under PCA. Our bit security analysis is inspired by the security analysis for LWE with the perfect inner-product hint given in [3], Our proposed method is applicable to all CCA-secure NIST candidate lattice-based KEMs. We applied our methods to NIST-PQC lattice-based KEMs, we get the bitsecurity loss of the lattice-based KEM under PCA. Take Kyber768 (original 182-bit-security) as an example, the bit security of Kyber768 is reduced to 128 after 444 PC-oracle queries and reduced to 64 after 998 PC-oracle queries, while  
   
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
   
  273  
   
  in Qin et al. [18] 1774 queries are required to recover the whole secret key. Our analysis also demonstrates the possibility of reducing the Oracle queries needed in PCA. The adversary may stop querying plaintext-checking oracle and solves the remaining part of reused secret oﬄine with the help of lattice reduction algorithms when the cost of lattice reduction algorithms becomes acceptable. The bit-security analysis in this paper works under perfect PC Oracle. Recently, Shen et al. [21] presents a new checking approach in the plaintextchecking attacks, which is preferable when the constructed PC Oracle is imperfect. Imperfect PC Oracle may occur due to environmental noises, or simply the measurement limitations in implementing the PC Oracle. Their basic idea is to design new detection codes that eﬃciently ﬁnd the problematic entries in the recovered secret key and corrects problematic entries with a small number of additional traces. When the raw oracle accuracy is ﬁxed, Their new attack requires only 41% of the EM traces needed in a majority-voting attack in our experiments. It is appealing to analyze the relationship between imperfect PC Oracle queries and bit-security of lattice-based KEMs. Acknowledgements. We thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper. Ruiqi Mi is supported by the National Key R&D Program of China (Grant No. 2021YFB3100100). Haodong Jiang is supported by National Natural Science Foundation of China (Grant No. 62002385).  
   
  References 1. Albrecht, M.R., Player, R., Scott, S.: On the concrete hardness of learning with errors. J. Math. Cryptol. 9(3), 169–203 (2015). http://www.degruyter.com/view/ j/jmc.2015.9.issue-3/jmc-2015-0016/jmc-2015-0016.xml 2. Avanzi, R., et al.: CRYSTALS-Kyber: Algorithm Speciﬁcations and Supporting Documentation (2019/2020). https://pq-crystals.org/kyber/index.shtml 3. Dachman-Soled, D., Ducas, L., Gong, H., Rossi, M.: LWE with side information: attacks and concrete security estimation. In: Micciancio, D., Ristenpart, T. (eds.) CRYPTO 2020, Part II. LNCS, vol. 12171, pp. 329–358. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-56880-1_12 4. D’Anvers, J.P., Karmakar, A., Roy, S.S., Vercauteren, F., et al.: SABER: ModLWR based KEM algorithm speciﬁcation and supporting documentation. Submission to the NIST post-quantum project (2019/2020). https://www.esat.kuleuven. be/cosic/pqcrypto/saber/ 5. Ding, J., Fluhrer, S., Rv, S.: Complete attack on RLWE key exchange with reused keys, without signal leakage. In: Susilo, W., Yang, G. (eds.) ACISP 2018. LNCS, vol. 10946, pp. 467–486. Springer, Cham (2018). https://doi.org/10.1007/978-3319-93638-3_27 6. Fujisaki, E., Okamoto, T.: Secure integration of asymmetric and symmetric encryption schemes. J. Cryptol. 26(1), 80–101 (2013). https://doi.org/10.1007/s00145011-9114-1 7. Goodwill, G., Jun, B., Jaﬀe, J., Rohatgi, P.: A testing methodology for side channel resistance (2011)  
   
  274  
   
  R. Mi et al.  
   
  8. Guo, Q., Mårtensson, E.: Do not bound to a single position: near-optimal multipositional mismatch attacks against Kyber and Saber. IACR Cryptology ePrint Archive, p. 983 (2022). https://eprint.iacr.org/2022/983 9. Huguenin-Dumittan, L., Vaudenay, S.: Classical misuse attacks on NIST round 2 PQC. In: Conti, M., Zhou, J., Casalicchio, E., Spognardi, A. (eds.) ACNS 2020, Part I. LNCS, vol. 12146, pp. 208–227. Springer, Cham (2020). https://doi.org/ 10.1007/978-3-030-57808-4_11 10. Lyubashevsky, V., Peikert, C., Regev, O.: On ideal lattices and learning with errors over rings. J. ACM 60(6), 43:1–43:5 (2013). https://doi.org/10.1145/2535925 11. Naehrig, M., Alkim, E., et al.: Frodokem learning with errors key encapsulation: algorithm speciﬁcation and supporting documentation. Submission to the NIST post-quantum project (2019/2020). https://frodokem.org/ 12. NIST: Call For Proposals. https://csrc.nist.gov/Projects/post-quantumcryptography/post-quantum-cryptography-standardization/Call-for-Proposals 13. NIST: Preparing for Post-Quantum Cryptography: Informatic (2021). https:// www.dhs.gov/sites/default/ﬁles/publications/post-quantum_cryptography_ infographic_october_2021_508.pdf 14. NIST: Selected algorithms 2022 (2022). https://csrc.nist.gov/Projects/postquantum-cryptography/selected-algorithms-2022 15. Okada, S., Wang, Y., Takagi, T.: Improving key mismatch attack on NewHope with fewer queries. In: Liu, J.K., Cui, H. (eds.) ACISP 2020. LNCS, vol. 12248, pp. 505– 524. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-55304-3_26 16. Poppelmann, T., Alkim, E., et al.: NewHope: algorithm speciﬁcation and supporting documentation (2019/2020). https://newhopecrypto.org/ 17. Qin, Y., Cheng, C., Ding, J.: An eﬃcient key mismatch attack on the NIST second round candidate Kyber. IACR Cryptology ePrint Archive, p. 1343 (2019). https:// eprint.iacr.org/2019/1343 18. Qin, Y., Cheng, C., Zhang, X., Pan, Y., Hu, L., Ding, J.: A systematic approach and analysis of key mismatch attacks on lattice-based NIST candidate KEMs. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021, Part IV. LNCS, vol. 13093, pp. 92–121. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-92068-5_4 19. Ravi, P., Roy, S.S., Chattopadhyay, A., Bhasin, S.: Generic side-channel attacks on CCA-secure lattice-based PKE and KEMs. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2020(3), 307–335 (2020). https://doi.org/10.13154/tches.v2020.i3.307-335 20. Regev, O.: On lattices, learning with errors, random linear codes, and cryptography. In: Gabow, H.N., Fagin, R. (eds.) Proceedings of the 37th Annual ACM Symposium on Theory of Computing, Baltimore, MD, USA, 22–24 May 2005, pp. 84–93. ACM (2005). https://doi.org/10.1145/1060590.1060603 21. Shen, M., Cheng, C., Zhang, X., Guo, Q., Jiang, T.: Find the bad apples: an eﬃcient method for perfect key recovery under imperfect SCA oracles - a case study of Kyber. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2023(1), 89–112 (2023). https://doi.org/10.46586/tches.v2023.i1.89-112  
   
  Quantum Cryptanalysis of OTR and OPP: Attacks on Confidentiality, and Key-Recovery Melanie Jauch and Varun Maram(B) Department of Computer Science, ETH Zurich, Zurich, Switzerland [email protected]  , [email protected]   
   
  Abstract. In this paper, we analyze the security of authenticated encryption modes OTR (Minematsu, Eurocrypt 2014) and OPP (Granger et al., Eurocrypt 2016) in a setting where an adversary is allowed to make encryption queries in quantum superposition. Starting with OTR – or more technically, AES-OTR, a third-round CAESAR candidate – we extend prior quantum attacks on the mode’s unforgeability in the literature to provide the ﬁrst attacks breaking conﬁdentiality, i.e., IND-qCPA security, of AES-OTR in diﬀerent settings depending on how the associated data is processed. On a technical level, one of our IND-qCPA attacks involves querying the quantum encryption oracle on a superposition of data with unequal length; to the best of our knowledge, such an attack has never been modelled before in the (post-)quantum cryptographic literature, and we hence believe our technique is of independent interest. Coming to OPP, we present the ﬁrst key-recovery attack against the scheme which uses only a single quantum encryption query. Keywords: AES-OTR · OPP · Authenticated Encryption · IND-qCPA Security · Key-Recovery · Simon’s Algorithm · Deutsch’s Algorithm  
   
  1  
   
  Introduction  
   
  With the development of large-scale quantum computers on the horizon, the security of widely-deployed cryptographic systems faces new threats. Speciﬁcally, public-key cryptosystems that rely on the hardness of factorization or computing discrete-logs would suﬀer from devastating attacks based on Shor’s algorithm [28]. This has led to the setting of so-called post-quantum secure public-key cryptography gaining a lot of attention over the last few years, which includes eﬀorts by NIST [3] to standardize such quantum-resistant algorithms. Coming to the symmetric-key cryptography setting however, the impact of quantum computers has been assumed to be signiﬁcantly less severe for a long time. It was widely believed that quantum attacks on symmetric primitives such as block ciphers would only improve by a quadratic speed up due to Grover’s algorithm [12], and that we cannot do any better. Naturally, it was hence assumed that c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 275–296, 2024. https://doi.org/10.1007/978-3-031-53368-6_14  
   
  276  
   
  M. Jauch and V. Maram  
   
  it is enough to simply double the key size of the aﬀected primitives to restore the same level of security as in the classical setting. However, the community quickly realized that it is not suﬃcient to just consider the quantum security of standalone primitives such as block ciphers – since they are rarely used in isolation in practice – but to also consider their associated modes of operation. Such modes are typically designed to provide enhanced security guarantees such as conﬁdentiality, integrity, and authenticity of encrypted messages. In this paper, we will be focusing on modes related to authenticated encryption (AE). Starting with the work of Kaplan et al. [18] which showed how to break the authenticity guarantees of classical AE modes such as GCM and OCB in the quantum setting in polynomial-time (in contrast to the generic quadratic speed up oﬀered by Grover’s algorithm), follow-up works by Bhaumik et al. [4] and Bonnetain et al. [7] improved the quantum attacks against the latter OCB modes, albeit still targeting authenticity. At a high-level, the above attacks fundamentally rely on other well-known quantum algorithms such as Simon’s period ﬁnding algorithm [29] and Deutsch’s algorithm [8]. Subsequently, Maram et al. [22] extended the aforementioned authenticity attacks to also break conﬁdentiality of the OCB modes in the quantum setting; more formally, the authors targeted a quantum security notion called “IND-qCPA security” [5], which diﬀers from the classical IND-CPA security notion in that the adversary is allowed to make quantum encryption queries. It is worth pointing out that the practicality of this model (also generically called “Q2 security” in the literature) where the attacker has quantum access to the secret-keyed encryption functionality is still currently debated in the community (e.g., see [2,4,6,16,18]); however we consider this discussion beyond the scope of our work. Now focusing on the OCB modes, they are one of the most well-studied and widely inﬂuential classical AE modes. OCB has three versions: OCB1 [27], OCB2 [26] and OCB3 [19]. While OCB1 and OCB3 are provably secure AE schemes in the classical setting, a breakthrough result by Inoue et al. [13] showed that OCB2 is classically broken as an AE mode. More speciﬁcally, Inoue and Minematsu [14] ﬁrst came up with classical attacks on the authenticity guarantees of OCB2. After their attacks became public, Poettering [25] and Iwata [15] proceeded to extend them to also break conﬁdentiality of OCB2 in the classical setting. In this context, the aforementioned work of Maram et al. [22] can be seen as translating this strategy to the quantum setting to break (IND-qCPA) conﬁdentiality of the three OCB modes starting with the quantum forgery attacks in [4,7,18]. However at the same time, Inoue et al. [13] observed that their classical attacks on OCB2 do not extend to other popular AE designs based on OCB such as OTR [23] and OPP [11]; this is due to some subtle diﬀerences in the structures of these modes when compared to OCB. In this paper, we analyze if one would observe something similar regarding the eﬀects of quantum insecurity of OCB on the OTR and OPP modes. Starting with OTR [23], it is technically a block cipher mode to realize a nonce-based authenticated encryption with associated data (AEAD) scheme. We speciﬁcally focus on an instantiation of the mode with AES as the underyling block cipher called AES-OTR [24], which also includes an additional way to  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  277  
   
  process associated data when compared to the generic OTR. AES-OTR was also a third-round candidate in the CAESAR competition [1] aimed at standardizing a portfolio of authenticated encryption schemes. In the classical setting, AESOTR was shown to oﬀer provable security guarantees as an AE scheme [24]. Coming to the quantum setting however, works by Kaplan et al. [18] and Chang et al. [20] proposed ways to attack the authenticity guarantees of AES-OTR in the quantum superposition model using Simon’s algorithm. However the conﬁdentiality of AES-OTR in the same quantum setting has not been addressed in the literature. Hence, following the work of Maram et al. [22] on the quantum (IND-qCPA) conﬁdentiality of OCB2, and given the status of AES-OTR as a third-round CAESAR candidate, this leads us to pose the question: Is the AES-OTR mode IND-qCPA secure? Coming to OPP [11], it is a (public) permutation-based AE mode unlike OCB and OTR. OPP essentially generalizes OCB3 by replacing the underlying block cipher by an eﬃciently invertible public permutation and a diﬀerent form of masking. This ensures fast encryption and full parallelization, hence making OPP an ideal candidate in applications requiring high eﬃciency of underlying primitives. However in contrast to the OCB and OTR modes, the quantum security of OPP – related to either authenticity or conﬁdentiality – has not been analyzed in the literature at all. This motivates us to ask the following question: Is the OPP mode quantum secure? 1.1  
   
  Our Contributions  
   
  In this paper, we answer the above questions concerning the quantum security of both AES-OTR and OPP modes by presenting tailor-made quantum attacks. Along the way, some of our attacks involve techniques that we believe will be of independent interest to the broader (post-)quantum cryptographic community. Our concrete results are listed below: Attacks on IND-qCPA Security of AES-OTR. In Sect. 3, we present the ﬁrst IND-qCPA attacks against AES-OTR. Speciﬁcally, our quantum attacks are tailored to three diﬀerent settings depending on how the associated data (AD) is processed: namely, the settings with parallel and serial processing of AD, and the setting where no AD is used at all. For the ﬁrst two settings with non-empty AD, our attacks work in the weak adversarial setting where the nonces used by the challenger to answer encryption queries in the IND-qCPA security game are generated uniformly at random (instead of the nonces being chosen by the adversary). On a high level, the attack breaking IND-qCPA security w.r.t. parallel AD processing uses Simon’s algorithm as well as Deutsch’s algorithm to gain raw block cipher access, i.e., the ability to evaluate the underlying block cipher on arbitrary inputs. With this access, it is straightforward to break IND-qCPA security. This attack strategy is  
   
  278  
   
  M. Jauch and V. Maram  
   
  similar to that used in [22] to break conﬁdentiality of the OCB modes. However, we need to make an extra assumption for our attack to be eﬃcient: namely that the authentication tags produced by the AES-OTR encryption oracle are not (signiﬁcantly) truncated. It is worth noting that the speciﬁcation of AESOTR [24] recommends parameters with untruncated tags. But interestingly, in the process we were also able to point out a gap in the quantum cryptanalysis of OCB2’s conﬁdentiality (with non-empty AD) in [22], since the corresponding IND-qCPA attack there uses a similar assumption of untruncated tags implicitly. This was later conﬁrmed by one of the authors [21]. Coming to the setting where no AD is used in AES-OTR, our attack assumes a stronger adversarial setting where the adversary is now allowed to adaptively choose the (classical) nonces for its quantum encryption queries in the IND-qCPA security game. This setting is the same as considered in [22, Section 4.4] with respect to their IND-qCPA attack against OCB2 as a “pure” AE (i.e., no AD) scheme. However, what makes our attack a non-trivial extension of the above attack on OCB2 is that for AES-OTR there is an additional formatting function applied to the nonce before it is AES-encrypted. We thus have to perform some additional steps that increase the overall complexity of our attack. The attack is described in detail in the full version of this paper [17]. Quantum Queries over Unequal -Length Data. Our IND-qCPA attack on AES-OTR w.r.t. serial AD processing involves a novel paradigm which, to the best of our knowledge, has never been considered in the (post-)quantum cryptanalytic literature. Note that in the IND-qCPA security deﬁnition, an adversary is allowed to make encryption queries on a quantum superposition of data. However, according to the laws of quantum physics, a superposition is deﬁned only over states with the same number of qubits. Hence in our IND-qCPA scenario, this translates to the seemingly implicit restriction of an adversary only being able to make superposition queries over equal -length data. In our work, we show how to overcome this restriction by modelling the quantum encryption oracle in the IND-qCPA security game in a way which allows an adversary to also make superposition queries over unequal -length data (see Sect. 3.3 for more details). Furthermore, to give evidence of the power of this new quantum cryptanalytic paradigm, we show how an adversary can immediately gain raw block cipher access in our IND-qCPA attack on AES-OTR with serial AD processing using only Simon’s algorithm; this is in contrast to the IND-qCPA attacks against OCB in [22] and against AES-OTR with parallel AD processing discussed above which also require Deutsch’s algorithm to obtain this raw access. It’s also worth pointing out that using this novel paradigm, we no longer have to rely on the above extra assumption of the AES-OTR authentication tags being untruncated in the serial AD case. Finally, our paradigm can also be extended to cryptanalysis in the more realistic post-quantum setting, i.e., where the adversary has quantum access only to public cryptographic oracles (also called “Q1 security” in the literature) such as hash functions in the so-called Quantum Random Oracle Model (QROM).  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  279  
   
  A Quantum Key-Recovery Attack on OPP. We present the ﬁrst quantum key-recovery attack on OPP in Sect. 4. Our attack is conducted in the weak adversarial setting similar to our IND-qCPA attacks on AES-OTR, where the nonces are chosen uniformly at random by the challenger. In contrast to AESOTR being based on a block cipher which may only be inverted knowing the key, OPP is built upon an eﬃciently invertible public permutation P . We exploit this speciﬁc property to formulate our key recovery attack. On a high level, we are able to recover a value Ω = P (. . . ||K) using only a single quantum encryption query via an application of Simon’s algorithm, where K is the key used in OPP; hence, applying P −1 to Ω allows us to recover the key K.  
   
  2  
   
  Preliminaries  
   
  Notation. Denote by {0, 1}∗ the set of all ﬁnite-length bit strings and {0, 1}8∗ the set of all ﬁnite-length byte strings. We let the parameters n, k, τ , κ ≥ 0 deﬁne the block length, the size of the key, tag, and nonce respectively. For b ∈ N we let [b] := {1, ..., b}. Given x, y ∈ {0, 1}∗ , the concatenation of x and y is denoted as x||y. We let the length of x in bits be denoted as |x| and we deﬁne |x|b := max{1, X/b}. We use the symbols ⊕, , , ≪, ≫ to denote bit-wise XOR, left-shift, right-shift, left-rotation and right-rotation, respectively. Further, we deﬁne the following padding function that, for a given input X, extends it to a desired length m pad0m : {0, 1}≤m → {0, 1}m , X → X||0m−|X| and for 0 ≤ |X| < m, we write X = X||10m−|X|−1 as the 10∗ padding. By msbl (x) we mean the sequence of ﬁrst l left-most bits of the bit sting x and for any non-negative integer q, let bin(q, m) denote the standard m-bit encoding of q. We want to highlight the diﬀerence in notation for the encryption functions used for AES-OTR in Sect. 3 and for OPP in Sect. 4. By OTR-EK,· (·) we indicate the encryption algorithm of AES-OTR with an underlying block cipher (AES to be precise) encryption function EK with key K. On the other hand we use OPP-E(K, ·) to denote the encryption algorithm of OPP with key K that uses a public permutation instead of a block cipher. In the context of AES-OTR we use notations such as 2X, 3X or 7X for an n-bit string X. Following [24], we here interpret X as a coeﬃcient vector of the polynomial in GF(2n ). So by 2X we essentially mean multiplying the generator of the ﬁeld GF(2n ), which is the polynomial x, and X over GF(2n ). This process is referred to as doubling. Similarly, 2i X denotes i-times doubling X and we denote 3X = X ⊕ 2X as well as 7X = 22 X ⊕ 2X ⊕ X. Field multiplication over GF(2n ) for n = 128 can be implemented as  X1 if msb 1 X = 0. 2X = 120 if msb1 X = 1. (X  1) ⊕ 0 10000111 We omit the details here and refer to [24] for further details.  
   
  280  
   
  M. Jauch and V. Maram  
   
  Simon’s Algorithm. Simon’s algorithm is a quantum algorithm that is able to solve the following problem referred to as Simon’s problem. This algorithm is the key element of most of our quantum attacks against AES-OTR and OPP. Definition 1. (Simon’s Problem) Given quantum access to a Boolean function f : {0, 1}n → {0, 1}n (called Simon’s function) for which it holds: ∃s ∈ {0, 1}n : ∀x, y ∈ {0, 1}n f (x) = f (y) ⇐⇒ y ∈ {x, x ⊕ s}, the goal is to ﬁnd the period s of f . This problem of course can be solved in a classical setting by searching for collisions in Θ(2n/2 ), when we are given classical access to the function f . However, when we are able to query the function f quantum-mechanically, and we are thus allowed to make queries of arbitrary quantum superpositions of the form |x|0 → |x|f (x), Simon’s algorithm can solve this problem with query complexity O(n). On a high level, Simon’s algorithm is able to recover a random vector y ∈ {0, 1}n in a single quantum query to f that is orthogonal to the period s, i.e. y · s = 0. This subroutine is repeated O(n) times such that one obtains n − 1 independent vectors where each is orthogonal to s with high probability. Therefore s can be recovered by solving the corresponding system of linear equations. For more details on the subroutine, we refer to [18]. Also [18] showed that Simon’s algorithm recovers the hidden period s with O(n) quantum queries even if f has some “unwanted periods” – i.e., values t = s such that f (x) = f (x ⊕ t) holds with probability ≤ 1/2 over a random choice of x. As we will show, this condition is always satisﬁed in our attacks. Deutsch’s Algorithm. Deutsch’s algorithm solves the following problem. Definition 2. Given quantum access to a Boolean function f : {0, 1} → {0, 1}, the goal is to decide whether f is constant, i.e. f (0) = f (1), or f is balanced, i.e. f (0) = f (1). The algorithm can solve this problem with a single quantum query to f with success probability 1; note that any algorithm with classical access to f would need two queries for the same. To be precise, Deutsch’s algorithm solves the above problem by computing the value f (0) ⊕ f (1) using a single quantum query to f . IND-qCPA Security of AEAD Schemes. Below we deﬁne IND-qCPA security for nonce-based authenticated encryption with associated data (AEAD) schemes; the formal deﬁnitions for such nonce-based AEAD schemes are provided in the full version [17]. Definition 3. (IND-qCPA with random nonces) A nonce-based AEAD scheme Π = (Enc, Dec) is indistinguishable under quantum chosen-plaintext attack (IND-qCPA secure) with random nonces, if there is no eﬃcient quantum adversary A that is able to win the following security game, except with probability at most 12 +  where  > 0 is negligible.  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  281  
   
  Key generation: A random key K ← K and a random bit b ← {0, 1} are chosen by the challenger. Queries: In any order the adversary A is allowed to make two types of queries: – Encryption queries: The challenger ﬁrst randomly chooses a nonce N ← {0, 1}κ and forwards it to A. The adversary now can choose a message-AD pair (M, A), possibly in superposition, and the challenger encrypts (N, A, M ) with the classical nonce N and returns the output (C, T ) to A. – Challenge query: The challenger picks a random nonce N ← {0, 1}κ once more and gives it to the adversary. Afterwards, A chooses two same sized classical message-AD pairs (M0 , A), (M1 , A) and forwards them to the challenger which in turn encrypts (N, A, Mb ) with the previously chosen classical nonce N . The output (C ∗ , T ∗ ) is again given to A. Guess: The adversary outputs a bit b and wins if b = b . Let p be the probability that A wins the above game. Then its IND-qCPA advantage with respect to the AEAD scheme Π is given by AdvIND-qCPA (A) = Π   p − 1 . So Π is said to be IND-qCPA secure under randomly chosen nonces if 2 AdvIND-qCPA (A) of any polynomial-time quantum adversary A is negligible. Π  
   
  3  
   
  Quantum Attacks on Confidentiality of OTR  
   
  The AES-OTR block cipher mode emerged from the Oﬀset Two-Round (OTR) mode [23] as a part of the CAESAR competition [1] and is based on the AES block cipher as proposed in [24]. It is a nonce based authenticated encryption with associated data (AEAD) scheme and provides two methods for associated data processing. AES-OTR has a provable security in the classical setting under the assumption that AES is a pseudorandom function as argued in [24]. However, in this section we will show that we can exploit the way AD is processed in both cases, namely in parallel and serial, to break IND-qCPA security. In this case, we assume a setting where the adversary has quantum access to an encryption oracle and the nonces the challenger uses to answer encryption queries are picked uniformly at random. We even go one step further and break IND-qCPA security of AES-OTR considered as a pure AE scheme, i.e., with empty AD. To do so, we consider a stronger adversarial setting in which the adversary is allowed to pick the classical nonces adaptively; this attack is described in detail in the full version [17]. In the following, we will be extending upon techniques as utilized in [22]. 3.1  
   
  Specifications of AES-OTR  
   
  We begin by describing the AES-OTR mode by following the speciﬁcations as proposed in [24] for the third round of the CAESAR competition. Let n, k, τ, κ as labeled in Sect. 2, where k ∈ {128, 192, 256}, τ ∈ {32, 40, ..., 128} and κ ∈ {8, 16, ..., 120} are of a ﬁxed length. Since AES-OTR uses AES as its underlying  
   
  282  
   
  M. Jauch and V. Maram  
   
  block cipher, n = 128 is ﬁxed as well and we assume EK to denote the AES encryption function with key K. Also, the lengths of both a plaintext M and associated data A are required to fulﬁll |M |, |A| ∈ {0, 1}8∗ such that |M |8 , |A|8 ≤ 264 . We note that [24] provides sets of recommended parameters which imply that for both instantiations of AES-OTR either with AES-128 or AES-256 a 16byte tag should be used. This recommendation becomes relevant for our attack in Sect. 3.2. For further details on the parameters we refer to [24]. Below, we provide a simpliﬁed description of AES-OTR for both variants of processing AD, namely in parallel (on the left) and in serial (on the right). To indicate how the AD is processed, we use p for parallel and s for serial processing and write OTR-EK,p (N, A, M ) or OTR-EK,s (N, A, M ) respectively. We omit the description of the decryption algorithm, as decryption is not relevant for our attacks. We again refer to [24] for the details. To be more precise, Algorithm 4 corresponds to the encryption core and Algorithms 5 and 6 describe the authentication core of the AEAD scheme described in Algorithm 1 and 2 for parallel and serial AD processing respectively. Note that the encryption core of AES-OTR with parallel (normal box) and serial (dashed box) AD processing only diﬀer in the way U is deﬁned. Algorithm 3 outlines how the nonce N is formatted before being incorporated into the mask U , used to encrypt the plaintext. (This formatting plays an important role for our attack with adaptive nonces, as described in the full version [17].) Notice that for a single block message AES-OTR only encrypts it by xor-ing it with some value depending on U . We will exploit this property for our IND-qCPA attacks. Algorithm 1. OTR-EK,p (N, A, M ) 1: 2: 3: 4: 5: 6:  
   
  (C, T E) ← EF-PK,τ (N, M ) if A = ε then T A ← AF-PK (A) else T A ← 0n T ← msbτ (T E ⊕ T A) return (C, T )  
   
  Algorithm 3. Format(τ, N ) return bin(τ mod n, 7)||0n−8−κ ||1||N  
   
  Algorithm 2. OTR-EK,s (N, A, M ) 1: 2: 3: 4: 5: 6:  
   
  if A = ε then T A ← AF-SK (A) else T A ← 0n (C, T E) ← EF-SK,τ (N, M, T A) T ← msbτ (T E) return (C, T )  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  283  
   
  Algorithm 4. EF-PK,τ (N, M ) , EF-SK,τ (N, M, T A) 1: Σ ← 0n 2: U ← EK (Format(τ, N ))   3: U ← 2 EK (Format(τ, N )) ⊕ T A 4: 5: 6: 7: 8: 9: 10: 11: 12: 13:  
   
  L ← U , L# ← 3U M1 ||...||Mm ← M s.t. |Mi | = n for i ∈ {1, ..., m/2 − 1} do C2i−1 ← EK (L ⊕ M2i−1 ) ⊕ M2i C2i ← EK (L# ⊕ C2i−1 ) ⊕ M2i−1 Σ ← Σ ⊕ M2i L ← L ⊕ L# , L# ← 2L# if m is even then Z ← EK (L ⊕ Mm−1 ) Cm ← msb|Mm | (Z) ⊕ Mm  
   
  14: Cm−1 ← EK (L# ⊕ Cm ) ⊕ Mm−1 15: Σ ← Σ ⊕ Z ⊕ Cm 16: L∗ ← L# 17: else if m is odd then 18: Cm ← msb|Mm | (EK (L)) ⊕ Mm 19: Σ ← Σ ⊕ Mm 20: L∗ ← L 21: if |Mm | = n then 22: 23: 24: 25:  
   
  T E ← EK (32 L∗ ⊕ Σ) else T E ← EK (7L∗ ⊕ Σ) C ← C1 ||...||Cm return (C, T E)  
   
  Algorithm 5. AF-PK (A)  
   
  Algorithm 6. AF-SK (A)  
   
  1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11:  
   
  1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11:  
   
  Ξ ← 0n Q ← EK (0n ) A1 ||...||Aa ← A, s.t. |Ai | = n for i ∈ {1, ..., a − 1} do Ξ ← Ξ ⊕ EK (Q ⊕ Ai ) Q ← 2Q Ξ ← Ξ ⊕ Aa if |Aa | = n then T A ← EK (3Q ⊕ Ξ) else T A ← EK (32 Q ⊕ Ξ) return T A  
   
  Fig. 1. Computation of the value T A of the authentication core of AES-OTR with parallel AD processing with Q = EK (0n ).  
   
  Ξ ← 0n Q ← EK (0n ) A1 ||...||Aa ← A, s.t. |Ai | = n for i ∈ {1, ..., a − 1} do Ξ ← EK (Ai ⊕ Ξ) Ξ ← Ξ ⊕ Aa if |Aa | = n then T A ← EK (2Q ⊕ Ξ) else T A ← EK (4Q ⊕ Ξ) return T A  
   
  Fig. 2. Computation of the value T A of the authentication core of AES-OTR with serial AD processing with Q = EK (0n ).  
   
  In the pictorial description of how the value T A in Algorithms 5 and 6 for parallel and serial AD processing is computed, the value Q is used as part of the  
   
  284  
   
  M. Jauch and V. Maram  
   
  masking and is deﬁned as Q = EK (0n ). Note that Q is a constant value and is independent of the nonce N . This is the key observation we use in our attacks in Sects. 3.2 and 3.3 that exploit the way AD is being processed. It is worth mentioning that there are some prior works (speciﬁcally [18,20]) which proposed approaches to attack unforgeability of AES-OTR using Simon’s algorithm when given quantum access to the corresponding encryption oracle. On a high level, the above works exploit the way AD is processed to compute collisions in the intermediate variable T A (see Figs. 1 and 2); we present a detailed description of their attacks in the full version [17]. We will use similar ideas for our following quantum attacks on the conﬁdentiality of AES-OTR. 3.2  
   
  IND-qCPA Attack on AES-OTR with Parallel AD Processing  
   
  In this section, we show that AES-OTR is insecure in the IND-qCPA setting with random nonces when it is used as an AEAD scheme with associated data processed in parallel. In our IND-qCPA attack, we exploit the way AD is processed to ﬁnd collisions for the output value T A of Algorithm 5, as well as exploit the fact that the encryption algorithm essentially performs a one-time pad encryption when given a single-block message. As a general attack strategy, we want to create a periodic function f1 , whose period can be computed using Simon’s algorithm. To construct our Simon’s function f1 , we use a similar approach as described in [22, Section 4.3] for breaking IND-qCPA security of OCB2. Note that the way OCB2 authenticates AD in [22, Figure 3] is (up to multiplication with constants) essentially the same as for AES-OTR in Algorithm 5. Thus, we can follow a very similar argument. We deﬁne f1 : {0, 1}n → {0, 1}τ f1 (A) = OTR-EK,p (N, A||A||0n , ε). As the plaintext is chosen to be empty, the ciphertext is empty as well which is the reason the quantum encryption oracle OTR-EK,p (N, ·) returns a tag of length τ only.1 Now notice that f1 has period s = 3Q = Q ⊕ 2Q since f1 (A) = msbτ (T A ⊕ T E)     = msbτ EK 22 32 Q ⊕ EK (A ⊕ Q) ⊕ EK (A ⊕ 2Q) ⊕ T E     = msbτ EK 22 32 Q ⊕ EK (A ⊕ Q ⊕ 2Q ⊕ 2Q) ⊕ EK (A ⊕ 2Q ⊕ Q ⊕ Q) ⊕ T E     = msbτ EK 22 32 Q ⊕ EK (A ⊕ 3Q ⊕ 2Q) ⊕ EK (A ⊕ 3Q ⊕ Q) ⊕ T E = f1 (A ⊕ s). 1  
   
  We notice that in the context of Simon’s algorithm the domain and the co-domain of Simon’s function are required to be of the same dimension. Since the size of the tag τ is possibly less than n, we technically need to append an additional n − τ bits of zeros (or any other ﬁxed bit string of size n − τ ). Importantly, this does not change the periodicity of f1 , as these bits are ﬁxed. For the sake of convenience however, we refrain from appending them in each step of the analysis of f1 .  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  285  
   
  Following the arguments made in [22, Section 3.1], using a single quantum query to the encryption oracle OTR-EK,p (N, ·), the function f1 can be computed in superposition. We need this to be done in a single quantum query, as the nonce N changes with each quantum query made to the oracle. We thus can apply Simon’s algorithm to f1 , which computes a vector y ∈ {0, 1}n that is orthogonal to the period s = 3Q = 3EK (0n ). Notice that there do not exist any “unwanted periods” as deﬁned in Sect. 2 with overwhelming probability, since we can apply the same reasoning as in [18, Section 5.3] using that AES is a PRP. It is important that the period is independent of the nonce N , such that despite the nonce changing with each quantum query, Simon’s algorithm still returns a random vector y orthogonal to the ﬁxed period. This means, after recovering O(n) such independent orthogonal vectors y, we can recover the value 3Q = 3EK (0n ) and thus the value EK (0n ) as well with O(n) quantum queries to the AES-OTR encryption oracle. For the rest of this section, we assume that AES-OTR is instantiated using the recommended parameter sets from [24]. In particular, we assume that the size of the tag is 16-bytes, i.e., τ = n. It turns out that not only is this assumption necessary for our attack to succeed eﬃciently but is also necessary for the INDqCPA attack on OCB2 in [22, Section 4.3]. We discuss this assumption in detail in the full version [17]. As a next step towards breaking IND-qCPA security we want to gain raw block cipher access i.e. the ability to compute EK (inp) for any given input inp ∈ {0, 1}n . To realize this, we use Deutsch’s algorithm like done in [22] as follows. Having recovered the value Q = EK (0n ), deﬁne two ﬁxed single-block associated data inputs α0 = 32 Q and α1 = 32 Q ⊕ inp for any given input inp ∈ {0, 1}n . We continue by considering the n functions f (i) : {0, 1} → {0, 1}, f (i) (b) = ith bit of { OTR-EK,p (N, αb , ε)}  
   
  (3.1)  
   
  for a random nonce N and empty message. Here again, the output of OTREK,p (N, ·) is only the tag of length τ = n, as the message is kept empty. Following the argument in [22], we can compute f (i) (b) in superposition with a single quantum query to the AES-OTR encryption oracle by also truncating out the unneeded n − 1 bits of the output of OTR-EK,p (N, ·). This gives us the ability to apply Deutsch’s algorithm on f (i) and recover the value f (i) (0) ⊕ f (i) (1) = ith bit of{T E ⊕ EK (α0 ⊕ 32 Q)} ⊕ ith bit of{T E ⊕ EK (α1 ⊕ 32 Q)} = ith bit of{EK (0n ) ⊕ EK (inp)}  
   
  (3.2)  
   
  with a single quantum query. Thus, by applying Deutsch’salgorithm to each of the n functions f (i) , we are able to recover all n bits of EK (0n ) ⊕ EK (inp) and from this, since we already know EK (0n ), we can recover EK (inp). It is worth pointing out, that despite the nonce N changes with each application of   Deutsch’s algorithm, we are still able to recover EK (0n ) ⊕ EK (inp) since it is independent of N ; note that the nonce-dependent value T E gets “xored-out” in Eq. (3.2).  
   
  286  
   
  M. Jauch and V. Maram  
   
  As a result of the observations above, we can now sketch our IND-qCPA attack against AES-OTR with parallel AD processing: 1. Recover the value 3Q and thus the value EK (0n ) using O(n) quantum queries with Simon’s algorithm as discussed above. 2. Pick arbitrary but diﬀerent single-block messages M0 and M1 and deﬁne the associated data to be empty, i.e. A = ε. We now give these values as an input to the challenger and record the nonce N which is used to encrypt Mb as well as the output (C ∗ , T ) of the challenger. 3. Using Deutsch’s algorithm 2n times (i.e., using raw cipher accesstwice)  block   as described above, we compute the value V = EK EK Format(τ, N ) using a total of 2n quantum queries. Here, we use the nonce N the challenger used to encrypt the challenge query. 4. Output the bit b = b if C ∗ = V ⊕ Mb . It remains to show that our attack outputs the correct bit b : To see this, we notice that for a single-block message M and empty AD the output of the encryption oracle is the ciphertext-tag pair (C ∗ , T ) where   C ∗ = msb|M | EK (L) ⊕ M with L = EK (Format(τ, N )) following the description of Algorithm 4. Since the attack relies on recomputing exactly the value used to encrypt Mb in a one-time pad-like manner, the attack succeeds with high probability. 3.3  
   
  IND-qCPA Attack on AES-OTR with Serial AD Processing  
   
  The aim of this section is to break IND-qCPA security of AES-OTR used as an AEAD scheme with random nonces but now with associated data processed in serial by Algorithm 6. We again exploit the way AD is processed to compute collisions for the variable T A. Similar to the attack above, we deﬁne a periodic function f2 whose period can be computed using Simon’s algorithm. In this case however, as a consequence of our choice of f2 , computing the period of the function already gives us raw block cipher access. This is in contrast to the IND-qCPA attack in [22, Section 4.3] and our previous attack in Sect. 3.2, where we ﬁrst recover EK (0n ) and then gain raw block cipher access via Deutsch’s algorithm. In this section, Simon’s function is deﬁned in a very diﬀerent fashion than before, as the function can distinguish two diﬀerent cases depending on an input bit b and treat them accordingly – this aﬀects either having one or two blocks of associated data as an input to the quantum encryption oracle respectively. We therefore also need to argue why we actually have quantum access to Simon’s function we deﬁne below. This is precisely the scenario we described in Sect. 1.1 w.r.t. an adversary being able to query a superposition of (associated) data with unequal length to the quantum encryption oracle. We achieve this by a novel modelling of the encryption oracle in our quantum circuit for f2 , such that the circuit also queries  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  287  
   
  the encryption oracle only once each time f2 is queried; we emphasize the latter aspect is due to the changing nonces as described in Sect. 3.2. At this point however, we make the assumption that we have quantum access to the Simon’s function f2 and are able to compute it with a single query to the quantum encryption oracle. We later validate this assumption and further discuss the quantum accessibility as well as the issue of having to achieve this with a single quantum query near the end of this section. We realize gaining raw block cipher access by choosing an arbitrary B ∈ {0, 1}n (for which we want to know its encryption EK (B)) and set Simon’s function to be f2 : {0, 1}n+1 → {0, 1}τ ,  OTR-EK,s (N, B||A, ε) if b = 0 f2 (b||A) = if b = 1 OTR-EK,s (N, A, ε) where b ∈ {0, 1} is a single bit and A ∈ {0, 1}n represents one block of associated data of size n. Note that here f2 gives us a value in {0, 1}τ , since the ciphertext is empty as a result of the plaintext being chosen as empty. More precisely, for a general set of associated data D and empty plaintext we get by Algorithm 4 and Algorithm 2 that      OTR-EK,s (N, D, ε) = msbτ EK 33 2 T AD ⊕ EK Format(τ, N ) where T AD = AF-SK (D). This implies that the period s of f2 only depends on the function AF-SK (D). Therefore, we deﬁne a new function g : {0, 1}n+1 → {0, 1}n ,  AF-SK (B||A) if b = 0 g(b||A) = if b = 1 AF-SK (A) We claim that g, and therefore f2 as well, has period s = 1||EK (B). Indeed:       g 0||A ⊕ 1||EK (B) = g 1||A ⊕ EK (B) = AF-SK A ⊕ EK (B) = EK (4Q ⊕ A ⊕ EK (B)) = AF-SK (B||A) = g(0||A) (3.3) g(1||A ⊕ 1||EK (B)) = g(0||A ⊕ EK (B)) = AF-SK (B||A ⊕ EK (B))   = EK 4Q ⊕ A ⊕ EK (B) ⊕ EK (B) = EK (4Q ⊕ A) = AF-SK (A) = g(1||A)  
   
  (3.4)  
   
  where Q = EK (0n ), and for Eqs. 3.3 and 3.4, we used the deﬁnition of Algorithm 6. Under the assumption that we can in fact compute f2 in superposition using a single quantum query to the encryption oracle OTR-EK,s (N, ·), we can apply Simon’s algorithm to f2 . Hence, with a similar argument as in Sect. 3.2 we can recover the value s = 1||EK (B) and thus the value EK (B) for any B ∈ {0, 1}n with O(n) quantum queries. It is important to mention that the period s is again  
   
  288  
   
  M. Jauch and V. Maram  
   
  independent of the nonce. So even though the nonce, and hence f2 changes with each quantum query, Simon’s algorithm still returns a random vector orthogonal to the ﬁxed period s = 1||EK (B) in each of the n iterations. We conclude that this grants us raw block cipher access without having to use Deutsch’s algorithm like in the attack described in Sect. 3.2. Unlike our IND-qCPA attack in Sect. 3.2, this attack succeeds with high probability even if the tags were truncated. This is justiﬁed because in the previous section truncation only became an issue when we applied Deutsch’s algorithm. Here, we do not use Deutsch’s algorithm but Simon’s algorithm only. Since the function f2 is still periodic as its periodicity is unaﬀected by the truncation of the tag, running Simon’s algorithm does not run into any issues. Now we can again sketch an IND-qCPA attack against AES-OTR but this time with serial AD processing: 1. Pick arbitrary but diﬀerent single-block messages M0 and M1 and deﬁne the AD to be empty. We give these values as an input to the challenger and record the nonce N which was used to encrypt either M0 or M1 as well as the output (C ∗ , T ) of the challenger.    2. Compute the value V = EK 2 · EK Format(τ, N ) using 2O(n) quantum encryption queries via two applications of Simon’s algorithm (using the raw block cipher access twice as discussed above.). 3. Output the bit b = b if Mb = C ∗ ⊕ V . To see that the above attack succeeds we note that for a single-block message M of size n and empty AD we have      OTR-EK,s (N, ε, M ) = EK 2 · EK Format(τ, N ) ⊕ M. C  
   
  where by |C we indicate truncating out the tag T . This is essentially a one-time pad encryption with mask V . Since we are able to compute V , we also recover the correct bit b . On the Quantum Accessibility of Function f2 . It remains to argue that we actually have quantum access to the function f2 with a single query to the encryption oracle each time we query f2 . This is done by coming up with a suitable quantum circuit that describes f2 such that it uses only a single encryption unitary gate; this is because the nonce N changes with each quantum query made to the encryption oracle. To achieve this, we have to modify the quantum encryption oracle queries in a slight way: we add an additional n-qubit input register which encodes the length of our message. By doing so, the encryption oracle knows how many bits of the message it should actually encrypt. Thus, we also need to deﬁne f2 in a diﬀerent manner:  OTR-E˜K,s (N, bin(2n, n)||B||A, ε) if b = 0. f2 (b||A) = if b = 1. OTR-E˜K,s (N, bin(n, n)||A||0n , ε)  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  289  
   
  The circuit also uses so-called Fredkin gate (as described e.g. in [30, Section 2.2]), which is a controlled swap gate. The Fredkin gate, upon input basis state (b, I1 , I2 ) with b a single bit, produces output (b, O1 , O2 ) where O1 = ¯bI1 + bI2 , O2 = bI1 + ¯bI2 . So the gate essentially swaps the inputs when b = 1 and does nothing if b = 0. Note that the way f2 is deﬁned, both |bin(2n, n)||B||A and |bin(n, n)||A||0n  are 3n qubit states and hence can be swapped (the swap gate can only operate on inputs that are of the same length, as the swapping is done qubit wise). In this case, when the encryption oracle gets such an input, it ﬁrst parses the ﬁrst n qubits of the query to ﬁgure out how many blocks of the input have to be encrypted. For b = 0 it just takes B||A as an input but if b = 1 then it ignores the remaining 0n block of the query and only encrypts A. The corresponding quantum circuit therefore looks as follows (Fig. 3):  
   
  Fig. 3. The quantum circuit implementing f2 using a single quantum query via the gate UOTR-E˜K,s . We indicate the n-bit encoding of n and 2n with bold letters.  
   
  Note that the output of the second register is set to |n||A||0n  if b = 0 and |2n||B||A if b = 1. For the third register it is the other way round. Comparison with the Quantum Attack on CMAC in [18]. Note that the serial AD processing component of AES-OTR (Algorithm 6) is essentially equivalent to the message authentication code CMAC [9]. And Kaplan et al. [18] show how to break the quantum unforgeability of CMAC using Simon’s algorithm (technically they present a quantum attack against the closely related CBC-MAC, but it is straightforward to extend the attack to CMAC). On a high-level, the extension of their CMAC attack to AES-OTR with serial AD processing would proceed as follows. Choose two arbitrary blocks B0 , B1 ∈ {0, 1}n and deﬁne the function g˜(b||A) = AF-SK (Bb ||A) (corresponding to Simon’s function f˜2 (b||A) = OTR-EK,s (N, Bb ||A, ε)). It is not hard to see that g˜ (and f˜2 ) has period s˜ = 1||(EK (B0 ) ⊕ EK (B1 )). Hence we can use Simon’s algorithm as in [18, Section 5.1] to recover the value EK (B0 ) ⊕ EK (B1 ). Also note that we no longer need to query the quantum encryption oracle on a superposition of unequal length AD, unlike our IND-qCPA attack above. However, just knowing EK (B0 ) ⊕ EK (B1 ) is not suﬃcient to obtain the raw block cipher access EK (·) we would need to break IND-qCPA security, even if B0 , B1 are under our control.  
   
  290  
   
  4  
   
  M. Jauch and V. Maram  
   
  Quantum Key-Recovery Attack on OPP  
   
  The Oﬀset Public Permutation Mode (OPP) was proposed in [10] and [11] and it essentially tries to generalize OCB3 by replacing the underlying block cipher by a public permutation and a diﬀerent form of masking. In this section we will show that using a public permutation the way OPP does, actually leads to a devastating key recovery attack in the quantum setting. Our attack uses a similar strategy as the IND-qCPA attack against OCB2 in [22, Section 4.4] with adaptively chosen nonces; but instead of choosing a new nonce adaptively to break IND-qCPA security, we are able to recover the value Ω := P (X||K) where P is an eﬃciently invertible public permutation and K is the key (X can be seen as a formatting of the nonce). In contrast to OTR being based on a block cipher that may only be inverted knowing the key, we are here dealing with a public permutation that can be inverted eﬃciently. This is the key issue of OPP and the reason we are able to recover the key knowing Ω. 4.1  
   
  Specification of OPP  
   
  In this section, it is assumed that the plaintexts we are dealing with always have a size that is a multiple of the block length. As a consequence, it is not necessary to treat the last block of the plaintext any diﬀerently in our analysis, and furthermore we are also excluding the speciﬁcations for how OPP encrypts plaintexts that do not meet this assumption. In the same manner this also applies to the way we describe the processing of associated data. Let n, k, τ, κ as labeled in Sect. 2 such that κ ≤ n − k − 1. We begin by describing the OPP mode as proposed in [10] in a simpliﬁed manner that also maintains consistent labeling of variables in previous descriptions of modes such as OTR. A set of functions Φ = {α, β, γ} is given by α, β, γ : {0, 1}n → {0, 1}n , α(x) = ϕ(x), β(x) = ϕ(x) ⊕ x and γ(x) = ϕ2 (x) ⊕ ϕ(x) ⊕ x where for x = x0 ||...||x15 and xi ∈ {0, 1}64 the function ϕ : {0, 1}1024 → {0, 1}1024 is deﬁned as   ϕ(x0 , ..., x15 ) = x1 , ..., x15 , (x0 ≪ 53) ⊕ (x5  13) . OPP uses the so called tweakable Even-Mansour construction MEM, where a tweak space T of the form T ⊆ {0, 1}n−k × N3 , as outlined in [10, Lemma 4], is considered. For further details about tweaks and tweakable block ciphers we refer to [10] as this speciﬁc notion is not relevant for the subsequent attack.  
   
  : {0, 1}k × T × {0, 1}n → {0, 1}n is then deﬁned The encryption function E as    
   
  E(K, X, ¯i, M ) = P δ(K, X, ¯i) ⊕ M ⊕ δ(K, X, ¯i) where δ : {0, 1}k × T → {0, 1}n is called the masking function and   for ¯i = (i0 , i1 , i2 ) ∈ N3 it is set to be δ(K, X, ¯i) = γ i2 ◦ β i1 ◦ αi0 P (X||K) . For  
   
  ¯i (M ) = E(K,  
   
  convenience the shorthand notation E X, ¯i, M ) is being used in K,X the algorithmic description of OPP.  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  291  
   
  corresponding to E  
   
  is deﬁned in a straightforward The decryption function D manner. However it should be noted that P is a public permutation such that P −1 can be computed eﬃciently, since the inverse permutation is necessary to perform decryption. Below, we present a simpliﬁed description of the OPP mode based on the speciﬁcation in [10]. We only provide a description of the encryption and authentication part of the algorithm, as the details regarding decryption are not relevant to our attack and are therefore omitted. To be precise, the authentication core of OPP is described in Algorithm 9 and the encryption core corresponds to Algorithm 8. Algorithm 7. OPP-E(K, N, AD, M ) 1: 2: 3: 4:  
   
  X ← pad0n−κ−k (N ) C, S ← OPPEnc(K, X, M ) T ← OPPAbs(K, X, AD, S) return C, T  
   
  Algorithm 8. OPPEnc(K, X, M ) 1: 2: 3: 4: 5: 6: 7: 8:  
   
  4.2  
   
  M0 ||...||Mm−1 ← M, s.t. |Mi | = n C←ε S ← 0n for i ∈ {0, ..., m − 1} do  i,0,1 (Mi ) Ci ← E K,X C ← C||Ci S ← S ⊕ Mi  m−1,2,1 (S) return C, E K,X  
   
  Algorithm 9. OPPAbs(K, X, A, S) 1: A0 ||...||Aa−1 ← A, s.t. |Ai | = n 2: S  ← 0n 3: for i ∈ {0, ..., a − 1} do  i,0,0 (Ai ) 4: S ← S ⊕ E K,X 5: return msbτ (S  ⊕ S)  
   
  Our Quantum Key-Recovery Attack  
   
  In this attack, we adapt the techniques used in [22, Section 4.4] for breaking IND-qCPA security of OCB2 with adaptively chosen nonces. However, in this case we are able to recover the key instead. Our attack is focused solely on the encryption part, so OPP is used as a pure AE scheme, and does not make use of the way associated data is processed. This is in contrast to our previous attacks in Sects. 3.2 and 3.3 as there we could exploit the fact that AD processing was not dependent on a nonce but rather on the constant value Q = Ek (0n ). In the case of OPP this is diﬀerent because the nonce is used in the associated data processing, as the value P (X||K) where X = pad0n−κ−k (N ) is dependent on the nonce N (see Algorithm 7). Since the nonce changes with each call to the encryption oracle, OPP never processes a ﬁxed set of AD the same way. This is the reason we can’t apply Simon’s algorithm (which calls the oracle multiple times) in the same manner. Before we formulate the attack itself, we observe a crucial property of the xor of two consecutive ciphertext blocks considered as a function of its corresponding  
   
  292  
   
  M. Jauch and V. Maram  
   
  plaintext blocks. Deﬁne Ω = P (X||K) and recall that OPP encrypts the i-th plaintext block Mi as Ci = P (δ(K, X, (i, 0, 1)) ⊕ Mi ) ⊕ δ(K, X, (i, 0, 1)) where δ(K, X, (i, 0, 1)) = ϕi+2 (Ω) ⊕ ϕi+1 (Ω) ⊕ ϕi (Ω). We now deﬁne functions fi : {0, 1}n → {0, 1}n such that   fi (M ) = P δ(K, X, (i, 0, 1)) ⊕ M ⊕ δ(K, X, (i, 0, 1)), which correspond to the i-th ciphertext block Ci considered as a function of its underlying plaintext block M . Further, by deﬁning s := ϕi+3 (Ω) ⊕ ϕi (Ω) we see that fi (M ⊕ s) ⊕ fi+1 (M ⊕ s)   = P ϕi+2 (Ω) ⊕ ϕi+1 (Ω) ⊕ ϕi (Ω) ⊕ M ⊕ s   ⊕ P ϕi+3 (Ω) ⊕ ϕi+2 (Ω) ⊕ ϕi+1 (Ω) ⊕ M ⊕ s ⊕ ϕi+3 (Ω) ⊕ ϕi (Ω)   = P ϕi+3 (Ω) ⊕ ϕi+2 (Ω) ⊕ ϕi+1 (Ω) ⊕ M   ⊕ P ϕi+2 (Ω) ⊕ ϕi+1 (Ω) ⊕ ϕi (Ω) ⊕ M ⊕ ϕi+3 (Ω) ⊕ ϕi (Ω) = fi+1 (M ) ⊕ fi (M ) So if we deﬁne Fi,i+1 : {0, 1}n → {0, 1}n as Fi,i+1 (M ) = fi (M ) ⊕ fi+1 (M ) we see from the above calculations that Fi,i+1 (M ⊕ s) = Fi,i+1 (M ), i.e., Fi,i+1 is a periodic function with period s = ϕi+3 (Ω) ⊕ ϕi (Ω). The idea is now to apply a linear function to 2n + 1 ciphertext blocks to capture this observation and create a periodic function that itself contains n copies of the periodic function Fi,i+1 from above. To do so consider the function g : {0, 1}(2n+1)n+τ → {0, 1}(n+1)n g(C0 , C1 , ..., C2n , t) = (C0 , C1 ⊕ C2 , ..., C2n−1 ⊕ C2n ). Here, the Ci ’s are n-bit blocks and t is a τ -bit block. It is not hard to see that g is in fact a linear function - i.e., it satisﬁes g(C ⊕ C  ) = g(C) ⊕ g(C  ) for any 2 valid inputs C and C  . Furthermore let f˜N : {0, 1}n → {0, 1}(n+1)n such that f˜N (M1 , ..., Mn ) = g ◦ OPP-E(K, N, ε, 0n ||M1 ||M1 ||M2 ||...||Mn ||Mn )  
   
  (4.1)  
   
  for some randomly chosen nonce N and empty associated data. We can also reformulate f˜N in terms of the functions fi and Fi,i+1 from above. To be precise, it holds   (4.2) f˜N (M1 , ..., Mn ) = f0 (0n ), F1,2 (M1 ), ..., F2n−1,2n (Mn ) . Moreover, we have included an all-zero plaintext block at the beginning, which will be useful later on for verifying correctness of the recovered key. The crucial property required for the success of the attack is that f˜N has n linearly independent periods si i∈[n] where   si = (0n )i−1 ||ϕ2i+2 (Ω) ⊕ ϕ2i−1 (Ω)||(0n )n−i .  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  293  
   
  This directly follows from the observation on the periodicity of Fi,i+1 and the fact that each pair of the two consecutive ciphertext blocks C2i−1 and C2i for i ∈ [n] encrypt the same plaintext block Mi but with diﬀerent mask δ. Following the same argument as in [22, Section 4.4], we can apply [4, Lemma 2] which assures the ability to compute a linear function of a quantum oracle’s output. Therefore, we can compute f˜N with a single quantum query to the OPPE(K, N, ·) oracle. Once more, we can apply Simon’s algorithm to f˜N which, 2 with a single quantum query, recovers a vector y = (y1 , ..., yn ) ∈ {0, 1}n with yi ∈ {0, 1}n ∀i ∈ [n] that is orthogonal to each of the periods si . The algorithm successfully computes such a vector with overwhelming probability as there do not exist any “unwanted periods” to which y could be orthogonal to. We justify this claim by building upon the argument presented in [4, Section 3.2], which treats the absence of “unwanted periods” in a very similar attack on a variant of OCB. We recall that OCB uses a block cipher instead of a public permutation like it is the case for OPP, so we need to adjust the reasoning to the setting of a public permutation. If we assume the existence of an unwanted period s˜ of f˜N with a probability greater than 12 , then at least one of the Fi,i+1 in Eq. 4.2 would also have to admit an unwanted period s˜i,i+1 1 with probability greater than 2n . We now draw upon the reasoning presented in [18, Section 3.2], which shows the non-occurrence of higher order diﬀerentials in the Even-Mansour construction. More accurately, Fi,i+1 admitting such an unwanted period is equivalent to saying that P admits a high-probability higher-order diﬀerential. But these only happen with negligible probability for a random choice of P according to [18]. This argument makes sure that the probability for unwanted periods to appear is bounded and thus Simon’s algorithm computes a vector y as described above with overwhelming probability. By orthogonality of y we get n equations of the form   (4.3) yi · ϕ2i+2 (Ω) ⊕ ϕ2i−1 (Ω) = 0. Before we can proceed, we recall that for x = x0 ||...||x15 and xi ∈ {0, 1}64 the function ϕ is deﬁned as   ϕ(x0 , ..., x15 ) = x1 , ..., x15 , (x0 ≪ 53) ⊕ (x5  13) As described in [10] we see that the function ϕ is in fact a linear map and it therefore can be represented by a matrix M . Following this, Eq. 4.3 is equivalent to   (4.4) yi · M 2i+2 · Ω ⊕ M 2i−1 · Ω = 0 Knowing M and using associativity of matrix multiplication, we are able to solve the n Equations in 4.4 and thus recover the value Ω = P (X||K). But since P is a public permutation and its inverse is assumed to be computable eﬃciently due P −1 being needed for decryption, we can just apply P −1 to Ω and we thus are able to acquire X||K. In particular, we gain possession of the key K. Observe, that in addition when running Simon’s algorithm, we recover the  
   
  0,0,1 (0n ) when we meaﬁxed classical value of the ﬁrst ciphertext block C0 = E K,X sure the quantum register corresponding to the output of f˜N as part of our  
   
  294  
   
  M. Jauch and V. Maram  
   
  application of Simon’s algorithm. It is important that we ﬁx the value to 0n (or any other arbitrary ﬁxed classical value of length n also works) in order for C0 to be a classical value. We sketch our key-recovery attack below: 1. Given access to a quantum encryption oracle of OPP for a random nonce N , i.e., OPP-E(K, N, ε, ·), we recover with a single quantum encryption query  
   
  0,0,1 (0n ) as discussed above. In the classical values Ω = P (X||K) and C0 = E K,X particular, we recover the classical value of the key K. 2. We perform a sanity check on the key: using Ω we recompute the encryption  
   
  of the one-block plaintext 0n with respect to the same key K and nonce N C as used in the above encryption oracle query as    
   
  = P ϕ2 (Ω) ⊕ ϕ(Ω) ⊕ Ω ⊕ ϕ2 (Ω) ⊕ ϕ(Ω) ⊕ Ω. C  
   
  If this turns out to be false we repeat step 1., else we are Check that C0 = C. certain to have recovered the right key K. It remains to argue why this attack is successful. We perform a sanity check in order to assure correctness of the key K. This is where the ﬁrst ciphertext block is useful. Indeed, having recovered the key K as described above, we can now just recompute the encryption of 0n , again with respect to the same key-nonce pair (K, N ) as in the provided encryption oracle, as    
   
  = P δ(K, X, (0, 0, 1)) ⊕ 0n ⊕ δ(K, X, (0, 0, 1)) C   = P ϕ2 (Ω) ⊕ ϕ(Ω) ⊕ Ω ⊕ ϕ2 (Ω) ⊕ ϕ(Ω) ⊕ Ω  
   
  = C0 , i.e. the encryption where Ω = P (X||K) and X = pad0n−κ−k (N ). If now C of the oracle and our manual computation coincide, we can be sure that we recovered the right key K. Else, we can just repeat the attack until the assertion returns to be true. With the included sanity check, we are certain to recover the key K at some point, as step one of our attack involves an application of Simon’s algorithm that already succeeds with high probability thanks to the non-existence of “unwanted periods” as argued before. Finally, we compare our quantum key-recovery attack on OPP with the corresponding attack on the generic Even-Mansour construction of [18] in the full version [17]. Acknowledgements. It is our pleasure to thank Xavier Bonnetain for helpful discussions, and the anonymous reviewers of SAC 2023 for their constructive comments and suggestions.  
   
  Quantum Cryptanalysis of OTR and OPP  
   
  295  
   
  References 1. Caesar: Competition for authenticated encryption: Security, applicability, and robustness, 2012-2019. https://competitions.cr.yp.to/caesar.html. Accessed 23 Mar 2023 2. Alagic, G., Bai, C., Katz, J., Majenz, C.: Post-quantum security of the EvenMansour cipher. In: Dunkelman, O., Dziembowski, S. (eds.) EUROCRYPT 2022, Part III. LNCS, vol. 13277, pp. 458–487. Springer, Cham (2022). https://doi.org/ 10.1007/978-3-031-07082-2 17 3. Alagic, G., et al.: Status report on the third round of the NIST post-quantum cryptography standardization process (2022) 4. Bhaumik, R., et al.: QCB: eﬃcient quantum-secure authenticated encryption. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021. LNCS, vol. 13090, pp. 668–698. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-92062-3 23 5. Boneh, D., Zhandry, M.: Secure signatures and chosen ciphertext security in a quantum computing world. In: Canetti, R., Garay, J.A. (eds.) CRYPTO 2013. LNCS, vol. 8043, pp. 361–379. Springer, Heidelberg (2013). https://doi.org/10. 1007/978-3-642-40084-1 21 6. Bonnetain, X., Hosoyamada, A., Naya-Plasencia, M., Sasaki, Yu., Schrottenloher, A.: Quantum attacks without superposition queries: the oﬄine Simon’s algorithm. In: Galbraith, S.D., Moriai, S. (eds.) ASIACRYPT 2019. LNCS, vol. 11921, pp. 552–583. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-34578-5 20 7. Bonnetain, X., Leurent, G., Naya-Plasencia, M., Schrottenloher, A.: Quantum linearization attacks. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021. LNCS, vol. 13090, pp. 422–452. Springer, Cham (2021). https://doi.org/10.1007/978-3030-92062-3 15 8. Deutsch, D.: Quantum theory, the Church-Turing principle and the universal quantum computer. Proc. R. Soc. Lond. Ser. A 400(1818), 97–117 (1985) 9. Dworkin, M.: Recommendation for block cipher modes of operation: the CMAC mode for authentication. Technical Report NIST Special Publication (SP) 800-38B, National Institute of Standards and Technology, Gaithersburg, MD (2005) 10. Granger, R., Jovanovic, P., Mennink, B., Neves, S.: Improved masking for tweakable blockciphers with applications to authenticated encryption. Cryptology ePrint Archive, Paper 2015/999 (2015). https://eprint.iacr.org/2015/999 11. Granger, R., Jovanovic, P., Mennink, B., Neves, S.: Improved masking for tweakable blockciphers with applications to authenticated encryption. In: Fischlin, M., Coron, J.-S. (eds.) EUROCRYPT 2016. LNCS, vol. 9665, pp. 263–293. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-49890-3 11 12. Grover, L.K.: A fast quantum mechanical algorithm for database search. In: Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, pp. 212–219 (1996) 13. Inoue, A., Iwata, T., Minematsu, K., Poettering, B.: Cryptanalysis of OCB2: attacks on authenticity and conﬁdentiality. In: Boldyreva, A., Micciancio, D. (eds.) CRYPTO 2019. LNCS, vol. 11692, pp. 3–31. Springer, Cham (2019). https://doi. org/10.1007/978-3-030-26948-7 1 14. Inoue, A., Minematsu, K.: Cryptanalysis of OCB2. Cryptology ePrint Archive, Report 2018/1040 (2018). https://eprint.iacr.org/2018/1040 15. Iwata, T.: Plaintext recovery attack of OCB2. Cryptology ePrint Archive, Report 2018/1090 (2018). https://eprint.iacr.org/2018/1090  
   
  296  
   
  M. Jauch and V. Maram  
   
  16. Jaeger, J., Song, F., Tessaro, S.: Quantum key-length extension. In: Nissim, K., Waters, B. (eds.) TCC 2021. LNCS, vol. 13042, pp. 209–239. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-90459-3 8 17. Jauch, M., Maram, V.: Quantum cryptanalysis of OTR and OPP: attacks on conﬁdentiality, and key-recovery. Cryptology ePrint Archive, Paper 2023/1157 (2023). https://eprint.iacr.org/2023/1157 18. Kaplan, M., Leurent, G., Leverrier, A., Naya-Plasencia, M.: Breaking symmetric cryptosystems using quantum period ﬁnding. In: Robshaw, M., Katz, J. (eds.) CRYPTO 2016. LNCS, vol. 9815, pp. 207–237. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-53008-5 8 19. Krovetz, T., Rogaway, P.: The software performance of authenticated-encryption modes. In: Joux, A. (ed.) FSE 2011. LNCS, vol. 6733, pp. 306–327. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-21702-9 18 20. Chang, L., Wei, Y., Wang, X., Pan, X.: Collision forgery attack on the AES-OTR algorithm under quantum computing. Symmetry (2022). https://doi.org/10.3390/ sym14071434 21. Maram, V.: Private communication (2023) 22. Maram, V., Masny, D., Patranabis, S., Raghuraman, S.: On the quantum security of OCB. IACR Trans. Symmetric Cryptol. 2022(2), 379–414 (2022) 23. Minematsu, K.: Parallelizable rate-1 authenticated encryption from pseudorandom functions. In: Nguyen, P.Q., Oswald, E. (eds.) EUROCRYPT 2014. LNCS, vol. 8441, pp. 275–292. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3642-55220-5 16 24. Minematsu, K.: AES-OTR V3.1. Third-Round Candidate Submission to CAESAR Competition (2016). https://competitions.cr.yp.to/round3/aesotrv31.pdf 25. Poettering, B.: Shorter double-authentication preventing signatures for small address spaces. Cryptology ePrint Archive, Report 2018/223 (2018). https:// eprint.iacr.org/2018/223 26. Rogaway, P.: Eﬃcient instantiations of tweakable blockciphers and reﬁnements to modes OCB and PMAC. In: Lee, P.J. (ed.) ASIACRYPT 2004. LNCS, vol. 3329, pp. 16–31. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-3053922 27. Rogaway, P., Bellare, M., Black, J., Krovetz, T.: OCB: a block-cipher mode of operation for eﬃcient authenticated encryption. In: ACM CCS 2001: 8th Conference on Computer and Communications Security, pp. 196–205 (2001) 28. Shor, P.W.: Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM Rev. 41(2), 303–332 (1999) 29. Simon, D.R.: On the power of quantum computation. SIAM J. Comput. 26(5), 1474–1483 (1997) 30. Thapliyal, H., Ranganathan, N., Kotiyal, S.: Reversible logic based design and test of ﬁeld coupled nanocomputing circuits. In: Anderson, N.G., Bhanja, S. (eds.) Field-Coupled Nanocomputing. LNCS, vol. 8280, pp. 133–172. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-43722-3 7  
   
  Fast and Eﬃcient Hardware Implementation of HQC Sanjay Deshpande1(B) , Chuanqi Xu1 , Mamuri Nawan2 , Kashif Nawaz2 , and Jakub Szefer1 1  
   
  2  
   
  CASLAB, Department of Electrical Engineering, Yale University, New Haven, USA {sanjay.deshpande,chuanqi.xu,jakub.szefer}@yale.edu Cryptography Research Centre, Technology Innovation Institute, Abu Dhabi, UAE {mamuri,kashif.nawaz}@tii.ae Abstract. This work presents a hardware design for constant-time implementation of the HQC (Hamming Quasi-Cyclic) code-based key encapsulation mechanism. HQC has been selected for the fourth round of NIST’s Post-Quantum Cryptography standardization process and this work presents the ﬁrst, hand-optimized design of HQC key generation, encapsulation, and decapsulation written in Verilog targeting implementation on FPGAs. The three modules further share a common SHAKE256 hash module to reduce area overhead. All the hardware modules are parametrizable at compile time so that designs for the diﬀerent security levels can be easily generated. The design currently outperforms the other hardware designs for HQC, and many of the fourth-round Post-Quantum Cryptography standardization process, with one of the best time-area products as well. For the combined HighSpeed design targeting the lowest security level, we show that the HQC design can perform key generation in 0.09 ms, encapsulation in 0.13 ms, and decapsulation in 0.21 ms when synthesized for an Xilinx Artix 7 FPGA. Our work shows that when hardware performance is compared, HQC can be a competitive alternative candidate from the fourth round of the NIST PQC competition. Keywords: HQC · Hamming Quasi-Cyclic · PQC · KEM · Key Encapsulation Mechanism · Post-Quantum Cryptography · FPGA Hardware Implementation  
   
  1  
   
  ·  
   
  Introduction  
   
  Since 2016 NIST has been conducting a standardization process with the goal to standardize cryptographic primitives that are secure against attacks aided by quantum computers. There are today ﬁve main families of post-quantum cryptographic algorithms: hash-based, code-based, lattice-based, multivariate, and isogeny-based cryptography. Very recently NIST has selected one algorithm for standardization in the key encapsulation mechanism (KEM) category, CRYSTALS-Kyber, and four fourth-round candidates that will continue in the process. One of the four fourth-round candidates is HQC. It is a code-based KEM based on structured codes. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 297–321, 2024. https://doi.org/10.1007/978-3-031-53368-6_15  
   
  298  
   
  S. Deshpande et al.  
   
  As the standardization process is coming to an end after the fourth round, the performance as well as hardware implementations of the algorithms are becoming very important factor in selection of the algorithms to be standardized. The motivation for our work is to understand how well hand-optimized HQC hardware implementation can be designed and realized on FPGAs. To date, most of the post-quantum cryptographic hardware has focused on lattice-based candidates, with code-based algorithms receiving much less attention. All existing hardware implementations for HQC are based on either high-level synthesis (HLS) [1,3] or are hardware-software co design [20]. Our design is ﬁrst full hardware, handoptimized design of HQC. While HLS can be used for rapid prototyping, in our experience it cannot yet outperform Verilog or other hand optimized designs. Indeed, as we show in this work, our design outperforms the existing HQC HLS design. Further, our design beats the existing hardware-software co-design implementation in terms of time taken to perform key generation, encapsulation, and decapsulation. In addition, our hardware design competes very well with the hardware designs for other candidates currently in the fourth round of NIST’s process: BIKE, Classic McEliece, and SIKE. The presented design has best time-area product as well as time for key generation and decapsulation compared to the hardware for these designs. We also achieve similar time-area product for encapsulation when compared to BIKE. Due to limited breakdown of data for SIKE’s hardware [16] comparison to SIKE for all aspects is more diﬃcult, but we believe our design is better since for similar area cost, their combined encapsulation and decapsulation times are two orders of magnitude larger. Detailed comparison to related work is given in Sect. 3. As this work aims to show, code-based designs such as HQC can be realized very eﬃciently when optimized hardware is developed. Further, our design is constant-time, eliminating timing-based attacks. We believe our work shows that HQC can be a strong contender in the fourth round of NIST’s process. The list of contributions our design includes: – We provide the ﬁrst hand-optimized, fully speciﬁcation-compliant FPGA implementation of HQC, that includes key generation, encapsulation, and decapsulation, as well as a joint design of all three operations, adherent to the latest (fourth-round) HQC speciﬁcation. – We provide an improved SHAKE256 module which is based on Keccak module given in [23]. With our improvement, our hash module design runs two time faster than the existing one from [23]. Also, improving the overall timearea product. – We provide ﬁrst hardware implementations and evaluation for two variants of constant-time ﬁxed-weight vector generation, namely Constant Weight Word ﬁxed-weight vector generation [22] and a novel Fast and Non-Biased ﬁxedweight vector generation algorithm, which is based on ﬁxed-weight vector generation process given in [1]. – We also provide an implementation of a parameterized binary ﬁeld polynomial multiplication unit that uses half the Block RAM when compared to the existing state-of-the-art while providing better performance.  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  299  
   
  – Our designs are constant-time providing protection against the timing sidechannel attacks and providing compile-time parameters to switch between diﬀerent security levels and performances. We evaluate the resource requirements and performance numbers of our designs on a Xilinx Artix 7 FPGA as it is a defacto standard for the evaluation of NIST PQC hardware designs. For all our hardware designs, we report the resource utilization in terms of Slices, Look Up Tables (LUTs)1 , DigitalSignal Processing Units (DSPs), FlipFlops (FF) Block RAM (BRAM). We also report Time which is computed by dividing the number of clock cycles taken per operation by design with the maximum clock frequency of the design. In order to consolidate the overall performance and for comparison with other hardware designs from the literature, we use Time Area Product (T × A = Time × Slices) as a metric. Functional correctness of our modules is ensured by generating the testvectors from the reference software implementation (provided in [2]). These testvectors are then fed into our design via testbenches performing pre- and postsynthesis simulations. The hardware design generated output is then compared with the reference software implementation output. 1.1  
   
  Open-Source Design  
   
  All our hardware designs reported in this paper are fully reproducible. The source code of our hardware designs is available under an open-source license at https://github.com/caslab-code/pqc-hqc-hardware.  
   
  2  
   
  Hardware Design of HQC  
   
  HQC Key Encapsulation Mechanism (HQC-KEM) consists of three main primitives: Key Generation, Encapsulation, and Decapsulation. The algorithms for each primitive are given in Figure 3 of speciﬁcation document [2]. These primitives are built upon the HQC Public Key Encryption (HQC-PKE) primitives also given in Figure 2 of the speciﬁcation document [2], which in turn are composed of more basic building blocks. In this work, we implement optimized and parameterizable hardware designs for all the primitives and the building blocks from scratch. In the following subsections, we brieﬂy discuss all the building blocks and provide comparisons with any existing designs. The main building blocks involved for each of the primitives are as follows: – Key Generation: Fixed weight vector generator, PRNG based random vector generator, polynomial multiplication, modular addition, and SHAKE256 – Encapsulation: Encrypt, SHAKE256 – Decapsulation: Decrypt, Encrypt, SHAKE256 1  
   
  We report both Slices and LUTs in our tables since slices can be often partially used based on the optimization strategy of the synthesis tool, which makes slice utilization not a complete indication of the density of the design.  
   
  300  
   
  2.1  
   
  S. Deshpande et al.  
   
  Modules Common Across the Design  
   
  In this section, we give a high-level overview of hardware designs of the building blocks that are used across the HQC-KEM and HQC-PKE. SHAKE256. HQC uses SHAKE256 for multiple purposes e.g., as a PRNG for ﬁxed weight vector generation and random vector generation in Key Generation, as a PRNG for ﬁxed weight vector generation in Encryption, and for hashing in encapsulation and decapsulation. We improve the SHAKE256 module described in [8] (which was originally designed based on Keccak design from [23]) to perform SHAKE256 operations. We further tailor the SHAKE256 hardware module as per the requirement for our hardware design. Following is a list of improvements we make to the design of the SHAKE256 module: The SHAKE256 from [23] module has a ﬁxed 32-bit data input and output ports, and has a performance parameter (parallel slices) which represents the number of combinatorial logic units that can be run in parallel inside the round function. The SHAKE256 from [23] did not work for parallel slices > 16. We made signiﬁcant changes in the control logic to ﬁx this issue. The design now supports up to parallel slices = 32. The time and area results can be seen in Table 1. We note from Table 1, that the time area product improves as we increase the parallel slices. However, we could not add the support parallel slices beyond 32 due to the other constraints of the state size of SHAKE256 (1600 bits) and ﬁxed data port sizes (32 bits) in the way that SHAKE256 is used in our HQC implementation. The existing SHAKE256 module from [23] operates with a command-based interface where the number of input bits to be processed, and the number of output bits required is speciﬁed before starting the hash operation. Based on the required weight, the ﬁxed weight vector generation process requires pseudorandom bits to be generated from the SHAKE256 module for a speciﬁed input seed. Suppose the generated pseudorandom bits fail to satisfy the conditions to achieve the necessary weight or need a second ﬁxed-weight vector generated from the same seed. In that case, another round of pseudorandom bits is generated from SHAKE256. As per the HQC speciﬁcation, the internal SHAKE256 state is maintained as starting point for generation of the next set of pseudorandom bits. The original SHAKE256 module from [23] was not optimized nor designed to support preserving of the SHAKE256 state between invocations. We modiﬁed parts of datapath and the control logic to preserve the state. Since our modiﬁcation of SHAKE256 holds the current state and does not automatically return to its new input loading state, we modify the operation of the existing forced exit signal to return the SHAKE256 module to the default state. Using the existing module from [23] directly, it was not possible to implement the constant-time solution for the ﬁxed weight vector generation since there is no command to request for additional bytes. We modify the existing design and add an additional command that can request additional bytes. The purpose of adding this command is to support and optimize the overall time taken to generate pseudo-random bits required for the ﬁxed weight vector generation process described in Sect. 2.1. This optimization gives a signiﬁcant amount of  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  301  
   
  Table 1. SHAKE256 module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. The formula for the time-area product, T × A, is (SLICES * Time). Parallel Slices Resources Logic (SLICES) 1 496 2 537 4 560 8 675 16 972 Our Improvement 32 1,654  
   
  (DSP) 0 0 0 0 0  
   
  Memory (FF) (BR) 498 0 466 0 370 0 280 0 236 0  
   
  F (MHz) 163 167 157 158 164  
   
  Cycles (cyc.) 2,408 1,206 604 302 150  
   
  Time (us) 14.77 7.22 3.85 1.91 0.91  
   
  T×A  
   
  (LUT) 1,437 1,558 1,625 1,958 2,819 4,797  
   
  0  
   
  191  
   
  166  
   
  74  
   
  0.45  
   
  744  
   
  0  
   
  7,325 3,877 2,156 1,289 884  
   
  improvement in terms of clock cycles (time), for e.g., in the case of the hqc128 parameter set, the number of clock cycles taken by the SHAKE256 module from [23] to facilitate the random bits needed for generating the second ﬁxed weight vector is equal to 639 clock cycles. With our improvements to the module, we achieve the same in 434 clock cycles (i.e., 32% improvement in time). And this improvement is up to 65% in larger parameter sets of HQC. In addition to the aforementioned changes, we further explored options for optimizing the maximum clock frequency by pipelining the critical path. We note that there are several such critical paths throughout the design, and pipelining each path added severe overhead in terms of clock cycles with minimal improvement in the maximum clock frequency. Consequently, the results presented in Table 1 are optimal time and area results for the given hardware architecture. We use a similar performance parameter parallel slices as described in the original keccak/SHAKE256 design in [23]. The SHAKE256 module has a ﬁxed 32-bit data ports, and data input and output is based on typical ready-valid protocol. The results targeting Xilinx Artix 7 xc7a200t FPGA are shown in Table 1. The clock cycle numbers provided in the Table 1 are for processing 320-bits input (sample input size chosen as per the seed size used in HQC) and generating one block of output (where each block size is 1088-bits). There are six options for the parallel slices, which provide diﬀerent time-area trade-oﬀs. We choose parallel slices = 32 as it provides the best time-area product. For brevity, we represent all the ports interfacing with the SHAKE256 module with ⇔ in all further block diagrams in this paper. Polynomial Multiplication. HQC uses polynomial multiplication operation in all the primitives of HQC-KEM. The polynomial multiplication operation is multiplication of two polynomials with n components in F2 . After proﬁling all the polynomial multiplication operations from the HQC speciﬁcation document and the reference design [2], we note that in all the polynomial multiplication operations, one of the inputs is a sparse ﬁxed weight vector (with weight w or wr given in Table 5 of the speciﬁcation document [2]) of width n-bits. Consequently, we design a sparse polynomial multiplication technique with an interleaved reduction X n −1 (values of n can be found in Table 5 of the speciﬁcation document [2].  
   
  302  
   
  S. Deshpande et al.  
   
  The motivation behind our polynomial multiplication unit is as follows: we represent the non-sparse arbitrary polynomial as arb poly and the sparse ﬁxedweight polynomial by sparse poly. For sparse poly, rather than storing the full polynomial we only store the indices for non-zero values. Then, the multiplication is performed by left shifting arb poly with each index of sparse poly and then performing reduction of the resultant vector in an interleaved fashion. Since the value of n is large in all parameter sets of HQC, we take a sequential approach for performing the left shift. We implement a sequential left shift module similar to one in [12]. The shift module described [12] uses a register based approach and is not scalable when the length of the input is as large as the n value for the HQC parameters (due to a larger resource utilization and complex routing). This issue is circumvented in our design by implementing a block RAM based sequential variable shift module with a dual port BRAM and small barrel rotation unit. The barrel rotation unit and the block RAM widths are used as performance parameter (BW - Block Width) for the shift module and in turn for the whole polynomial multiplication unit. A similar implementation of sequential variable shift module was previously described in [10], however we could not readily use their implementation because the shift module is tightly embedded with the other modules for a diﬀerent application and we re-implemented our version. The hardware design of our polynomial multiplication module (poly mult) is given in Fig. 7b of our extended online version of this work [11]. The arb poly input to the poly mult module is loaded sequentially and the width is of each chunk of arb poly is equal to BW (making total number of chunks in polynomial equal to RAMDEPTH = ceil(n/BW)). We store the least signiﬁcant part of the polynomial at the lowest address of the block RAM and the most signiﬁcant part at the highest address. Since the polynomial length in HQC parameters is equal to n and is not divisible by BW (n is a prime) we pad the most signiﬁcant part of the polynomial with zeros. For sparse poly, one index is loaded at a time. While performing the shift operation we also perform the reduction (X n − 1) in an interleaved fashion. As the result of multiplying two n-bit polynomials could be a 2n-bit polynomial and reduction of 2n-bit polynomial to (X n − 1) in F2 is equivalent to slicing of the 2n-bit polynomial into two parts of n-bit polynomials and then performing a bitwise XOR. As result, when the shift operation is performed on each chunk we also compute the address value (ADDR 2N) (signifying the degree of the resultant polynomial). If we notice that this degree of the resultant polynomial is greater that n we perform XOR of this chunk to the lower chunk by decoding the address based on the value of ADDR 2N. We perform similar operation over all the indices of the sparse poly to achieve the ﬁnal multiplied resultant value. The clock cycles taken by our poly mult module for one polynomial multiplication can be computed using the following formula where WSP ARSE is weight of the sparse polynomial, n is length of the polynomial, BW is the block width, 3 cycles represents the number of pipeline stages and 2 cycles are for the start and done synchronization with interfacing modules. The clock cycles taken for shift and interleaved reduction for one index is (3 +ceil(n/BW)). Our poly mult module is constant time and we achieve that by ﬁxing the WSP ARSE to a speciﬁc value (w and wr ) based on the parameter set.  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  303  
   
  Table 2. Comparison of our area and timing information poly mult module with the other sparse polynomial multiplication units targeting Artix 7 board with xc7a200t FPGA chip. BW (bits)  
   
  Resources Logic Memory F Cycles (SLICES) (LUT) (DSP) (FF) (BR) (MHz) (cyc.)  
   
  Time T × A (us)  
   
  ∗ Our poly mult module, Polynomial Length∗ = 12,323, WSP ARSE = 71 32 134 396 0 181 1 270 27,621 0.10 14 202 599 0 205 2 277 13,918 0.05 10 64 486 1,438 0 456 4 238 7,102 0.03 14 128 ∗ General Sparse Multiplier, Polynomial Length∗ = 12,323, WSP ARSE = 71 [17] 32 132 319 0 127 2 234 27,691 0.12 16 64 197 549 0 190 4 222 13,988 0.06 12 378 1,136 0 381 8 185 7,172 0.04 15 128 ∗ Sparse Multiplier, Polynomial Length∗ = 10,163, WSP ARSE = 71 [15] 32 100 — — 2 240 15,8614 0.66 66 157 — — 3 220 90,880 0.41 64 64 292 — — 5 210 51,688 0.24 70 128 †=  
   
  Slices (no info on LUTs), + Length of the non-sparse arbitrary polynomial, Weight of the sparse polynomial input  
   
  ∗  
   
  =  
   
  latencypoly mult = WSP ARSE × (3 + ceil(n/BW)) + 2 Table 2 shows the results for our poly mult module compared with the related work. We note that our sparse polynomial multiplication module performs better in terms of time while utilizing half the Block RAM resources when compared to the existing designs. Table 3 shows results for our poly mult module for the parameter sizes used for HQC hardware design. Polynomial Addition/Subtraction. HQC uses polynomial addition/ subtraction in all of its primitives. Since all addition and subtraction operations happen in F2 , the addition and subtraction could be realized as the same operation. We design two variants of constant-time adders namely xor based adder and location based adder that could be attached with our polynomial multiplication module described in Sect. 2.1. We design our adder modules as an extension for polynomial multiplication because the addition/subtraction always appears with the polynomial multiplication as shown in Fig. 3 of the speciﬁcation document [2]. The adders operate on the contents of block RAM since the polynomials are stored inside the block RAM. Both of the adder module designs do not use any additional block RAM resources, they load the polynomial multiplication output, perform the addition, and write the value back to the same block RAM inside the polynomial. The xor based adder design performs addition in a regular F2 fashion by performing bit-wise exclusive-OR operation. The module performs addition sequentially by generating one block RAM address per clock cycle to load inputs from two block RAMs and then performs addition and writes them back to one of the speciﬁed block RAMs at the same block RAM address.  
   
  304  
   
  S. Deshpande et al.  
   
  Table 3. Time and area information of our poly mult module for diﬀerent HQC parameter sizes with diﬀerent performance parameter (BW) sizes, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Input Length+ (bits)  
   
  ∗ Wsparse Resources Logic Memory F Cycles (SLICES) (LUT) (DSP) (FF) (BR) (MHz) (cyc.)  
   
  Time T × A (us)  
   
  Our poly mult module (BW = 32) 17,669 (hqc128) 66 35,851 (hqc192) 100 57,637 (hqc256) 131  
   
  139 131 134  
   
  412 387 397  
   
  0 0 0  
   
  189 193 199  
   
  1 2 2  
   
  287 257 267  
   
  36,698 0.13 112,402 0.44 236,457 0.89  
   
  18 57 119  
   
  620 649 644  
   
  0 0 0  
   
  245 249 223  
   
  2 2 2  
   
  270 286 283  
   
  18,482 0.07 56,402 0.20 118,426 0.42  
   
  14 43 91  
   
  1,439 1,445 1,448  
   
  0 0 0  
   
  496 500 474  
   
  4 4 4  
   
  238 240 245  
   
  9,374 28,402 59,476  
   
  19 58 119  
   
  Our poly mult module (BW = 64) 17,669 (hqc128) 66 35,851 (hqc192) 100 57,637 (hqc256) 131  
   
  209 219 218  
   
  Our poly mult module (BW = 128) 17,669 (hqc128) 66 35,851 (hqc192) 100 57,637 (hqc256) 131 +  
   
  486 488 489  
   
  Length of the non-sparse polynomial,  
   
  ∗  
   
  0.04 0.12 0.24  
   
  = Weight of the sparse polynomial input  
   
  Table 4. Polynomial addition modules (xor based adder and loc based adder with datapath width 128-bits) area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Input Length (bits)  
   
  Resources Logic Memory F Cycles Time T × A (SLICES) (LUT) (DSP) (FF) (BR) (MHz) (cyc.) (us)  
   
  xor based adder (BW = 128) 17,669 74 143 73 142 35,851 73 142 57,637 loc based adder (BW = 128) 17,669 86 160 35,851 88 161 92 161 57,637  
   
  0 0 0  
   
  159 161 161  
   
  0 0 0  
   
  330 318 311  
   
  142 284 455  
   
  0.43 0.89 1.46  
   
  31 65 106  
   
  0 0 0  
   
  174 174 175  
   
  0 0 0  
   
  316 300 300  
   
  69 103 134  
   
  0.22 0.34 0.45  
   
  18 30 41  
   
  The location based adder is an optimized adder designed to perform addition when one of the input is a sparse vector. This module is mainly designed to perform operations x + h · y from KeyGen algorithm from Fig. 3 of speciﬁcation document [2] and r1 + h · r2 and s · r2 + e from Encrypt Algorithm from Fig. 2 of speciﬁcation document [2]. In these operations the values of x, r1 , and e are sparse, ﬁxed-weight vectors so the addition is optimized by only ﬂipping the bits of the other input in the position of one. The location based adder module takes location of ones from the sparse vector as input and computes the address to load out the part of non-sparse polynomial from the block RAM and ﬂips the bit on the appropriate location and writes it back to the same location. The process is repeated until all locations with ones are covered. Since there are a ﬁxed, and known number of ones in the ﬁxed-weight vector, there is a ﬁxed  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  305  
   
  Fig. 1. Hardware design of Constant Weight Word Fixed-Weight vector generator (fixed weight vector cww) module.  
   
  number of operations and timing does not reveal any sensitive information. Results of our polynomial addition location based adder module for one performance parameter (width = 128) are shown in Table 4. Fixed-Weight Vector Generator. The ﬁxed-weight vector generator function generates a uniformly random n-bit ﬁxed-weight vector of a speciﬁed input weight (w). The algorithm for a ﬁxed-weight generation as speciﬁed in [1] ﬁrst generates 24×w random bits. These random bits are then arranged into w 24-bit integers. These 24-bit integers undergo a threshold check and are rejected if the integer value is beyond the threshold. (949 × 17, 669, 467 × 35, 851, 291 × 57, 637 for hqc-128, hqc-192 and hqc-256 respectively). After the threshold check, these integers are reduced modulo n. After the threshold check and reduction process, if the weight is not equal to w, then more random bits are drawn from RNG, and the process is repeated until w integers are achieved. After the threshold check and reduction then, a check for duplicates is performed over all the reduced integers. In case any duplicate is found, that integer is discarded, and more random bits are requested drawn from the RNG, which again undergoes threshold check, reduction, and duplicate check. This process is repeated until a uniform ﬁxed-weight vector is generated. The main pitfall with the ﬁxed-weight vector approach proposed in [1] is that it may show non-constant-time behavior in the rejection sampling process (i.e., the threshold check and duplicate detection as discussed earlier). A timing attack on existing software reference implementation of HQC [1] was performed in [13]. The authors use the information of rejection sampling routine (that is part of ﬁxed-weight generation) being invoked during the deterministic reencryption process in decapsulation and show that this leaks secret-dependent timing information. The timing of the rejection sampling routine depends upon the given seed. This seed is derived for the encrypt function in encapsulation and decapsulation procedures using the message. The decapsulation operation is dependent on the decoded message and this dependency allows to construct a plaintext distinguisher (described in detail in [13]) which is then used to mount the timing attack. Although the attack has not been demonstrated on a hardware implementation of HQC yet, we implement two variants of ﬁxed-weight generation to  
   
  306  
   
  S. Deshpande et al.  
   
  Table 5. Constant Weight Word (CWW) and Fast Non-Biased (FNB) ﬁxed-weight vector module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. For FNB design (discussed in Appendix 1.A), the wr parameter is derived from Table 5 of the speciﬁcation document [2]. The CWW design is fully constant-time so no ACCEPTABLE REJECTIONS parameter is required. Design  
   
  Weight (wr )  
   
  Resources Logic (SLICES)  
   
  (LUT)  
   
  Memory  
   
  F  
   
  Cycles  
   
  Time  
   
  (DSP)  
   
  (FF)  
   
  (MHz)  
   
  (cyc.)  
   
  (us)  
   
  (BR)  
   
  T × A  
   
  Failure+ Prob.  
   
  Constant Weight Word (CWW) hqc128  
   
  75  
   
  67  
   
  201  
   
  4  
   
  229  
   
  1.0  
   
  201  
   
  3,062  
   
  15.23  
   
  1,020  
   
  0  
   
  hqc192  
   
  114  
   
  71  
   
  211  
   
  5  
   
  245  
   
  1.0  
   
  200  
   
  6,817  
   
  34.09  
   
  2,420  
   
  0  
   
  hqc256  
   
  149  
   
  72  
   
  216  
   
  5  
   
  248  
   
  1.0  
   
  204  
   
  11,487  
   
  56.31  
   
  4,054  
   
  0  
   
  Fast and Non-Biased Design with ACCEPTABLE REJECTIONS = wr (discussed in Appendix 1.A) hqc128  
   
  75  
   
  106  
   
  316  
   
  0  
   
  124  
   
  2.0  
   
  223  
   
  1,479  
   
  6.63  
   
  702  
   
  2.8 × 2−199  
   
  hqc192  
   
  114  
   
  100  
   
  295  
   
  0  
   
  125  
   
  2.0  
   
  236  
   
  2,226  
   
  9.43  
   
  1,075  
   
  1.1 × 2−280  
   
  hqc256  
   
  149  
   
  107  
   
  314  
   
  0  
   
  192  
   
  2.5  
   
  242  
   
  3,248  
   
  13.42  
   
  1,435  
   
  4.9 × 2−355  
   
  +  
   
  = Probability of our design failing to behave constant-time.  
   
  Algorithm 1.  
   
  Constant Weight Word Fixed Weight Vector Generation  
   
  Input: N , w, seed Output: w distinct elements in range 0 to N − 1 1: rand bits ← prng(input = seed, output size = 32 × w) 2: for i ← w − 1 to 0 do 3: pos[i] = i + (rand bits[32 + 32 ∗ i − 1 : 32 ∗ i])%(N − i) 4: end for 5: for j ← w − 1 to 0 do 6: duplicate f ound ← 0 7: for k ← j + 1 to w − 1 do 8: if pos[j] == pos[k] then 9: duplicate f ound ← 1 10: end if 11: end for 12: if duplicate f ound == 1 then 13: pos[j] = j 14: end if 15: end for 16: return pos  
   
  prevent the attack from being possible in hardware. The two variants are Constant Weight Word Fixed-Weight Vector Generation and Fast and Non-Biased Fixed-Weight Vector Generation; discussed in Appendix 1.A due to limited space. Constant Weight Word (CWW) Fixed Weight Vector Generation: The CWW ﬁxed-weight vector generation variant comes as a fourth-round recommendation from the HQC authors [2]. It was introduced as a ﬁx for the non-constant time behavior of the earlier ﬁxed-weight algorithm (given in [1]) at the cost of small bias. The CWW was originally proposed by Sendrier in (Algorithm 5 of [22]). Shown in Algorithm 1, we rewrite the CWW ﬁxed-weight vector generation algorithm as implemented in our hardware design. The CWW ﬁxed-weight algorithm ﬁrst generates 32 × w random bits. These random bits are arranged into 32-bit integer array with indices 0 to w − 1. Each 32-bit integer from the array is then  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  307  
   
  modulo-reduced to N - ARRAY INDEX, and the reduced number is then added with the ARRAY INDEX. After the reduction, a compare and swap is performed, as shown in steps 5–14 of Algorithm 1. This compare and swap step ensures no duplicate elements exist in the ﬁnal ﬁxed-weight vector. Our hardware design uses the SHAKE256 module (described in Sect. 2.1) as the PRNG. The 32-bit interface from our SHAKE256 module helps us avoid the 32-bit arrangement of random bits (given in steps 2–4 of Algorithm 1). We design a pipelined Barrett reduction [6] unit to perform the modular reduction (where both the input and modulo value can be changed at runtime, note that in most other design the modulo is ﬁxed at compile time which makes the design of the reduction unit simpler). The operation is shown in step 3 of Algorithm 1. To perform the integer multiplication inside the Barrett reduction, we design karatsuba multiplication [7] unit using the Digital Signal Processing (DSP) units available on the target FPGA. If the DSP resources are unavailable on the target FPGA, we note that our design can naturally be synthesized to use LUTs. We store the reduced values in a dual-port BRAM (pos BRAM shown in Fig. 1) of depth w. Once the BRAM is ﬁlled, we perform the compare and swap step with the help of the control logic interfaced with the two ports on the BRAM. We note that the pseudorandom number generation, Barrett reduction is performed in constant-time and since the compare and swap procedure is always over a ﬁxed number of memory locations, we achieve a fully constant time hardware implementation for the ﬁxed-weight vector generation process. Although the CWW algorithm ensures the constant time behavior in generating ﬁxed-weight vectors, there is a small bias between the uniform distribution and the algorithm’s output. The security analysis performed in [22] for BIKE’s parameters [4] shows that this bias has negligible impact on security. The time and area results for our hardware design are given in Table 5. Because the compare and swap operation requires combinatorial logic between two ports of the dual port BRAM, this becomes the critical path for the design. The compare and scan step takes w × (w − 1)/2 clock cycles. Consequently, as shown in the Table 5, as w increases, the number clock cycles also increases. 2.2  
   
  Encode and Decode Modules  
   
  The encode and decode modules are building blocks of the encrypt and decrypt modules, respectively. We describe the encode and decode modules here, before describing the bigger encrypt and decrypt modules in Sect. 2.3. Encode Module. As speciﬁed in [2], HQC Encode uses concatenation of two codes namely Reed–Muller and Reed–Solomon codes. The hardware design of our encode module is shown in Fig. 2. The Encode function takes K-bit input and ﬁrst encodes it with the Reed–Solomon code. The Reed–Solomon encoding process involves systematic encoding using a linear feedback shift register (LFSR) with a feedback connection based on the generator polynomial (shown on page 23, section 2.5.2 of [2]). The Reed–Solomon code generates a n1 -bit output (as given in [2] the value for n1 is 368, 448, and 720 for hqc128, hqc192,  
   
  308  
   
  S. Deshpande et al.  
   
  Fig. 2. Hardware design of encode module (formed by concatenating two encode functionalities, Reed-Solomon on the left side and Reed-Muller on the right side). Table 6. Encode module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design Resources Logic Memory F Cycles Time T × A (SLICES) (LUT) (DSP) (FF) (BR) (MHz) (cyc.) (us) hqc128 280 858 0 922 2 270.34 97 1,011 0 1,088 2 298.32 131 hqc192 358 1,503 0 1,689 2 293.51 189 hqc256 514 HLS design - {Reed–Solomon Encode + Reed–Muller Encode} [3] hqc128 — 2,019 0 603 0 — 7,244  
   
  0.36 0.44 0.64  
   
  100 157 331  
   
  47.18 —  
   
  and hqc256 respectively). For the Galois ﬁeld multiplication unit (for the ﬁeld F2 [x]/(x8 + x4 + x3 + x2 + 1)) we design an LFSR-based optimized multiplication unit similar to the one described in [19]. The number of Galois ﬁeld multiplication units we run in parallel is equal to the degree of the generator polynomial. The outputs from Galois ﬁeld multipliers are fed in to a LFSR after each cycle. At the end of encoding process the module generates a n1 -bit output. The n1 -bit output from Reed–Solomon code is then encoded by Reed–Muller code. The Reed–Muller encoding is achieved by performing vector-matrix multiplication where each byte from input is the vector and the matrix is the generator matrix (G) shown in Appendix 1A of our extended online version of this work [11]. In our design we store the generator matrix rows (each row is of length 128bits) in ROM and we select the matrix rows based on each input byte. We store the output after multiplying input byte into a block RAM in chunks of 128-bits. Based on the security parameter set the code word output from Reed–Muller code has a multiplicity value (i.e., number of times a code word or in our case number of times each block RAM location is repeated). As per the speciﬁcation [2], hqc128 has multiplicity value of 3 and hqc192 and hqc256 have multiplicity value of 5. To optimize the storage, we only store one copy of code word, and while accessing the code word we compute the block RAM address in a way that the multiplicity is achieved. The time and area results for our encode module targeting Artix 7 board with xc7a200t-3 FPGA are shown in Table 6.  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  309  
   
  Table 7. Decode module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design Resources Logic Memory F Cycles Time (SLICES) (LUT) (DSP) (FF) (BR) (MHz) (cyc.) (ms)  
   
  T×A  
   
  hqc128 952 hqc192 1,100 hqc256 1,243  
   
  19 33 50  
   
  2,817 3,257 3,679  
   
  0 0 0  
   
  3,779 2.5 4,727 2.5 5,574 2.5  
   
  205 212 206  
   
  4,611 5,485 9,199  
   
  0.02 0.03 0.04  
   
  HLS design - {Reed–Muller Decode + Reed–Solomon Decode} [3] hqc128 —  
   
  10,154 0  
   
  2,569 3  
   
  —  
   
  68,619 592.00 —  
   
  Decode Module As given in the speciﬁcation [2], the ciphertext is ﬁrst decoded with duplicated Reed-Muller code and then with shortened Reed-Solomon code. To decode duplicated Reed-Muller code, the transformation module expands and adds multiple code words into expanded code word, and then the Hadamard transformation is applied to the expanded code word. Finally, Find Peak module ﬁnds the location of the highest absolute value of the Hadamard Transformation output. Figure 6 in our extended online version of this work [11] describes detailed hardware design of Reed-Muller Decoder. expand and sum module collects data inputs into m x 128-bit shift register, then add and shift the last 2-bit lsb of each shift register to produce a pair of data outputs. The data pair is then processed in hadamard transformation module which consist of 7 layers of similar blocks of radix-2 butterﬂy structure. With the outputs from hadamard transformation coming in pairs, ﬁnding peak can be done in parallel inside the Find Peak module and compare the peaks of each to be the ﬁnal peak. The whole processes then repeated n1 times to produce n1 data output to Reed-Solomon Decoder. To decode Reed-Solomon code, we need to sequentially compute syndromes Si , coeﬃcients σi of error location polynomial σ(x), roots of error location polynomial (αi )−1 , pre-deﬁned helper polynomial Z((αi )−1 ), errors ei , and ﬁnally correct the output of decode of Reed-Muller code based on the errors. Evaluation. Table 6 and Table 7 show time and area results for our decode module. Out of the existing other hardware designs [2,3,20] (i.e., HLS and hardware-software codesign), only [3] provides a breakdown of the performance for encode and decode modules. As shown in Table 7 and Table 6 our hardware design outperforms the other designs by a signiﬁcant margin in all aspects. To the best of our knowledge our implementation of encode and decode is the ﬁrst hand-optimized hardware implementation of concatenated encode (shortened Reed–Solomon encode + duplicated Reed–Muller encode) and decode (duplicated Reed–Muller decode and shortened Reed–Solomon decode) modules. There are other hand-optimized hardware designs in the literature for Reed–Solomon and Reed–Muller encode and decode (given in [5,14,21]), but their target was not a cryptographic application. Hence, the implementation strategy is highly  
   
  310  
   
  S. Deshpande et al.  
   
  focused on timing, throughput, and area performance rather than a secure implementation (e.g., constant-time). Consequently, although our design is very eﬃcient, it will not be fair to compare our hardware implementations with them. 2.3  
   
  Encrypt and Decrypt Modules  
   
  The encrypt and decrypt modules are building blocks of the encapsulation and decapsulation modules, respectively. We describe the encrypt and decrypt modules here, before describing the bigger encapsulation and decapsulation modules later.  
   
  Fig. 3. Hardware design of decode module (formed by concatenating two decode functionalities, Reed-Muller on the left side and Reed-Solomon on the right side).  
   
  Fig. 4. Hardware design of encrypt and decrypt modules.  
   
  Encrypt Module. The encrypt module (shown in Fig. 2 of the speciﬁcation document [2]) takes public key (h, s), message m, and seed (θ) and generates a ciphertext (u,v) as the output. The hardware design for the encrypt module is shown in Fig. 4a. We use our CWW ﬁxed-weight vector module (fixed weight vector cww) described in Sect. 2.1.A to generate r1 , r2 , and e ﬁxed-weight vectors of weight wr by expanding theta in and in parallel we run encode module (described in Sect. 2.2). After the generation of r2 we start the polynomial multiplication of h.r2 in parallel to the e generation. For polynomial multiplication, we use the poly mult module with BW = 128 described in Sect. 2.1. The addition of r1 in u computation and e in v computation is performed by our location based adder and addition with t is performed by xor based adder (described in Sect. 2.1).  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  311  
   
  From the encrypt algorithm (given in Fig. 2 of the speciﬁcation document [2]), we observe that both h.r2 and s.r2 multiplications can be performed in parallel, consequently, we design a parallel encrypt module targeting higher performance where we use two polynomial multiplications in parallel (shown in Fig. 8a of our extended online version of this work [11]). We provide a choice of using either encrypt or parallel encrypt module as a parameter. Table 8 shows results for both encrypt hardware implementations targeting Xilinx Artix 7 xc7a200t FPGA. We note that the major contributor to the overall time in encrypt operation is due to polynomial multiplication and using two poly mult modules in parallel reduces the overall time by 40–60% across diﬀerent parameter sets. The area results do not include the SHAKE256 module as the SHAKE256 is shared among all primitives. Figure 3 shows the hardware block design on our decode module and Table 7 shows the time and area results for our decode module. Table 8. Encrypt module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design Resources† Logic  
   
  Memory  
   
  (SLICES) (LUT) (DSP) (FF)  
   
  F  
   
  Cycles  
   
  (BR) (MHz) (cyc.)  
   
  Time T × A (us)  
   
  encrypt module – uses one poly mult module with with BW = 128 hqc128 1,230  
   
  3,642  
   
  4  
   
  1,773 10  
   
  179  
   
  28,217  
   
  0.16  
   
  194  
   
  hqc192 1,283  
   
  3,797  
   
  5  
   
  1,966 10  
   
  182  
   
  79,889  
   
  0.44  
   
  563  
   
  hqc256 1,438  
   
  4,256  
   
  5  
   
  2,542 10  
   
  192  
   
  160,489 0.84  
   
  1,202  
   
  parallel encrypt module – two poly mult modules with BW = 128 running in parallel hqc128 1,734  
   
  5,132  
   
  4  
   
  2,179 12  
   
  179  
   
  17,202  
   
  0.10  
   
  173  
   
  hqc192 1,793  
   
  5,308  
   
  5  
   
  2,376 12  
   
  196  
   
  46,857  
   
  0.24  
   
  429  
   
  hqc256 1,934  
   
  5,725  
   
  5  
   
  2,931 12  
   
  196  
   
  91,862  
   
  0.47  
   
  908  
   
  †  
   
  = Given resources does not include the area for SHAKE256 module.  
   
  Table 9. Decrypt module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design Resources† Logic Memory F Cycles Time T × A (SLICES) (LUT) (DSP) (FF) (BR) (MHz) (cyc.) (us) hqc128 2,146  
   
  6,352  
   
  0  
   
  5,730 10.5  
   
  194  
   
  14,198 0.07  
   
  150  
   
  hqc192 2,378  
   
  7,038  
   
  0  
   
  6,787 10.5  
   
  187  
   
  34,313 0.18  
   
  428  
   
  hqc256 2,886  
   
  8,544  
   
  0  
   
  8,740 13  
   
  186  
   
  69,356 0.37  
   
  1,067  
   
  Decrypt Module. The decrypt module (shown in Fig. 2 of the speciﬁcation document [2]) takes secret key (x, y), ciphertext (u,v), and generates the message (m’). Figure 4b shows our hardware design for decrypt module. The module accepts part of the secret key (y) as locations with ones (since it is a sparse  
   
  312  
   
  S. Deshpande et al.  
   
  ﬁxed weight vector). We use our poly mult module with BW = 128 described in Sect. 2.1 to compute u.y and use xor based adder module (described in Sect. 2.1) to compute v − u.y. We then use the decode module (described in Sect. 2.2) to decode v−u.y and retrieve the message. Table 9 shows our hardware implementation results for decrypt module targeting Xilinx Artix 7 xc7a200t FPGA. 2.4  
   
  Key Generation  
   
  We now begin to describe the top-level modules, starting with the key generation, followed in later sections with encapsulation and decapsulation. The key generation (shown in Fig. 2 of the speciﬁcation document [2]) takes the secret key seed and public key seed as an input and generates secret key (x, y) and public key (h, s) respectively as output. Figure 8b in our extended online version of this work [11] shows the hardware design of our keygen module. Our keygen module assumes that the public key seed and the secret key seed are generated by some other hardware module implementing a true random number generator. We use our CWW ﬁxed-weight vector module (fixed weight vector cww) module described in Sect. 2.1.A to generate (x, y) from the secret key seed. x and y are ﬁxed weight vectors of weight w and length n-bits. To optimize the storage, rather than storing full n-bit sparse vector we only output locations of ones. There is also an optional provision to output the full vector as described in Sect. 2.1. The vector set random uses the SHAKE256 module to expand the public key seed and generates h. We then use poly mult module with BW = 128 (described in Sect. 2.1) to compute (h.y and ﬁnally use location based adder module (described in Sect. 2.1) to compute s. We note that in the Fig. 8b (in our extended online version of this work [11]) only a block RAM for x storage (X RAM) is visible because the y, h, s are stored in the block RAMs which are inside fixed weight vector, poly mult, and location based adder modules respectively. Table 10 shows the results for the keygen module. The area results do not include the SHAKE256 module because it is shared among all other primitives. When we compare our hqc128 keygen design with existing designs from literature, we note that our design runs at least 3× faster than existing hardware designs while utilizing 80% lesser FPGA footprint. We highlight that this improvement is due to our optimized fixed weight vector cww and poly mult modules discussed in Sect. 2.1 and Sect. 2.1 respectively.  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  313  
   
  Table 10. Keygen module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design  
   
  Resources† Logic  
   
  F  
   
  Cycles  
   
  Time  
   
  (SLICES)  
   
  (LUT)  
   
  (DSP)  
   
  (FF)  
   
  Memory (BR)  
   
  (MHz)  
   
  (cyc.)  
   
  (ms)  
   
  T × A  
   
  hqc128  
   
  809  
   
  2,396  
   
  4  
   
  901  
   
  10  
   
  179  
   
  15,759  
   
  0.09  
   
  72  
   
  hqc192  
   
  791  
   
  2,342  
   
  5  
   
  926  
   
  10  
   
  189  
   
  42,106  
   
  0.22  
   
  177  
   
  hqc256  
   
  791  
   
  2,342  
   
  5  
   
  942  
   
  10  
   
  188  
   
  82,331  
   
  0.44  
   
  347  
   
  hqc128-perf HLS∗ [2]  
   
  3,900  
   
  12,000  
   
  0  
   
  9,000  
   
  3  
   
  150  
   
  40,000  
   
  0.27  
   
  1,053  
   
  hqc128-comp. HLS∗ [2]  
   
  1,500  
   
  4,700  
   
  0  
   
  2,700  
   
  3  
   
  129  
   
  630,000  
   
  4.80  
   
  7,200  
   
  hqc128-optimized. HLS∗ [3]  
   
  3,921  
   
  11,484  
   
  0  
   
  8,798  
   
  6  
   
  150  
   
  40,427  
   
  0.27  
   
  1,058  
   
  hqc128-pure HLS∗ [3]  
   
  8,359  
   
  24,746  
   
  0  
   
  21,746  
   
  7  
   
  153  
   
  40,427  
   
  0.27  
   
  2,256  
   
  †  
   
  = Given resources does not include the area for SHAKE256 module, ∗ = Target FGPA is Artix-7 xc7a100t-1  
   
  2.5  
   
  Encapsulation Module  
   
  The encapsulate operation (shown in Fig. 3 of the speciﬁcation document [2]) takes public key (h, s) and message m as an input and generates shared secret (K) and ciphertext (c = (u,v)) and d. The hardware design of the encap module is shown in Fig. 9a of our extended online version of this work [11]. Our encap module assumes that m is generated by some other hardware module implementing a true random number generator and provided as an input to our module. Since the SHAKE256 module is extensively used in encapsulate operation we design a HASH processor module which handles all the communication with the SHAKE256 module. HASH processor modules reduces the multiplexing logic of inputs to the SHAKE256 module signiﬁcantly. The Hash processor modules helps in expanding m to generate θ. We then use our encrypt module (described in Sect. 2.3) to encrypt m using θ and the public key as inputs and generates ciphertext. After the generation of r1 , r2 , and e inside the encrypt module (described in Sect. 2.3) we then run HASH processor module in parallel to encrypt module to generate d. After the encryption of m we then use the HASH processor to compute K(m, c) to generate the shared secret K. Our design is constant-time since all the underlying modules are constant-time and the control logic from the encap module does not depend on any secret input. Table 11 shows the results for the encap module with our encrypt and parallel encrypt. The area results do not include the SHAKE256 module because it is shared among all other primitives. We note that our hqc128 encap with parallel encrypt design runs at least 4.5× faster than existing hardware designs from the literature while using 64% lesser FPGA footprint. Hence, achieving the best Time-Area product. We highlight that this improvement comes mainly from our optimized encode, and poly mult hardware designs discussed in Sect. 2.2, and Sect. 2.1 respectively.  
   
  314  
   
  S. Deshpande et al.  
   
  Table 11. Encap module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design  
   
  Resources† Logic (SLICES)  
   
  F  
   
  Cycles  
   
  Time  
   
  (LUT)  
   
  (DSP)  
   
  (FF)  
   
  Memory (BR)  
   
  (MHz)  
   
  (cyc.)  
   
  (ms)  
   
  T × A  
   
  our encap module with encrypt hqc128  
   
  1,400  
   
  4,145  
   
  4  
   
  2,128  
   
  13  
   
  179  
   
  33,438  
   
  0.19  
   
  hqc192  
   
  1,445  
   
  4,278  
   
  5  
   
  2,412  
   
  15  
   
  182  
   
  90,346  
   
  0.50  
   
  262 716  
   
  hqc256  
   
  1,625  
   
  4,809  
   
  5  
   
  3,041  
   
  15  
   
  182  
   
  177,154  
   
  0.97  
   
  1,582 247  
   
  our encap module with parallel encrypt hqc128  
   
  1,969  
   
  5,828  
   
  4  
   
  2,531  
   
  15  
   
  179  
   
  22,423  
   
  0.13  
   
  hqc192  
   
  2,174  
   
  6,434  
   
  5  
   
  2,821  
   
  17  
   
  196  
   
  57,314  
   
  0.29  
   
  636  
   
  hqc256  
   
  2,330  
   
  6,898  
   
  5  
   
  3,417  
   
  17  
   
  196  
   
  108,527  
   
  0.55  
   
  1,292  
   
  hqc128 perf HLS∗ [2]  
   
  5,500  
   
  16,000  
   
  0  
   
  13,000  
   
  5  
   
  151  
   
  890,00  
   
  0.59  
   
  3,245  
   
  hqc128 comp. HLS∗ [2]  
   
  2,100  
   
  6,400  
   
  0  
   
  4,100  
   
  5  
   
  127  
   
  1,500,000  
   
  12.00  
   
  25,200  
   
  hqc128 optimized HLS∗ [3]  
   
  5,575  
   
  16,487  
   
  0  
   
  13,390  
   
  10  
   
  152  
   
  89,110  
   
  0.59  
   
  3,289  
   
  hqc128 pure HLS∗ [3]  
   
  9,955  
   
  29,496  
   
  0  
   
  26,333  
   
  11  
   
  148  
   
  89,131  
   
  0.59  
   
  †  
   
  5,873 = Given resources does not include the area for SHAKE256 module, ∗ = Target FGPA is Artix-7 xc7a100t-1  
   
  2.6  
   
  Decapsulation Module  
   
  The decapsulate operation (shown in Fig. 3 of the speciﬁcation document [2]) takes secret key (x, y), public key (h, s), ciphertext (c = (u, v)), d as an input and generates shared secret (K). Figure 9b of our extended online version of this work [11] shows hardware design the decap module. We use our decrypt module (described in Sect. 2.3) to decrypt the input ciphertext using secret key (y) and generate the m . We then use encap module to perform re-encryption of m and generate u , v and d . We then pause the encap module to verify the u , v and d against u, v and d. After the veriﬁcation we set a signal (optional port mprime fail) if the veriﬁcation fails. Irrespective of veriﬁcation result we still continue with the generation of the shared secret K to maintain the constanttime behavior. Table 12 shows the results for the decap module using encrypt and parallel encrypt (for performing the encryption). The area results do not include the SHAKE256 module because it is shared among all the primitives. When we compare our hqc128 decap with parallel encrypt design with the existing designs from literature, we note that our design runs at least 5.7× faster while using 40% lower FPGA footprint. Hence, achieving the best Time Area product. We highlight that this improvement comes mainly from our optimized decode, encode, and poly mult designs discussed in Sect. 2.2, Sect. 2.2, and Sect. 2.1 respectively.  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  315  
   
  Table 12. Decap module area and timing information, data based on synthesis results for Artix 7 board with xc7a200t-3 FPGA chip. Design  
   
  Resources† Logic (SLICES)  
   
  F  
   
  Cycles  
   
  Time  
   
  (LUT)  
   
  (DSP)  
   
  (FF)  
   
  Memory (BR)  
   
  (MHz)  
   
  (cyc.)  
   
  (ms)  
   
  T × A  
   
  our decap module with encrypt hqc128  
   
  3,035  
   
  8,984  
   
  4  
   
  6,596  
   
  20  
   
  192  
   
  48,212  
   
  0.25  
   
  758  
   
  hqc192  
   
  3,368  
   
  9,969  
   
  5  
   
  7,911  
   
  22  
   
  186  
   
  125,805  
   
  0.68  
   
  2,290  
   
  hqc256  
   
  3,693  
   
  10,931  
   
  5  
   
  9,424  
   
  22  
   
  186  
   
  248,338  
   
  1.33  
   
  4,,911 777  
   
  our decap module with parallel encrypt hqc128  
   
  3,702  
   
  10,959  
   
  4  
   
  7,003  
   
  22  
   
  179  
   
  37,197  
   
  0.21  
   
  hqc192  
   
  4,025  
   
  11,915  
   
  5  
   
  8,320  
   
  24  
   
  186  
   
  92,773  
   
  0.50  
   
  2,012  
   
  hqc256  
   
  4,347  
   
  12,868  
   
  5  
   
  9,794  
   
  24  
   
  186  
   
  179,711  
   
  0.97  
   
  4,216  
   
  hqc128 perf HLS∗ [2]  
   
  6,200  
   
  19,000  
   
  0  
   
  15,000  
   
  9.0  
   
  152  
   
  190,000  
   
  1.20  
   
  7,440  
   
  hqc128 comp. HLS∗ [2]  
   
  2,700  
   
  7,700  
   
  0  
   
  5,600  
   
  10.5  
   
  130  
   
  2,100,000  
   
  16.00  
   
  43,200  
   
  hqc128 optimized HLS∗ [3]  
   
  6,223  
   
  18,739  
   
  0  
   
  15,243  
   
  18.0  
   
  152  
   
  193,082  
   
  1.27  
   
  7,903  
   
  hqc128 pure HLS∗ [3]  
   
  8,434  
   
  24,898  
   
  0  
   
  21,680  
   
  18.0  
   
  150  
   
  193,004  
   
  1.27  
   
  †  
   
  10,711 = Given resources does not include the area for SHAKE256 module, ∗ = Target FGPA is Artix-7 xc7a100t-1  
   
  3  
   
  HQC Joint Design and Related Work  
   
  In this section, we present our joint hardware design of a HQC combining our keygen, encap, and decap modules (described in Sect. 2.4, Sect. 2.5, and Sect. 2.6 respectively) into one overall design. Following that we compare our joint design with other HQC combined designs from the literature in Sect. 3.2. In addition to that, we also conduct a comprehensive literature survey focusing on full hardware designs of the other three fourth-round public-key encryption and key-establishment algorithms in NIST’s standardization process: BIKE, Classic McEliece, and SIKE. We also include the CRYSTALS-Kyber, a publickey encryption and key-establishment algorithm selected for standardization at the end of the prior third round. Due to limited space we discuss this part in Appendix 1.A. 3.1  
   
  HQC Joint Design  
   
  In this work, we present two designs, Balanced and HighSpeed. The main difference between our Balanced and HighSpeed designs is that our Balanced design uses the regular encrypt module (shown in Fig. 4a), and our HighSpeed design uses the parallel encrypt module (shown in Fig. 8a of our extended online version of this work [11]) for performing the encryption and re-encryption operations in encapsulation and decapsulation. The In order to build a resourceeﬃcient yet performant joint design, we start by identifying the common submodules among the three keygen, encap, and decap by using hqc128 parameter set as an example:  
   
  316  
   
  S. Deshpande et al.  
   
  SHAKE256: The SHAKE256 module is used in all the primitives (keygen, encap, and decap) in HQC. As shown in Table 1, the SHAKE256 with parallel slices = 32 has a high area utilization. Consequently, we share one SHAKE256 module among all the primitives. Polynomial Multiplication: The poly mult module is also common among all the primitives. Table 3 shows the area utilization of the poly mult module. In our Balanced design, we use only one poly mult module which takes up 60%, 35% and 16% of area resources in overall keygen, encap, and decap modules respectively. And in our HighSpeed design, we use two poly mult modules for faster Encrypt and Re-encrypt operations as described in Sect. 2.5 and Sect. 2.6 this takes up 50% and 26% of overall area resources in encap, and decap modules respectively. Encapsulation: As speciﬁed in Sect. 2.6, we use the encap module (described in Sect. 2.5) inside decap module to perform re-encryption and hash computation. This encap module takes up 46% of the overall decap resources in the Balanced design and 53% in the HighSpeed design. Consequently, sharing one encap module to perform both encapsulation and decapsulation would save a signiﬁcant amount of area. In order to save the resource overhead due to the duplication of modules, we decide to share the aforementioned modules between the three primitives in our joint design. To diﬀerentiate between diﬀerent operations, we provide a 2-bit port in the interface, which helps in choosing the operation between Key Generation, Encapsulation, and Decapsulation. The results for our combined Balanced and HighSpeed implementations are shown in Table 13 in comparison with the most recent related work. Our results are generated targeting the Artix 7 (xc7a100t-csg324-3) FPGA. This is the same target FPGA family type as used in the related works [2,3,20]. Our data is from synthesis and implementation reports, while data for the other related works are from the cited papers. 3.2  
   
  Evaluation and Comparison to Existing HQC Hardware Designs  
   
  Previously, a hardware design for HQC has been generated using high-level synthesis (HLS) [2], and code targeting Artix-7 is available online.2 The generated code can obtain the performance numbers: 0.3 ms for key generation, 0.6ms for encapsulation, and 1.2ms for decapsulation, the times correspond to the HighSpeed implementation of the lowest security level. Authors also provide LightW eight version for the lowest security level, but did not provide hardware designs for other security levels. A diﬀerent HLS-based design with better results has been presented in [3]. This HLS design can achieve the performance of: 0.27 ms for key generation, 0.59 ms for encapsulation, and 1.27 ms for decapsulation with their HighSpeed version. Apart from the HLS designs, a recent hardware 2  
   
  https://pqc-hqc.org/implementation.html.  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  317  
   
  Table 13. Comparison of our HQC hardware design with the related work. Design  
   
  Resources Logic  
   
  Memory  
   
  (Slices) (LUT) (DSP) (FF)  
   
  F  
   
  Encap  
   
  (BR) (MHz) (Mcyc.) (ms)  
   
  Decap  
   
  KeyGen  
   
  (Mcyc.) (ms)  
   
  (Mcyc.) (ms)  
   
  Security Level 1 — Classical 128-bit Security HQC – Our Work, HDL design, Artix 7 (xc7a100t) Balanced  
   
  4,684  
   
  13,865 8  
   
  6,897  
   
  22  
   
  164  
   
  0.03  
   
  0.20  
   
  0.05  
   
  0.29  
   
  0.02  
   
  0.10  
   
  HighSpeed  
   
  5,246  
   
  15,214 8  
   
  7,293  
   
  24  
   
  178  
   
  0.02  
   
  0.13  
   
  0.04  
   
  0.21  
   
  0.02  
   
  0.09  
   
  0.56  
   
  5.6  
   
  0.06  
   
  0.56  
   
  HQC – [20], HW/SW codesign, Artix 7 (xc7a100t) HW/SW  
   
  —  
   
  8,000  
   
  0  
   
  2,400  
   
  3  
   
  100  
   
  0.13  
   
  1.3  
   
  28.0  
   
  132  
   
  1.48  
   
  11.85 2.15  
   
  17.21 0.62  
   
  5.01  
   
  148  
   
  0.09  
   
  0.59  
   
  1.27  
   
  0.04  
   
  0.27  
   
  132  
   
  1.50  
   
  12.00 2.10  
   
  16.00 0.63  
   
  4.80  
   
  1.20  
   
  0.30  
   
  HQC – [3], HLS design, Artix 7 (xc7a100t) LightW eight —  
   
  8,876  
   
  HighSpeed  
   
  20,169 0  
   
  —  
   
  0  
   
  6,405  
   
  16,374 25  
   
  0.19  
   
  HQC – [2], HLS design, Artix 7 (xc7a100t) LightW eight 3,100  
   
  8,900  
   
  0  
   
  6,400  
   
  14.0  
   
  6,600 20,000 0 16,000 12.5 148 0.09 0.60 0.19 HighSpeed HW/SW = Hardware-Software CoDesign, FF = ﬂip-ﬂop, F = Fmax , BR = BRAM  
   
  0.04  
   
  design [20] presented a hardware-software codesign approach and reports better performance numbers than that of LightW eight versions of both the HLS designs. Note, however, that there is area overhead of the CPU core. The HLS designs and hardware-software codesign only provide the lowest security level version. Meanwhile, both Balanced and HighSpeed variants of our design are faster for all three operations when compared to all existing designs. We also achieve the best time area product, and cover all three security levels.  
   
  4  
   
  Conclusion  
   
  This work presented two performance-targeted and constant-time hardware designs of the HQC KEM. This work presented ﬁrst, hand-optimized design of HQC key generation, encapsulation, and decapsulation written in Verilog targeting FPGAs and provides compile-time parameters to switch between all security levels and performances. This work also presented a memory-optimized Polynomial Multiplication module and a SHAKE256 module, which runs two times faster when compared to the existing work. This work also presented the ﬁrst hardware implementation of two variants of constant-time solutions for the ﬁxed-weight vector generation process. Our HQC design currently outperforms the other existing hardware designs for HQC. As this work showed, code-based designs such as HQC can be realized very eﬃciently in optimized hardware. Acknowledgement. We would like to thank the reviewers for the valuable feedback and Dr. Cuauhtemoc Mancillas L´ opez for constructive comments and shepherding our article. We would like to thank Dr. Victor Mateu and Dr. Carlos Aguilar Melchor for their helpful discussions. We would like to thank Dr. Shanquan Tian for his optimization recommendations for the SHAKE256 module. The work was supported in part by a research grant from Technology Innovation Institute.  
   
  318  
   
  S. Deshpande et al.  
   
  Appendix 1.A Fast and Non-Biased (FNB) Fixed-Weight Vector Generation Although the CWW design is constant in time, it does have a small bias. As an alternative, we propose a new FNB ﬁxed-weight vector generation design which is based on ﬁxed-weight vector generation technique given in [1]. Our FNB ﬁxed-weight generation module can be parametrized to create design with an arbitrarily small probability of timing attack being possible. In our hardware module, have a parameter ACCEPTABLE REJECTIONS, which can be used to specify how many indices could be rejected in either rejection sampling or in duplicated detection and still, the design will behave constant time. The parameter (ACCEPTABLE REJECTIONS) can be set based on user’s target failure probability. If the actual failures are within the failure probability set by the selected parameter value, then the timing side channel given in [13] is not possible. We use SHAKE256 module described in Sect. 2.1 to expand 320-bit seed to a 24 × w-bit string. Since the SHAKE256 module has 32-bit interface, the seed is loaded in 32-bit chunks, and the seed is stored in a BRAM. The 32-bit chunk from SHAKE256 is broken into 24-bit integer by preprocess unit and stored in the ctx RAM then threshold check and reduction are performed. For the reduction, we use Barrett reduction [6]. Unlike the variable Barrett reduction discussed in Sect. 2.1.A, this speciﬁc Barrett reduction is optimized as we always reduce the inputs to a speciﬁc ﬁxed value (n). After the reduction, the integer values (locations) are stored in a BRAM. Once the locations BRAM is ﬁlled, the duplicate detection module is triggered. The duplicate detection module helps detect potential duplicates values in the locations BRAM by traversing through all address locations and updating the value stored in a dual-ported BRAM. While the duplicate detection module checks for duplicates, the SHAKE256 module generates the next 24 × w-bit string to tackle any potential duplicates and stores them in another BRAM. This way, we can hide any clock cycles taken for seed expansion. Our hardware design uses a PRNG to generate the uniformly random bits required for the ﬁxed weight vector generation from an input seed of length 320-bits. Our hardware design includes this PRNG in the form of SHAKE256 and assumes that the seed will be initialized by some other hardware module implementing a true random number generator. Our FNB ﬁxed-weight generation module can be parametrized to create design with an arbitrarily small probability of timing attack being possible. In our hardware module, have a parameter name is ACCEPTABLE REJECTIONS, which can be used to specify how many indices could be rejected, and still, the design will behave constant time (at the cost of extra area for more storage and extra cycles). The extra area is needed because we generate additional (based on parameter value) uniformly random bits in advance and store them in the a BRAM. The extra clock cycles are needed because even after we found the required number of indices that are under the threshold value, we still go over all the locations to maintain constant time behavior. For the duplicate detection logic inside the duplicate detection module, the control logic is programmed to take the same cycles in both cases of duplicate being detected or not. The parameter (ACCEPTABLE REJECTIONS) can be set based on the user’s target failure probability. If the actual failures  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  319  
   
  Table 14. Comparison of the time and area of state-of-the-art hardware implementations of other (NIST PQC competition) round 4 KEM candidates. Design Resources Logic  
   
  Memory  
   
  (SLICES) (LUT) (DSP) (FF)  
   
  F  
   
  Encap  
   
  Decap  
   
  KeyGen  
   
  (BR) (MHz) (Mcyc.) (ms) (Mcyc.) (ms) (Mcyc.) (ms)  
   
  Security Level 1 — Classical 128-bit Security HQC – Our Work, HDL design, Artix 7 (xc7a200t) BAL  
   
  4,560  
   
  13,481 8  
   
  6,897  
   
  22  
   
  164  
   
  0.03  
   
  0.20 0.05  
   
  0.29 0.02  
   
  0.10  
   
  HS  
   
  5,133  
   
  15,195 8  
   
  7,293  
   
  24  
   
  178  
   
  0.02  
   
  0.13 0.04  
   
  0.21 0.02  
   
  0.09  
   
  BIKE – [18], HDL design, Artix 7 (xc7a35t) LW  
   
  4,078  
   
  12,868 7  
   
  5,354  
   
  17.0  
   
  121  
   
  0.20  
   
  1.2  
   
  1.62  
   
  13.3 2.67  
   
  21.9  
   
  HS  
   
  15,187  
   
  52,967 13  
   
  7,035  
   
  49.0  
   
  96  
   
  0.01  
   
  0.1  
   
  0.19  
   
  1.9  
   
  0.26  
   
  2.7  
   
  BIKE – [17], HDL design, Artix 7 (xc7a200t) LW  
   
  3,777  
   
  12,319 7  
   
  3,896  
   
  9.0  
   
  121  
   
  0.05  
   
  0.4  
   
  0.84  
   
  6.89 0.46  
   
  3.8  
   
  TO  
   
  5,617  
   
  19,607 9  
   
  5,008  
   
  17.0  
   
  100  
   
  0.03  
   
  0.3  
   
  0.42  
   
  4.2  
   
  0.18  
   
  1.9  
   
  HS  
   
  7,332  
   
  25,549 13  
   
  5,462  
   
  34.0  
   
  113  
   
  0.01  
   
  0.1  
   
  0.21  
   
  1.9  
   
  0.19  
   
  1.7  
   
  Classic McEliece – [8], HDL design, Artix 7 (xc7a200t) LW  
   
  —  
   
  23,890 5  
   
  45,658 138.5 112  
   
  0.13  
   
  1.1  
   
  0.17  
   
  1.5  
   
  8.88  
   
  79.2  
   
  HS  
   
  —  
   
  40,018 4  
   
  61,881 177.5 113  
   
  0.03  
   
  0.3  
   
  0.10  
   
  0.9  
   
  0.97  
   
  8.6  
   
  SIKE – [16], HDL design, Artix 7 (xc7a100t) LW  
   
  3,415  
   
  —  
   
  57  
   
  7,202  
   
  21  
   
  145  
   
  —  
   
  25.6 —  
   
  27.2 —  
   
  15.1  
   
  HS  
   
  7,408  
   
  —  
   
  162  
   
  11,661 37  
   
  109  
   
  —  
   
  15.3 —  
   
  16.3 —  
   
  9.1  
   
  220  
   
  0.003  
   
  0.01 0.004  
   
  0.02 0.002  
   
  0.01  
   
  Kyber – [9], HDL design, (xc7a200t) HS  
   
  —  
   
  9,457  
   
  4  
   
  8,543  
   
  4.5  
   
  Kyber – [24], HDL design, (xc7a12t-1) BAL 2,126 7,412 2 4,644 3 161 0.005 0.23 0.006 0.04 0.003 0.02 LW = LightWeight, HS = HighSpeed, T O = TradeOﬀ, BAL = Balanced, FF = ﬂip-ﬂop, F = Fmax , BR = BRAM  
   
  are within the failure probability set by the selected parameter value, then the timing side channel given in [13] is not possible. Table 5 shows the comparison of our new FNB design to the CCW design. The area results shown in Table 5 exclude SHAKE256 module as the SHAKE256 is shared among all primitives. The reported frequency in Although the CWW algorithm ensures the constant time behavior in generating ﬁxed-weight vectors, there is a small bias between the uniform distribution and the algorithm’s output. Meanwhile, for the new FNB algorithm, there is no bias. Further, FNB is faster than CWW, and the time-area product is better. These beneﬁts come at the cost of extremely small probabilities that the design is not constant time, but only if it happens that there are more rejections than wr . Table 5 shows that the probability of non-constant time behavior for FNB can be 2−200 or even smaller. To compute the failure probability (given in Table 5) for each parameter set, we take into account both threshold check failure and duplicate detection probabilities for the respective parameter sets.  
   
  Comparison to Hardware Designs for Other Round 4 Algorithms We also provide Table 14 where we tabulate latest hardware implementations of all other post-quantum cryptographic algorithm hardware implementations from  
   
  320  
   
  S. Deshpande et al.  
   
  the fourth round of NIST’s standardization process, plus the to-be standardized Kyber algorithm. We focus on comparison of the hardware designs for lowest level of security, Level 1, as all publications give clear time and area numbers.  
   
  References 1. Aguilar Melchor, C., et al.: HQC. Technical report, National Institute of Standards and Technology (2020). https://pqc-hqc.org/doc/hqc-speciﬁcation 021-06-06.pdf 2. Aguilar Melchor, C., et al.: HQC. Technical report, National Institute of Standards and Technology (2023). http://pqc-hqc.org/doc/hqc-speciﬁcation 2023-04-30.pdf 3. Aguilar-Melchor, C., et al.: Towards automating cryptographic hardware implementations: a case study of HQC. Cryptology ePrint Archive, Paper 2022/1425 (2022). https://eprint.iacr.org/2022/1425 4. Aragon, N., et al.: BIKE. Technical report, National Institute of Standards and Technology (2020). https://csrc.nist.gov/projects/post-quantum-cryptography/ round-3-submissions 5. Azad, A.A., Shahed, I.: A compact and fast FPGA based implementation of encoding and decoding algorithm using Reed Solomon codes. Int. J. Future Comput. Commun. 31–35 (2014) 6. Barrett, P.: Implementing the Rivest Shamir and Adleman public key encryption algorithm on a standard digital signal processor. In: Odlyzko, A.M. (ed.) CRYPTO 1986. LNCS, vol. 263, pp. 311–323. Springer, Heidelberg (1987). https://doi.org/ 10.1007/3-540-47721-7 24 7. Bernstein, D.D.: Fast multiplication and its applications (2008) 8. Chen, P., et al.: Complete and improved FPGA implementation. https://doi.org/ 10.46586/tches.v2022.i3.71-113 9. Dang, V.B., Mohajerani, K., Gaj, K.: High-speed hardware architectures and FPGA benchmarking of CRYSTALS-Kyber, NTRU, and saber. Cryptology ePrint Archive, Paper 2021/1508 (2021). https://eprint.iacr.org/2021/1508 10. Deshpande, S., del Pozo, S.M., Mateu, V., Manzano, M., Aaraj, N., Szefer, J.: Modular inverse for integers using fast constant time GCD algorithm and its applications. In: Proceedings of the International Conference on Field Programmable Logic and Applications. FPL (2021) 11. Deshpande, S., Xu, C., Nawan, M., Nawaz, K., Szefer, J.: Fast and eﬃcient hardware implementation of HQC. Cryptology ePrint Archive, Paper 2022/1183 (2022). https://eprint.iacr.org/2022/1183 12. Gigliotti, P.: Implementing barrel shifters using multipliers. Technical report, XAPP195, Xilinx (2004). https://www.xilinx.com/support/documentation/ application notes/xapp195.pdf 13. Guo, Q., Hlauschek, C., Johansson, T., Lahr, N., Nilsson, A., Schr¨ oder, R.L.: Don’t reject this: key-recovery timing attacks due to rejection-sampling in HQC and bike. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(3), 223– 263 (2022). https://doi.org/10.46586/tches.v2022.i3.223-263. https://tches.iacr. org/index.php/TCHES/article/view/9700 14. Hashemipour-Nazari, M., Goossens, K., Balatsoukas-Stimming, A.: Hardware implementation of iterative projection-aggregation decoding of reed-muller codes. In: 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), ICASSP 2021, pp. 8293–8297 (2021). https://doi.org/10.1109/ ICASSP39728.2021.9414655  
   
  Fast and Eﬃcient Hardware Implementation of HQC  
   
  321  
   
  15. Hu, J., Wang, W., Cheung, R.C., Wang, H.: Optimized polynomial multiplier over commutative rings on FPGAS: a case study on bike. In: 2019 International Conference on Field-Programmable Technology (ICFPT), pp. 231–234 (2019). https:// doi.org/10.1109/ICFPT47387.2019.00035 16. Massolino, P.M.C., Longa, P., Renes, J., Batina, L.: A compact and scalable hardware/software co-design of SIKE. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2020(2), 245–271 (2020). https://doi.org/10.13154/tches.v2020.i2.245-271. https://tches.iacr.org/index.php/TCHES/article/view/8551 17. Richter-Brockmann, J., Chen, M.S., Ghosh, S., G¨ uneysu, T.: Racing bike: improved polynomial multiplication and inversion in hardware. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2022(1), 557–588 (2021). https://doi.org/10.46586/tches. v2022.i1.557-588. https://tches.iacr.org/index.php/TCHES/article/view/9307 18. Richter-Brockmann, J., Mono, J., Guneysu, T.: Folding bike: scalable hardware implementation for reconﬁgurable devices. IEEE Trans. Comput. 71(5), 1204–1215 (2022). https://doi.org/10.1109/TC.2021.3078294 19. Sandoval-Ruiz, C.: VHDL optimized model of a multiplier in ﬁnite ﬁelds. Ingenieria y Universidad 21(2), 195–212 (2017). https://doi.org/10.11144/Javeriana.iyu21-2. vhdl. https://revistas.javeriana.edu.co/index.php/iyu/article/view/195 20. Sch¨ oﬀel, M., Feldmann, J., Wehn, N.: Code-based cryptography in IoT: a HW/SW co-design of HQC. CoRR abs/2301.04888 (2023). https://doi.org/10.48550/arXiv. 2301.04888 21. Scholl, S., Wehn, N.: Hardware implementation of a Reed-Solomon soft decoder based on information set decoding. In: 2014 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 1–6 (2014). https://doi.org/10.7873/DATE. 2014.222 22. Sendrier, N.: Secure sampling of constant-weight words - application to bike. Cryptology ePrint Archive, Paper 2021/1631 (2021). https://eprint.iacr.org/2021/1631 23. Wang, W., Tian, S., Jungk, B., Bindel, N., Longa, P., Szefer, J.: Parameterized hardware accelerators for lattice-based cryptography and their application to the HW/SW co-design of qTESLA. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2020(3), 269–306 (2020). https://doi.org/10.13154/tches.v2020.i3.269-306. https://tches.iacr.org/index.php/TCHES/article/view/8591 24. Xing, Y., Li, S.: A compact hardware implementation of CCA-secure key exchange mechanism CRYSTALS-KYBER on FPGA. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2021(2), 328–356 (2021). https://doi.org/10.46586/tches.v2021.i2. 328-356. https://tches.iacr.org/index.php/TCHES/article/view/8797  
   
  Homomorphic Encryption  
   
  On the Precision Loss in Approximate Homomorphic Encryption Anamaria Costache1(B) , Benjamin R. Curtis2 , Erin Hales3 , Sean Murphy3 , Tabitha Ogilvie3 , and Rachel Player3 1  
   
  Norwegian University of Science and Technology (NTNU), Trondheim, Norway [email protected]  2 Zama, Paris, France [email protected]  3 Royal Holloway, University of London, Egham, UK {erin.hales.2018,tabitha.ogilvie.2019}@live.rhul.ac.uk, {s.murphy,rachel.player}@rhul.ac.uk  
   
  Abstract. Since its introduction at Asiacrypt 2017, the CKKS approximate homomorphic encryption scheme has become one of the most widely used and implemented homomorphic encryption schemes. Due to the approximate nature of the scheme, application developers using CKKS must ensure that the evaluation output is within a tolerable error of the corresponding plaintext computation. Choosing appropriate parameters requires a good understanding of how the noise will grow through the computation. A strong understanding of the noise growth is also necessary to limit the performance impact of mitigations to the attacks on CKKS presented by Li and Micciancio (Eurocrypt [34]). In this work, we present a comprehensive noise analysis of CKKS, that considers noise coming both from the encoding and homomorphic operations. Our main contribution is the ﬁrst average-case analysis for CKKS noise, and we also introduce reﬁnements to prior worst-case noise analyses. We develop noise heuristics both for the original CKKS scheme and the RNS variant presented at SAC 2018. We then evaluate these heuristics by comparing the predicted noise growth with experiments in the HEAAN and FullRNS-HEAAN libraries, and by comparing with a worst-case noise analysis as done in prior work. Our ﬁndings show mixed results: while our new analyses lead to heuristic estimates that more closely model the observed noise growth than prior approaches, the new heuristics sometimes slightly underestimate the observed noise growth. This evidences the need for implementation-speciﬁc noise analyses for CKKS, which recent work has shown to be eﬀective for implementations of similar schemes.  
   
  1  
   
  Introduction  
   
  Homomorphic Encryption (HE) enables computation on ciphertexts without revealing any information about the underlying plaintexts. The ﬁrst scheme was proposed by Gentry [19] and since then, many homomorphic encryption c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 325–345, 2024. https://doi.org/10.1007/978-3-031-53368-6_16  
   
  326  
   
  A. Costache et al.  
   
  schemes have been proposed [5,10,11,18], based on the security of the Learning with Errors (LWE) problem [39] and its variants. One of the most popular schemes is the approximate homomorphic encryption scheme CKKS [10]. Ciphertexts in all homomorphic encryption schemes based on LWE variants contain noise, which grows with each evaluation operation, and must be carefully controlled to ensure correct decryption. The main insight of [10] is that it may be tolerable for decryption to be approximate, for example in applications where we expect small errors to occur. This enables the CKKS scheme to natively support real-valued plaintexts, making it attractive for application settings such as privacy-preserving machine learning [4,27,37]. In contrast, other similar schemes such as BGV [5] or BFV [18], are exact, and thus have a ﬁnite plaintext space that data must be encoded into. CKKS has been extensively optimised [8,9,28] and is implemented in many prominent open-source homomorphic encryption libraries [2,23,25,30,38,40]. Homomorphic encryption schemes involve many diﬀerent parameters, and it can be a challenge to choose appropriate parameters that balance eﬃciency, security and noise growth. This is particularly true for the CKKS scheme, for two main reasons. Firstly, unlike for exact schemes, encoding and encryption noises must be considered together. Secondly, in CKKS, we have to track not only the level of ciphertexts (as in BFV and BGV), but we must also track the scaling factor Δ. Unfortunately, there is no clear guidance for choosing Δ and a trial-and-error approach is usually advised1 . Prior noise analyses for CKKS [8–10,21,28] employ a worst-case analysis in the canonical embedding, in analogue to the line of work [13,15,20] for analysing noise growth in BGV and BFV. In particular, a worst-case bound on the noise of each ciphertext in the canonical embedding is tracked through each homomorphic operation. This leads to a bound on the noise in the output ciphertext, which can be used to set parameters for correctness. These worst-case bounds are developed assuming that the random variable falls within a certain multiple of standard deviations (e.g. six [20] or ten [21]) from its mean. We can thus expect the bounds to be loose even from the beginning of the computation (as a freshly sampled noise is likely to be closer to the mean than several standard deviations away), and that the looseness will compound as we move further through the computation. This intuition was conﬁrmed in experiments of [15] for the BFV and BGV schemes, whose operations are similar to those of CKKS. An alternative, average-case approach to noise analysis was proposed in [12] for the CGGI scheme [11], in which the noise is modelled as a Gaussian, and its variance is tracked through each homomorphic operation. The noise in the output ciphertext is ﬁnally bounded from the output variance, in order to pick parameters for correctness. Adopting a similar approach for CKKS appears challenging, as the noise after a homomorphic multiplication is a product of the noises in the two input ciphertexts, whereas as the output distribution of the product of two subgaussians is not necessarily subgaussian, and can have a much heavier tail [36]. 1  
   
  See e.g. https://ibm.github.io/fhe-toolkit-linux/html/helib/md opt i b m f h edistro h elib c k k s-security.html.  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  327  
   
  In this work, we will demonstrate how a Central Limit Theorem approach can be applied to give a heuristic average-case noise growth analysis for CKKS. Contributions: Our ﬁrst contribution is a new result relating the CKKS message and plaintext spaces. Recall that CKKS encoding maps an element from the (complex) message space into an element in the (polynomial ring) plaintext space via a scaled restriction of the inverse canonical embedding. In Theorem 1, we provide a new, tighter bound relating the size of an error in the plaintext space to the size of the induced error in the message space. Moreover, we prove that this bound is the best possible. In addition, we show that the worst case expansion factor in either the real or complex part of our message equals the worst case expansion factor of the entire embedding. This means that, perhaps surprisingly, bounding a decrypted and decoded message over only the real part, rather than the whole embedding, provides no beneﬁt for worst-case analyses. Our next contribution is to present the ﬁrst average-case noise analysis for CKKS. In Theorem 3 we give a result showing that the product of two Normally distributed polynomials has Normally distributed coeﬃcients under a Central Limit assumption. Using this result we are able to heuristically model all CKKS noise operations as operations on Normal random variables, thus recovering an analysis similar to [12]. We present our noise analyses for ‘Textbook’ CKKS as originally presented in [10] and the RNS variant presented in [9]. In order to evaluate the eﬃcacy of our average-case noise analysis for CKKS, we compare the noise heuristics developed under this analysis with the worstcase bounds of prior work. We also present reﬁnements to these prior worst-case noise analyses using the techniques of [26]. We parameterise all our noise bounds in terms of a failure probability, α, rather than a-priori ﬁxing a one dimensional failure probability as in prior work [13,15,20]. We evaluate the bounds arising from all these noise analyses with experiments in HEAAN v1.0 [24] and FullRNSHEAAN [22]. We note that neither the Textbook CKKS nor the RNS variant noise analysis is implementation-speciﬁc, and we chose the HEAAN library as it is the implementation that most closely resembles the theoretical description of both variants of the scheme. Our experimental results are given in Table 4 for Textbook CKKS heuristics as compared with HEAAN v1.0 [24] and in Table 5 for heuristics for the RNS variant [9] as compared with FullRNS-HEAAN [22]. Our results show that our new heuristics improve upon prior noise analyses in terms of modelling more closely the observed noise. However, we also observe that our heuristics may underestimate the noise growth observed in practice. Prior work [16] for BGV has noted another example of a noise analysis that was not implementation speciﬁc that also led to underestimates of the observed noise. Our work can therefore be seen as an improved starting point for a tight noise analysis for CKKS, but an implementation-speciﬁc analysis may be more suitable for applications that cannot aﬀord this underestimate. Related work: Average-case noise analyses were presented for the BGV scheme in [35], by applying results from the present work. An implementation-speciﬁc  
   
  328  
   
  A. Costache et al.  
   
  average-case noise analysis for the BGV scheme was presented in [16]. An average-case noise analysis for BFV was presented in [3]. Lee et al. [32] use the signal-to-noise ratio to analyse CKKS noise, and proposes to track the variance of the errors, rather than an upper bound. The variances of the noise in multi-key BFV and CKKS operations were tracked in [7], but a proof that the noises are distributed as Gaussians was not presented. Our work thus provides a theoretical justiﬁcation for these approaches. Our study of encoding also provides theoretical support for the heuristics in [21]. Structure: In Sect. 2 we introduce relevant background material and notation, including the Textbook CKKS scheme [10] and its RNS variant [9]. In Sect. 3 we study the precision loss coming from encoding and decoding in CKKS. In Sect. 4 we describe the three methods for noise analysis that we will apply to Textbook CKKS and its RNS variant. We then apply this to Textbook CKKS. In Sect. 5 we describe the modiﬁcations required to the noise analysis methods for the RNS setting, and provide heuristics for this setting. In Sect. 6 we report on experimental results to evaluate the noise analysis approaches that we introduced and we draw conclusions. For reasons of space, we are omitting proofs from Sects. 3 and 4 and refer the reader to the full version of the paper instead [14].  
   
  2  
   
  Preliminaries  
   
  Notation: Vectors are denoted in small bold font z, and zj refers to the j th element of a vector, indexing from zero. The notation · is used for rounding to the nearest integer and [·]q represents reduction modulo q. For z = x + iy ∈ C, we denote by z := x + i y the rounding of both its real and imaginary components, and extend this componentwise to deﬁne the rounding z of a complex vector z ∈ CN/2 . Unless otherwise stated, log will always mean log2 . In this work, we will consider several diﬀerent norms. We denote the p-norm by ·p and the inﬁnity norm by ·∞ . We consider norms on a polynomial m both as a vector of its coeﬃcients and under the canonical embedding, and can denote these norms by m and m respectively. We use s ← D to denote sampling s according to the distribution D. We use the notation N(μ, σ 2 ) to refer to a univariate Normal distribution with mean μ and variance σ 2 , and N(µ; Σ) to refer to an N -dimensional multivariate Normal distribution with N -dimensional mean vector µ and N × N covariance matrix Σ. For a polynomial Z(X) ∈ R[X]/(X N + 1), we will write Z ∼ N(µ, ρ2 IN ) to indicate that each coeﬃcient of Z is independently and identically normally distributed, i.e., Zi ∼ N(μi , ρ2 ). We denote by erf the (Gauss) error function, by erf−1 its inverse, and by erfc the complementary function. The Textbook CKKS Scheme: The CKKS scheme as originally presented in [10] is a levelled HE scheme that we refer to as Textbook CKKS. For reasons of space, we omit the presentation of the scheme, and only detail that of its RNS variant.  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  329  
   
  The Textbook CKKS scheme is parameterised by L, p, q0 , N , λ, χ, S, V , and Δ. The base p > 0 and modulus q0 are used to form the scale parameter and the chain of moduli (one for each level) as follows: Δ = 2p and Q = Δ q0 for 1   L. The dimension N is typically chosen as a power of two, and we will only use such N in this work. The dimension N and the chain of moduli parameterise the underlying plaintext and ciphertext rings. The plaintext space is R = Z[X]/(X N +1). We denote by Q some ﬁxed level in the description below, so that the ciphertext space at any given moment is RQ = ZQ [X]/(X N + 1). The security parameter is λ. The Ring-LWE error distribution is denoted by χ and is such that each coeﬃcient is sampled as a discrete Gaussian with standard deviation σ = 3.2 [1]. The parameter S denotes the secret key distribution, which is speciﬁed in [10] to be HW T (h), i.e. the secret is ternary with Hamming weight exactly h. The parameter V denotes the ephemeral secret distribution, which is speciﬁed in [10] to be ZO(ρ) with ρ = 0.5, i.e. the secret is ternary with coeﬃcients having probability ρ/2 for each of −1 and 1, and probability 1 − ρ of being 0. The CKKS scheme uses the canonical embedding to deﬁne an encoding from the message space CN/2 to the plaintext space Z[X]/(X N + 1) in the following way: an isomorphism τ : R[X]/(X N + 1) → CN/2 can be deﬁned via considering the canonical embedding restricted to N/2 of the 2N th primitive roots and discarding conjugates. Encoding and decoding then use this map τ , as well as a precision parameter Δ, as follows: Encode(z, Δ) = Δτ −1 (z) and 1 τ (m), where z ∈ CN/2 , m ∈ Z[X]/(X N + 1), and · is taken Decode(m, Δ) = Δ coeﬃcient-wise. RNS Variants of CKKS: Variants of CKKS using RNS have been proposed [9, 28]. In this work, we focus on the RNS-CKKS scheme as described in [9]. This scheme is speciﬁed in Fig. 1. In RNS variants of CKKS [9,28], the chain of ciphertext moduli changes compared to the original scheme. The th ciphertext modulus is given by Q =  th ciphertext slot is with respect to the modulus qj . In the j=0 qj where the j RNS variant [9], the key switching procedure requires the large modulus P to be k formed similarly from a set of k pairwise coprime pi as P = i=0 pi . The other parameters as speciﬁed in [9], and the encoding and decoding, are the same as for Textbook CKKS. Precision Loss: in this work we are concerned with bounding the precision loss in CKKS, which we can deﬁne informally as the diﬀerence between evaluating a circuit in the clear and evaluating the same circuit homomorphically. A more formal description is given below. Deﬁnition 1. Consider a normed space (M, || · ||), messages m1 , ..., mn ∈ M, and a circuit C : Mn → M. Then we deﬁne the precision loss associated with calculating the circuit C homomorphically as the distance ||m ˜ − m||, where m ˜ is the output of the homomorphic evaluation of C(m1 , ..., mn ), and m is the true value of the circuit.  
   
  330  
   
  A. Costache et al.  
   
  SecretKeyGen(λ): Sample s ← S and output sk = (1, s). PublicKeyGen(sk): For sk = (1, s), for all 0 ≤ j ≤ L, a representative a(j) is sampled uniformly from Rqj , and b(j) ← −a(j) s + e mod qj is set. Output pk = (pk(j) )0≤j≤L = (b(j) , a(j) )0≤j≤L . EvaluationKeyGen(sk, w): Sample e ← χ. Output (evk(0) , . . . , evk(k+L) ) = ((b(0) , a(0) ), . . . , (b(k+L) , a(k+L) )), where, for 0 ≤ i < k, a(i) ← Rpi uniformly and b(i) = −a(i) s + e mod pi ; and for 0 ≤ j ≤ L, a(k+j) ← Rqj uniformly and b(k+j) = −a(k+j) s + [P ]qj s2 + e mod qj . Encrypt(pk, m): For m ∈ R. Sample v ← V and e0 , e1 ← χ. For all 0 ≤ j ≤ L, and for the public key pk = (pk(j) )0≤j≤L = (b(j) , a(j) )0≤j≤L , output ct = (ct(j) )0≤j≤L where ct(j) = (b(j) v + e0 + m, a(j) v + e1 ) ∈ Rq2j .   Decrypt(sk, ct): For ct = (ct(j) )0≤j≤ , output m = ct(0) , sk mod q0 . (j)  
   
  (j)  
   
  Add(ct0 , ct1 ): For 0 ≤ j ≤ , for input ciphertexts ct1 = {ct1 } and ct2 = {ct2 }, (j) (j) (j) (j) output ctadd = (ctadd )0≤j≤ where ctadd = ct1 + ct2 mod qj . (j) input ciphertexts ct1 = {ct1 } = Pre-Multiply(ct 0 , ct1 ): For 0 ≤ j ≤ , for   (j)  
   
  (j)  
   
  c0 , c 1  
   
  (j)  
   
  and ct2 = {ct2 } =  
   
  (j) {ctpre-mult }0≤j≤ (j) (j) (j) (j) c0 C1 + c1 C0  
   
  (j)  
   
  (j)  
   
  C0 , C1  
   
  (j) (j) (j) = {(d0 , d1 , d2 )} where (j) (j) (j) mod qj , and d2 = c1 C1  
   
  (j) d0  
   
  , output ctpre-mult = (j) (j) c0 C0  
   
  =  
   
  (j)  
   
  mod qj , d1  
   
  =  
   
  mod qj . KeySwitch(ct, evk): For 0 ≤ j ≤ , for input ciphertext ctpre-mult = (j) (j) (j) (j) (j) {ctpre-mult }0≤j≤ = {(d0 , d1 , d2 )}, output ctks = {ctks }0≤j≤ =         (0) (0) () () (j) (j) (j) (j) (j) (j) c0 , c1 , . . . , c0 , c1 , where c0 , c1 = [d0 + cˆ0 ]qj , [d1 + cˆ1 ]qj , (j)  
   
  (j)  
   
  for cˆ0 and cˆ1 as defined in the full version [14]. (j) Rescale(ct) : For   0 ≤ j ≤ , for input ciphertext ct = {ct }0≤j≤ (j)  
   
  (j)  
   
  (c0 , c1 )  (j)  
   
   (j)  
   
  (j)  
   
  output ctrs = {ctrs }0≤j≤−1 = {(c0  
   
  0≤j≤ (j) ()  
   
   (j)  
   
   (j)  
   
  , c1  
   
  =  
   
  )}0≤j≤−1 , where  
   
  c0 = q−1 (c0 − c0 ) mod qj and c1 = q−1 (c1 − c1 ) mod qj . Multiply(ct0 , ct1 ): Output Rescale(KeySwitch(Pre-Multiply(ct0 , ct1 ), evk)). (j)  
   
  ()  
   
  Fig. 1. The RNS-CKKS Scheme  
   
  This deﬁnition is similar to Deﬁnition 10 of [33]. We will consider precision loss in three spaces: ﬁrstly, the plaintext space R with inﬁnity norm on the vector of coeﬃcients; secondly, the message space CN/2 with inﬁnity norm, which is equivalent to R with inﬁnity canonical norm; and lastly the projection to the real part RN/2 with inﬁnity norm.  
   
  3  
   
  Encoding Analysis  
   
  In this section, we give theoretical bounds on the precision loss from encoding and decoding. Proofs of the results can be found in the full version [14]. To understand precision loss due to encoding, as well as translate noise bounds derived in the plaintext space to noise bounds in the message space, we investigate how distance  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  331  
   
  measured in R[X]/(X N + 1) corresponds to distance measured in CN/2 , when we move between the two via τ for N a power of 2. If we measure using the 2-norm in both spaces, these two  distances correspond  
   
  exactly as here τ gives a scaled isometry with τ (m)2 = N2 m2 . However, we will use the inﬁnity norm in both spaces to support our Worst Case in the Ring analysis (see Sect. 4). We ﬁnd that, in the worst case, there is an O(N ) expansion in the inﬁnity norm under the map τ and unlike the 2-norm, there is no contraction under the map τ −1 . The section is organised as follows. In Sect. 3.1, we develop new theoretical results on the relationships between distances in the two spaces. In Sect. 3.2, we then apply these results in the context of CKKS encoding and decoding. 3.1  
   
  Mapping Theory  
   
  Lemma 1 ([17]). Let m ∈ RN . Then m∞  m∞ . can  
   
  This inequality is best possible in the sense that it is achieved: let m = τ −1 (z) can and let zk = Bζkj for 0  k  N2 − 1, so that z∞ = B. Then we ﬁnd m∞ = z∞ = B = |mj | = m∞ . In particular, there is no contraction as we move from CN/2 , ·∞ to RN , ·∞ but there is an expansion as we move the other way. The prior result on this bound is as follows. Lemma 2 ([17,20]). Let m ∈ RN . Then m∞  N m∞ . can  
   
  Using generic√proof methods and properties of the norm, we can reduce this factor to N/ 2. Before improving further, we require some deﬁnitions and a Lemma. The proof technique of the following Lemmas 3 and 4 is adapted from [6]. We introduce the notation I(N, j) and I(N ) as follows:  
   
  I(N, j) :=  
   
  N −1   k=0  
   
   sin   
   
    
   
   jkπ  , N   
   
  I(N ) :=  
   
  max  
   
  0jN −1  
   
  I(N, 2j + 1).  
   
  Lemma 3. For j ∈ Z, we have that I(N, 2j + 1) = I(N, 1), so that I(N ) = I(N, 1). 1 I(N ) N →∞ N  
   
  Lemma 4. lim  
   
  =  
   
  2 π,  
   
  and this limit is approached from below.  
   
  Theorem 1. Let m ∈ RN . Then m∞  is the best possible for ﬁxed N . can  
   
    
   
  I(N )2 + 1 m∞ , and this bound  
   
  Corollary 1. Suppose for all m ∈ RN we have m∞  N · M (N ) m∞ with M (N ) a least upper bound. Then M (N ) → π2 as N → ∞. can  
   
  332  
   
  A. Costache et al.  
   
  We now bound just the real component of the canonical embedding of m, although the following results apply equally to the imaginary component. We can,Re = maxj |Re(m(ζj ))|. We ﬁnd that, in the limit, the use the notation m∞ upper bound on expansion of just the real component equals the upper bound on the entire expansion. Lemma 5. Let m ∈ RN . Then m∞ possible.  
   
  can,Re  
   
   I(N ) m∞ , and this result is best  
   
  Corollary 2. Let m ∈ RN . If for all N we have that m∞ then k  π2 and k → π2 as N → ∞.  
   
  can,Re  
   
  3.2  
   
   kN m∞  
   
  Application to Encoding  
   
  In this section, we apply the results from Sect. 3.1 to produce bounds on the growth of polynomials under encoding and decoding. Our ﬁrst result enables us to produce bounds in the plaintext space given bounds in the message space. Lemma 6. Suppose m ∈ RN and z ∈ CN/2 are such that m = Encode(z, Δ). Then m∞  Δ z∞ + 12 . The result in Theorem 1 enables us to give bounds in the message space given bounds in the plaintext space. Lemma 7. Suppose m ∈ RN has m∞  B. If z = Decode(m, Δ) we have √ I(N )2 +1 that z∞  B, and this bound is the best possible. Δ Due to the fast convergence of I(N ) to 2N π , we can replace this result by its limiting value. We can therefore precisely bound the error introduced during encoding. Corollary 3. Suppose z ∈ CN/2 is encoded under scale factor√Δ. Then the I(N )2 +1 precision lost in each slot as a result of encoding is bounded by , and 2Δ N this bound tends to πΔ as N → ∞. We can also give analogous results for the real and imaginary components alone. Lemma 8. Suppose m ∈ RN has m∞  B. Then if z = Decode(m, Δ) we 2N B, and this bound is the best possible. have that Re(z)∞ , Im(z)∞  πΔ Corollary 4. Suppose z ∈ CN/2 is encoded under scale factor Δ. Then the precision lost on both the real and imaginary components of each slot is bounded N . by πΔ This shows that, perhaps surprisingly, if using a worst case analysis, it is not possible to achieve a tighter analysis of precision loss by considering only the error on the real part of the message. To beneﬁt from restricting our attention to only the real part, we must be able to specify statistical, rather than worst case, behaviour.  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  4  
   
  333  
   
  Noise Analysis Methods  
   
  In this section, we present the three noise analysis methods considered in this work, and apply them to give noise heuristics for the Textbook CKKS scheme [10]. Proofs of the results can be found in the full version [14]. We ﬁrst introduce some notation and deﬁnitions. Noise Deﬁnitions and Notation: For a Textbook CKKS ciphertext (ct0 , ct1 ) at level encrypting a message m, we deﬁne its noise as the polynomial such that ct, sk = m + mod Q where this noise is small. We denote by ρ2 the (component) variance of a noise polynomial . Some operations, such as key switching, introduce an additive noise term, whose variance we denote by η 2 . We treat noise polynomials as continuous random variables for simplicity, but the distributional results are applicable to the corresponding discrete random variables for practical purposes. Variance: We will use the following variance results. A polynomial f with coefﬁcients distributed uniformly in [−k/2, k/2], has coeﬃcient variance ρ2f = k 2 /12. A polynomial sampled from ZO(ρ) has coeﬃcient variance ρ. A polynomial sampled from the Ring-LWE error distribution has coeﬃcient variance σ 2 . 4.1  
   
  Bounding Noise Random Variables  
   
  In this subsection, we introduce our reﬁnement for bounding a random variable of a given variance. Given a (multivariate) random variable, we wish to identify a reasonable upper bound on the size of the components of the random variable(s). It has been common practice [13,15,20] to give an upper bound using the fact that erfc(6) ≈ 2−55 . Instead of deferring to such a bound in all contexts, we express our bounds on distributions in terms of a new failure probability parameter α, deﬁned as follows. Deﬁnition 2. Suppose a random variable Z has real support. We will say B is a probability 1 − α bound on Z if Pr(Z > B) = α. Equivalently, we will say B has failure probability α, or that B has error tolerance α. This reﬁnement enables us to determine bounds on a random variable that hold with probability (1 − α). In Theorem 2, we give bounds both for the canonical embedding as in prior CKKS analyses (e.g. [10]), and for the plaintext ring. When applying Theorem 2 in real and complex settings respectively, we use the following functions for notational convenience: 1  
   
  2  
   
  1  
   
  HR (α, N ) := erf−1 ((1 − α) N ) and HC (α, N ) := (− ln(1 − (1 − α) N )) 2 . For example, a√ ciphertext with noise variance ρ2 can be bounded in the canonical embedding as N · ρ · HC (α, N ) using Theorem 2 part(3).  
   
  334  
   
  A. Costache et al.  
   
  Theorem 2. Suppose Z ∼ N(0, ρ2 IN ). Then: 1. A probability (1 − α) bound on the random variable Z∞ is given by √ 1 B = 2 ρ erf−1 ((1 − α) N ). 2. Let τ denote the map used in encoding and decoding and consider τ (Z). Then we have that Re(τ (Z)), Im(τ (Z)) ∼ N(0, N2 ρ2 IN/2 ), and a probability (1 − α) bound on both Re(τ (Z))∞ and Im(τ (Z))∞ is given by √ 2 B = N ρ erf−1 ((1 − α) N ). can  
   
  3. A probability (1 − α) bound on the random variable Z∞ is given by √ 2 1 B = N ρ (− ln(1 − (1 − α) N )) 2 . 4.2  
   
  Worst-Case Noise Analysis Methods  
   
  In this subsection, we introduce the two worst-case noise analysis methods considered in this work. Worst-Case Canonical Embedding Analysis: The ﬁrst noise analysis method we consider is a reﬁnement of the standard approach for analysis of CKKS noise, as e.g. in [10]. This method tracks bounds on the noise polycan nomials under the canonical embedding,  ∞ , i.e. the bounds are presented in the message space. We improve on the canonical embedding bounds in [10] by following the Iliashenko approach [26]. For a noise polynomial that consists of several summands, this approach calculates the coeﬃcient variance of the whole sum and then maps under the canonical embedding to obtain a bound on the noise. In contrast, the prior approach relies on repeated applications of the triangle inequality to bound individual summands that are then combined into a ﬁnal bound. The Iliashenko approach is expected to provide tighter bounds than the prior approach [15]. We use Theorem 2 or triangle inequalities to derive bounds in the canonical embedding. We use the fact that can can can p(X)q(X)∞  p(X)∞ q(X)∞ is the worst case bound on a product of polynomials. Worst-Case Analysis in the Ring: In this method, like the other worst-case analyses, we track how a bound  ∞ on the size of the largest coeﬃcient of the noise polynomial grows with each homomorphic operation. The diﬀerence is that we give the bound ‘in the ring’, i.e. the bounds are presented in the plaintext space, into which decryption takes place. We again use triangle inequalities and Theorem 2 to derive bounds in the ring. We use the fact that p(X)q(X)∞  N p(X)∞ q(X)∞ is the worst case bound on a product. We note that this noise analysis method has has been considered for other homomorphic encryption schemes, as e.g. in [31], and this is the ﬁrst work that considers it for CKKS.  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  4.3  
   
  335  
   
  Average-Case Noise Analysis Method  
   
  We present the main result of this section, Theorem 3, that considers the product of two Normally distributed polynomials. We then show how Theorem 3 enables us to develop the ﬁrst average-case noise analysis for CKKS using Heuristic 1. Theorem 3. Suppose that Z ∼ N(µ; ρ2 IN ) and Z  ∼ N(µ ; ρ2 IN ), then the polynomial product ZZ  (modulo X N + 1) has mean vector E(ZZ  ) and covariance matrix Cov(ZZ  ) given by E(ZZ  ) = µ∗  
   
  and  
   
  Cov(ZZ  ) = ρ2∗ IN + S,  
   
  where µ∗ is the polynomial product of µ and µ , ρ2∗ = N ρ2 ρ2 +ρ2 µ2 +ρ2 µ 2 and S is an oﬀ-diagonal matrix with entries 2  
   
  Si,i = ρ2  
   
  N −1  j=0  
   
  ξ(i − j)ξ(i − j)μi−j μi −j + ρ2  
   
  N −1   
   
  2  
   
  ξ(i − j)ξ(i − j)μi−j μi −j ,  
   
  j=0  
   
  for a modiﬁed sign function ξ given by ξ(z) = Sign(z) for z = 0 and ξ(0) = 1. Furthermore, the components (ZZ  )i of this polynomial product can be approximated as a Normal N(μ∗i , ρ2∗ ) distribution. Theorem 3 gives the mean and covariance of the product Y = ZZ  , and shows the components Yi of Y can be well-approximated as Normal. Our average-case analysis will model ZZ  as a multivariate Normal distribution of the established mean and covariance. This is expressed in Heuristic 1 and will be justiﬁed below. Heuristic 1. Suppose that Z ∼ N(µ; ρ2 IN ) and Z  ∼ N(µ ; ρ2 IN ). Then, for µ∗ , ρ2∗ and S as speciﬁed in Theorem 3, the polynomial product ZZ  (modulo X N + 1) can be approximated as a multivariate Normal distribution as  
   
  ZZ  ∼ N µ∗ ; ρ2∗ IN + S . Small-S Assumption: To simplify our analysis, we make the assumption that the oﬀ-diagonal matrix S encountered in Theorem 3 is negligible. While we believe this assumption is reasonable in many circumstances of interest, we note that it would not hold e.g. if the mean vectors have large constant components. Deﬁnition 3. A covariance matrix of the form ρ2∗ IN + S with constant component covariance ρ2∗ and oﬀ-diagonal matrix S satisﬁes the Small-S assumption if this oﬀ-diagonal matrix S is negligible compared to ρ2∗ IN . Average-Case Noise Analysis: In an average-case noise analysis, we track the how the variance of the noise polynomial develops with each homomorphic operation, rather than tracking how a bound on the coeﬃcients of develops. In our average-case noise analysis of CKKS, we consider how the variance of develops ‘in the ring’, i.e., in the plaintext space. Heuristic 1 shows that, in the ring,  
   
  336  
   
  A. Costache et al.  
   
  the polynomial product (modulo X N + 1) of multivariate Normal vectors can be well-approximated as a multivariate Normal distribution. Moreover, under the Small-S assumption, we can model the input and output polynomials in an application of Heuristic 1 as Normal random variables of a speciﬁed component variance. This enables us to track the variance of the noise polynomial through each homomorphic operation using the results presented below in Corollary 5. Given the variance in an output ciphertext, we can then use Theorem 2 to derive a bound on the noise in the output ciphertext. Corollary 5. Suppose that Z ∼ N(µ; ρ2 IN ) and Z  ∼ N(µ ; ρ2 IN ) are independent, λ is a constant vector. Approximations to the distribution of Z + Z  , λZ and the rounding Z are then given by:  
   
  2 1 . Z +Z  ∼ N(0, (ρ2 +ρ2 )IN ), λZ ∼ N λµ ; ρ2 λ2 IN , Z ∼ N µ , ρ2 + 12 Furthermore, an approximation to the distribution of ZZ  when the Small-S assumption is valid for ZZ  and an approximation to the distribution of Z 2 when the Small-S assumption is valid for Z 2 are given by: 2 2 ZZ  ∼ N µµ ; (N ρ2 ρ2 + ρ2 µ2 + ρ2 µ 2 )IN 2 and Z 2 ∼ N µ2 ; 2ρ2 (N ρ2 + 2 µ2 )IN . 4.4  
   
  Summary of Textbook Noise Heuristics  
   
  In this subsection, we summarise the noise heuristics obtained when analysing the Textbook CKKS scheme [10] according to the three diﬀerent noise analysis methods presented in this work. Table 1 gives the worst-case analyses in the ring (WCR) and in the canonical embedding (CE) for Textbook CKKS [10]. Table 2 gives the average-case analysis in terms of the variance of the noise after each homomorphic operation, and illustrates how this variance could be converted to a bound on the noise in the output ciphertext using Theorem 2. The full justiﬁcation for the distributional results leading to the noise heuristics in Tables 1 and 2 is given in the full version [14]. This gives a variance for the noise polynomial after each operation, directly giving the average-case analysis. The variances can then be converted to a bound in either the canonical embedding or the ring after each operation to give the respective worst-case analyses, using Theorem 2. The worst-case bounds developed in this work can be contrasted with the worst-case canonical embedding bounds given in prior work (as e.g. in [10]). These are restated for completeness in the full version [14].  
   
  5  
   
  Application of Methods to RNS-CKKS  
   
  In this section we discuss the application of the noise analysis methods described in Sect. 4 to RNS variants of CKKS [9,28]. We focus mainly on [9].  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  337  
   
  Table 1. Worst-case bounds for Textbook CKKS [10]. Here, B1 , B2 , and B denote input noise bounds in the ring or canonical embedding, as appropriate, and ηks =    1 P −2 N Q2 σ 2 + 1P Q (h + 1) . 12 Operation Fresh  
   
  √  
   
  WCR  
   
  σ N + 2h + 2 · HR (α, N )  
   
  σ  
   
    
   
  CE N2 2  
   
  + hN + N · HC (α, N )  
   
  B1 + B2   can PreMult N · m1 ∞ B2 + m2 ∞ B1 + B1 B2 m1 ∞ B2 + m2 can ∞ B1 + B1 B2 √ √ Key-Switch B + 2 · ηks · HR (α, N ) B + N · ηks · HC (α, N )   N Rescale Δ−1 B + 16 (h + 1) HR (α, N ) Δ−1 B + 12 (h + 1)HC (α, N ) Add  
   
  B1 + B2  
   
  Table 2. Average-case noise analysis for Textbook CKKS [10]. Here, ρ1 , ρ2 , and ρ denote input noise variances. The ﬁnal output variance can be converted to (e.g.) a canonical embedding bound using Theorem 2. Operation  
   
  Output Variance  
   
  Final output bound (CE) √ Fresh = +h+ N · ρfresh · HC (α, N ) √ Add ρ2add = ρ21 + ρ22 N · ρadd · HC (α, N ) √ 2 2 2 2 2 2 2 ρpre-mult = N ρ1 ρ2 + ρ2 m1 2 + ρ1 m2 2 N · ρpre-mult · HC (α, N ) PreMult √  −2  1 2 2 2 2 N · ρks · HC (α, N ) Key-Switch ρks = ρ + 12 P N Q σ + 1P Q (h + 1) √ ρ2 1 Rescale ρ2rs = Δ + (h + 1) N · ρrs · HC (α, N ) 2 12 ρ2fresh  
   
  5.1  
   
  (N 2  
   
  1)σ 2  
   
  Diﬀerences from Textbook CKKS  
   
  The operations in RNS variants of CKKS are performed ‘slotwise’ with respect to  the constituent moduli qj making up the th ciphertext modulus Q = j=0 qj . In [9], for all 0 ≤ j ≤ L, a distinct qj = 1 mod 2N is chosen to support NTT operations in each slot. It is also required that Δ/qj ≈ 1 for all 1 ≤ j ≤ L and q0 is suﬃciently large for correctness. The need for distinct qj that are not exactly equal to Δ incurs an approximation error not present in Textbook CKKS. The changes in parameters in the RNS variants require modiﬁcations to the rescale and key switch operations. The other operations carry over to the RNS case in a more straightforward way. When rescaling from Q to Q−1 in RNS variants, instead of dividing by Δ, we divide by q . The key switching procedure presented in [9] translates the key switching approach of [10] to the RNS setting and so requires the klarge modulus P to be formed from a set of k pairwise coprime pi as P = i=0 pi . We also note that a hybrid key switching is possible in the RNS setting, for example as done in [28]. The deﬁnition of noise in the RNS variant [9] also diﬀers from the Textbook CKKS deﬁnition. A RNS ciphertext ct at level can be expressed as a vector of its RNS representatives (ct(j) )0≤j≤ . The noise in a ciphertext is deﬁned in [9] as such that ct(0) , sk = m + mod q0 .  
   
  338  
   
  5.2  
   
  A. Costache et al.  
   
  Distribution of Noise Polynomials for the RNS Variant [9]  
   
  In this subsection we derive the distributions of the noise polynomials for the RNS variant [9] that diﬀer from Textbook CKKS, namely, for the rescale and key switch operations. The analysis for the other operations is analogous to the Textbook CKKS case as presented in Sect. 4.4. Proofs for the results in this subsection are presented in the full version [14]. Lemma 9 [Key Switch – RNS]. The RNS-CKKS Key Switch operation applied to a ciphertext at level introduces an additive error such that the output noise is given by ks := + εks if the input noise is given by . The additive error εks has a Normal distribution given by 2 2 εks ∼ N(0, ηks IN ), where ηks =  
   
  −2 1 N Q2 ( 2 12 P  
   
  + 1)σ 2 +  
   
  2 1 12 (k  
   
  + 1)(s22 + 1) .  
   
  For example, if the secret is sparse with ﬁxed Hamming weight h, we have 1 1 2 ηks = 12 P −2 N Q2 ( 2 + 1)σ 2 + 12 (k 2 + 1)(h + 1). Lemma 10 [Rescale – RNS]. Let ct be a ciphertext encrypting m with noise  
   
  . Let ctrs encrypting m be the ciphertext with noise rs resulting from the Rescale operation. The Rescale noise rs ∼ N(0; ρ2rs IN ), where the component variance ρ2rs is given by ρ2rs =  
   
  ρ2 1 2 + (s + 1) . 2 12 q2  
   
  For example, if the secret is sparse with ﬁxed Hamming weight h, we have 2 1 ρ2rs = ρq2 + 12 (h + 1).   
   
  5.3  
   
  Summary Tables of Noise Bounds  
   
  In this subsection, we present noise heuristics for the RNS variant [9] that were developed by applying the noise analyses of Sect. 4 to this variant. Table 3 summarises the worst-case canonical embedding and average-case noise heuristics for the RNS variant [9]. These heuristics can be justiﬁed in the same manner as explained in Sect. 4.4 for the Textbook CKKS case, using the distributional results presented in Sect. 5.2.  
   
  6  
   
  Experimental Results  
   
  In this section we evaluate the eﬃcacy of the noise analyses developed in this work for Textbook CKKS and the RNS variant of [9] as compared with their implementations HEAAN [24] and FullRNS-HEAAN [22] respectively. We also compare the new heuristics with those obtained from a worst-case canonical embedding approach as in prior work (denoted as P-CE). The code used to generate our results is available at https://github.com/bencrts/CKKS noise.  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  339  
   
  Table 3. Worst-case canonical embedding bounds (CE) and average-case noise analysis (CLT) for RNS CKKS [9]. Here, B1 and B2 denote input noise bounds in the canonical 2 1 = 12 P −2 N Q2 (2 + embedding; ρ1 , ρ2 , and ρ denote input noise variances; and ηks 2 2 1 1)σ + 12 (k + 1)(h + 1). Operation Fresh Add  
   
  σ  
   
    
   
  CE N2 2  
   
  + hN + N · HC (α, N ) B1 + B2  
   
  Output variance (CLT) ρ2fresh = ( N + h + 1)σ 2 2 ρ2add = ρ21 + ρ22  
   
  can 2 2 2 2 2 2 2 m1 can ∞ B2 + m2 ∞ B1 + B1 B2 ρpre-mult = N ρ1 ρ2 + ρ2 m1 2 + ρ1 m2 2 √ 2 Key-Switch B + N ηks HC (α, N ) ρ2ks = ρ2 + ηks  ρ2 −1 N 1 2 Rescale q B + 12 (h + 1) HC (α, N ) ρrs = q2 + 12 (h + 1)  
   
  PreMult  
   
    
   
  Experimental Framework: We run experiments in the HEAAN v1.0 [24] and FullRNS-HEAAN [22] libraries. We note that neither the Textbook CKKS nor the RNS variant noise analysis is implementation-speciﬁc, and we chose the HEAAN library as it is the implementation most closely resembles the theoretical description of both variants of the scheme. The LWE parameters (ring dimension N , ciphertext modulus q, error standard deviation σ, secret distribution S) were set as follows. Following [1], we used (log2 (N ), log2 (q)) ∈ {(13, 109), (14, 219), (15, 443)} in HEAAN v1.0. We used (log2 (N ), log2 (q)) ∈ {(12, 100), (13, 100), (14, 220), (15, 420)} in FullRNSHEAAN. We used σ = 3.2 and the default secret distribution in both libraries. We set the error tolerance as α = 0.0001 and the scale parameter as Δ = 240 . For FullRNS-HEAAN the moduli chains are parameterised by L and k. The bit-size of the top-level modulus is generated by FullRNS-HEAAN as 60+(L−1)· log2 (Δ). For log(N ) ∈ {13, 14, 15}, we choose L to allow for a top-level modulus which is close to the choices in HEAAN v1.0. For log(N ) = 12 we choose a modulus large enough to support one multiplication. We always set the default library selection of k = L + 1. For both libraries we evaluate the following circuit, similarly to [15]. We generate fresh ciphertexts ct0 , ct1 and ct2 and evaluate the circuit ct2 ∗(ct1 +ct0 ), i.e. a homomorphic addition, followed by a (full) homomorphic multiplication. In each experiment, we iterate 1000 times and record the average, and maximum, observed noise. In Tables 4 and 5 we report the observed noises together with the noise predicted from the heuristics developed in this work: the average-case approach (CLT), and the worst-case heuristics (WCR and CE). We also compare with the noise predicted from the prior heuristics (P-CE). For the multiplication operation estimates we use worst-case message bounds, speciﬁcally Δ for WCR, N Δ2 for CLT, and 2NπΔ for CE. In Table 4 we report the experimental results for HEAAN v1.0 [24] in two settings. We ﬁrst (in the rows marked as ‘Ring’) report the observed noise in the plaintext space. In these experiments, in each trial, we generate a random plaintext with coeﬃcients in [−Δ, Δ], evaluate the speciﬁed circuit, and measure  
   
  340  
   
  A. Costache et al.  
   
  noise in the ring after each operation. We also (in the rows marked as ‘Real’ and ‘Complex’) report the observed noise in the message space. In Table 5 we report the experimental results for FullRNS-HEAAN [22] in the message space. For the HEAAN v1.0 [24] and FullRNS-HEAAN [22] experiments in the message space, in each trial, we generate a vector of random numbers, encode them, encrypt them, and homomorphically evaluate the circuit as described above. Then, we decrypt and decode and measure the precision loss. The rows marked as ‘Real’ correspond to generating numbers with real part and imaginary part both uniform in [0, 1], encoding and decoding with scale factor Δ, and reporting only the real error on the computation. The rows marks as ‘Complex’ correspond to generating numbers with real part and imaginary part both uniform in [0, 1] and reporting the magnitude of the largest error. While in exact schemes, it is trivial to observe the noise, this is not so straightforward for CKKS. Our methodology was to generate three plaintexts m1 , m2 and m3 , and to run the circuit both in the plaintext space and in the ciphertext space. In other words, the noise reported in Tables 4 and Table 5 is the result of ((m1 + m2 ) · m3 ) − Dec((Enc(m1 ) + Enc(m2 )) · Enc(m3 )) . Results: The plaintext space experiments of Table 4 illustrate that for Textbook CKKS [10], the average case noise approach (CLT) introduced in this work, and our reﬁnements to the prior worst case canonical embedding approach (CE), both improve on the heuristics given in prior work (P-CE), in the sense of predicting a value closer to the observed noise. For CLT compared to P-CE, the heuristicto-practical gap reduces from around 8 bits to less than 1 bit. However, the CLT approach slightly underestimates the maximal noise, and sometimes slightly underestimates the maximum noise (as illustrated in the column gap). The WCR approach leads to a large heuristic-to-practical gap after multiplication, which we also observed in the Complex experiments (in the message space), so we omit it in the FullRNS-HEAAN experiments. For the message space results in Table 4, the addition and multiplication results are similar for both the Real and Complex case. The CLT and CE approaches both underestimate the average and maximum noise by 3 to 7 bits. The WCR approach correctly bounds the noise: tightly for addition, but very loosely after multiplication. The results in Table 5 illustrate that for the RNS variant of [9], the CLT approach and the CE approach both improve on the prior approach (P-CE), in the sense of predicting a value closer to the observed noise. For CLT compared to P-CE, the heuristic-to-practical gap typically reduces from around 6 bits to less than 1 bit. However, we again very frequently observe the CLT giving an underestimate. At log N = 14, a jump is seen in the observed noise values, and in this case the prior approach P-CE gives a tight bound on the noise. Discussion: Our results illustrate that, for the plaintext space for Textbook CKKS, and for the message space in the RNS variant of [9], both the averagecase noise analysis introduced in this work and the reﬁnement of the prior  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  341  
   
  Table 4. Average and maximum bits of noise observed in the ring and message space over 1000 trials in HEAAN compared with noise predicted by the CLT, WCR and CE noise analyses. The column gap denotes the diﬀerence between the predicted CLT noise value and the maximum experimental observation, with a negative value representing a heuristic underestimate. log(N )  
   
  log(q)  
   
  Average  
   
  Maximum  
   
  CLT  
   
  WCR  
   
  CE  
   
  P-CE  
   
  gap  
   
  10.87 11.40 11.92  
   
  12.77 13.27 13.77  
   
  –1.20 –1.04 –1.12  
   
  12.61 13.13 13.66  
   
  14.32 14.82 15.32  
   
  –0.52 –0.34 –0.37  
   
  –29.13 –28.60 –28.08  
   
  –27.22 –26.72 –26.22  
   
  –6.28 –6.63 –7.33  
   
  –27.39 –26.87 –26.34  
   
  –25.68 –25.18 –24.68  
   
  –5.35 –6.06 –6.32  
   
  –29.13 –28.60 –28.08  
   
  –27.22 –26.72 –26.22  
   
  –6.51 –6.88 –7.41  
   
  –27.39 –26.87 –26.34  
   
  –25.68 –25.18 –24.68  
   
  –5.76 –6.22 –6.62  
   
  Ring Addition noise. 13 14 15  
   
  109 219 443  
   
  4.58 4.63 4.68  
   
  13 14 15  
   
  109 219 443  
   
  5.18 5.21 5.27  
   
  5.52 5.39 5.49  
   
  4.32 4.35 4.37  
   
  4.82 4.85 4.87  
   
  Ring Multiplication noise. 6.19 6.04 6.09  
   
  5.67 5.70 5.72  
   
  19.32 20.35 21.37  
   
  Real Addition error. 13 14 15  
   
  109 219 443  
   
  –25.37 –24.41 –23.35  
   
  13 14 15  
   
  109 219 443  
   
  –25.07 –24.03 –23.03  
   
  13 14 15  
   
  109 219 443  
   
  –24.81 –23.81 –22.76  
   
  –23.42 –22.55 –21.32  
   
  –29.70 –29.18 –28.65  
   
  –22.83 –21.80 –20.78  
   
  Real Multiplication error. –23.00 –21.77 –20.98  
   
  –28.35 –27.83 –27.30  
   
  –8.33 –6.30 –4.28  
   
  Complex Addition error. –23.12 –22.22 –21.17  
   
  –29.63 –29.10 –28.58  
   
  –22.83 –21.80 –20.78  
   
  Complex Multiplication error. 13 14 15  
   
  109 219 443  
   
  –24.45 –23.47 –22.41  
   
  –22.52 –21.53 –20.61  
   
  –28.28 –27.75 –27.23  
   
  –8.33 –6.30 –4.28  
   
  worst-case canonical embedding approach improve upon prior noise analyses in terms of modelling more closely the observed noise. Our work can thus represent an improved starting point for manual parameter selection compared to prior approaches. However, some discrepancies can be seen between the observed results and the predictions from the heuristic analyses. For example, in the multiplication results in the ring in Table 4, the WCR respectively CE bounds seem to increase by 1 respectively 0.5 bits as log N increases by 1 bit, while the average observed noise shows a much slower growth. As another example, in the message space results in Table 4, the prior canonical embedding approach (P-CE) also leads  
   
  342  
   
  A. Costache et al.  
   
  Table 5. Average and maximum bits of noise observed in the message space over 1000 trials in FullRNS-HEAAN compared with noise predicted by the CLT, CE and P-CE noise analyses. The column gap denotes the diﬀerence between the predicted CLT noise value and the maximum experimental observation, with a negative value representing a heuristic underestimate. log(N )  
   
  log(q)  
   
  L  
   
  k  
   
  Average  
   
  CLT  
   
  CE  
   
  P-CE  
   
  gap  
   
  –24.25 –23.23 –22.21 –21.19  
   
  –23.63 –22.61 –21.59 –20.57  
   
  –18.89 –17.89 –16.89 –15.89  
   
  –0.04 –0.30 –0.46 –0.45  
   
  –21.62 –20.61 –19.59 –18.57  
   
  –17.39 –16.39 –15.39 –14.39  
   
  –1.16 –0.53 –3.25 –3.17  
   
  –23.63 –22.61 –21.59 –20.57  
   
  –18.89 –17.89 –16.89 –15.89  
   
  –0.39 –0.74 –0.62 -0.55  
   
  –21.62 –20.61 –19.59 –18.57  
   
  –17.39 –16.39 –15.39 –14.39  
   
  –1.80 –0.91 –4.02 –3.92  
   
  Maximum  
   
  Real Addition error. 12 13 14 15  
   
  100 100 220 420  
   
  2 2 5 10  
   
  3 3 6 11  
   
  –24.38 –23.16 –22.07 –21.00  
   
  –24.21 –22.93 –21.75 –20.74  
   
  Real Multiplication error. 12 13 14 15  
   
  100 100 220 420  
   
  2 2 5 10  
   
  3 3 6 11  
   
  –21.86 –21.70 –17.79 –16.77  
   
  12 13 14 15  
   
  100 100 220 420  
   
  2 2 5 10  
   
  3 3 6 11  
   
  –24.03 –22.83 –21.84 –20.76  
   
  12 13 14 15  
   
  100 100 220 420  
   
  2 2 5 10  
   
  3 3 6 11  
   
  –21.80 –21.41 –17.67 –16.73  
   
  –22.96 –21.94 –20.92 –19.90  
   
  Complex Addition error. –23.78 –22.42 –21.52 –20.57  
   
  –24.17 –23.16 –22.14 –21.12  
   
  Complex Multiplication error. –21.17 –21.03 –16.94 –15.93  
   
  -21.08 –20.95 –16.82 –15.90  
   
  –22.88 –21.86 –20.84 -19.82  
   
  to underestimates of the predicted noise. Moreover, in the multiplication results of Table 5, there seems to be a jump in the observed noise after log N = 14, whereas the noise heuristics all grow more smoothly as log N grows. This means that, for larger log N , the P-CE approach gives a correct and tight noise growth prediction, while for smaller log N , the CLT and CE approaches give a closer prediction of the observed noise. This discussion suggests a fundamental issue with the modelling in all existing noise analysis approaches, including those prior to this work, suggesting that a reﬁned theory of CKKS noise is needed. One of the most crucial observations is that our heuristics underestimate the noise growth in many places (denoted by negative gap values in Tables 4 and 5). Similar underestimates have been observed in the literature before [3,16]. In more detail, the authors of [3] show that an average-case analysis of the BFV scheme that assumes independence of the coeﬃcients of the noise leads to underestimates of the multiplication noise, and they develop a correcting function to account  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  343  
   
  for this discrepancy. The authors of [16] compare experimental results of the BGV scheme as implemented in HElib to the theoretical bounds from the work of [29], and observe that the latter also underestimates the noise growth in practice. In contrast, the implementation-speciﬁc analysis of [16] is shown to very closely match the observed noise growth. Our heuristics are not speciﬁc to the implementations in HEAAN v1.0 [24] or Full-RNS HEAAN [22], and assumptions on which the heuristics rely may not hold for each implementation. For example, our experiments indicate that in HEAAN v1.0 [24] (though not in Full-RNS HEAAN [22]), the independence heuristic between coeﬃcients of the noise polynomial fails at encryption. We believe that developing implementationspeciﬁc noise analyses for CKKS is an important direction for future work.  
   
  References 1. Albrecht, M., et al.: Homomorphic encryption security standard. HomomorphicEncryption.org, Technical report (2018) 2. Al Badawi, A., et al.: Openfhe: open-source fully homomorphic encryption library. Cryptology ePrint Archive, Paper 2022/915 (2022). https://eprint.iacr.org/2022/ 915 3. Biasioli, B., Marcolla, C., Calderini, M., Mono, J.: Improving and automating BFV parameters selection: an average-case approach. Cryptology ePrint Archive, Paper 2023/600 (2023). https://eprint.iacr.org/2023/600 4. Boemer, F., Costache, A., Cammarota, R., Wierzynski, C.: ngraph-he2: a highthroughput framework for neural network inference on encrypted data. In: Brenner, M., Lepoint, T., Rohloﬀ, K. (eds.) Proceedings of the 7th ACM Workshop on Encrypted Computing & Applied Homomorphic Cryptography, WAHC@CCS 2019, London, UK, 11–15 November 2019, pp. 45–56. ACM (2019) 5. Brakerski, Z., Gentry, C., Vaikuntanathan, V.: (Leveled) fully homomorphic encryption without bootstrapping. In: Goldwasser, S. (ed.) ITCS 2012, pp. 309– 325. ACM (2012) 6. Brisebarre, N., Jolde¸s, M., Muller, J.-M., Nane¸s, A.-M., Picot, J.: Error analysis of some operations involved in the cooley-tukey fast fourier transform. ACM Trans. Math. Softw. (TOMS) 46(2), 1–27 (2020) 7. Chen, H., Dai, W., Kim, M., Song, Y.: Eﬃcient multi-key homomorphic encryption with packed ciphertexts with application to oblivious neural network inference. In: Cavallaro, L., Kinder, J., Wang, X.F., Katz, J. (eds.) ACM CCS 2019, pp. 395–412. ACM Press (2019) 8. Cheon, J.H., Han, K., Kim, A., Kim, M., Song, Y.: Bootstrapping for approximate homomorphic encryption. In: Nielsen, J.B., Rijmen, V. (eds.) EUROCRYPT 2018. LNCS, vol. 10820, pp. 360–384. Springer, Cham (2018). https://doi.org/10.1007/ 978-3-319-78381-9 14 9. Cheon, J.H., Han, K., Kim, A., Kim, M., Song, Y.: A full RNS variant of approximate homomorphic encryption. In: Cid, C., Jacobson Jr, M.J. (eds.) SAC 2018, vol. 11349 of LNCS, pp. 347–368. Springer, Heidelberg (2019). https://doi.org/10. 1007/978-3-030-10970-7 16 10. Cheon, J.H., Kim, A., Kim, M., Song, Y.: Homomorphic encryption for arithmetic of approximate numbers. In: Takagi, T., Peyrin, T. (eds.) ASIACRYPT 2017. LNCS, vol. 10624, pp. 409–437. Springer, Cham (2017). https://doi.org/10. 1007/978-3-319-70694-8 15  
   
  344  
   
  A. Costache et al.  
   
  11. Chillotti, I., Gama, N., Georgieva, M., Izabach`ene, M.: Faster fully homomorphic encryption: bootstrapping in less than 0.1 seconds. In: Cheon, J.H., Takagi, T. (eds.) ASIACRYPT 2016. LNCS, vol. 10031, pp. 3–33. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-53887-6 1 12. Chillotti, I., Gama, N., Georgieva, M., Izabach`ene, M.: TFHE: fast fully homomorphic encryption over the torus. J. Cryptology 33(1), 34–91 (2020) 13. Costache, A., Smart, N.P.: Which ring based somewhat homomorphic encryption scheme is best? In: Sako, K. (ed.) CT-RSA 2016. LNCS, vol. 9610, pp. 325–340. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-29485-8 19 14. Costache, A., Curtis, B.R., Hales, E., Murphy, S., Ogilvie, T., Player, R.: On the precision loss in approximate homomorphic encryption. Cryptology ePrint Archive, Paper 2022/162 (2022). https://eprint.iacr.org/2022/162 15. Costache, A., Laine, K., Player, R.: Evaluating the eﬀectiveness of heuristic worstcase noise analysis in FHE. In: Chen, L., Li, N., Liang, K., Schneider, S. (eds.) ESORICS 2020. LNCS, vol. 12309, pp. 546–565. Springer, Cham (2020). https:// doi.org/10.1007/978-3-030-59013-0 27 16. Costache, A., N¨ urnberger, L., Player, R.: Optimisations and tradeoﬀs for helib. In: Topics in Cryptology-CT-RSA 2023: Cryptographers’ Track at the RSA Conference 2023, San Francisco, CA, USA, 24–27 April 2023, Proceedings, pp. 29–53. Springer, Heidelberg (2023). https://doi.org/10.1007/978-3-031-30872-7 2 17. Damg˚ ard, I., Pastro, V., Smart, N., Zakarias, S.: Multiparty computation from somewhat homomorphic encryption. In: Safavi-Naini, R., Canetti, R. (eds.) CRYPTO 2012. LNCS, vol. 7417, pp. 643–662. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-32009-5 38 18. Fan, J., Vercauteren, F.: Somewhat practical fully homomorphic encryption. Cryptology ePrint Archive, Report 2012/144 (2012). http://eprint.iacr.org/2012/144 19. Gentry, C.: Fully homomorphic encryption using ideal lattices. In: Mitzenmacher, M. (ed.) 41st ACM STOC, pp. 169–178. ACM Press (2009) 20. Gentry, C., Halevi, S., Smart, N.P.: Homomorphic evaluation of the AES circuit. In: Safavi-Naini, R., Canetti, R. (eds.) CRYPTO 2012. LNCS, vol. 7417, pp. 850–867. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-32009-5 49 21. Halevi, S., Shoup, V.: Design and implementation of HElib: a homomorphic encryption library. Cryptology ePrint Archive, Report 2020/1481 (2020). https://eprint. iacr.org/2020/1481 22. Fullrns-heaan. https://github.com/KyoohyungHan/FullRNS-HEAAN. Version as at October 2018 23. Heaan v2.1. https://github.com/snucrypto/HEAAN. Version as at September 2021 24. Heaan v1.0. https://github.com/snucrypto/HEAAN/releases/tag/1.0. Version as at September 2018 25. HElib. https://github.com/shaih/HElib. Version as at January 2019 26. Iliashenko, I.: Optimisations of fully homomorphic encryption. PhD thesis, KU Leuven (2019) 27. Kim, A., Song, Y., Kim, M., Lee, K., Cheon, J.H.: Logistic regression model training based on the approximate homomorphic encryption. BMC Med. Genom. 11(4), 83 (2018) 28. Kim, A., Papadimitriou, A., Polyakov, Y.: Approximate homomorphic encryption with reduced approximation error. In: Galbraith, S.D. (ed.) CT-RSA 2022. LNCS, vol. 13161, pp. 120–144. Springer, Cham (2022). https://doi.org/10.1007/978-3030-95312-6 6  
   
  On the Precision Loss in Approximate Homomorphic Encryption  
   
  345  
   
  29. Kim, A., Polyakov, Y., Zucca, V.: Revisiting homomorphic encryption schemes for ﬁnite ﬁelds. In: Tibouchi, M., Wang, H. (eds.) ASIACRYPT 2021. LNCS, vol. 13092, pp. 608–639. Springer, Cham (2021). https://doi.org/10.1007/978-3-03092078-4 21 30. Lattigo v2.2.0. http://github.com/ldsec/lattigo. Version as at July 2021. EPFLLDS 31. Lepoint, T., Naehrig, M.: A comparison of the homomorphic encryption schemes FV and YASHE. In: Pointcheval, D., Vergnaud, D. (eds.) AFRICACRYPT 2014. LNCS, vol. 8469, pp. 318–335. Springer, Cham (2014). https://doi.org/10.1007/ 978-3-319-06734-6 20 32. Lee, Y., Lee, J.W., Kim, Y.S., Kim, Y., No, J.S., Kang, H.: High-Precision Bootstrapping for Approximate Homomorphic Encryption by Error Variance Minimization. In: Dunkelman, O., Dziembowski, S. (eds.) EUROCRYPT 2022. LNCS, vol. 13275, pp. 551–580. Springer, Cham (2022). https://doi.org/10.1007/978-3-03106944-4 19 33. Li, B., Micciancio, D., Schultz, M., Sorrell, J.: Securing approximate homomorphic encryption using diﬀerential privacy. In: Annual International Cryptology Conference, pp. 560–589. Springer, Heidelberg (2022). https://doi.org/10.1007/978-3031-15802-5 20 34. Li, B., Micciancio, D.: On the security of homomorphic encryption on approximate numbers. In: Canteaut, A., Standaert, F.-X. (eds.) EUROCRYPT 2021. LNCS, vol. 12696, pp. 648–677. Springer, Cham (2021). https://doi.org/10.1007/978-3030-77870-5 23 35. Murphy, S., Player, R.: A central limit framework for ring-lwe decryption. Cryptology ePrint Archive, Report 2019/452 (2019). https://eprint.iacr.org/2019/452 36. Murphy, S., Player, R.: Discretisation and product distributions in Ring-LWE. J. Math. Cryptol. 15(1), 45–59 (2021) 37. Ogilvie, T., Player, R., Rowell, J.: Improved privacy-preserving training using ﬁxed-hessian minimisation. In: Brenner, M., Lepoint, T. (eds.) Proceedings of the 8th Workshop on Encrypted Computing and Applied Homomorphic Cryptography (WAHC 2020) (2020). https://doi.org/10.25835/0072999 38. PALISADE Lattice Cryptography Library (release 1.11.5). https://palisadecrypto.org/. Accessed Sept 2021 39. Regev, O.: On lattices, learning with errors, random linear codes, and cryptography. J. ACM (JACM) 56(6), 1–40 (2009) 40. Microsoft SEAL (release 3.6). Microsoft Research, Redmond, WA. https://github. com/Microsoft/SEAL. Version as at November 2020  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems Mounika Pratapa(B)  
   
  and Aleksander Essex  
   
  Western University, London, Canada {mpratapa,aessex}@uwo.ca Abstract. The number-theoretic literature has long studied the question of distributions of sequences of quadratic residue symbols modulo a prime number. In this paper, we present an eﬃcient algorithm for generating primes containing chosen sequences of quadratic residue symbols and use it as the basis of a method extending the functionality of additively homomorphic cryptosystems. We present an algorithm for encoding a chosen Boolean function into the public key and an eﬃcient two-party protocol for evaluating this function on an encrypted sum. We demonstrate concrete parameters for secure function evaluation on encrypted sums up to eight bits at standard key sizes in the integer factorization setting. Although the approach is limited to applications involving small sums, it is a practical way to extend the functionality of existing secure protocols built on partially homomorphic encryption schemes. Keywords: Secure computation · Additive homomorphic encryption Quadratic residues · Residue symbol sequences  
   
  1  
   
  ·  
   
  Introduction  
   
  Ever since Yao’s Millionaires’ problem [29], distrusting parties have been computing things of mutual interest without sharing their respective inputs. Secure function evaluation (SFE) has many interesting applications in areas such as privacy-preserving machine learning [24], private information retrieval [10], similarity search in private databases such as genotype and other medical data [25], online voting [2], auctions [11] and private credit checking [18]. Despite recent advances in fully homomorphic encryption, partially homomorphic schemes (i.e., those oﬀering homomorphic operations with respect to a single operation) still play an important role in secure computation. For example, Switzerland requires internet-based elections to be cryptographically veriﬁable1 and the ﬁrst certiﬁed implementation is based around mix nets built from additively homomorphic encryption.2 1 2  
   
  Swiss Federal Chancellery Ordinance on Electronic Voting. Available: https://www. fedlex.admin.ch/eli/cc/2022/336/en. The Swiss Post E-voting System. Available: https://gitlab.com/swisspost-evoting.  
   
  c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 346–366, 2024. https://doi.org/10.1007/978-3-031-53368-6_17  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  347  
   
  In applications where partially homomorphic encryption is suﬃcient, such schemes can oﬀer more clear-cut parameterizations, more mature hardness assumptions, more straightforward implementations, and faster executions relative to their fully homomorphic counterparts. This paper presents a method for extending the functionality of additive homomorphic encryption schemes (speciﬁcally those with eﬃcient full decryption) by working in groups containing sequences of quadratic residues and nonresidues with a correspondence to a chosen Boolean function. Given an encrypted value Enc(x) and a Boolean function f : Zt → {0, 1}, we present an eﬃcient method for homomorphically evaluating Enc(f (x)) in a single public-key operation across a short (but non-trivial) interval 0 ≤ x < t. Let QR : Z × Z → {0, 1} be a function testing the quadratic residuosity of an integer x ∈ Zp , deﬁned as  0 if x is a quadratic residue modulo p. QR(x, p) = 1 otherwise. Given f (·) and an integer sequence of the form [αx+β | 0 ≤ x < t, and α, β > 0], our approach involves three components: 1. An eﬃcient algorithm for ﬁnding a prime p for which QR(αx + β, p) = f (x). 2. An additively homomorphic public-key cryptosystem embedding the required quadratic residue symbol sequence into the plaintext space, i.e., M ⊂ Zp . 3. A public homomorphic operation that can blind the encryption of αx + β while preserving its quadratic residue symbol modulo p (and hence the output of the function f (x)). Taken together, these components allow f (x) to be securely evaluated on an encrypted sum in the range 0 ≤ x < t for small (but non-trivial) values of t in a single public-key operation. Previously, patterns in quadratic residues have been exploited for the evaluation of speciﬁc functions such as secure integer comparison [17], sign function evaluation [1,30], and threshold functions [16]. However, secure evaluation of arbitrary functions using quadratic residue patterns appears to be a novel direction. Our work extends the approach of [16] to the general case. Contribution. We present an algorithm for generating primes with arithmetic sequences containing chosen quadratic residue symbols. These sequences extend the functionality of additively homomorphic cryptosystems and generalize the approach to secure evaluation of arbitrary functions by generating the candidate primes that facilitate the required quadratic residue symbol sequences. Using an additively homomorphic scheme with eﬃcient full decryption (such as the schemes due to Paillier [22] and Okamoto-Uchiyama [21]), given an encrypted sum Enc(x), we present parameters for evaluating arbitrary functions Enc(f (x)) for x up to t = 256 at the 4096-bit prime range and sums up to t = 512 where larger public-keys are acceptable.  
   
  348  
   
  2  
   
  M. Pratapa and A. Essex  
   
  Related Work  
   
  Finding patterns in quadratic residues and non-residues has been a subject in the number theory literature for a long time. Gauss posed the problem of ﬁnding the smallest quadratic non-residue np modulo a prime p [19], and papers over the past century have continued to reﬁne this bound, most recently by Carella [8] showing np  (log p)(log log p). Much of the subsequent literature has focused on distributions of consecutive runs of residues and non-residues, providing bounds on√ the size of a prime necessary to observe a speciﬁed run length. For some a ≥ 14 e, a sequence of length p1/4 for a given p contains at least one residue and one non-residue according to Burgess [7]. Research has long explored primes with at least  consecutive quadratic residues or non-residues. Studies by Brauer [5] and Davenport [14] examined arbitrary combinations of residues and non-residues, but these were restricted to very short sequences (t < 10). Records for run lengths were improved with the rise of scientiﬁc computing in the 1980s. For example, Buell [6] experimentally studied the smallest primes exhibiting a consecutive sequence of  residues followed by  non-residues. For a residue symbol sequences of the form QR(a + x, p) for 0 ≤ x < t, they achieved  = 9 and thus t = 18 for p = 414463. Since there are an equal number of residues and non-residues modulo an odd prime p, the probability that a particular integer will be a residue or nonresidue would be 1/2 across all primes if the distribution were uniform and random. Peralta [23] presented the probability of ﬁnding an arbitrary residue symbol sequence modulo p given the length of sequence t and found it deviates √ from random by a factor no more than t(3 + p)/p. Later research exploited the random-looking distributions of residue symbols for applications in watermarking [4] and pseudorandom bit generators [13,26,27], approximate pattern matching [15]. While there are several interesting applications exploiting the patterns in quadratic residues, Feige et al. [17] proposed a minimal model for secure integercomparison by exploiting the fact that, for p = 7, the Legendre symbols  x p ∀x = a − b | a, b ∈ [−2, 2] coincide with the sign function of x ∈ [−2, 2]. Following this, improved secure function evaluation protocols were proposed [1,30] to evaluate sign function by generating primes with the required quadratic residue symbol patterns modulo a prime number, speciﬁcally a Blum prime (p ≡ 3 mod 4). However, both these approaches [1,30] rely on ﬁnding consecutive long runs of quadratic residues alone, thus only useful to perform secure comparison based on sign function. The work in [16] used brute force to search for consecutive sequences of  residues followed by  non-residues, where the quadratic residuosity function is calculated as QR(x + a, p). The maximum value attained in this approach is t = 52, for a = 1134844 and p = 2269739. Such runs were exploited to evaluate the threshold function, i.e., a Heaviside function H(x), which is oﬀ until x = c.  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  349  
   
  Our approach. As we prove in the next section, the number of primes exhibiting a given residue symbol sequence is inﬁnite. However, in contrast to previous work, instead of relying on the Legendre symbols of consecutive numbers modulo a prime p, we present parameters to ﬁnd arithmetic sequences of the form αx + β, which can be used to generate primes toward secure evaluation of an arbitrary function with an integer domain and Boolean range.  
   
  3  
   
  Cryptographic Preliminaries  
   
  Let f be a function where, f : Zt → {0, 1} is deﬁned over an integer input x for 0 ≤ x < t. Our objective is to securely evaluate f without revealing its inputs. To achieve this, we present relevant notations and an algorithm that can provably generate primes embedding arithmetic residue symbol sequences that implement f . Such primes are useful to extend the functionality of additively homomorphic cryptosystems. Definition 1 (Legendre Symbol). The Legendre symbol is a function L : Z × Z → {−1, 0, 1} defined as: ⎧   ⎪ ⎨ 1 if x is quadratic residue mod p x ≡ −1 if x is quadratic non-residue mod p ⎪ p ⎩ 0 if x ≡ 0 mod p. It can be directly established that the quadratic residuosity function QR : Z × Z → {0, 1}, as deﬁned in the introduction:  0 if x is a quadratic residue modulo p. QR(x, p) = 1 otherwise. is a modiﬁcation of the Legendre symbol’s co-domain where   x p +1 QR(x, p) = . 2 Since the Legendre symbol is a completely multiplicative function of its top argument, so is the quadratic residuosity function. 3.1  
   
  Linear Embeddings of Boolean Functions in Residue Sequences  
   
  We deal with the following functions: – A function f : Zt → {0, 1} that needs to be securely evaluated over an input x, for 0 ≤ x < t and t ∈ Z+ . – The quadratic residuosity function QR : Z × Z → {0, 1} for secure evaluation of f .  
   
  350  
   
  M. Pratapa and A. Essex  
   
  – A mapping function h : Z → Zp , that maps an input x into h(x) = (αx + β) mod p. Here, {α, β} ∈ Z+ and are given as input parameters to facilitate the application of QR function. We are interested in locating primes p which contain some residue symbol sequences modulo a prime p that imitate the range of f . In other words, given and integers {α, β} we are looking for some p that can compute: QR(h(x), p) = f (x) for 0 ≤ x < t. Approach to Secure Computation. We can homomorphically evaluate f using an additive scheme as follows. Let CS = {Gen, Enc, Dec} be an additively homomorphic public-key cryptosystem that display the following homomorphisms: Enc(x1 ) · Enc(x2 ) = Enc(x1 + x2 mod p) and Enc(x1 )x2 = Enc(x1 x2 mod p). Given an encrypted value Enc(x) in the range 0 ≤ x < t, and an α, β > 0, one can homomorphically compute Enc(h(x)) as follows: Enc(h(x)) = Enc(x)α · Enc(β) = Enc(αx + β mod (p)). Applying the quadratic residue function to the decryption Enc(h(x)) yields. QR(Dec(Enc(h(x))), p) = QR(h(x), p) = f (x). This demonstrates the basic mechanics of the secure evaluation of f . Clearly, however, the decrypter could recover x from seeing h(x), and thus a homomorphic blinding function will be presented later in Sect. 4. 3.2  
   
  Prime Numbers with Chosen Residue Symbol Sequences  
   
  We begin with the Legendre symbol as a standard notation and later re-frame the discussion in terms of a quadratic residuosity function QR. To proceed further, we propose two theorems. Theorem 1. Given a list of t distinct primes {a1 , . . . , at } and a list of Legendre symbols {1 , . . . , t } where x ∈ {−1, 1}. For all 1 ≤ x ≤ t, a prime p can be generated such that   p = x . ax  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  351  
   
  Proof. The proof proceeds in two parts. In the ﬁrst part, we prove the existence of an integer p containing the chosen residue symbol sequence. In the second part, we prove the existence of p implies the existence of a prime p with the same properties. Because each ax is prime, each 0 < bx < ax is to be co-prime to ax , Chinese Remainder Theorem guarantees the existence of a unique solution p to the system of congruences formed by [ax ], [bx ]: p ≡ b0 mod a0 .. . p ≡ bt mod at . Since  
   
    
   
  bx ax for each 1 ≤ x < t we have  
   
    
   
  = x , andp ≡ bx mod ax ,   
   
  p ax  
   
   = x .  
   
  Now we show the existence of an integer p implies the existence of a prime p with the same congruences. Since p ≡ p mod Aprod , and therefore p ≡ bx mod ax , then   p = x . ax Finally, since p is relatively prime to Aprod , Dirichlet’s theorem guarantees there  
   
  are inﬁnitely many primes of the form kAprod + p . Theorem 2. For all t ∈ Z+ and all functions f : Zt → {0, 1} there exists a prime p and two integers 0 < α, β < p such that for all 0 ≤ x < t   αx + β +1 p = f (x) 2   denotes the Legendre symbol of αx + β modulo p. where αx+β p Proof. Let α, β, t be positive integers such that αx + β is prime for all 0 ≤ x < t. The existence of such an α, β is guaranteed for all t > 0 by a theorem due to Green and Tao [20], which proves the primes contain arbitrarily long arithmetic sequences, and, therefore, there exists an α, β for all t > 0 such that αx + β is prime for all 0 ≤ x < t. Given such a linear sequence of (αx + β)’s where all of them are prime valued,3 Theorem 1 guarantees there exists a prime p such that for all 0 ≤ x < t,   p = 2f (x) − 1. αx + β 3  
   
  Requiring all (αx + β) be prime is only done to facilitate the existence proof. In practice, Algorithm Gen (see Sect. 4.1) can generate suitable keypairs in the presence of composite (αx + β)’s.  
   
  352  
   
  M. Pratapa and A. Essex  
   
  Suppose there existed a p such that p ≡ 1 mod 4. By the law of quadratic reciprocity,     αx + β p = = 2f (x) − 1, (1) p αx + β and therefore,  
   
    
   
   αx + β +1 p . f (x) = 2 In the alternate case where all such primes p were congruent to 3 mod 4, Theorem 1 also guarantees there exists a prime p such that    2f (x) − 1 if αx + b ≡ 1 mod 4 p = αx + β 1 − 2f (x) if αx + b ≡ 3 mod 4.  
   
  For all αx + β ≡ 1 mod 4, quadratic reciprocity again gives us     αx + β p = = 2f (x) − 1. p αx + β Finally, for all αx + β ≡ 3 mod 4,     αx + β p =− = −(1 − 2f (x)) = 2f (x) − 1. p αx + β   
   
  Therefore f (x) = for all 0 ≤ x < t.  
   
  4  
   
  αx + β p 2  
   
   +1 (2)  

  Our Cryptosystem  
   
  Let CS = {Gen, Enc, Dec, Add, Smul, Eval} be an additively homomorphic publickey cryptosystem. Let M be the plaintext space and m ∈ M be a message. Without loss of generality and for the sake of a concrete description, we build CS based on the cryptosystem due to Okomoto and Uchiyama [21], which has a message space cardinality |M| = p for a large prime p. Given pre-computed sequence parameters α, β, and a Boolean function f : Zt → {0, 1} we deﬁne CS with the following functionalities: – Gen(1ρ , α, β, f ): Outputs secret key SK = {p, q} and public key PK = {n} where n = p2 q. Here p is chosen such that QR(αx + β, p) = f (x) for 0 ≤ x < t. To facilitate eﬃcient generation for non-trivial values of t, p is generated using the algorithm presented in Sect. 4.1. By contrast, q is randomly chosen using standard methods. Both |p| = |q| = λ, where λ is a standard length at the ρ-bit security level in the integer factorization setting.  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  353  
   
  – Enc(PK, m): Encryption function accepting a public key PK, plaintext m and outputting a ciphertext c = m. – Dec(SK, c): Decryption function accepting a private key SK, ciphertext c = m and outputting plaintext m. – Add(c1 , c2 ): Homomorphic addition accepting two encrypted messages c1 = m1  and c2 = m2  and outputting a ciphertext c = (m1 + m2 ) mod p. – Smul(s, c): Scalar homomorphic multiplication accepting a ciphertext c1 = m1  and a scalar m2 , outputting a ciphertext c = (m1 m2 ) mod p. The properties of CS can be further used to securely evaluate f on an encrypted plaintext. Toward that end, a sixth functionality Eval, is deﬁned: – Eval(PK, α, β, c): Secure evaluation of f on an encrypted plaintext c = m. First, a non-zero blinding parameter rc is uniformly sampled from the message space M. Since M = Zp and p is a private value, we sample uniformly from the public interval rc ← [1, 2λ ].4 The function computes: Smul(rc2 , Add(Smul(m, α), Enc(β)) = rc2 · (αm + β) mod p. Note that decrypting (αm + β) mod p directly would reveal m, as α, β are public. Hence, a blinding operation is applied to randomize this value while preserving its residuosity. The result of Eval is the encryption of uniform quadratic residue in Zp if QR(αm + β) = 1, and a uniform non-residue otherwise. The output of Eval can then be decrypted by the private key holder and the quadratic residuosity of the plaintext tested to reveal the outcome of f (m). 4.1  
   
  Key Generation  
   
  To securely evaluate a function of the form f : Zt → {0, 1}, we work in an additive group modulo a prime p which contains an arithmetic sequence S ⊂ Z∗p such that for each sm ∈ S, QR(sm ) = f (m), or, in Legendre symbol form, where:   sm = 1 − 2 · f (m). p This section describes a method for generating a prime p containing such a sequence.5 Step 1: Let S = {sm | sm = αm + β, 0 ≤ m < t} be an odd sequence for some α, β ∈ Z+ .  
   
  4  
   
  5  
   
  If implementing CS based on an additive cryptosystem in which |M| = n is a public value, such as in the case of DGK [12] or Paillier [22], blinding factor rc can be chosen from Zn . See our Python3 implementation of the key generation algorithm with example parameters: https://github.com/mounikapratapa/SFEPHE.  
   
  354  
   
  M. Pratapa and A. Essex (e  
   
  )  
   
  (e  
   
  )  
   
  m,ρm m,0 Step 2: For each sm ∈ S, let sm,0 , . . . , sm,ρ represent all of the prime m factors of sm for which em,j is an odd power.6 The multiplicative properties of the Legendre symbol give us    (em,0 )    (em,ρm ) sm,0 · . . . · sm,ρ sm,0 sm sm,ρm m = = · ... · = 1 − 2 · f (m) p p p p  
   
  This can be rewritten in terms of applications of QR by replacing multiplications with additions when expressing the factorization of individual sequence elements: QR(sm , p) = QR(sm,0 , p) + . . . + QR(sm,ρm , p) ≡ f (m) mod 2.  
   
  (3)  
   
  Expressing residues and non-residues in this form (i.e., as an addition modulo 2) instead of Legendre symbols (as a multiplication of signs) allows us to obtain a system of equations capturing the relationship between the unique prime factors of a sequence element and the function evaluated at that position. Step 3: Let A = {a0 , . . . , au−1 } represent the set of u unique prime factors from the combined set of all sequence factors sm,j across all sequence elements sm . That is, ai ∈ A if there is some sm such that ai | sm and ai  sj for all other j = m. Non-unique factors will not be utilized in this calculation and are assigned a ﬁxed, implicit target residue symbol of 1. For each sequence element sm ∈ S and each unique prime factor aj ∈ A, we deﬁne a function d(aj , sm ) such that  1 if aj | sm d(aj , sm ) = 0 otherwise. Step 4: Deﬁne a (t × u) matrix M . Let the last column represent function f evaluated at m. Form an augmented matrix representing the system of equations arising from Equation (3): s0 s1 .. . st−1  
   
  a1 ... au−1 d(a1 , s0 ) . . . d(au−1 , s0 ) f (0) ⎞ d(a1 , s1 ) . . . d(au−1 , s1 ) f (1) ⎟ ⎟ .. .. .. ⎠ . . . d(a0 , st−1 ) d(a1 , st−1 ) . . . d(au−1 , st−1 ) f (t − 1)  
   
  a0 ⎛ d(a , s ) 0 0 ⎜ d(a0 , s1 ) ⎜ .. ⎝ .  
   
  Step 5: Using Gaussian elimination, convert M into reduced row echelon form, i.e., compute M  ← RREF(M ). If the system of equations implied by M  is consistent and exactly determined, each aj ∈ A implies a 6  
   
  e  
   
  m,j Factors sm,j of even power will have a ﬁxed residue symbol of 1 and are excluded from the generation algorithm (having no bearing on the outcome).  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  355  
   
  residue value σj ∈ {0, 1} which will satisfy the overall requirement that QR(sm ) = f (m) for 0 ≤ m < t. If the system is consistent and underdetermined, select a single valid solution uniformly at random and proceed to Step 6. Otherwise repeat the same process from Step 1 with a diﬀerent α, β. Step 6: For each factor aj ∈ A and each residue value σj ∈ {0, 1} computed in the previous step, select bj uniformly from [1, aj ) such that QR(bj , aj ) = σj . Step 7: For each pair aj , bj , apply Chinese remaindering to compute a p satisfying the following system of congruences: p ≡ b0 mod a0 p ≡ b1 mod a1 .. .  p ≡ bu−1 mod au−1 . Step 8: Compute: p←k  
   
   u−1   
   
   aj + p  
   
  j=0 R  
   
  for k ← − [kmin , kmax ] sampled uniformly from the largest interval such that |p| = λ. Step 9: If p ≡ 1 mod 4 and p is prime, continue to the next step, otherwise repeat Steps 6–8 until such a p is found. Step 10: Generate a random prime q of length λ using a standard generation method suitable for the integer factorization setting. Step 11: Output p, q. 4.2  
   
  Encryption and Decryption  
   
  We recall the Okamoto-Uchiyama cryptosystem [21], which is the basis for CS. Key Generation: Run the key generation algorithm in Sect. 4.1 to obtain large primes p, q. Compute n = p2 q. Select a uniform g ∈ {2, . . . , n − 1} such that: g p−1 ≡ 1 mod p2 . Set h ← g n mod n. Return SK ← {p, q} and PK ← {n, g, h}. Observe: Z∗n is a cyclic group of order p(p − 1)(q − 1). Let us deﬁne two subgroups: Gp ⊂ Z∗n , the subgroup of order p, and Gφ ⊂ Z∗n , the subgroup of order φ = (p − 1)(q − 1). An isomorphism Z∗n ∼ = Gp × Gφ exists such that g ∈ Z∗n can be rewritten as gp gφ mod n, for some gp ∈ Gp and gp ∈ Gφ respectively. Thus 2 2 2 2 g has order p · φ whereas h = g n = g p q = gpp q gφp q = gp0 gφp q ≡ gφrˆ mod n has order φ.  
   
  356  
   
  M. Pratapa and A. Essex  
   
  Encryption: A plaintext 0 ≤ m < p is encrypted as follows. Uniformly sample r ← Z∗n . Output: c ← g m hr mod n. Observe: Following our notation, c = (gp gφ )m (gφrˆ )r = gpm gφm+ˆrr ≡ gpm gφr¯ mod n. In other words, m is captured in the subgroup of order p and gφr¯ is indistinguishable from a uniform element in Gφ . Decryption: A ciphertext c is decrypted as follows. First compute cˆ ← cφ mod n. Observe: cˆ = cφ = (gpm gφr¯ )φ = (gpm )φ (gφr¯ )φ = gpmφ gφ0 = gpmφ mod n. Compute the discrete logarithm of gpmφ to recover mφ. An eﬃcient algorithm exists for computing discrete logarithms in Gp ⊂ Z∗p2 q given knowledge of p (which we omit for space). Finally, compute (mφ) · φ−1 ≡ m mod p and return m. 4.3  
   
  Correctness of the Evaluation Function  
   
  The Eval function deﬁned in Sect. 4 is used to homomorphically evaluate f (m) on encrypted plaintext m. Theorem 3. Given c = Enc(m), QR(Dec(SK, Eval(PK, α, β, c)), p) = f (m) in the range 0 ≤ m < t. Proof. Given ciphertext c = Enc(m), a blinding factor rc sampled from the public interval rc ← [1, 2λ ], and public sequence parameters α, β, Eval computes: 2  
   
  c = Eval(PK, α, β, c) = (cα · β)rc mod n 2  
   
  = (mα · β)rc  
   
  = (αm + β) · rc2 . Decrypting c returns (αm + β) · rc2 mod p. Therefore applying the QR-function to the decryption result we have QR((αm + β) · rc2 , p) = QR(αm + β, p) · QR(rc2 , p) = f (m) · 1 = f (m).  

  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  5  
   
  357  
   
  Semantic Security of CS  
   
  We prove that CS is semantically secure under common hardness assumptions. While CS partially depends on the semantic security of the underlying OkamotoUchiyama [21] cryptosystem, the key-generation algorithm has been modiﬁed from the general case by the quadratic residuosity function. Here we argue that the modiﬁed key-generation function does not aﬀect the semantic security of the underlying cryptosystem. First, we recall some deﬁnitions. Definition 2. A function f : N → R is negligible with respect to n ∈ N, if for all 1 positive polynomials p, there exists some M ∈ Z such that it holds f (n) < p(n) every time n > M . Definition 3. Given an encryption function Enck (m), message length |m|, random number r and a security parameter 1n , a cryptosystem CS is semantically secure if there exists a pair of probabilistic polynomial time algorithms A and A such that, for every such pair: P r[A(1n , Enck (m), r) = m] − P r[A (1n , |m|, r) = m]  
  0, the output of B(c) depends on the residuosity of (m + γ 2 ), which in turn depends on the distribution of quadratic residues modulo p, giving us   1 P r[B(c) = T rue | m > 0] = + . 2 The overall probability P r[B(c) = T rue | m ← [0, λ]] = (P r[B(c) = T rue | m = 0] · P r[m = 0]) + (P r[B(c) = T rue | m > 0] · P r[m > 0])       1 1 1 + · 1− λ ≥  
   
  = 1· λ + 2 2 2 is non-negligible. This implies that as long as the semantic security of underlying encryption scheme holds, the QR() function does not reveal any additional information about the underlying plaintext message.  
   
  Security in the Presence of Factor Base [ax ]. Since the input parameters α, β and the range of f (x) are public, it is easy to determine the sequence and factor base [ax ]. The array [bx ] used for CRT is chosen randomly and it remains hidden. Since the chosen [bx ] and k varies with each iteration of the keygeneration algorithm, this adds additional randomness to the choice of primes. Leaking the Quadratic Residuosity of Elements in Zp . The public nature of α, β and the function f (x) creates access to a limited oracle that provides  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  359  
   
  information about quadratic residuosity QR(x, p) for t elements, where t is the function size. Due to this reason, the security of our system relies on a slightly weaker assumption than factoring n = p2 q. The alternate hardness assumption we propose here is factoring n = p2 q in the presence of the quadratic residuosity oracle QR(x, p). Although the information provided by such an oracle is highly restricted relative to Zn , whether this information can be exploited to factorize n remains an open question and requires further cryptanalytic eﬀorts.  
   
  6  
   
  Protocol  
   
  Protocol π is conducted between two semi-honest parties PA and PB . Let PA input a vector of plaintexts X = {x1 , . . . , xa } and PB input a vector of plaintexts Y = {y1 , . . . , yb } for xi , yj ∈ M. Let CS = {Gen, Enc, Dec, Add, Smul, Eval} be an additively homomorphic cryptosystem with the functionalities deﬁned in Sect. 4. Let PA be the holder of the private-key SK. Since our protocol is framed as an extension of existing protocols based on additive schemes (e.g., vector addition, weighted sums etc.). We begin by deﬁning a sub-protocol πsub , which capturing the existing protocol conducted on X and Y using the conventional functionalities {Enc, Add, Smul}. Suppose the output of πsub results in PA receiving a single ciphertext m where m represents the nominal outcome of πsub . Protocol π extends πsub with the Eval functionality to homomorphically compute f (m) given m. The full protocol π is presented in Fig. 1.  
   
  Fig. 1. Secure Function Evaluation Protocol π  
   
  Theorem 5. The secure evaluation of function f : Zt → {0, 1} by the protocol π is correct.  
   
  360  
   
  M. Pratapa and A. Essex   
   
  Proof. From Theorem 3 we previously established that QR(Dec(SK, c )) returns f (m) as  QR(Dec(SK, c ), p)) = f (m). 6.1  
   
  Participant Privacy During the Protocol π  
   
  Participant privacy while running the protocol π in two party setting guarantees that there is no inadvertent leakage of information in the presence of semi-honest adversaries. Such adversaries follow the protocol exactly but try to learn more information than allowed based on their respective inputs and any intermediate transcripts during the protocol execution. To formalize this idea, we adopt privacy by simulation approach by creating the view of the parties without the knowledge of any keys. Security proof is established by constructing a simulator S that generates a view for adversary that is computationally indistinguishable from its real view. Privacy by simulation requires certain notations to proceed further. – f = (fPA , fPB ) is the two-party functionality that is computed by the protocol π. – The view of PA and PB denoted as V iewPπA and V iewπ PB are deﬁned as: ViewπPA (x) = (xPA , rPA , mPB ), ViewπPB (x) = (xPB , rPB , mPA ). where, x = (xPA , xPB ) represent the inputs of the participants to the protocol, r = (rPA , rPB ) are the random values generated during the transaction and m = (mPA , mPB ) are the messages sent by the respective parties during the protocol. – We say that π securely computes f in the presence of a semi-honest adversary if we are able to construct the algorithms SPB to simulate PB ’s view to establish PA ’s privacy and SPA to simulate PA ’s view to establish PB ’s privacy. PA ’s Privacy For PA ’s privacy, we need to establish that PB ’s view is simulatable given PB ’s input xPB and output fPB (xPA , xPB ) = ⊥. We have output by PA denoted as OutputπPA , which is a result of the execution of protocol π on the combined input from both the parties. Theorem 6 (PA ’s privacy). There exists a probabilistic polynomial time algorithm SPB such that   c   SPB (xPB , ⊥), f (xPA , xPB ) ≡ ViewπPB , OutputπPA (x) . c  
   
  where ≡ indicates ciphertext indistinguishability.  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  361  
   
  Proof. The proof of PA ’s privacy is simple because PB only receives the ciphertext c = Enc(x), i.e., mPA = c. To simulate PB ’s view the simulator just needs to sample the messages of the form c ← Z∗n . Due to the semantic security of the CS, it is easy to establish that a ciphertext c = Enc(x) is computationally indistinguishable from an element of Z∗n . SPB can now compute the output using public key PK, c and PB ’s input xPB and computing f (xPA , xPB ) homomorphically,  
   
  thereby simulating V iewPB . 6.2  
   
  PB ’s Privacy  
   
  We rely on the similar approach to the proof of PA ’s privacy. We establish that ViewPA is simulatable given PA ’s input denoted as xPA . Note that, the privacy proof relies on the hiding properties of blinding factor b. Let QR, N R ⊂ Zn respectively denote the subsets of quadratic residues and non-residues modulo prime n. PA decrypts ciphertext c , this results in random looking message m = (f (xPA , xPB )α + β) · rc2 mod n where rc is a uniform element modulo n and thus rc2 is uniform in QR. Theorem 7 (PB ’s privacy). There exists a probabilistic polynomial time algorithm SPA such that   c  SPA (xPA , f (xPA , xPB )) ≡ ViewπPA , ⊥] Proof. Algorithm SPA will begin by directly computing encryptions using PK and the PA ’s input xPA and outputs f (xPA , xPB ). SPA now computes the quadratic residuosity using QR(f (xPA , xPB )) to check for the output, for 0 the decrypted plaintext would be of the form m ∈ N R. SPA samples m ←R N R for each 0 ≤ j < k − 1 and computes the corresponding ciphertexts c = Enc(m ) using PK. If f (m) = 1, then, plaintext would be of the form m ∈ QR. If (f (xPA , xPB )α + β) is a quadratic residue (resp. non-residue), then the term (f (xPA , xPB )α + β) · rc2 is indistinguishable from a random element in QR (resp. N R) as proved in [16], implying m values are indistinguishable from a real-world  
   
  plaintext (f (xPA , xPB )α + β) · rc2 mod p.  
   
  7 7.1  
   
  Results and Discussion Experiments  
   
  We implemented all the experiments in Python3 on a local Intel i7 quad-core processor @ 1.8 GHz with 8 GB RAM. Finding Sequence Parameters α,β. As part of the key generation algorithm in Sect. 4.1, our goal is to ﬁnd some linear sequence deﬁned by sm = αm + β and some prime p for which the Legendre symbol of sm modulo p matches a given Boolean function evaluated at f (m) over 0 ≤ m < t. For this to be true for all possible Boolean functions f , we require the residue symbols of each sm to be, in essence, independently programmable. This is not  
   
  362  
   
  M. Pratapa and A. Essex  
   
  possible for most sequences. For example, the factorization of elements of the sequence 3m + 2 is: 2, 5, 23 , 11, . . . . Here, no matter what prime p is chosen, the Legendre symbol of s0 is always the same as s2 . Conversely, the factorization of 4m + 3 is: 3, 7, 11, 3 · 5, . . . . Here, no matter the Legendre symbol of s0 , the symbol for s3 can be set independently by choosing a prime p for which 5 has the necessary symbol to give s3 the required symbol to match f (3). In general, we can independently “program” each symbol of the sequence so long as each sm contains at least one unique odd-powered prime factor. This condition is simple and suﬃcient, although not strictly necessary (cf. steps 3–5 of Sect. 4.1) We took a brute-force approach to ﬁnding sequence parameters α, β for different values of t. We began with the smallest step size (α) and starting point (β), incrementing β across a heuristically chosen intervals at each function size t. For example, at t = 512, we searched in the range α < 6000, β < 100 and recorded the parameters yielding the minimal bit-length |p|. These parameters are not optimal. The goal was to demonstrate practical, concrete values of α, β. Finding more eﬃcient algorithms and computing optimal bounds on |p| is left to future work. For each candidate α/β, we generated the sequence [αx + β] for 0 ≤ x < t. We factored each sequence element, and for each prime factor (sm )em for which set factors A as  deﬁned in Step 3 of Sect. 4.1. em is odd, we added sm to the  Let the product of this set be A. Since p > A (see step 8 in Sect. 4.1),    we set |p| = log2 ( A) representing the lower bound of |p|. For each domain cardinality t, we report the α, β leading to the smallest |p| found in our search. For example, the linear sequence formed by α = 342, β = 787 contains unique factors suﬃcient to produce sub 3000-bit primes for evaluating 8-bit Boolean functions. See Table 1 for our experimentally found sequence parameters. Generating Prime p . Once suitable α, β are found, the steps to ﬁnd p are implemented according to the key-generation algorithm described in Sect. 4.1. We ran our implementation of the key generation function Gen for various function sizes. From table 2, the most time is taken to iteratively ﬁnd the right set of bx . To speed up this step, we built a look up table containing sets of all quadratic residues and non-residues with respect to each ax and for each iteration, the suitable bx is randomly chosen. Additionally, the CRT step has to be computed each time we ﬁnd a new set of bx ’s. CRT has complexity O((S1 +S2 )2 ), where Si denotes the number of digits in the modulus, making it proportional to the number of congruences. We also performed the comparison of our approach with its predecessor from [16]. Whereas the previous work focused on a speciﬁc function class (thresholds), our approach works across the entire class of Boolean functions, and at larger domain sizes. In fact, due to the usage of pre-determined α, β values, generating the right prime for the largest function size of 512 took less than 2 minutes, including all the steps. The search-based approach introduced in [16] takes more than 20 min to produce a sequence of size 26.  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  363  
   
  Table 1. Sample α, β Domain cardinality t α  
   
  β  
   
  |p| (bits)  
   
  8  
   
  2  
   
  27  
   
  46  
   
  16  
   
  6  
   
  29  
   
  97  
   
  32  
   
  20  
   
  53  
   
  263  
   
  64  
   
  84  
   
  305 719  
   
  128  
   
  90  
   
  197 1218  
   
  256  
   
  342  
   
  787 2858  
   
  512  
   
  1938 31  
   
  7066  
   
  Table 2. Run times for Gen Function size (domain cardinality t) 512  
   
  7.2  
   
  256  
   
  128  
   
  50  
   
  Gaussian Elimination  
   
  0.236 0.078 0.015 0.004  
   
  Test for consistency  
   
  0.016 0.009 0.002 0.001  
   
  Finding the right bx  
   
  87.30 21.00 3.900 0.560  
   
  CRT  
   
  25.24 3.6  
   
  0.142 0.062  
   
  An Example Case of Secure Function Evaluation Using π  
   
  Our scheme has several potential applications that involve secure function evaluation. Speciﬁcally, we eliminate the need to display intermediate computations to either party involved in the transaction. For example: in case of similar patients query, the state of the art approaches [3,9,25,28,31] using either homomorphic encryption or other multiparty computation techniques require several communication rounds to retrieve the records of patients sharing similar genetic makeup. To reduce the communication overhead, they display the similarity score for each record directly to the querying party leading to regression based database re-identiﬁcation attacks. There is a scope for such attacks in other applications such as secure machine learning inference, especially in classiﬁcation problems. Our scheme can be applied to display the class labels directly while hiding intermediate scores. To test the performance of our parameters, we implemented a simple secure function evaluation on threshold functions protocol from Sect. 6. The threshold function denoted as τt (x) is similar to that of the one implemented in [16]. Note that we implemented the same threshold function to demonstrate the eﬃciency of our protocol. However, we can implement any function with a boolean co-domain using the same protocol. The range of a threshold function is {0, 0, 0, ...., 1, 1, 1}, with a maximum length of k. For a ﬁxed threshold value t, the inputs to Gen would be {α, β, f (x) = {0}tx=0  {1}kx=t+1 }. With p, q from Gen and n = p2 q, compute g ∈ 2, · · · , n − 1 such that g p−1 ≡ 1 mod p2 and h = g n mod n. Finally, PK = (α, β, n, g, h) and SK = (p, q). Similar to  
   
  364  
   
  M. Pratapa and A. Essex  
   
  the work in [16], the protocol uses Dice coeﬃcient as a similarity metric to perform linkage between two datasets that consist of user names. The records are linked approximately to address any variations or errors in the strings being compared. Accordingly, the records will be considered a match if the Dice coeﬃcient between two strings is above a threshold value. The protocol is performed between two parties PA , PB as shown below. The details for sub-protocols 1 and 2 can be referred from [16]. The diﬀerence with our protocol is during computing the evaluation function, which is in Step 6 of sub-protocol 2 in [16]. Particularly, Protocol 1 in [16] is modiﬁed as follows: – Public parameters: PK, α, β and for a threshold value t maximum set cardinality μ, where μ = t – Private parameters: Party PA holds a list of strings [a1 , . . . , an ] and private keys. Party PB holds a list of strings [b1 , . . . , bn ] – PA , PB produce set intersection cardinalities between the private inputs using sub-protocol 1 from [16] – By modifying the ﬁnal step in sub-protocol 2 in [16] both parties compute threshold dice coeﬃcient dij such that dij = Eval(PK, α, β, θb ) = ((θb )α · 2 β)rc – Output: For all threshold dice coeﬃcient values, if QR(Dec(SK, dij ), p) = 1, PA outputs the index. For a given threshold function, we can compute the dice-coeﬃcient for more precise threshold values compared to [16]. Due to the ability of Gen to generate primes as per the f (x) s range, we can compute any kind of functions securely unlike the approaches in [1,16,30] (see table 3).  
   
  Table 3. Comparison between secure function evaluation protocols that rely on the runs of quadratic residues. Performance Indicator  
   
  Noisy Legendre Symbol [1]  
   
  Domain cardinality 623 (t)  
   
  Yu’s Protocol [30]  
   
  Residue PHE [16]  
   
  Our Protocol  
   
  Ω(log(p))  
   
  26  
   
  512  
   
  Residue symbol sequence type  
   
  {1}t  
   
  {1}t  
   
  [0]t || [1]t  
   
  {0, 1}t  
   
  Secure function evaluation type  
   
  Speciﬁc (sign functions)  
   
  Speciﬁc (sign functions)  
   
  Speciﬁc (thresholds)  
   
  General (Boolean)  
   
  8  
   
  Conclusion  
   
  This paper discusses a method to extend the functionality of additively homomorphic schemes in applications where the encrypted sum is below a threshold t  
   
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
   
  365  
   
  based on chosen patterns on quadratic residues modulo a prime. We developed a novel algorithm to encode such patterns into the private keys of cryptosystems in the integer factorization setting. We presented a protocol with concrete parameterizations for eﬃciently evaluating arbitrary Boolean functions on encrypted sums up to t = 512. Future work will seek to push the domain cardinality t to higher values and will also explore the possibility of integrating of this technique in the fully homomorphic setting.  
   
  References 1. Abspoel, M., Bouman, N.J., Schoenmakers, B., de Vreede, N.: Fast secure comparison for medium-sized integers and its application in binarized neural networks. In: Matsui, M. (ed.) CT-RSA 2019. LNCS, vol. 11405, pp. 453–472. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-12612-4_23 2. Adida, B.: Helios: web-based open-audit voting. In: USENIX Security Symposium, vol. 17, pp. 335–348 (2008) 3. Asharov, G., Halevi, S., Lindell, Y., Rabin, T.: Privacy-preserving search of similar patients in genomic data. In: Proceedings on Privacy Enhancing Technologies, vol. 2018, no. 4, pp. 104–124 (2018) 4. Atallah, M.J., Wagstaﬀ Jr, S.S.: Watermarking with quadratic residues. In: Security and Watermarking of Multimedia Contents, vol. 3657, pp. 283–288. International Society for Optics and Photonics (1999) 5. Brauer, A.: Combinatorial methods in the distribution of k-th. power residues. Comb. Math. Appl. 14–37 (1969) 6. Buell, D.A., Hudson, R.H.: On runs of consecutive quadratic residues and quadratic nonresidues. BIT Numer. Math. 24(2), 243–247 (1984) 7. Burgess, D.A.: The distribution of quadratic residues and non-residues. Mathematika 4(2), 106–112 (1957) 8. Carella, N.A.: Consecutive quadratic residues and quadratic nonresidue modulo p. arXiv preprint arXiv:2011.11054 (2020) 9. Cheng, K., Hou, Y., Wang, L.: Secure similar sequence query on outsourced genomic data. In: Proceedings of the 2018 on Asia Conference on Computer and Communications Security, pp. 237–251 (2018) 10. Chor, B., Goldreich, O., Kushilevitz, E., Sudan, M.: Private information retrieval. In: Proceedings of IEEE 36th Annual Foundations of Computer Science, pp. 41–50. IEEE (1995) 11. Damgard, I., Geisler, M., Kroigaard, M.: Homomorphic encryption and secure comparison. Int. J. Appl. Cryptogr. 1(1), 22–31 (2008) 12. Damgård, I., Geisler, M., Krøigaard, M.: Eﬃcient and secure comparison for online auctions. In: Pieprzyk, J., Ghodosi, H., Dawson, E. (eds.) ACISP 2007. LNCS, vol. 4586, pp. 416–430. Springer, Heidelberg (2007). https://doi.org/10.1007/9783-540-73458-1_30 13. Damgård, I.B.: On the randomness of legendre and jacobi sequences. In: Goldwasser, S. (ed.) CRYPTO 1988. LNCS, vol. 403, pp. 163–172. Springer, New York (1990). https://doi.org/10.1007/0-387-34799-2_13 14. Davenport, H.: On the distribution of quadratic residues (mod p). J. Lond. Math. Soc. 1(1), 49–54 (1931)  
   
  366  
   
  M. Pratapa and A. Essex  
   
  15. Egidi, L., Manzini, G.: Better spaced seeds using quadratic residues. J. Comput. Syst. Sci. 79(7), 1144–1155 (2013) 16. Essex, A.: Secure approximate string matching for privacy-preserving record linkage. IEEE Trans. Inf. Forensics Secur. 14(10), 2623–2632 (2019) 17. Feige, U., Killian, J., Naor, M.: A minimal model for secure computation. In: Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, pp. 554–563 (1994) 18. Frikken, K., Atallah, M., Zhang, C.: Privacy-preserving credit checking. In: Proceedings of the 6th ACM Conference on Electronic Commerce, pp. 147–154 (2005) 19. Gauss, C.F.: Disquisitiones arithmeticae, vol. 157. Yale University Press, Yale (1966) 20. Green, B., Tao, T.: The primes contain arbitrarily long arithmetic progressions (2004) 21. Okamoto, T., Uchiyama, S.: A new public-key cryptosystem as secure as factoring. In: Nyberg, K. (ed.) EUROCRYPT 1998. LNCS, vol. 1403, pp. 308–318. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0054135 22. Paillier, P.: Public-key cryptosystems based on composite degree residuosity classes. In: International Conference on the Theory and Applications of Cryptographic Techniques, pp. 223–238. Springer, Heidelberg (1999). https://doi.org/ 10.1007/3-540-48910-x_16 23. Peralta, R.: On the distribution of quadratic residues and nonresidues modulo a prime number. Math. Comput. 58(197), 433–440 (1992) 24. Riazi, M.S., Weinert, C., Tkachenko, O., Songhori, E.M., Schneider, T., Koushanfar, E.: Chameleon: a hybrid secure computation framework for machine learning applications. In: Proceedings of the 2018 on Asia Conference on Computer and Communications Security, ASIACCS ’18, pp. 707–721. Association for Computing Machinery, New York (2018) 25. Salem, A., Berrang, P., Humbert, M., Backes, M.: Privacy-preserving similar patient queries for combined biomedical data. In: Proceedings on Privacy Enhancing Technologies, vol. 2019, no. 1, pp. 47–67 (2019) 26. Sárközy, A.: On ﬁnite pseudorandom binary sequences and their applications in cryptography. Tatra Mt. Math. Publ. 37, 123–136 (2007) 27. Sárközy, A., Stewart, C.L.: On pseudorandomness in families of sequences derived from the legendre symbol. Period. Math. Hung. 54(2), 163–173 (2007) 28. Schneider, T., Tkachenko, O.: Episode: eﬃcient privacy-preserving similar sequence queries on outsourced genomic databases. In: Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security, pp. 315–327 (2019) 29. Yao, A.C.C.: How to generate and exchange secrets. In: 27th Annual Symposium on Foundations of Computer Science (SFCS 1986), pp. 162–167. IEEE (1986) 30. Yu, C.H.: Sign modules in secure arithmetic circuits. Cryptology ePrint Archive (2011) 31. Zhu, R., Huang, Y.: Eﬃcient privacy-preserving general edit distance and beyond. IACR Cryptol. ePrint Arch. 2017, 683 (2017)  
   
  Public-Key Cryptography  
   
  Generalized Implicit Factorization Problem Yansong Feng1,2 , Abderrahmane Nitaj3(B) , and Yanbin Pan1,2(B) 1  
   
  2  
   
  Key Laboratory of Mathematics Mechanization, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China [email protected]  School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China 3 Normandie Univ, UNICAEN, CNRS, LMNO, 14000 Caen, France [email protected]   
   
  Abstract. The Implicit Factorization Problem (IFP) was ﬁrst introduced by May and Ritzenhofen at PKC’09, which concerns the factorization of two RSA moduli N1 = p1 q1 and N2 = p2 q2 , where p1 and p2 share a certain consecutive number of least signiﬁcant bits. Since its introduction, many diﬀerent variants of IFP have been considered, such as the cases where p1 and p2 share most signiﬁcant bits or middle bits at the same positions. In this paper, we consider a more generalized case of IFP, in which the shared consecutive bits can be located at any positions in each prime, not necessarily required to be located at the same positions as before. We propose a lattice-based algorithm to solve this problem under speciﬁc conditions, and also provide some experimental results to verify our analysis.  
   
  Keywords: Implicit Factorization Problem algorithm · Coppersmith’s algorithm  
   
  1  
   
  · Lattice · LLL  
   
  Introduction  
   
  In 1977, Rivest, Shamir, and Adleman proposed the famous RSA encryption scheme [18], whose security is based on the hardness of factoring large integers. RSA is now a very popular scheme with many applications in industry for information security protection. Therefore, its security has been widely analyzed. Although it seems infeasible to break RSA with large modulus entirely with a classical computer now, there still exist many vulnerable RSA instances. For instance, small public key [7,8] or small secret key [4] can lead to some attacks against RSA. In addition, side-channel attacks pose a great threat to RSA [2,5,6], targeting the decryption device to obtain more information about the private key. It is well known that additional information on the private keys or the prime factors can help attack the RSA scheme eﬃciently. In 1997, Coppersmith [8,14] c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 369–384, 2024. https://doi.org/10.1007/978-3-031-53368-6_18  
   
  370  
   
  Y. Feng et al.  
   
  proposed an attack that can factor the RSA modulus N = pq in polynomial time if at least half of the most (or least) signiﬁcant bits of p are given. In 2013, by using Coppersmith’s method, Bernstein et al. [3] showed that an attacker can eﬃciently factor 184 distinct RSA keys generated by government-issued smart cards. At PKC 2009, May and Ritzenhofen [15] introduced the Implicit Factorization Problem (IFP). It concerns the question of factoring two n-bit RSA moduli N1 = p1 q1 and N2 = p2 q2 , given the implicit information that p1 and p2 share γn of their consecutive least signiﬁcant bits, while q1 and q2 are αn-bit. Using a two-dimensional lattice, May and Ritzenhofen obtained a heuristic result that this implicit information is suﬃcient to factor N1 and N2 with a lattice-based algorithm, provided that γn > 2αn + 2. In a follow-up work at PKC 2010, Faug`ere et al. [9] generalized the Implicit Factorization Problem to the case where the most signiﬁcant bits (MSBs) or the middle bits of p1 and p2 are shared. Speciﬁcally, they established the bound of γn > 2αn + 2 for the case where the MSBs are shared, using a two-dimensional lattice. For the case where the middle bits of p1 and p2 are shared, Faug`ere et al. obtained a heuristic result that q1 and q2 could be found from a threedimensional lattice if γn > 4αn + 6. In 2011, Sarkar and Maitra [21] further expanded the Implicit Factorization Problem by revealing the relations between the Approximate Common Divisor Problem (ACDP) and the Implicit Factorization Problem, and presented the bound of γ > 2α − α2 for the following three cases. 1. the primes p1 , p2 share an amount of the least signiﬁcant bits (LSBs); 2. the primes p1 , p2 share an amount of most signiﬁcant bits (MSBs); 3. the primes p1 , p2 share both an amount of least signiﬁcant bits and an amount of most signiﬁcant bits. In 2016, Lu et al. [13] presented a novel algorithm and improved the bounds to γ > 2α − 2α2 for all the above three cases of the Implicit Factorization Problem. In 2015, Peng et al. [17] revisited the Implicit Factorization Problem with shared middle bits and improved the bound of Faug`ere et al. [9] up to γ > 4α − 3α2 . The √ bound was further enhanced by Wang et al. [22] in 2018 up to γ > 4α − 4α α. It is worth noting that in the previous cases, the shared bits are located at the same position for the primes p1 and p2 . In this paper, we present a more generalized case of the Implicit Factorization Problem that allows for arbitrary consecutive shared locations, rather than requiring them to be identical in the primes, as in previous research. More precisely, we propose the Generalized Implicit Factorization Problem (GIFP), which concerns the factorization of two n-bit RSA moduli N1 = p1 q1 and N2 = p2 q2 when p1 and p2 share γn consecutive bits, where the shared bits are not necessarily required to be located at the same positions. See Fig. 1 for an example, where the starting positions for the shared bits in p1 and p2 may be diﬀerent.  
   
  Generalized Implicit Factorization Problem  
   
  371  
   
  Fig. 1. Shared bits M for p1 and p2  
   
  We transform the GIFP into the Approximate Common Divisor Problem and then, employ Coppersmith’s method with some optimization√strategy, we propose a polynomial time algorithm to solve it when γ > 4α(1 − α). In Table 1, we present a comparison of our new bound on γ with the known former bounds obtained by various methods to solve the Implicit Factorization Problem. Table 1. Asymptotic lower bound of γ in the Implicit Factorization Problem for n-bit N1 = p1 q2 and N2 = p2 q2 where the number of shared bits is γn, q1 and q2 are αn-bit. MSBs  
   
  both LSBs-MSBs Middle bits General  
   
  May, Ritzenhofen [15] 2α  
   
  LSBs  
   
  –  
   
  –  
   
  –  
   
  –  
   
  Faug` ere, et al. [9]  
   
  2α  
   
  -  
   
  -  
   
  4α  
   
  –  
   
  Sarkar, Maitra [21]  
   
  2α − α2  
   
  2α − α2  
   
  2α − α2  
   
  -  
   
  –  
   
  Lu, et al. [13]  
   
  2α − 2α2 2α − 2α2 2α − 2α2  
   
  -  
   
  -  
   
  Peng, et al. [17]  
   
  –  
   
  –  
   
  -  
   
  Wang, et al. [22]  
   
  –  
   
  –  
   
  –  
   
  4α − 3α2 √ 4α(1 − α) –  
   
  This work  
   
  –  
   
  –  
   
  –  
   
  –  
   
  4α(1 −  
   
  √  
   
  α)  
   
  It can be seen in Table 1 that the bounds for the Implicit Factorization Problem for sharing middle bits are inferior to those of other variants. This is because the unshared bits in the Implicit Factorization Problem for LSBs or MSBs or both LSBs and MSBs are continuous, and only one variable is necessary to represent the unshared bits while at least two variables are needed to represent the unshared bits in the Implicit Factorization Problem sharing middle bits or GIFP. In addition, our bound for GIFP is identical to the variant of IFP sharing the middle bits located in the same position. However, it is obvious that the GIFP relaxes the constraints for the positions of the shared bits. Therefore, with the same bound for the number of shared bits as in the IFP sharing middle bits at the same position, we show that the Implicit Factorization Problem can still be solved eﬃciently when the positions for the sharing bits are located diﬀerently. There are still open √ problems, and the most important one is: can we improve our bound 4α (1 − α) for GIFP to 2α − 2α2 or even better? A positive answer  
   
  372  
   
  Y. Feng et al.  
   
  seems not easy since the bound for GIFP directly yields a bound for any known √ variant of IFP. Improving the bound for GIFP to the one better than 4α (1 − α) means that we can improve the bound for the variant of IFP sharing the middle bits located in the same position, and improving the bound for GIFP to the one better than 2α − 2α2 means that we can improve the bound for any known variant of IFP. Roadmap. Our paper is structured as follows. Section 2 presents some required background for our approaches. In Sect. 3, we present our analysis of the Generalized Implicit Factorization Problem, which constitutes our main result. Section 4 details the experimental results used to validate our analysis. Finally, we provide a brief conclusion in Sect. 5.  
   
  2  
   
  Notations and Preliminaries  
   
  Notations. Let Z denote the ring of integers, i.e., the set of all integers. We use lowercase bold letters (e.g., v) for vectors and uppercase bold letters (e.g., A) n represents the number of ways to select m items for matrices. The notation m n n! = 0. out of n items, which is deﬁned as m!(n−m)! . If m > n, we set m 2.1  
   
  Lattices, SVP, and LLL  
   
  Let m ≥ 2 be an integer. A lattice is a discrete additive subgroup of Rm . A more explicit deﬁnition is presented as follows. Definition 1 (Lattice). Let v1 , v2 , . . . , vn ∈ Rm be n linearly independent vectors with n ≤ m. The lattice L spanned by {v1 , v2 , . . . , vn } is the set of all integer linear combinations of {v1 , v2 , . . . , vn }, i.e.,   n  ai vi , ai ∈ Z . L = v ∈ Rm | v = i=1  
   
  The integer n denotes the rank of the lattice L, while m represents its dimension. The lattice L is said to be full rank if n = m. We use the matrix B ∈ Rn×m , where each vector vi contributes a row to B. The determinant of L is deﬁned  as det(L) = det (BBt ), where Bt is the transpose of B. If L is full rank, this reduces to det(L) = |det (B)|. The Shortest Vector Problem (SVP) is one of the famous computational problems in lattices. Definition 2 (Shortest Vector Problem (SVP)). Given a lattice L, the Shortest Vector Problem (SVP) asks to ﬁnd a non-zero lattice vector v ∈ L of minimum Euclidean norm, i.e., ﬁnd v ∈ L\{0} such that v ≤ w for all non-zero w ∈ L.  
   
  Generalized Implicit Factorization Problem  
   
  373  
   
  Although SVP is NP-hard under randomized reductions [1], there exist algorithms that can ﬁnd a relatively short vector, instead of the exactly shortest vector, in polynomial time, such as the famous LLL algorithm proposed by Lenstra, Lenstra, and Lovasz [12] in 1982. The following result is useful for our analysis [14]. Theorem 1 (LLL Algorithm). Given an n-dimensional lattice L, we can ﬁnd an LLL-reduced basis {v1 , v2 , . . . , vn } of L in polynomial time, which satisﬁes n(n−1)  
   
  1  
   
  vi  ≤ 2 4(n+1−i) det(L) n+1−i ,  
   
  for  
   
  i = 1, . . . , n.  
   
  Theorem 1 presents the upper bounds for the norm of the i-th vector in the LLL-basis using the determinant of the lattice. 2.2  
   
  Coppersmith’s Method  
   
  In 1996, Coppersmith [8,14] proposed a lattice-based method for ﬁnding small solutions of univariate modular polynomial equations modulo a positive integer M , and another lattice-based method for ﬁnding the small roots of bivariate polynomial equations. The methods are based on ﬁnding short vectors in a lattice. We brieﬂy sketch the idea below. More details can be found in [14]. Let M be a positive integer, and f (x1 , . . . , xk ) be a polynomial with integer coeﬃcients. Suppose we want to ﬁnd a small solution (y1 , . . . , yk ) of the modular equation f (x1 , . . . , xk ) ≡ 0 (mod M ) with the bounds yi < Xi for i = 1, . . . , k. The ﬁrst step is to construct a set G of k-variate polynomial equations such that, for each gi ∈ G with i = 1, . . . , k, we have gi (y1 , . . . , yk ) ≡ 0 (mod M ). Then we use the coeﬃcient vectors of gi (x1 X1 , . . . , xk Xk ), i = 1, . . . , k, to construct a k-dimensional lattice L. Applying the LLL algorithm to L, we get a new set H of k polynomial equations hi (x1 , . . . , xk ), i = 1, . . . , k, with integer coeﬃcients such that hi (y1 , . . . , yk ) ≡ 0 (mod M ). The following result shows (y1 , . . . , yk ) = 0 over the integers in some cases, where that one can get hi for h(x1 , . . . , xk ) = i1 ...ik ai1 ...ik xi11 · · · xi1k , the Euclidean norm is deﬁned by  2 h(x1 , . . . , xk ) = i1 ...ik ai1 ...ik . Theorem 2 (Howgrave-Graham [11]). Let h(x1 , . . . , xk ) ∈ Z[x1 , . . . , xk ] be a polynomial with at most ω monomials. Let M be a positive integer. If there exist k integers (y1 , . . . , yk ) satisfying the following two conditions: 1. h(y1 , . . . , yk ) ≡ 0 (mod M ), and there exist k positive integers X1 , . . . , Xk such that |y1 | ≤ X1 , . . . , |yk | ≤ Xk , 2. h(x1 X1 , . . . , xk Xk )  
  4α 1 − α , provided that α + γ ≤ 1. Proof. Without loss of generality, we can assume that the starting and ending positions of the shared bits are known. When these positions are unknown, we can simply traverse the possible starting positions of the shared bits, which will just scale the time complexity for the case that we know the position by a factor O(n2 ). Hence, we suppose that p1 shares γn-bits from the β1 n-th bit to (β1 + γ)n-th bit, and p2 shares bits from β2 n-th bit to (β2 + γ)n-th bit, where β1 and β2 are known with β1 ≤ β2 (see Fig. 1 ). Then we can write p1 = x1 + M0 2β1 n + x2 2(β1 +γ)n ,  
   
  p2 = x3 + M0 2β2 n + x4 2(β2 +γ)n ,  
   
  with M0 < 2γn , x1 < 2β1 n , x2 < 2(β−β1 )n , x3 < 2β2 n , x4 < 2(β−β2 )n where β = 1 − α − γ. From this, we deduce 2(β2 −β1 )n p1 = x1 2(β2 −β1 )n + M0 2β2 n + x2 2(β2 +γ)n = x1 2(β2 −β1 )n + (p2 − x3 − x4 2(β2 +γ)n ) + x2 2(β2 +γ)n = p2 + (x1 2(β2 −β1 )n − x3 ) + (x2 − x4 )2(β2 +γ)n . Then, multiplying by q2 , we get N2 + (x1 2(β2 −β1 )n − x3 )q2 + (x2 − x4 )q2 2(β2 +γ)n = 2(β2 −β1 )n p1 q2 .  
   
  376  
   
  Y. Feng et al.  
   
  Next, we deﬁne the polynomial f (x, y, z) = xz + 2(β2 +γ)n yz + N2 , which shows that (x1 2(β2 −β1 )n − x3 , x2 − x4 , q2 ) is a solutions of f (x, y, z) ≡ 0  
   
  (mod 2(β2 −β1 )n p1 ).  
   
  Let m and t be integers to be optimized later with 0 ≤ t ≤ m. To apply Coppersmith’s method, we consider a family of polynomials gi,j (x, y, z) for 0 ≤ i ≤ m and 0 ≤ j ≤ m − i:  
   
  m−i max(t−i,0) N1 . gi,j (x, y, z) = (yz)j f (x, y, z)i 2(β2 −β1 )n These polynomials satisfy  
   
  gi,j x1 2(β2 −β1 )n − x3 , x2 − x4 , q2  
   
  i  
   
  m−i max(t−i,0) 2(β2 −β1 )n N1 = (x2 − x4 )j q2j 2(β2 −β1 )n p1 q2  
   
  m max(t−i,0) max(t−i,0)+i 2(β2 −β1 )n = (x2 − x4 )j q2j+i q1 p1 m  

  ≡ 0 mod 2(β2 −β1 )n pt1 . On the other hand, we have  
   
  x1 2(β2 −β1 )n − x3 ≤ max x1 2(β2 −β1 )n , x3  
   
  ≤ max 2β1 n 2(β2 −β1 )n , 2β1 n = 2β2 n , and |x2 − x4 | ≤ max(x2 , x4 ) = 2(β−β1 )n . Also, we have q2 = 2αn . We then set X = 2β2 n , Y = 2(β−β1 )n , Z = 2αn . To reduce the determinant of the lattice, we introduce a new variable w for p2 , and multiply the polynomials gi,j (x, y, z) by a power ws for some s that will be optimized later. Similar to t, we also require 0 ≤ s ≤ m Note that we can replace zw in gi,j (x, y, z)ws by N2 . We want to eliminate this multiple. Since gcd(N2 , 2N1 ) = 1, there inverse of N2 , denoted  exists an m as N2−1 , such that N2 N2−1 ≡ 1 (mod ∗) 2(β2 −β1 )n N1t . We then eliminate − min(s,i+j) (zw)min(s,i+j) mthe t original polynomial by multiplying it by N2  (β −βfrom )n (mod ∗) 2 2 1 N1 , while ensuring that the resulting polynomial evaluam  tion is still a multiple of 2(β2 −β1 )n pt1 . By selecting the appropriate parameter  
   
  Generalized Implicit Factorization Problem  
   
  377  
   
  s, we aim to reduce the determinant of the lattice. To this end, we consider a new family of polynomials Gi,j (x, y, z, w) for 0 ≤ i ≤ m and 0 ≤ j ≤ m − i:  
   
  m−i max(t−i,0) − min(s,i+j) N1 N2 , Gi,j (x, y, z, w) = (yz)j ws f (x, y, z)i 2(β2 −β1 )n m  − min(s,i+j) where N2 is computed modulo 2(β2 −β1 )n N1t , and each term zw is replaced by N2 . For example, suppose s ≥ 1, then  
   
  m G0,1 (x, y, z, w) = yws−1 N2 2(β2 −β1 )n N1t N2−1 . Next, consider the lattice L spanned by the matrix B whose rows are the coeﬃcients of the polynomials Gi,j (Xx, Y y, Zz, W w) where, for 0 ≤ i ≤ m, 0 ≤ j ≤ m − i, The rows are ordered following the rule that Gi,j ≺ Gi ,j  if i < i or if i = i and j < j  . The columns are ordered following the monomials so that         xi y j z i+j−min(s,i+j) ws−min(s,i+j) ≺ xi y j z i +j −min(s,i +j ) ws−min(s,i +j ) if i < i or if i = i and j < j  . Table 2 presents a matrix B with m = 3, s = 2, t = 2 where ∗ represents a nonzero term. Table 2. The matrix of the lattice with m = 3, s = 2, t = 2 and M = 2(β2 −β1 )n . Gi,j w2  
   
  y2  
   
  y3 z  
   
  xw  
   
  xy  
   
  xy 2 z  
   
  x2  
   
  x2 yz  
   
  x3 z  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  G0,1 0  
   
  Y W M 3 N12 0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  G0,2 0  
   
  0  
   
  Y 2 M 3 N12 0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  G0,3 0  
   
  0  
   
  0  
   
  Y 3 ZM 3 N12 0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  G1,0 ∗  
   
  ∗  
   
  0  
   
  0  
   
  XW M 2 N1 0  
   
  0  
   
  0  
   
  0  
   
  0  
   
  G1,1 0  
   
  ∗  
   
  ∗  
   
  0  
   
  0  
   
  XY M 2 N1 0  
   
  0  
   
  0  
   
  0  
   
  G1,2 0  
   
  0  
   
  ∗  
   
  ∗  
   
  0  
   
  0  
   
  XY 2 ZM 2 N1 0  
   
  0  
   
  0  
   
  G2,0 ∗  
   
  ∗  
   
  ∗  
   
  0  
   
  ∗  
   
  ∗  
   
  0  
   
  X 2M 0  
   
  0  
   
  G2,1 0  
   
  ∗  
   
  ∗  
   
  ∗  
   
  0  
   
  ∗  
   
  ∗  
   
  0  
   
  X 2 Y ZM 0  
   
  G3,0 ∗  
   
  ∗  
   
  ∗  
   
  ∗  
   
  ∗  
   
  ∗  
   
  ∗  
   
  ∗  
   
  ∗  
   
  yw  
   
  G0,0 W 2 M 3 N12 0  
   
  X 3Z  
   
  By construction, the square matrix B is left triangular. Hence, the dimension of the lattice is ω=  
   
  m m−i   i=0 j=0  
   
  1=  
   
  m  i=0  
   
  (m − i + 1) =  
   
  1 (m + 1)(m + 2) 2  
   
  and its determinant is det(B) = det(L) = X eX Y eY Z eZ W eW 2(β2 −β1 )neM N1eN ,  
   
  378  
   
  Y. Feng et al.  
   
  with eX =  
   
  m m−i    
   
  i=  
   
  1 m(m + 1)(m + 2), 6  
   
  j=  
   
  1 m(m + 1)(m + 2), 6  
   
  i=0 j=0  
   
  eY =  
   
  m m−i   i=0 j=0  
   
  m m−i   eZ = (i + j − min(s, i + j)) i=0 j=0  
   
  1 1 1 m(m + 1)(m + 2) + s(s + 1)(s + 2) − s(m + 1)(m + 2), 3 6 2 m m−i   1 = (s − min(s, i + j)) = s(s + 1)(s + 2), 6 i=0 j=0  
   
  = eW  
   
  eN =  
   
  t m−i   1 (t − i) = t(t + 1)(3m − t + 4), 6 i=0 j=0  
   
  eM =  
   
  m m−i   1 (m − i) = m(m + 1)(m + 2). 3 i=0 j=0  
   
  The former results are detailed in Appendix A. To combine Theorem 1 and Theorem 2, we set  (β −β )n m t ω(ω−1) 2 2 1 p1 1 √ 2 4(ω+1−i) det(L) ω+1−i < , ω with i = 2. Then det(L) <  

  1  
   
  2  
   
  ω−1 √ 4  
   
  ω  
   
  2(β2 −β1 )n  
   
  ωm  
   
  ptω 1 ,  
   
  and X eX Y eY Z eZ W eW 2(β2 −β1 )neM N1eN  
  4α 1 − α .  
   
  By Assumption 1, we can get (x0 , y0 , z0 ) = (x1 2(β2 −β1 )n − x3 , x2 − x4 , q2 ), so we have q2 = z0 , and we calculate N2 . p2 = q2 Next, we have 2(β2 −β1 )n p1 = p2 + (x1 2(β2 −β1 )n − x3 ) + (x2 − x4 )2(β2 +γ)n = p2 + y0 + z0 2(β2 +γ)n . Therefore, we can calculate p1 and q1 =  
   
  N1 p1 .  
   
  This terminates the proof.  

  380  
   
  4  
   
  Y. Feng et al.  
   
  Experimental Results  
   
  We provide some experiments to verify Assumption 1 and the correctness of our analysis. The experiments were run on a computer conﬁgured with AMD Ryzen 5 2500U with Radeon Vega Mobile Gfx (2.00 GHz). We selected the parameter n = log(N ) using gradients, validated our theory starting from small-scale experiments, and continually increased the scale of our experiments. The results are presented in Table 3: Table 3. Some experimental results for the GIFP. n  
   
  αn  
   
  βn  
   
  β1 n β2 n γn  
   
  200  
   
  20  
   
  40  
   
  20  
   
  30  
   
  140 6  
   
  m dim(L) Time for LLL(s) Time for Gr¨ obner Basis(s) 28  
   
  1.8620  
   
  0.0033  
   
  200  
   
  20  
   
  60  
   
  20  
   
  30  
   
  140 6  
   
  28  
   
  1.8046  
   
  0.0034  
   
  500  
   
  50  
   
  100 50  
   
  75  
   
  350 6  
   
  28  
   
  3.1158  
   
  0.0043  
   
  500  
   
  50  
   
  150 50  
   
  75  
   
  300 6  
   
  28  
   
  4.23898  
   
  0.0048  
   
  1000 100 200 100 150 700 6  
   
  28  
   
  8.2277  
   
  0.0147  
   
  As can be seen from Table 3, we chose various values of n, αn, βn, β1 n, β2 n and γn to investigate the behavior of our proposed algorithm. For each set of parameters, we recorded the time taken by the LLL algorithm and Gr¨ obner basis algorithm to solve the Generalized Integer Factorization Problem (GIFP). Our experiments conﬁrm Assumption 1 and also the eﬃciency of our algorithm in handling various values of n and related parameters. As the size of the problem increases, the computation time for LLL and Gr¨ obner basis algorithms also increases. Nevertheless, our algorithm’s time complexity grows moderately compared to the problem size. Therefore, we can conclude that our algorithm is suitable for practical applications in the Generalized Integer Factorization Problem (GIFP). Besides the Generalized Implicit Factoring Problem, we also conducted experiments on a special case, called the least-most signiﬁcant bits case (LMSBs). This case is characterized by β1 = 0 and β2 = β. The results of these experiments are outlined below  
   
  5  
   
  Conclusion and Open Problem  
   
  In this paper, we considered the Generalized Implicit Factoring Problem (GIFP), where the shared bits are not necessarily required to be located at the same positions. We proposed a lattice-based algorithm that can eﬃciently factor two RSA moduli, N1 = p1 q1 and N2 = p2 q2 , in polynomial time, when the primes share a suﬃcient number of bits. √ Our analysis shows that if p1 and p2 share γn > 4α (1 − α) n consecutive bits, not necessarily at the same positions, then N1 and N2 can be factored in  
   
  Generalized Implicit Factorization Problem  
   
  381  
   
  Table 4. Some experimental results for the LMSBs case. n  
   
  αn βn  
   
  γn  
   
  m dim(L) Time for LLL(s) Time for Gr¨ obner Basis(s)  
   
  256 25 75  
   
  156 5  
   
  21  
   
  1.3068  
   
  0.0029  
   
  256 25 75  
   
  156 5  
   
  21  
   
  1.2325  
   
  0.0023  
   
  256 25 75  
   
  156 6  
   
  21  
   
  1.2931  
   
  0.0023  
   
  512 50 150 212 6  
   
  28  
   
  2.0612  
   
  0.0028  
   
  512 50 150 212 6  
   
  28  
   
  2.4889  
   
  0.0086  
   
  512 50 150 212 6  
   
  28  
   
  2.0193  
   
  0.0022  
   
  polynomial time. However, this bound is valid when pi and qi , i = 1, 2, are not assumed to have the same bit length, i.e., N1 and N2 are unbalanced moduli [16] (Table 4). √ So our work raises an open question on improving the bound 4α (1 − α), which would lead to better bounds for speciﬁc cases such as sharing some middle bits. It is known that the unshared bits in the Most Signiﬁcant Bits (MSBs) or the Least Signiﬁcant Bits (LSBs) are continuous, and only one variable is required when using variables to represent the unshared bits. This makes the MSBs or LSBs case easier to solve than the generalized case and achieves a better bound of 2α (1 − α). However, the bound of the MSBs is not linear with the bound of the GIFP, which is unnatural. We hope that the gap between the bounds of the MSBs or LSBs and the GIFP case can be reduced. Acknowledgement. The authors would like to thank the reviewers of SAC 2023 for their helpful comments and suggestions. Yansong Feng and Yanbin Pan were supported in part by National Key Research and Development Project (No. 2018YFA0704705), National Natural Science Foundation of China (No. 62032009, 12226006) and Innovation Program for Quantum Science and Technology under Grant 2021ZD0302902.  
   
  A  
   
  Details of calculations in Section 3.2  
   
  In this appendix, we present the details of calculations for the quantities eX , eY , eZ , eW , eN , and eM used in Sect. 3.2. We begin by a lemma that will be easily proven by induction. This lemma is well-known and can be found in many textbooks and references on combinatorics and discrete mathematics, such as Table 174 on page 174 in [10].  n    holds for any integer n. Lemma 1. The equation i=0 2i = n+1 3 Moving on, we provide the calculations for eX as:    m m−i m m    m−i+1 i+1 j= eX = = 2 2 i=0 j=0 i=0 i=0  
   
   m+2 1 = = m(m + 1)(m + 2). 3 6  
   
  382  
   
  Y. Feng et al.  
   
  The calculation of eY is the same as eX . Next, we provide the calculation for eZ : eZ =  
   
  m m−i   (i + j − min(s, i + j)) i=0 j=0  
   
  =  
   
  m m−i    
   
  max{i + j − s, 0}  
   
  i=0 j=0  
   
  =  
   
  = = =  
   
  t m    
   
  (t − s) (Let t = i + j)  
   
  t=s+1 j=0 m   
   
  (t − s)(t + 1)  
   
  t=s+1 m   
   
  (t − s)(t + 1) −  
   
  t=0 m   
   
  t(t + 1) −  
   
  t=0 m   
   
  m   
   
  s   
   
  (t − s)(t + 1)  
   
  t=0  
   
  s(t + 1) −  
   
  t=0  
   
  s   
   
  t(t + 1) +  
   
  t=0  
   
  s   
   
  s(t + 1)  
   
  t=0  
   
    m s s    t+1 t+1 =2 (t + 1) − 2 (t + 1) −s +s 2 2 t=0 t=0 t=0 t=0  
   
    
   
    
   
   m+2 m+2 1 s+2 =2 −s + 3 2 6 3 1 1 1 = m(m + 1)(m + 2) + s(s + 1)(s + 2) − s(m + 1)(m + 2). 3 6 2 Then, we provide the calculation for eW : eW =  
   
  m m−i   (s − min(s, i + j)) i=0 j=0  
   
  =  
   
  s  s−i   
   
  (s − i − j)  
   
  i=0 j=0  
   
  =  
   
  s  s−i  i=0 j=0  
   
  s−  
   
  s  s−i  i=0 j=0  
   
  i−  
   
  s  s−i   
   
  j  
   
  i=0 j=0  
   
  1 1 1 s(s + 1)(s + 2) − s(s + 1)(s + 2) − s(s + 1)(s + 2) 2 6 6 1 = s(s + 1)(s + 2). 6  
   
  =  
   
  Generalized Implicit Factorization Problem  
   
  383  
   
  Furthermore, we provide the calculation for eN : eN =  
   
  t m−i t t     (t − i) = (t − i)(m − i + 1) = (t − i)(m + 2 − i − 1) i=0 j=0  
   
  i=0  
   
  i=0  
   
  t t t t t + 1     (t − i) − (t − i)(i + 1) = (m + 2) − t(i + 1) + i(i + 1) = (m + 2) 2 i=0 i=0 i=0 i=0  
   
  = (m + 2) =  
   
  t i + 1  t + 1  t + 2 t + 2 t + 1 t + 2  2 = (m + 2) −t +2 −t + 3 2 2 2 2 2 i=0  
   
  1 t(t + 1)(3m − t + 4). 6  
   
  Finally, we provide the calculation for eM :  m m−i m m     m−i+1 (m − i) = (m − i + 1)(m − i) = 2 2 i=0 j=0 i=0 i=0  
   
    
   
   m  i+1 m+2 1 = 2 =2 = m(m + 1)(m + 2). 2 3 3 i=0  
   
  eM =  
   
  References 1. Ajtai, M.: The shortest vector problem in L2 is NP-hard for randomized reductions (extended abstract). In: Symposium on the Theory of Computing (1998) 2. Bauer, A., Jaulmes, E., Lomn´e, V., Prouﬀ, E., Roche, T.: Side-channel attack against RSA key generation algorithms. In: Batina, L., Robshaw, M. (eds.) CHES 2014. LNCS, vol. 8731, pp. 223–241. Springer, Heidelberg (2014). https://doi.org/ 10.1007/978-3-662-44709-3 13 3. Bernstein, D.J., et al.: Factoring RSA keys from certiﬁed smart cards: coppersmith in the wild. In: Sako, K., Sarkar, P. (eds.) ASIACRYPT 2013. LNCS, vol. 8270, pp. 341–360. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-420450 18 4. Boneh, D., Durfee, G.: Cryptanalysis of RSA with private key d less than N 0.292 . In: Stern, J. (ed.) EUROCRYPT 1999. LNCS, vol. 1592, pp. 1–11. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48910-X 1 5. Brumley, D., Boneh, D.: Remote timing attacks are practical. In: Proceedings of the 12th USENIX Security Symposium, Washington, D.C., USA, 4–8 August 2003. USENIX Association (2003). https://www.usenix.org/conference/ 12th-usenix-security-symposium/remote-timing-attacks-are-practical 6. Carmon, E., Seifert, J., Wool, A.: Photonic side channel attacks against RSA. In: 2017 IEEE International Symposium on Hardware Oriented Security and Trust, HOST 2017, McLean, VA, USA, 1–5 May 2017, pp. 74–78. IEEE Computer Society (2017). https://doi.org/10.1109/HST.2017.7951801 7. Coppersmith, D.: Finding a small root of a univariate modular equation. In: Maurer, U. (ed.) EUROCRYPT 1996. LNCS, vol. 1070, pp. 155–165. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-68339-9 14 8. Coppersmith, D.: Small solutions to polynomial equations, and low exponent RSA vulnerabilities. J. Cryptol. 10(4), 233–260 (1997). https://doi.org/10.1007/ s001459900030  
   
  384  
   
  Y. Feng et al.  
   
  9. Faug`ere, J.-C., Marinier, R., Renault, G.: Implicit factoring with shared most signiﬁcant and middle bits. In: Nguyen, P.Q., Pointcheval, D. (eds.) PKC 2010. LNCS, vol. 6056, pp. 70–87. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3642-13013-7 5 10. Graham, R.L., Knuth, D.E., Patashnik, O.: Concrete Mathematics: A Foundation for Computer Science. 2nd edn. Addison-Wesley Longman Publishing Co., Inc, USA (1994) 11. Howgrave-Graham, N.: Finding small roots of univariate modular equations revisited. In: Darnell, M. (ed.) Cryptography and Coding 1997. LNCS, vol. 1355, pp. 131–142. Springer, Heidelberg (1997). https://doi.org/10.1007/BFb0024458 12. Lenstra, A.K., Lenstra, H.W., Lov´ asz, L.: Factoring polynomials with rational coeﬃcients. Math. Ann. 261, 515–534 (1982) 13. Lu, Y., Peng, L., Zhang, R., Hu, L., Lin, D.: Towards optimal bounds for implicit factorization problem. In: Dunkelman, O., Keliher, L. (eds.) SAC 2015. LNCS, vol. 9566, pp. 462–476. Springer, Cham (2016). https://doi.org/10.1007/978-3-31931301-6 26 14. May, A.: New RSA vulnerabilities using lattice reduction methods. Ph.D. thesis, University of Paderborn (2003). http://ubdata.uni-paderborn.de/ediss/17/2003/ may/disserta.pdf 15. May, A., Ritzenhofen, M.: Implicit factoring: on polynomial time factoring given only an implicit hint. In: Jarecki, S., Tsudik, G. (eds.) PKC 2009. LNCS, vol. 5443, pp. 1–14. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-0046811 16. Nitaj, A., Ariﬃn, M.R.K.: Implicit factorization of unbalanced RSA moduli. IACR Cryptol. ePrint Arch. p. 548 (2014). http://eprint.iacr.org/2014/548 17. Peng, L., Hu, L., Lu, Y., Huang, Z., Xu, J.: Implicit factorization of RSA moduli revisited (Short Paper). In: Tanaka, K., Suga, Y. (eds.) IWSEC 2015. LNCS, vol. 9241, pp. 67–76. Springer, Cham (2015). https://doi.org/10.1007/978-3-31922425-1 5 18. Rivest, R.L., Shamir, A., Adleman, L.M.: Cryptographic communications system and method (1983). US Patent 4,405,829 19. Sarkar, S., Maitra, S.: Further results on implicit factoring in polynomial time. Adv. Math. Commun. 3(2), 205–217 (2009). https://doi.org/10.3934/amc.2009.3. 205 20. Sarkar, S., Maitra, S.: Some applications of lattice based root ﬁnding techniques. Adv. Math. Commun. 4(4), 519–531 (2010). https://doi.org/10.3934/amc.2010.4. 519 21. Sarkar, S., Maitra, S.: Approximate integer common divisor problem relates to implicit factorization. IEEE Trans. Inf. Theory 57(6), 4002–4013 (2011). https:// doi.org/10.1109/TIT.2011.2137270 22. Wang, S., Qu, L., Li, C., Fu, S.: A better bound for implicit factorization problem with shared middle bits. Sci. China Inf. Sci. 61(3), 032109:1–032109:10 (2018). https://doi.org/10.1007/s11432-017-9176-5  
   
  Differential Cryptanalysis  
   
  CLAASP: A Cryptographic Library for the Automated Analysis of Symmetric Primitives Emanuele Bellini , David Gerault , Juan Grados(B) , Yun Ju Huang , Rusydi Makarim , Mohamed Rachidi , and Sharwan Tiwari Cryptography Research Center, Technology Innovation Institute, Abu Dhabi, UAE {emanuele.bellini,david.gerault,juan.grados,yunju.huang,mohamed.rachidi, sharwan.tiwari}@tii.ae  
   
  Abstract. This paper introduces CLAASP, a Cryptographic Library for the Automated Analysis of Symmetric Primitives. The library is designed to be modular, extendable, easy to use, generic, eﬃcient and fully automated. It is an extensive toolbox gathering state-of-the-art techniques aimed at simplifying the manual tasks of symmetric primitive designers and analysts. CLAASP is built on top of Sagemath and is open-source under the GPLv3 license. The central input of CLAASP is the description of a cryptographic primitive as a list of connected components in the form of a directed acyclic graph. From this representation, the library can automatically: (1) generate the Python or C code of the primitive evaluation function, (2) execute a wide range of statistical and avalanche tests on the primitive, (3) generate SAT, SMT, CP and MILP models to search, for example, diﬀerential and linear trails, (4) measure algebraic properties of the primitive, (5) test neural-based distinguishers. We demonstrate that CLAASP can reproduce many of the results that were obtained in the literature and even produce new results. In this work, we also present a comprehensive survey and comparison of other software libraries aiming at similar goals as CLAASP. Keywords: Cryptographic library primitives  
   
  1  
   
  · Automated analysis · Symmetric  
   
  Introduction  
   
  The security targets for cryptographic primitives are well-deﬁned, and relatively stable, after decades of cryptanalysis. In particular, a symmetric cipher should behave like a random keyed permutation, a hash function should behave like a random function, and a MAC scheme should be unforgeable. Testing a cryptographic primitive for these properties is, on the other hand, a vastly diﬃcult task that relies on testing for known weaknesses. Such a process generally involves determining the most likely diﬀerential or linear characteristic, evaluating the resistance of the primitive to various cryptanalysis techniques such as c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 387–408, 2024. https://doi.org/10.1007/978-3-031-53368-6_19  
   
  388  
   
  E. Bellini et al.  
   
  integral attacks, and running generic randomness tests. Fortunately, automatic techniques exist to help designers and cryptographers run such evaluations; for instance, SAT/SMT, Mixed Integer Linear Programming (MILP) or Constraint Programming (CP) are frequently used to ﬁnd optimal diﬀerential and linear characteristics. These tools have, over time, become more accessible to nonexperts, through libraries such as [43], that generate models (in this case, SMT) automatically from a description of the cipher. However, such tools generally focus on a single aspect, such as generating models in a given paradigm, and there is currently no single-stop toolkit that combines automated model generation, statistical testing and machine learning based analysis. We aim to ﬁll this gap with CLAASP, a Cryptographic Library for the Automated Analysis of Symmetric Primitives. This paper introduces the ﬁrst public version of CLAASP; the ambition of the project is to keep adding analysis tools in line with the state of the art, to provide cryptanalysts with a click-of-a-button solution to run all the standard analysis tools and gain an overview of the security of a given primitive. The library’s source code has been made available to the wider community and is publicly accessible at Github: https://github.com/Crypto-TII/claasp. Also, in Github: https://github.com/peacker/claasp_white_paper, you can ﬁnd the scripts used to accompany this paper. We ﬁrst present existing cryptanalysis libraries in Sect. 1.1, before introducing the building blocks of CLAASP: the cipher object in Sect. 2, and the evaluators in Sect. 3. We then present the battery of tests and tools implemented in CLAASP in Sect. 4, and ﬁnish with a comparison with other cryptographic libraries in Sect. 5. 1.1  
   
  Related Works  
   
  Automated tools to support cryptanalysts have become a cornerstone for the design of new primitives. Over time, such tools were made more generic and gathered into libraries; we describe the most prominent ones in this section. The lineartrails library [23] is dedicated to the search for linear characteristics on SPN ciphers. ARX toolkit [32,33] and YAARX [56] focus on ARX ciphers, the former testing conditions for trails to be possible, and the latter performing various analysis techniques on the components. On the algebraic cryptanalysis side, the Automated Algebraic Cryptanalysis tool [51] tests properties of block and stream ciphers; in particular, it evaluates the randomness of a cipher through Maximum Degree Monomial tests [52]. Autoguess [27] is a tool to automate the technique guess-and-determine. This technique involves making a calculated guess of a subset of the unknown variables, which enables the deduction of the remaining unknowns using the information obtained from the guessed variables and some given relations. In order to automate this technique, SAT/SMT, MILP, and Gröbner basis solvers are used and several new modeling techniques to exploit these solver proposed. For instance, the authors of the library introduce new encodings in CP and  
   
  CLAASP  
   
  389  
   
  SAT/SMT to solve the problem of determining the minimal guess, i.e., the subset of guessed variables from which the remaining variables can be deduced. Autoguess also allows to automate the key-bridging technique. This technique is utilized in key-recovery attacks on block ciphers, wherein the attacker seeks to determine the minimum number of sub-key guesses needed to deduce all the involved sub-keys through the key schedule. The signiﬁcant contribution of this work lies in integrating key-bridging techniques into tools that were previously only capable of searching for distinguishers. As a result, these enhanced tools can now be utilized as fully automatic methods for recovering keys. CryptoSMT [53] is the ﬁrst large-scale solver-based library dedicated to cryptanalysis. Based on SMT and SAT solvers, it provides an extensive toolkit, permitting the search for optimal diﬀerential and linear trails, the evaluation of the probability of a diﬀerential, the search for hash function preimages, and secret key search. The study described in [28] presents an innovative approach to explore diﬀerentials and linear approximations. Diﬀerent from methods that rely on SAT or MILP techniques, this approach transforms the search for diﬀerential and linear trails into a problem of identifying multiple long paths within a multistage graph. A practical implementation of this research, called CryptaGraph, is available in [29]. One notable feature of CryptaGraph is its automatic conversion capability, enabling C or Rust implementations of ciphers to be transformed into models for searching diﬀerentials or linear approximations using the graph-based approach mentioned earlier. An improvement of [28] can be found in [30] and its implementation was named PathFinder. Another SMT-based library, based on ArxPy [43] is the CASCADA framework [44], which also implements techniques to search for rotational-XOR diﬀerentials, impossible-rotational-XOR, but also related-key impossible-diﬀerentials, linear approximations, and zero-correlation characteristics. The generated SMT models are expressed through the theory of bit-vectors [5], and follow the general methodology of Mouha and Preneel [39] for diﬀerential properties, Sasaki’s [48] technique for impossible diﬀerentials, an SMT-based miss-in-the middle search for related-key impossible diﬀerentials of ARX ciphers [4], and a novel method proposed for zero-probability global properties. If a search can not use the previous methods, then a generic method, based on the constructions of statistical tables, such as the Diﬀerential Distribution Table (DDT), is used. Depending on the sizes of the inputs of the block cipher, these generic models could be costly, so they also proposed heuristic models by relaxing the accuracy of their properties; they called them weak models. Finally, their framework implements methods to check the properties mentioned above experimentally. Finally, TAGADA [34] is a tool which generates Minizinc [41] models for the search for diﬀerential properties on word-based SPN ciphers, such as the AES. The search for such ciphers is typically divided into two steps, one where the word variables are abstracted as boolean values denoting the presence or absence of a diﬀerence, and one where the abstracted solutions from step 1 are instantiated to word values, when possible. The models generated by TAGADA implement  
   
  390  
   
  E. Bellini et al.  
   
  the ﬁrst step, including optimisations based on inferred equalities through XOR operators, in order to drastically reduce the number of incorrect solutions to be passed to step 2. Such constraints are deduced naturally from a Directed Acyclic Graph (DAG) representation of the cipher under study. The genericity of Minizinc models enables solving with a range of CP, SAT and SMT solvers, in particular, the ones participating in the MiniZinc competition, that provide an interface to MiniZinc. On the other hand, solver-speciﬁc optimisations and perks are abstracted away by the Minizinc interface, compared to models developped in the native language of a solver. A summary of the functionalities of these libraries is presented in Table 1. Table 1. Comparison of cryptanalysis libraries features with CLAASP. -∗ means that the functionality is not supported, but could easily be added from the existing code. ∗∗ means the algebraic tests works on algebraic model for cipher preimages. TAGADA  
   
  CASCADA  
   
  CryptoSMT  
   
  lineartrails YAARX  
   
  Autoguess  
   
  CLAASP  
   
  Cipher types  
   
  SPN  
   
  All  
   
  All  
   
  SPN  
   
  ARX  
   
  All  
   
  All  
   
  Cipher representation  
   
  DAG  
   
  Python code  
   
  Python code  
   
  C++ code  
   
  C code  
   
  Algebraic DAG representation  
   
  Statistical/Avalanche tests  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes  
   
  Continuous diﬀusion tests  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes  
   
  Components analysis tests  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes  
   
  Constraint Diﬀerential solvers trails Diﬀerentials Impossible diﬀerential Linear trails Linear hull Zero correlation approximation Supported solvers  
   
  Truncated  
   
  Yes  
   
  Yes  
   
  -  
   
  Yes  
   
  -  
   
  Yes  
   
  -  
   
  Yes Yes  
   
  Yes -*  
   
  -  
   
  Yes -  
   
  -  
   
  Yes Yes  
   
  -  
   
  Yes -∗ Yes  
   
  Yes -∗ -∗  
   
  Yes -  
   
  -  
   
  -  
   
  Yes Yes Yes  
   
  CP, (MiniZinc)  
   
  SMT  
   
  SMT  
   
  -  
   
  -  
   
  single-key related-key  
   
  single-key related-key  
   
  single-key related-key  
   
  single-key  
   
  Supported Scenarios  
   
  SAT, SMT, MILP, CP, Groebner basis single-key single-key related-key single-tweak related-tweak  
   
  SAT, SMT, MILP, CP, Groebner basis single-key related-key single-tweak related-tweak  
   
  Algebraic tests  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes∗∗  
   
  Neural-based tests  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes  
   
  State Recovery  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes  
   
  -  
   
  Key-bridging  
   
  -  
   
  -  
   
  -  
   
  -  
   
  -  
   
  Yes  
   
  -  
   
  1.2  
   
  Our Contribution  
   
  We introduce CLAASP, a Cryptographic Library for the Automated Analysis of Symmetric Primitives. CLAASP has been designed to simplify the manual tasks of symmetric cipher designers and analysts. CLAASP has been designed with the following goals:  
   
  CLAASP  
   
  391  
   
  – Be open-source with a GPLv3 licence. – Be modular. For this reason it is built on top of Sagemath, thus inheriting Python modularity. – Be extendable. The Python/Sagemath environment allows to easily integrate other powerful libraries: constraint solvers such as Cryptominisat, Cadical or Gurobi, machine learning engines such as Tensorﬂow, Grobner basis solvers, parallelization packages such as NumPy, etc. – Be usable. Much eﬀort has been dedicated to provide a smooth user experience for both designing and analyzing a cipher. This includes a comprehensive documentation for users and developers, and a Docker image to easily start with the library without the need of installing all the dependencies. – Be generic. The wide range of pre-deﬁned components, allows to implement a wide range of iterated symmetric ciphers, ranging from block ciphers (possibly with a tweak), cryptographic permutations, hash functions, and covering several design types such as Feistel, SPN, ARX, etc. – Be automated. The concept of the library revolves around providing a cipher design as the input and getting an analysis of the cipher design as the output with respect to some desired property. – Be eﬃcient. In spite of being the most generic and fully automated tool of its kind, this library is competitive in terms of eﬃciency with similar tools targeting speciﬁc sectors. The central objects of CLAASP are symmetric ciphers. They are described as directed acyclic graphs whose nodes are components (S-Boxes, linear layers, constants, Input/Output, etc.) and whose edges are input/output component connections. From this representation, the library can automatically: 1. generate the Python or C code of the evaluation function; 2. execute a wide range of statistical and avalanche tests on the primitive, including continuous diﬀusion tests; 3. generate a report containing the main properties of the cipher components; 4. generate SAT, SMT, CP and MILP models to search, for example, diﬀerential and linear trails; 5. measure algebraic properties of the primitive; 6. test neural-based distinguishers. We demonstrate that CLAASP can reproduce many of the results that were obtained in the literature: in terms of diﬀerential cryptanalysis, we retrieve similar results to CASCADA for the 1 to 7 rounds of SPECK32, 64, and LEA128. Furthermore, we were able to ﬁnd an optimal diﬀerential trail for Speck 128-128 reduced to 10 rounds. To the best of our knowledge, optimal trails for this speciﬁc version of Speck were only known for up to 9 rounds. This achievement was made possible by seamlessly integrating a Parallel SAT solver into CLAASP. In particular, we successfully incorporated ParKissat, the winner of the SAT competition 2022 (parallel track) [57], into the SAT module of CLAASP. In addition, we show how to use CLAASP to retrieve the known 17-rounds impossible diﬀerential on HIGHT, as well as 6-round impossible diﬀerentials on SPECK32.  
   
  392  
   
  E. Bellini et al.  
   
  Regarding linear cryptanalysis, we obtain a linear trail of Salsa with better correlation than the one reported in [17]. This discovery has the potential to enhance the correlation of the diﬀerential-linear distinguisher against Salsa reduced to 8 rounds presented in the aforementioned paper. Finally, in terms of neural cryptanalysis, CLAASP implements (and can reproduce the results of) [10], in addition to the seminal results of [26]. In addition, researchers willing to apply neural cryptanalysis to new ciphers using the techniques from [10] can do so in a straight-forward manner using the library functions. Besides the presentation of the library, important contributions of this work are a survey and a comparison (where possible) of the main software tools trying to achieve the same goals as CLAASP.  
   
  2  
   
  Symmetric Primitives in CLAASP  
   
  In this section, we describe how a symmetric primitive is represented in CLAASP. We also present the main pre-implemented primitives that are available for testing and give some indications on how to build a custom cipher. 2.1  
   
  The Component Class  
   
  In CLAASP, a symmetric cipher is represented as a list of “connected components”. The components of a symmetric cipher are its building blocks (S-Boxes, linear layers, etc.). Two components are connected when the output bits of the ﬁrst component become the input bits of the second component, in a one-to-one correspondence. The library supports the following primitive components: the S-Box component, linear layer components (ﬁxed and variable rotation, ﬁxed and variable shift, bit and word permutation, multiplication by a binary or word matrix), word operations components (NOT, AND, OR, XOR, modular addition and subtraction), and the constant component. It also supports composite components, which are a combination of primitive components: the sigma function used in ASCON, the theta function used in Keccak, and the theta function used in Xoodoo. For example, the linear layer in ASCON can be presented by the combination of several XOR and ROTATE components, or as a composite component. Composite components can also be created at a user level. Finally, some special components are used to represent the inputs of the cipher, and cipher intermediate and ﬁnal outputs. In CLAASP, each component requires the following minimal information: a unique component ID (e.g. “sbox_0_0”); a component type (e.g. “sbox”); the input and output bit size of the component; a list of the components that are connected to the input of the component (a list of IDs); a list of lists of bits positions specifying which output bits of the input components are connected to the component; a description containing the necessary information to ﬁnalize the deﬁnition of the component (e.g., the list of integers deﬁning an SBox).  
   
  CLAASP  
   
  2.2  
   
  393  
   
  The Cipher Class  
   
  Ciphers as Directed Acyclic Graphs. In CLAASP, a symmetric cipher is represented as a list of connected components, forming a directed acyclic graph, and a list of basic properties, listed in Table 2. Table 2. Parameters that are used to deﬁne a cipher in CLAASP. Property  
   
  Description  
   
  id  
   
  unique identiﬁer of the cipher, composed by cipher name and parameters  
   
  family_name  
   
  name of the cipher family, such as AES, ASCON, etc  
   
  type  
   
  type of the cipher (block cipher, permutation, hash or stream cipher)  
   
  inputs  
   
  inputs of the cipher, such as key and plaintext  
   
  inputs_bit_size  
   
  list of number of bits of each input parameters  
   
  output_bit_size  
   
  number of bits of the cipher output  
   
  number_of_rounds number of rounds in the cipher rounds  
   
  list of rounds each containing a list of components  
   
  reference_code  
   
  [optional] Python reference code (as a string) of the cipher evaluation function, used to verify the cipher correctness  
   
  CLAASP supports iterated symmetric ciphers, based on the composition of several round functions, which are themselves a list of connected components; each cipher must have at least one round. The round decomposition is useful and common in symmetric cipher design and cryptanalysis; in most tests, a given property is studied round by round. CLAASP natively implements a range of well-known block ciphers (AES, TEA, DES, XTEA, LEA, Twoﬁsh, LowMC, Threeﬁsh, Midori, HIGHT, PRESENT, SKINNY, Raiden, Sparx, SIMON, SPECK), permutations (ASCON, Xoodoo, ChaCha, Spongent-π, GIFT-128, TinyJAMBU, GIMLI, Grain core, KECCAK-p, PHOTON, SPARKLE) and hash functions (SHA-1, SHA-2, MD5, BLAKE, BLAKE2). This list is meant to be expanded over time. How to Create the Cipher Object. While native support for more primitives will be added over time, CLAASP exposes a simple interface for users to add new ones as well. This process is illustrated through a toy example of a 2-rounds cipher with 6-bit block, 6-bit key injected in every round with a XOR operation, 2 3-bit S-boxes, and a linear layer made of a left rotation of 1 bit, shown in Fig. 2, and the corresponding CLAASP implementation in Fig. 1. The main concern of a user implementing a primitive is to correctly link the components at a bit level, and mark which component or group of components need to be reported in the output of the tests. This is because a user might be  
   
  394  
   
  E. Bellini et al.  
   
  interested not only in getting reports at every round, but, for example, after the linear and the nonlinear layer of an SPN. Cipher Inputs. It is important to notice that, in order to be generic, the library has been designed to accept multiple inputs which can be labeled with diﬀerent names: for example, a key, a plaintext and a tweak, or a message and a nonce. On the other hand, to better exploit the features of some tests, a naming convention has been introduced for inputs such as “key” or “plaintext”. The cipher Representation is not unique. The cipher representation as a list of connected components is not unique. For example, the nonlinear layer of ASCON permutation can be represented as a circuit made of word operation components (XOR, AND and NOT) or with a layer of parallel S-boxes.  
   
  Fig. 1. ToySPN class deﬁnition.  
   
  Fig. 2. ToySPN diagram.  
   
  Diﬀerent cipher representations may aﬀect the output of tests; for instance, a naive diﬀerential cryptanalysis model built for an ASCON implementation using the circuit representation is less accurate than one using a S-Box representation. In general, the circuit model is useful when a user wishes to monitor the action of every gate (i.e. word operation) on a single bit. On the other hand an S-Boxbased model often allows a faster evaluation function. CLAASP implements both S-Box and circuit representations for some primitives, such as ASCON, Xoodoo, Keccak and Gimli, as well as the bit-based and word-based such as TinyJambu representations.  
   
  CLAASP  
   
  3  
   
  395  
   
  Library: Evaluation Modules  
   
  The most basic functionality of CLAASP is to evaluate a cryptographic primitive on a given input. Basic C and Python evaluation functions provide nonoptimized evaluation for single inputs, and a vectorized evaluation functions permits fast batch evaluation. For instance, using the vectorized implementation, it takes around 3 s to perform one million AES encryptions. A detailed performance review is given in the long version of this paper [8]. Base Evaluator in Python and C. In CLAASP, users can create a cipher object and call an evaluation method to evaluate a particular input. This functionality is also used internally in CLAASP by some of the modules, to run, for example, avalanche or statistical tests. The corresponding Python and C code1 is generated automatically by scanning the list of components. Vectorized Implementations. In Python, the NumPy library allows to vectorize function evaluations on an array, rather than a single input. NumPy arrays are typed and homogeneous, which, combined with NumPy’s optimisations, enables signiﬁcant performance gains compared to Python native lists and loops. The cipher object provides the NumPy-based evaluate_vectorized function. The inputs are speciﬁed as NumPy arrays, of 8-bit unsigned integer values, arranged as one column per data point. The return value is encoded as a list containing a single NumPy array of 8-bit unsigned integer values, this time arranged as one row per data point. The choice of using bytes stems from NumPy’s lack of support for integers over 64 bits.  
   
  4  
   
  Library: Test Modules  
   
  In this section, we describe all automated analysis modules that are currently supported in CLAASP. Many of the analysis tools presented here are derived from diﬀerential and linear cryptanalysis [13], the cornerstones of modern symmetric primitives evaluation. Let SK (X) be a symmetric primitive; diﬀerential cryptanalysis focuses on the probability, over all inputs, for a diﬀerence δ to propagate to γ, i.e., X, P r[Sk (X) ⊕ SK (X ⊕ δ)] = γ. Conversely, linear cryptanalysis focuses on the correlation for a linear mask Γ0 to propagate to Γ1 , P r[Sk (X) · Γ1 = X · Γ0 ]. In both cases, the cryptographer is interested in ﬁnding diﬀerences (resp. masks) for which this probability (resp. correlation) is high. 4.1  
   
  Component Analysis  
   
  This module allows the visualization of the “quality” of certain properties of the components used in a cipher, by means of radar charts. These properties include: 1  
   
  When possible a word-oriented implementation is used, opposed to a slower bitoriented implementation for primitives with mixed type of components.  
   
  396  
   
  E. Bellini et al.  
   
  Boolean function properties (number of terms, algebraic degree, number of variables, whether the Boolean function is APN or balanced), vectorial Boolean function properties (diﬀerential uniformity, boomerang uniformity, nonlinearity, etc.), linear layer properties (order, linear and diﬀerential branch number). More precisely, this module allows to retrieve the list of the components used in the cipher, the number of occurrences of each component, and the corresponding properties. For example, for 2 rounds of AES-128, the user will notice that a XOR operation between 2 inputs of 128 bits each occurs 3 times. If one considers XOR output bits expressed as a Boolean function, then each of these XOR components has an algebraic degree of 1 with 2 terms, and 2 variables. The generated radar chart visualisations are provided in [8]. 4.2  
   
  Statistical and Avalanche Tests  
   
  Statistical Tests. Statistical tests, such as Diehard [37], or its successor Dieharder [16], evaluate the randomness of a set of bit strings. Such tests were applied to evaluate AES candidates [7,49,50] through the NIST Statistical Test Suite (NIST STS) [6,46]. CLAASP implements both the NIST STS and Dieharder suites within the statistical test module. The statistical test process is divided into two phases, dataset generation and analysis. Dataset Generator. The datasets used in CLAASP which covers keyed primitives are deﬁned in [49]. Keyless primitives datasets are somehow special cases of the keyed ones. As an example, the illustration of the avalanche dataset generator is shown in Appendix B of [8]. The dataset generator, which returns a set of bit strings, is based on CLAASP’s vectorized evaluation method. Statistical Test Tools. The results of NIST STS and Dieharder are exported into a ﬁle and additionally returned as a Python dictionary for easy integration into scripts. CLAASP also features visualization of the results, as shown in Fig. 3.  
   
  Fig. 3. CLAASP plot for the 188 NIST statistical tests pass rate of ASCON round 3 and round 4.  
   
  CLAASP  
   
  397  
   
  Performance and Experiments. To generate the plaintext avalanche test for all supported primitives (191 Gigabits), it takes 4 h. For a 100 Mbits dataset, it takes around 30 min to ﬁnish the NIST statistical tests. Figure 4 shows the number of tests that pass for each round of ASCON (left) and the percentage of the rounds needed to pass all statistical tests with respect to the 9 possible datasets for several primitives. Avalanche Tests. This module focuses on the avalanche properties, presented in [20], of a symmetric iterated primitive. These tests evaluate the cipher with respect to three diﬀerent metrics that represent what usually the literature calls full diﬀusion, avalanche and strict avalanche criteria. The goal of the tests is to compare how these metric evolve with respect to the computational cost of the round function; each metric is expected to satisfy a certain criterion (namely to pass a threshold) after a few rounds.  
   
  Fig. 4. Randomness graphs of ASCON generated by CLAASP. Left side is the statistical test result of avalanche dataset. Right side are all the statistical results of ASCON compared with other primitives.  
   
  Measure Avalanche Criteria. The results of the avalanche tests allow a user to: check if a criterion is satisﬁed at a certain round for a speciﬁc input bit diﬀerence; obtain the worst input bit diﬀerences, that are the input bit diﬀerences for which the criterion is satisﬁed after more rounds than the rest of the input bit diﬀerences; obtain the value of the criterion for a speciﬁc round and a speciﬁc input bit diﬀerence; obtain the average value of the criterion among all the input bit diﬀerences for a speciﬁc round. For better visualization, CLAASP can generate a heatmap graph of the output returned by the avalanche tests, as illustrated in the long version of the paper [8]. Performance. Figure 5 reports the timings of the avalanche tests for 5 rounds of some popular ciphers, using the vectorized evaluation function, up to 50,000 samples; all tests run globally in less than 5 min.  
   
  398  
   
  E. Bellini et al.  
   
  Truncated Diﬀerential Search. This module oﬀers a range of features, including the ability to easily discover truncated diﬀerentials with only one active bit in both the input and output states. Such diﬀerentials, when paired with linear approximations, can be very useful for increasing the correlations of diﬀerentiallinear distinguishers. For instance, we successfully used this module to rediscover the truncated diﬀerential outlined in [3], which has been cited and studied extensively in various papers (such as [22]). To view the script we used to rediscover this diﬀerential, you can refer to the accompanying repository for this paper.  
   
  Fig. 5. Timings of the avalanche tests for ﬁve rounds of popular ciphers  
   
  4.3  
   
  Fig. 6. Time comparison of the CAF computation for Speck 128-128, AES128, the iterated permutation in ASCON320 cipher, and the iterated permutation in ChaCha cipher, ﬁxed to 5 rounds each for several random samples.  
   
  Constraint Solvers for Diﬀerential and Linear Cryptanalysis  
   
  The search for strong diﬀerential or linear properties often relies on trails, i.e., round by round propagation of the property under study; the ﬁnal probability of the trail, under the Markov assumption that all round keys are independent, is computed as the product of the probabilities of each round. Finding such trails is a diﬃcult combinatorial problem, traditionally handled with Matsui’s algorithm [38] variations. In recent years, the use of automatic solvers, such as Mixed Integer Linear Programming (MILP), SAT, SMT, and more recently Constraint Programming (CP), have become a simpler alternative. These tools have the beneﬁt of being extensively studied and optimized by the AI and OR communities, so that the focus shifts from implementing a search algorithm to modeling the problem properly. CLAASP can automatically generate MILP, SAT, SMT and CP models for diﬀerential and linear cryptanalysis, from a primitive’s description.  
   
  CLAASP  
   
  399  
   
  Diﬀerential and Linear Models for ARX Ciphers. In order to implement the search for diﬀerential and linear trails on ARX ciphers, we utilized the techniques outlined in [19] and in [35]. Speciﬁcally, we implemented the MILP constraints described in those papers for the ARX components and were able to successfully replicate the trails reported therein. In addition to the MILP constraints described there, we also implemented SAT, CP, and SMT equivalent constraints not only for ARX ciphers but also for SPN ciphers. To accelerate the search for trails using SAT techniques, we implemented the sequential encoding method presented in [54] on CLAASP. Moreover, through modeling the cipher evaluation process (as discussed in Sect. 3) using MILP, SAT, CP, and SMT, we were able to implement the techniques outlined in [47]. Those techniques aimed to verify the validity of diﬀerential trails. In particular, by using those techniques, their authors reported some invalid trails presented in [36]. The scripts accompanying this paper demonstrate how we used CLAASP to verify diﬀerential trails. For linear trails on ARX ciphers, we were able to rediscover trails presented in recent papers, such as those presented for two well-studied ciphers, such as Speck and ChaCha. Speciﬁcally, we rediscover the linear trails presented at [17] and the linear trails outlined in [9]. The former presents the best attacks against ChaCha reduced to 7 rounds, while the second is a recent paper attacking Speck. Again, the scripts accompanying this paper demonstrate how we used CLAASP to verify these linear trails. New Results. Our library supports a range of SAT solvers, including parallel solvers, which we believe is a unique feature not found in other cryptanalysis libraries. By utilizing the CLAASP interface, we were able to search for differential and linear trails using parallel SAT solvers. We managed to ﬁnd an optimal diﬀerential trail for 10 rounds of Speck 128-128. This accomplishment was made possible by utilizing the power of 125 AMD EPYC 7763 cores on a Ubuntu machine with 1TB of memory. To conﬁrm the optimality of this trail, we used CLAASP in conjunction with ParKissat [57] to search for a 10-round trail of this version of Speck with a probability weight of 48. It took approximately 2.23 days to obtain as output UNSAT. The script accompanying this paper contains the details of this ﬁnding. Regarding linear cryptanalysis, we obtain a linear trail for Salsa with better theoretical correlation than the one reported in [17]. We start from the same input bit mask described in Lemma 10 and Lemma 11 of [17]. Speciﬁcally, we found a trail with a theoretical correlation of 2−31 instead of 2−34 as described in [17]. This accomplishment was made possible by utilizing the SAT module of CLAASP. We use only 1 AMD EPYC core, and the trail was found in less than 1 min. We attempt to ﬁnd a trail with a theoretical correlation of 2−30 , but the solver outputs UNSAT. This discovery has the potential to enhance the correlation of the diﬀerential-linear distinguisher against Salsa reduced to 8 rounds presented in the aforementioned paper. The reader can reproduce the trail by using the script accompanying this paper.  
   
  400  
   
  E. Bellini et al.  
   
  Diﬀerential and Linear Models for SPN Ciphers. SPN ciphers use Substitution Boxes (SBoxes) as their non-linear component. In addition, their linear layers can typically be expressed as a matrix multiplication. The representation of SBoxes in diﬀerential search models typically uses its Diﬀerential Distribution Table, or DDT. The DDT is a 2 dimensional object such that #{x∈Fn 2 :x⊕y=δ,SB[x]⊕SB[y]=γ} . Models for linear trails use the Linear DDTδ,γ = 2n Approximation Table (LAT) built in a similar fashion instead. To represent the DDT in SAT, SMT or MILP, a constraint to forbid each invalid triplet (δ, γ, P r[δ → γ]) is typically introduced [55]. Techniques such as the QuineMcCluskey algorithm [42] or the heuristic Espresso are used to reduce the number of generated equations. In the case of Constraint Programming (CP), table constraints permit to directly enforce the constraint (δ, γ, P r[δ → γ]) ∈ DDT, where DDT is the set of valid tuples [25]. These techniques are implemented in CLAASP. Diﬀerential and Linear Trails Search. CLAASP exposes functions to generate, for either paradigm among SAT, SMT, MILP or CP, a model for the search of diﬀerential or linear trails. More speciﬁcally, CLAASP implements the generation of models to ﬁnd: (1) One optimal (highest objective value) trail; or (2) All trails for which the objective value is within a ﬁxed range. The functions generating these models take, as an additional parameter, a list of variables for which the values are to be ﬁxed, and the corresponding values. Single-key trails are found by setting the key variables to zero, while related-key trails are found by placing no restrictions. Application to Diﬀerential Probability and Linear Hull Evaluation. Trails with identical input and output can be combined into a diﬀerential or a linear hull with higher probability than single trails. Observing diﬀerentials (or linear hulls), rather than single trails, can result in attacks on more rounds; the gap between the two cases is studied in [1]. CLAASP permits the enumeration of trails with ﬁxed variables, so that the evaluation of the probability of a diﬀerential, by enumerating all trails better than a certain weight with a ﬁxed input and output, is straightforward. Application to Impossible Diﬀerential and Zero-correlation Linear Approximation Search. Impossible diﬀerentials, as well as their counterpart in the linear world, zero-correlation linear hulls, are also of interest to cryptographer. CLAASP implements a technique similar to [19] to ﬁnd such properties; the main idea is to ﬁx an input and output diﬀerence, and to look for a trail with a solver; if no trail is found, then we have an impossible diﬀerential. As an example, we reproduce the 3 impossible diﬀerentials for 6 rounds of SPECK32/64 presented in [45] in less than 30 s using the SMT model. 4.4  
   
  Continuous Diﬀusion Tests  
   
  In [18], Coutinho et. al, describe a framework to construct continuous functions from Boolean ones. Assuming independence, these functions provide the proba-  
   
  CLAASP  
   
  401  
   
  bility or correlation between the output bits being 1 based on an input of real numbers that represent the probability of each input bit being 1. They are also able to generalize various cryptographic operations, leading to the creation of continuous versions of entire cryptographic algorithms. Upon these continuous versions of cryptographic algorithms, they construct three metrics, namely Continuous Avalanche Factor (CAF), Continuous Neutrality Measure (CNM), and Diﬀusion Factor (DF). The CAF is the continuous equivalent of the avalanche factor [21], which measures the proportion of output bits that change for input Hamming distances equal to 1 on average; this proportion is expected to be 0.5 for a random permutation. In the continuous version, since there is no concept of Hamming distance, the Euclidean Distance (ED) is used to evaluate CAF. The idea behind CAF is to measure how much the output of a continuous version of an algorithm changes, on average, when the input bit’s probability of being equal to 1 of a chosen random bit is slightly altered by a small real number λ. In other words, we need to evaluate, on average, the behavior of the ED between the outputs y0 = f (x0 ) and y1 = f (x1 ) for x0 , x1 ∈ B, when the ED of x0 and x1 is lesser than λ. It is expected for “good ciphers” that even with small values of λ, higher values on the ED of the propagation of these alterations, on average. For more information on the other two metrics (CNM and DF), see [18]. Within the continuous diﬀusion test module, CLAASP implements the continuous versions of several cryptographic operations, following Theorem 1 and Deﬁnitions 1 to 12 from [18], which can be combined to obtain the continuous version of entire primitives. The performance of Speck 128-128, AES-128, the iterated permutations in ASCON320 and the iterated permutation in ChaCha with respect to CAF, subject to λ = 0.001, is presented in Table 3. For the iterated permutation in ChaCha, a single round is equivalent to four half-quarter rounds in the table. Figure 6 displays the timing comparison of these ciphers for various sample sizes used in computing CAF. The experiments were conducted on a Ubuntu 22.04.1 machine equipped with 256 AMD core processors and 1TB of memory. When comparing Table 3 to Table 2 in [18], we observed slight variations in the CAF values reported in Fig. 6 compared to the values presented in [18]. This diﬀerence is due to our use of the Python Decimal package to handle small numbers, while the implementation of Table 2 in [18] employed the Relic library [2]. For instance, for ﬁve rounds of AES-128, we obtained a value of 0.777, whereas [18] reports 0.734.  
   
  402  
   
  E. Bellini et al.  
   
  Table 3. Continuous Avalanche Factor comparison for AES-128, ASCON320 permutation, ChaCha permutation, and Speck 128-128 using λ = 0.001.  
   
  4.5  
   
  Rounds AES  
   
  ASCON ChaCha Speck -  
   
  1 to 4  
   
  0  
   
  0  
   
  5  
   
  0.777 0.008  
   
  0  
   
  0  
   
  6  
   
  0.971 0.761  
   
  0.019  
   
  0  
   
  7  
   
  0.999 0.962  
   
  0.257  
   
  0.002  
   
  8  
   
  –  
   
  0.998  
   
  0.694  
   
  0.067  
   
  9  
   
  –  
   
  0.999  
   
  0.939  
   
  0.318  
   
  10  
   
  –  
   
  –  
   
  0.993  
   
  0.613  
   
  11  
   
  –  
   
  –  
   
  –  
   
  0.828  
   
  12  
   
  –  
   
  –  
   
  –  
   
  0.941  
   
  13  
   
  –  
   
  –  
   
  –  
   
  0.98  
   
  14  
   
  –  
   
  –  
   
  –  
   
  0.997  
   
  0  
   
  0  
   
  Algebraic Module  
   
  The objective of this module is to study the algebraic properties and algebraic attack resistance of a speciﬁed cipher. In algebraic cryptanalysis, breaking a block or stream cipher, involves solving a set of multivariate polynomial equations over a ﬁnite ﬁeld Fq , which often has one or a few solutions in Fq , but solving a system of multivariate random polynomials is generally a hard task. This module generates a multivariate algebraic polynomial system corresponding to the “sbox”, “linear_ layer”, “mix_column”, and “constant” components, together with the “XOR”, “AND”, “OR”, “SHIFT”, “ROTATE”, and “NOT” operations. It provides a set of polynomials representing the components and operations involved in a particular input cipher along with connection polynomials, which represent the links between the various components. From the polynomial system, it is possible to retrieve its algebraic degree, number of polynomials, and number of variables in order to analyze its algebraic features and the diﬃculty of solving the system. The security of a cipher (up to a particular number of rounds) against algebraic attacks could be evaluated by solving the corresponding algebraic system up to that many rounds. The module now oﬀers a method to test it by solving the system in a time limit using only the Gröbner basis computation [15] available on the SAGE platform. The algebraic module is currently in its preliminary stage and will be improved in upcoming releases. 4.6  
   
  Neural Aided Cryptanalysis Module  
   
  Following Aron Gohr’s seminal paper at CRYPTO’19 [26], improving the stateof-the-art diﬀerential cryptanalysis result on the SPECK32-64 cipher, neuralbased approaches to cryptanalysis have gained traction in the community. In  
   
  CLAASP  
   
  403  
   
  Gohr’s approach, a neural network is trained to distinguish, from an input composed of 2 ciphertexts in binary format, whether they correspond to the encryption of two unrelated plaintexts, or of two plaintexts with a given XOR diﬀerence. CLAASP implements such approaches, and other neural-based analysis tools. Single Ciphertext Approach: Neural Network Black box Distinguisher Tests. Diﬀerential neural cryptanalysis examines pairs of plaintexts. The black box test implemented by CLAASP takes a step back, and focuses on single ciphertexts. Built from [11], this test investigates whether a neural network can ﬁnd a relation between the inputs of a primitive and its output. The neural network is trained to label samples [P, C] as 0 (if Y is random) or 1 if Y is the output of a given component of the primitive. This test returns the accuracy of distinguishing a ciphertext coming from an instance of the cipher with a certain key and the output of a random permutation. After a certain amount of rounds, the accuracy will converge to 0.5, meaning that the black box distinguisher is not able to distinguish the cipher output from random. Pairs of Ciphertexts: Neural Network Diﬀerential Distinguisher Tests. This test implements the neural distinguisher described by Gohr in [26], with the simpliﬁed training pipeline described in [10], where a depth-1 neural distinguisher trained on n rounds is iteratively retrained for n + 1, . . . n + t rounds, where n + t is the ﬁrst round where the neural distinguisher fails to learn. Speciﬁcally, the neural distinguisher is trained to label samples [C0 = EK (P0 ), C1 = EK (P1 )] as 0 (if P0 ⊕ P1 is random) or 1 if P0 ⊕ P1 is a given, ﬁxed value δ. Helper Function: Truncated Diﬀerential Search For Neural Distinguishers. The previous test relies on an input diﬀerence with good propagation properties. It has been observed [26] that the input diﬀerence that starts the most likely diﬀerential does not result in the best neural distinguishers. Further research [12] suggested diﬀerential-linear properties, based on highly likely truncated diﬀerentials a few rounds before the studied round, may be at play. This assumption was used as the basis to an input diﬀerence search technique [10], where a genetic algorithm explores potential input diﬀerences and ranks them based on the cumulative biases of the resulting output diﬀerence bits. This algorithm is implemented by CLAASP, and can be used to retrieve Gohr’s original input diﬀerence. These functions are illustrated in the supplementary material. The script ﬁrst runs the black box test on 1 round of Speck 64, then runs the input diﬀerence search for Speck 64, and trains Gohr’s neural network using the optimal diﬀerence returned by the optimizer. Note that the optimizer is not deterministic, and its parameters are adapted for a reasonably fast execution time for demonstration purposes; therefore, it may, in some rare instance, fail to ﬁnd the optimal input diﬀerence 0x00400000.  
   
  404  
   
  5  
   
  E. Bellini et al.  
   
  Benchmark Comparison with Other Libraries  
   
  In this section we compare CLAASP to similar libraries. 5.1  
   
  TAGADA  
   
  The TAGADA library focuses on the diﬀerential cryptanalysis of word-oriented ciphers with an SPN structure. For such ciphers, it is common (e.g., [14]) to divide the search into two steps. The ﬁrst step aims to ﬁnd truncated diﬀerential characteristics through the minimization of the non-linear operators utilized in this process. The second step enumerates the truncated diﬀerential characteristic passing to the minimum number of non-linear operators found in the previous step. It was shown [25] that the ﬁltering of the ﬁrst step may be insufﬁcient so that too many solutions are left to explore in step 2. More advanced ﬁltering is, therefore, beneﬁcial and enables scaling to more rounds. This is done through additional constraints that capture linear dependencies between variables during step 1. The TAGADA library generalizes such constraints, making it very eﬃcient for word-based ciphers. These techniques are not, at the moment, included in CLAASP, so TAGADA is expected to perform signiﬁcantly better on word-based characteristics search. We are planning to include these additional constraints in the next releases of CLAASP. On the other hand, the basic version of the ﬁrst step, searching for the minimum number of active SBoxes of SPN ciphers, is implemented in CLAASP. TAGADA implements the option of running the ﬁrst step search with the basic technique used in CLAASP; we attempted to run the search for 3 and 4 rounds of AES-128, but we were not able to reproduce the known results from [24,31,40] with TAGADA, which reported 2 and 7 SBoxes respectively, rather than the expected 3 and 9. On the other hand, CLAASP returned the expected solution. Note that TAGADA can only generate MiniZinc models, while CLAASP allows to directly write the model in the language supported by the solvers (including a MiniZinc interface). 5.2  
   
  CASCADA  
   
  We compared CLAASP and CASCADA based on the time spent seeking optimal characteristics in single-key scenarios for ciphers Speck 32-64, Speck 64-128, and LEA across multiple rounds, using SMT solvers: MathSAT, Yices, and Z3. Average timings from ﬁve repetitions per round were considered. Testing was on an Ubuntu 22.04.1 machine with 256 AMD cores and 1TB RAM. With the Yices solver, CLAASP’s performance mirrored CASCADA. However, it outperformed with MathSAT and Z3. See a detailed graph in the appendix of the longer version of this paper [8]. In terms of functionalities, CASCADA includes the search for impossible diﬀerentials, in particular through the method of [19]. In this method, the variables corresponding to the input and output diﬀerences of a diﬀerential are ﬁxed to a value that the analyst wants to test, and the solver is run. If the solver  
   
  CLAASP  
   
  405  
   
  ﬁnds a solution, then the diﬀerential is possible; otherwise, it is impossible. In this method, the analyst usually tests all the pairs of input and output diﬀerences of low hamming weight (typically 1). A similar technique can be used for zero-correlation linear approximations. Using this method, CLAASP can for instance retrieve the 17-rounds impossible diﬀerential on HIGHT presented in [19] in under 10 min on a single core.  
   
  6  
   
  Conclusion  
   
  The fast-paced publication of new cryptanalysis techniques, of improvement of existing ones, makes it crucial to have an eﬃcient way to test a given property on a large number of primitives; CLAASP aims to fulﬁll this need. In its current form, it already oﬀers a vast array of cipher analysis techniques, from component analysis, to automatic models building, through neural cryptanalysis. Future releases will add more primitives, as well as further analysis techniques, such as guess-and-determine or meet-in-the-middle techniques. More importantly, the CLAASP team is strongly committed to include new state-of-the-art improvements to automated techniques as it evolves, and provide a one-stop shop to evaluate, compare and experiment with modiﬁcations on existing methods. Finally, the open-source status of the library is an invitation to researchers from the community to not only use, but also improve CLAASP as they see ﬁt.  
   
  References 1. Ankele, R., Kölbl, S.: Mind the gap - a closer look at the security of block ciphers against diﬀerential cryptanalysis. In: Cid, C., Jr., M.J.J. (eds.) SAC 2018. LNCS, vol. 11349, pp. 163–190. Springer, Cham (2018). https://doi.org/10.1007/978-3030-10970-7_8 2. Aranha, D.F., Gouvêa, C.P.L., Markmann, T., Wahby, R.S., Liao, K.: RELIC is an eﬃcient library for cryptography. https://github.com/relic-toolkit/relic 3. Aumasson, J.-P., Çalık, Ç., Meier, W., Özen, O., Phan, R.C.-W., Varıcı, K.: Improved cryptanalysis of skein. In: Matsui, M. (ed.) ASIACRYPT 2009. LNCS, vol. 5912, pp. 542–559. Springer, Heidelberg (2009). https://doi.org/10.1007/9783-642-10366-7_32 4. Azimi, S.A., Ranea, A., Salmasizadeh, M., Mohajeri, J., Aref, M.R., Rijmen, V.: A bit-vector diﬀerential model for the modular addition by a constant and its applications to diﬀerential and impossible-diﬀerential cryptanalysis. Des. Codes Cryptogr. 90(8), 1797–1855 (2022) 5. Barrett, C., Fontaine, P., Tinelli, C.: The Satisﬁability Modulo Theories Library (SMT-LIB). www.SMT-LIB.org (2016) 6. Bassham, L., et al.: Special Publication (NIST SP) - 800–22 Rev 1a: A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications (2010) 7. Bassham, L., Soto, J.: NISTIR 6483: randomness testing of the advanced encryption standard ﬁnalist candidates. NIST Internal or Interagency Reports (2000)  
   
  406  
   
  E. Bellini et al.  
   
  8. Bellini, E., et al.: CLAASP: a cryptographic library for the automated analysis of symmetric primitives. Cryptology ePrint Archive, Paper 2023/622 (2023). https:// eprint.iacr.org/2023/622 9. Bellini, E., Gérault, D., Grados, J., Makarim, R.H., Peyrin, T.: Fully automated diﬀerential-linear attacks against ARX ciphers. In: Rosulek, M. (ed.) CT-RSA 2023. LNCS, vol. 13871, pp. 252–276. Springer, Cham (2023). https://doi.org/10. 1007/978-3-031-30872-7_10 10. Bellini, E., Gerault, D., Hambitzer, A., Rossi, M.: A Cipher-agnostic neural training pipeline with automated ﬁnding of good input diﬀerences. Cryptology ePrint Archive, Paper 2022/1467 (2022). https://eprint.iacr.org/2022/1467 11. Bellini, E., Hambitzer, A., Protopapa, M., Rossi, M.: Limitations of the use of neural networks in black box cryptanalysis. In: Ryan, P.Y., Toma, C. (eds.) SecITC 2021. LNCS, vol. 13195, pp. 100–124. Springer, Heidelberg (2021). https://doi.org/ 10.1007/978-3-031-17510-7_8 12. Benamira, A., Gerault, D., Peyrin, T., Tan, Q.Q.: A deeper look at machine learning-based cryptanalysis. In: Canteaut, A., Standaert, F.-X. (eds.) EUROCRYPT 2021. LNCS, vol. 12696, pp. 805–835. Springer, Cham (2021). https:// doi.org/10.1007/978-3-030-77870-5_28 13. Biham, E., Shamir, A.: Diﬀerential cryptanalysis of des-like cryptosystems. J. Cryptol. 4(1), 3–72 (1991) 14. Biryukov, A., Nikolić, I.: Automatic search for related-key diﬀerential characteristics in byte-oriented block ciphers: application to AES, Camellia, Khazad and others. In: Gilbert, H. (ed.) EUROCRYPT 2010. LNCS, vol. 6110, pp. 322–344. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-13190-5_17 15. Brickenstein, M., Dreyer, A.: Polybori: a framework for Gröbner-basis computations with Boolean polynomials. J. Symb. Comput. 44(9), 1326–1345 (2009) 16. Brown, R.G.: DieHarder: A Random Number Test Suite Version 3.31.1 (2021). https://webhome.phy.duke.edu/~rgb/General/dieharder.php 17. Coutinho, M., Passos, I., Vásquez, J.C.G., de Mendonça, F.L.L., de Sousa, R.T., Borges, F.: Latin dances reloaded: improved cryptanalysis against salsa and chacha, and the proposal of forró. In: Agrawal, S., Lin, D. (eds.) Advances in Cryptology - ASIACRYPT 2022, LNCS, vol. 13791, pp. 256–286. Springer (2022) 18. Coutinho, M., de Sousa Júnior, R.T., Borges, F.: Continuous diﬀusion analysis. IEEE Access 8, 123735–123745 (2020) 19. Cui, T., Chen, S., Fu, K., Wang, M., Jia, K.: New automatic tool for ﬁnding impossible diﬀerentials and zero-correlation linear approximations. Sci. China Inf. Sci. 64(2) (2021) 20. Daemen, J., Hoﬀert, S., Assche, G.V., Keer, R.V.: The design of Xoodoo and Xooﬀf. IACR Trans. Symmetric Cryptol. 2018(4), 1–38 (2018) 21. Daum, M.: Cryptanalysis of Hash functions of the MD4-family (2005) 22. Dey, S., Garai, H.K., Maitra, S.: Cryptanalysis of reduced round chacha - new attack & deeper analysis. IACR Trans. Symmetric Cryptol. 2023(1), 89–110 (2023) 23. Dobraunig, C., Eichlseder, M., Mendel, F.: Heuristic tool for linear cryptanalysis with applications to CAESAR candidates. In: Iwata, T., Cheon, J.H. (eds.) ASIACRYPT 2015. LNCS, vol. 9453, pp. 490–509. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48800-3_20 24. Fouque, P.-A., Jean, J., Peyrin, T.: Structural evaluation of AES and chosen-key distinguisher of 9-round AES-128. In: Canetti, R., Garay, J.A. (eds.) CRYPTO 2013. LNCS, vol. 8042, pp. 183–203. Springer, Heidelberg (2013). https://doi.org/ 10.1007/978-3-642-40041-4_11  
   
  CLAASP  
   
  407  
   
  25. Gérault, D., Lafourcade, P., Minier, M., Solnon, C.: Computing AES related-key diﬀerential characteristics with constraint programming. Artif. Intell. 278, 103183 (2020) 26. Gohr, A.: Improving attacks on round-reduced Speck32/64 using deep learning. In: Boldyreva, A., Micciancio, D. (eds.) CRYPTO 2019. LNCS, vol. 11693, pp. 150–179. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-26951-7_6 27. Hadipour, H., Eichlseder, M.: Autoguess: a tool for ﬁnding guess-and-determine attacks and key bridges. In: Ateniese, G., Venturi, D. (eds.) ACNS 2022. LNCS, vol. 13269, pp. 230–250. Springer, Cham (2022). https://doi.org/10.1007/978-3031-09234-3_12 28. Hall-Andersen, M., Vejre, P.S.: Generating graphs packed with paths estimation of linear approximations and diﬀerentials. IACR Trans. Symmetric Cryptol. 2018(3), 265–289 (2018) 29. Hall-Andersen, M., Vejre, P.S.: Cryptagraph. https://github.com/psve/ cryptagraph (2019) 30. Indrøy, J.P., Raddum, H.: Trail search with CRHS equations. IACR Cryptol. ePrint Arch, p. 1329 (2021) 31. Khoo, K., Lee, E., Peyrin, T., Sim, S.M.: Human-readable proof of the related-key security of AES-128. IACR Trans. Symmetric Cryptol. 2017(2), 59–83 (2017) 32. Leurent, G.: Analysis of diﬀerential attacks in ARX constructions. In: Wang, X., Sako, K. (eds.) ASIACRYPT 2012. LNCS, vol. 7658, pp. 226–243. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-34961-4_15 33. Leurent, G.: Construction of diﬀerential characteristics in ARX designs application to skein. In: Canetti, R., Garay, J.A. (eds.) CRYPTO 2013. LNCS, vol. 8042, pp. 241–258. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-400414_14 34. Libralesso, L., Delobel, F., Lafourcade, P., Solnon, C.: Automatic Generation of Declarative Models For Diﬀerential Cryptanalysis. In: Michel, L.D. (ed.) 27th International Conference on Principles and Practice of Constraint Programming, CP 2021, Montpellier, France (Virtual Conference), October 25–29, 2021. LIPIcs, vol. 210, pp. 40:1–40:18. Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2021) 35. Lipmaa, H., Moriai, S.: Eﬃcient algorithms for computing diﬀerential properties of addition. In: Matsui, M. (ed.) FSE 2001. LNCS, vol. 2355, pp. 336–350. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45473-X_28 36. Liu, Y., Witte, G.D., Ranea, A., Ashur, T.: Rotational-XOR cryptanalysis of reduced-round SPECK. IACR Trans. Symmetric Cryptol. 2017(3), 24–36 (2017) 37. Marsaglia, G.: The Marsaglia Random Number CDROM including the Diehard Battery of Tests of Randomness (1995). https://web.archive.org/web/ 20160125103112. http://stat.fsu.edu/pub/diehard/ 38. Matsui, M.: Linear cryptanalysis method for DES Cipher. In: Helleseth, T. (ed.) EUROCRYPT 1993. LNCS, vol. 765, pp. 386–397. Springer, Heidelberg (1994). https://doi.org/10.1007/3-540-48285-7_33 39. Mouha, N., Preneel, B.: A Proof that the ARX Cipher Salsa20 is secure against diﬀerential cryptanalysis. IACR Cryptol. ePrint Arch, p. 328 (2013) 40. Mouha, N., Wang, Q., Gu, D., Preneel, B.: Diﬀerential and linear cryptanalysis using mixed-integer linear programming. In: Wu, C.-K., Yung, M., Lin, D. (eds.) Inscrypt 2011. LNCS, vol. 7537, pp. 57–76. Springer, Heidelberg (2012). https:// doi.org/10.1007/978-3-642-34704-7_5 41. Nethercote, N., Stuckey, P.J., Becket, R., Brand, S., Duck, G.J., Tack, G.: MiniZinc: towards a standard CP modelling language. In: Bessière, C. (ed.) CP 2007. LNCS,  
   
  408  
   
  42. 43. 44.  
   
  45. 46.  
   
  47.  
   
  48.  
   
  49. 50.  
   
  51. 52.  
   
  53. 54.  
   
  55.  
   
  56.  
   
  57.  
   
  E. Bellini et al. vol. 4741, pp. 529–543. Springer, Heidelberg (2007). https://doi.org/10.1007/9783-540-74970-7_38 Quine, W.V.: A way to simplify truth functions. Amer. Math. Monthly 62, 627–631 (1955) Ranea, A., Liu, Y., Ashur, T.: An easy-to-use tool for rotational-XOR cryptanalysis of ARX block ciphers. IACR Cryptol. ePrint Arch, p. 727 (2020) Ranea, A., Rijmen, V.: Characteristic automated search of cryptographic algorithms for distinguishing attacks (CASCADA). IET Inf. Secur. 16(6), 470–481 (2022) Ren, J., Chen, S.: Cryptanalysis of reduced-round speck. IEEE Access 7, 63045– 63056 (2019) Rukhin, A., et al.: Special Publication (NIST SP) - 800–22: A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications (2001) Sadeghi, S., Rijmen, V., Bagheri, N.: Proposing an MILP-based method for the experimental veriﬁcation of diﬀerence-based trails: application to SPECK. SIMECK. Des. Codes Cryptogr. 89(9), 2113–2155 (2021) Sasaki, Yu., Todo, Y.: New impossible diﬀerential search tool from design and cryptanalysis aspects. In: Coron, J.-S., Nielsen, J.B. (eds.) EUROCRYPT 2017. LNCS, vol. 10212, pp. 185–215. Springer, Cham (2017). https://doi.org/10.1007/ 978-3-319-56617-7_7 Soto, J.: NISTIR 6390: Randomness testing of the advanced encryption standard candidate algorithms. NIST Internal or Interagency Reports (1999) Soto, J.: Statistical testing of random number generators. In: Proceedings of the 22nd National Information Systems Security Conference, vol. 10, p. 12. NIST Gaithersburg, MD (1999). https://csrc.nist.gov/CSRC/media/Publications/ conference-paper/1999/10/21/proceedings-of-the-22nd-nissc-1999/documents/ papers/p24.pdf Stankovski, P.: Automated algebraic cryptanalysis, pp. 11. ECRYPT II (2010). tools for Cryptanalysis 2010; Conference date: 22–06-2010 Through 23–06-2010 Stankovski, P.: Greedy distinguishers and nonrandomness detectors. In: Gong, G., Gupta, K.C. (eds.) INDOCRYPT 2010. LNCS, vol. 6498, pp. 210–226. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-17401-8_16 Stefan Kölbl: CryptoSMT: an easy to use tool for cryptanalysis of symmetric primitives. https://github.com/kste/cryptosmt Sun, L., Wang, W., Wang, M.: Accelerating the search of diﬀerential and linear characteristics with the SAT method. IACR Trans. Symmetric Cryptol. 2021(1), 269–315 (2021) Sun, S., Hu, L., Wang, P., Qiao, K., Ma, X., Song, L.: Automatic security evaluation and (related-key) diﬀerential characteristic search: application to SIMON, PRESENT, LBlock, DES(L) and other bit-oriented block ciphers. In: Sarkar, P., Iwata, T. (eds.) ASIACRYPT 2014. LNCS, vol. 8873, pp. 158–178. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-45611-8_9 Vesselinux, Laboratory of Algorithmics, C., of Luxembourg University, S.L.: Vesselinux/yaarx: Yet another toolkit for analysis of ARX cryptographic algorithms. https://github.com/vesselinux/yaarx Zhang, X., Chen, Z., Cai, S.: Parkissat: Random shuﬄe based and pre-processing extended parallel solvers with clause sharing. SAT COMPETITION, 51 (2022)  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics and Its Applications Kosei Sakamoto1(B) , Ryoma Ito2 , and Takanori Isobe3 1  
   
  Mitsubishi Electric Corporation, Kamakura, Japan [email protected]  2 NICT, Koganei, Japan [email protected]  3 University of Hyogo, Kobe, Japan [email protected]   
   
  Abstract. The most crucial but time-consuming task for diﬀerential cryptanalysis is to ﬁnd a diﬀerential with a high probability. To tackle this task, we propose a new SAT-based automatic search framework to eﬃciently ﬁgure out a diﬀerential with the highest probability under a speciﬁed condition. As the previous SAT methods (e.g., the Sun et al.’s method proposed at ToSC 2021(1)) focused on accelerating the search of an optimal single diﬀerential characteristic, these are not optimized for evaluating a clustering eﬀect to obtain a tighter diﬀerential probability of diﬀerentials. In contrast, our framework takes advantage of a method to solve incremental SAT problems in parallel using a multi-threading technique, and consequently, it oﬀers the following advantages compared with the previous methods: (1) speedy identiﬁcation of a diﬀerential with the highest probability under the speciﬁed conditions; (2) eﬃcient construction of the truncated diﬀerential with the highest probability from the obtained multiple diﬀerentials; and (3) applicability to a wide class of the symmetric-key primitives. To demonstrate the eﬀectiveness of our framework, we apply it to the block cipher PRINCE and the tweakable block cipher QARMA. We successfully ﬁgure out the tight diﬀerential bounds for all variants of PRINCE and QARMA within the practical time, thereby identifying the longest distinguisher for all the variants, which improves existing ones by one to four more rounds. Besides, we uncover notable diﬀerences between PRINCE and QARMA in the behavior of differential, especially for the clustering eﬀect. We believe that our ﬁndings shed light on new structural properties of these important primitives. Keywords: Diﬀerential · SAT-based automatic search SAT problem · Low-latency primitives  
   
  · Incremental  
   
  Due to the page limitation, we leave the part of (1) descriptions of several basic algorithms and SAT models, (2) a detailed explanation of our investigation about the impact of multi-threading techniques, (3) key-recovery attacks, and (4) discussion of good parameters in our algorithms to the full version of this paper (https://eprint.iacr. org/2023/1227). c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 409–428, 2024. https://doi.org/10.1007/978-3-031-53368-6_20  
   
  410  
   
  1  
   
  K. Sakamoto et al.  
   
  Introduction  
   
  Background. The most crucial but time-consuming part of diﬀerential cryptanlysis [7] is to determine a pair of plaintext diﬀerences and the corresponding ciphertext diﬀerences and construct a diﬀerential distinguisher with high probability. To this end, cryptographers frequently use a diﬀerential characteristic, which is a sequence of the internal diﬀerences in each round. However, from the attackers’ viewpoint, they are interested in not the internal diﬀerences but only a pair of the input and output diﬀerences, which is called a diﬀerential in literature. A diﬀerential is more useful than a diﬀerential characteristic for the attackers, as a diﬀerential has a higher probability than that of a diﬀerential characteristic. Several studies investigated the relationship between a diﬀerential characteristic and a diﬀerential, revealing that a gap between their probabilities can be signiﬁcant [2,8,18]. Most of these studies focused on a diﬀerential constructed by only a diﬀerential characteristic with the highest probability, which is called the optimal diﬀerential characteristic. This seems reasonable, as the probability of the optimal diﬀerential characteristic dominates the probability of a diﬀerential in numerous designs. However, K¨ olbl and Roy [19] demonstrated an interesting case in Simeck32 [29] where a diﬀerential with a higher probability can be constructed by the non-optimal diﬀerential characteristic. Although this appears to be a special case, it can be valid for any design. From these aspects, ﬁnding a diﬀerential with a higher probability still remains a challenging task. Finding such a diﬀerential is not only useful from the attackers’ aspect but also crucial from the designers’ aspect. In particular, the ultra-low-latency designs must be carefully designed against diﬀerential cryptanalysis, because they are usually based on a substitution-permutation network with a small number of rounds, and the growth of the diﬀerential probability is not suﬃcient at the beginning of the rounds. In fact, the designers of MANTIS [6] and SPEEDY [21] invested signiﬁcant eﬀorts into guaranteeing the resistance against diﬀerential cryptanalysis in their works. Nevertheless, they were broken by diﬀerential cryptanalysis [10,15]. Furthermore, the best attack to the ﬁrst low-latency design PRINCE [9] is also (multiple) diﬀerential cryptanalysis on 10 (out of 12) rounds proposed by Canteaut et al. [12]. Hence, it is evidently important to investigate a diﬀerential in detail, especially for low-latency designs. Limitations of SAT-Based Automatic Search Tools. The existing SAT-based automatic search tools, proposed by Sun et al. [26,27], focused on accelerating the search for an optimal diﬀerential characteristic by incorporating the Matsui’s bounding conditions [24]. These tools are valid for evaluating a single diﬀerential characteristic, but the Matsui’s bounding conditions are not suitable for the purpose of evaluating the clustering eﬀect of multiple diﬀerential characteristics; thus, the existing tools are not suitable for eﬃciently ﬁnding a diﬀerential with a higher probability. Certainly, it can be applied to evaluate the clustering eﬀect of multiple diﬀerential characteristics by removing the Matsui’s bounding conditions and adding some new conditions. However, such  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  411  
   
  a straightforward adjustment can be ineﬃcient because Sun et al. assumed only an environment with a single-thread execution even though their SAT solver accepts an execution on multiple threads. Considering that the evaluation for the clustering eﬀect of multiple diﬀerential characteristics having diﬀerent input and output diﬀerences requires a much more computational cost than that for ﬁnding the optimal diﬀerential characteristic, the tool for ﬁnding a diﬀerential with the highest possible probability should be optimized for execution on multiple threads. Moreover, it is also of great importance to investigate the impact of the relation on the eﬃciency between the number of threads to be assigned to solve a single SAT problem and the degree of the parallelization for the evaluation of the clustering eﬀect, as we have to evaluate the clustering eﬀect for each found diﬀerential characteristic having diﬀerent input and output diﬀerences with a high probability. Therefore, without these considerations, it is hard to eﬃciently investigate the clustering eﬀect of numerous diﬀerential characteristics having diﬀerent input and output diﬀerences in detail. This investigation leads to understanding the behavior of the probability about diﬀerentials more deeply; thus, optimizing these SAT-based tools to evaluate for the clustering eﬀect of diﬀerential characteristics is crucial. Our Contributions. In this study, we propose a new generic SAT-based automatic search framework that aims to ﬁgure out a diﬀerential with a higher probability under the speciﬁed condition, in contrast to existing approaches. The main concept of the framework involves investigating the clustering eﬀect of all diﬀerential characteristics having diﬀerent input and output diﬀerences with a speciﬁed range of weight and identifying the good diﬀerential. Our framework fully leverages a method to solve incremental SAT problems, which can eﬃciently solve a SAT problem with small modiﬁcations multiple times, in parallel using a multithreading technique. As an incremental SAT problem can be eﬃciently solved by the bounded variable elimination method [16], it is known that we can eﬃciently evaluate the clustering eﬀect by converting the evaluation of the clustering eﬀect into an incremental SAT problem. In our method, we also take advantage of an incremental SAT problem to eﬃciently ﬁnd all diﬀerential characteristics having diﬀerent input and output diﬀerences that are seeds to construct diﬀerentials, as well as the evaluation of the clustering eﬀect. By carefully investigating the most suitable parameters, such as the number of threads to be assigned to solve a single incremental SAT problem and the degree of the parallelization for the evaluation of the clustering eﬀect, to solve multiple incremental SAT problems eﬃciently, our framework enables us to thoroughly evaluate the clustering eﬀect of such all differential characteristics not only with the highest probability but also with any probability. Hence, we evaluate the probability of diﬀerentials more comprehensively than any other previous methods. Identifying Good Diﬀerentials on PRINCE and QARMA. To demonstrate the eﬀectiveness of our framework, we apply it to PRINCE [9] and QARMA [3], which are the reﬂection ciphers for low-latency applications. As a result, we signiﬁcantly improve previous diﬀerential bounds for all variants of these ciphers as shown in  
   
  412  
   
  K. Sakamoto et al.  
   
  Table 1. Comparison of our results with existing ones regarding distinguishers. Cipher  
   
  Total Attacked Setting† Type‡ Time/Data Reference # Rounds # Rounds  
   
  PRINCE PRINCEv2 12  
   
  4 6 6 6 7  
   
  SK SK SK SK SK  
   
  ID D I D D  
   
  – 262 262 256.42 255.771  
   
  [14] [2] [11] [12] Section 4.1  
   
  QARMA64  
   
  16  
   
  6 7 4.5 7 8 9 10  
   
  SK SK RT RT RT RT RT  
   
  ID D ID ID SS ZC/I D  
   
  – 258.921 – – 257 244 260.831  
   
  [28] Section 4.2 [23] [30] [22] [1] Section 4.2  
   
  QARMA128  
   
  24  
   
  6 SK ID – [28] 10 SK D 2121.549 Section 4.2 6.5 RT ID – [23] 8 RT TDIB 2124.1 [22] 12 RT D 2120.024 Section 4.2 † SK: Single-Key, RT: Related-Tweak ‡ D: Diﬀerential, I: Integral, ID: Impossible Diﬀerential, SS: Statistical Saturation, ZC: Zero-Correlation, TDIB: Tweak Diﬀerence Invariant Bias  
   
  Table 1, and our diﬀerential distinguishers are the longest ones among existing ones. It is important to note that while the previous attacks may have been adjusted for key recovery, identifying the longest distinguisher is very important to deeply comprehend the structural properties of these primitives as pseudo random permutations. These results demonstrate that the proposed framework is eﬀective for evaluating the tight diﬀerential bounds. Diﬀerence in Behavior of Clustering Eﬀect Between PRINCE and QARMA. We look into the diﬀerence between PRINCE and QARMA in the behavior of a differential. Our experiments observe that the gaps in the probability between a diﬀerential characteristic and a diﬀerential can be large in QARMA under the SK setting compared to that in PRINCE. Speciﬁcally, QARMA under the single-key (SK) setting has a large impact on the clustering eﬀect, and the case reported by K¨ olbl and Roy [19] can occur in QARMA under the SK setting. A detailed investigation of such gaps reveals that they are inﬂuenced by diﬀerent design strategies for the linear layers (i.e., matrices). After conducting the additional experiments using four types of matrices with diﬀerent properties, we ﬁnd that the target cipher has a good resistance to a clustering eﬀect when each output bit of the round function depends on as many input bits of the round function  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  413  
   
  as possible. We conclude that a cipher using a matrix with the same property as that used in QARMA has a large impact on a clustering eﬀect, and a clustering eﬀect in non-optimal weights can strongly aﬀect the probability of a diﬀerential. Our framework can be applied to any symmetric-key primitive. Also, it is very important to analyze the tight diﬀerential bound in the ﬁeld of symmetric-key cryptanalysis. Therefore, we believe that our work is a signiﬁcant contribution in terms of the tight security analysis for a wide class of symmetric-key primitives.  
   
  2 2.1  
   
  Preliminaries Deﬁnitions of Diﬀerential Characteristic and Diﬀerential  
   
  We frequently use terms diﬀerential characteristic and diﬀerential throughout this paper. To avoid mixing these terms, we specify their deﬁnitions and how to calculate their probabilities. Further, we provide the deﬁnition of weight that is also frequently used in this paper. Notably, we explain a diﬀerential characteristic and diﬀerential over an r-round iterated block cipher E(·) = fr (·) ◦ · · · ◦ f1 (·). Deﬁnition 1 (Diﬀerential characteristic). A diﬀerential characteristic is a sequence of diﬀerences over E deﬁned as follows: f1  
   
  f2  
   
  fr  
   
  C = (c0 −→ c1 −→ · · · −→ cr ) := (c0 , c1 , · · · , cr ), where (c0 , c1 , · · · , cr ) denotes the diﬀerences in the output of each round, i.e., c0 and cr denote the diﬀerences in a plaintext and ciphertext, respectively. The probability of a diﬀerential characteristic is estimated by the product of the corresponding diﬀerential probabilities for each round on the Markov cipher assumption [20] as follows: Pr(C) =  
   
  r   
   
  fi  
   
  Pr(ci−1 −→ ci ).  
   
  i=1  
   
  Deﬁnition 2 (Diﬀerential). A diﬀerential is a pair of the input and output diﬀerences (c0 , cr ). The probability of a diﬀerential is estimated by a sum of probabilities for all diﬀerential characteristics sharing the same input and output diﬀerences (c0 , cr ) as follows:  fr f1 f2 E Pr(c0 − → cr ) = Pr(c0 −→ c1 −→ · · · −→ cr ). c 1 ,c 2 ,···c r −1  
   
  We ﬁnally provide the deﬁnition of weight which corresponds to the probability of a diﬀerential characteristic. Deﬁnition 3 (Weight). A weight w is a negated value of the binary logarithm of the probability Pr deﬁned as follows: w = − log2 Pr  
   
  414  
   
  2.2  
   
  K. Sakamoto et al.  
   
  SAT-Based Automatic Search for Diﬀerential Characteristics  
   
  SAT. When a formula consists of only AND (∧), OR (∨), and NOT (·) operations based on Boolean variables, we refer to it as a Boolean formula. In a SAT problem, a SAT solver checks whether there is an assignment of Boolean variables that can validate a Boolean formula or not. If such an assignment exists, a SAT solver returns satisﬁable or “SAT”. Generally, a SAT problem is an NPcomplete [13]. However, owing to numerous eﬀorts for SAT problems, nowadays, there are numerous excellent SAT solvers that can solve a SAT problem very eﬃciently, such as CaDiCaL, Kissat, and CryptoMiniSat5. A Boolean formula can be converted into a Conjunctive Normal Form (CNF), which is expressed by the conjunction (∧) of the disjunction (∨) on (possibly ja i ci,j ), where ci,j is a Boolean negated) Boolean variables, such as a=0 ( b=0 ja variable. We call each disjunction b=0 ci,j in a Boolean formula a clause. SAT-Based Automatic Tools. SAT-based automatic tools are known as a valid approach to ﬁnd optimal diﬀerential/linear characteristics and are more powerful than MILP-based ones as shown in [27]. To implement its approach with the SAT method, the diﬀerential/linear propagation over all operations in a primitive must be converted into a CNF, and then we check if there exists a diﬀerential/linear characteristic along with a speciﬁed weight as a SAT problem. We can know the optimal diﬀerential/linear characteristics by solving some SAT problems by changing the number of speciﬁed weights. SAT Models for Basic Operations. Our framework is based on a pure-SAT model proposed by Sun et al. [26,27]. Due to the page limitation, we do not give the detailed modeling method (for more information, please refer to Sun et al.’s work). Herein, we specify some basic notations that are used in this study to construct a whole SAT model as follows: MSAT : A whole SAT model that we solve. Mcla.operations : Clauses to express the propagation of diﬀerences in a certain operation. These clauses also contain variables to express a weight corresponding to the propagation of diﬀerences in a probabilistic operation. Mvar : Variables to construct clauses. In this study, we use Mcla.xor , Mcla.matrix , and Mcla.sbox as clauses to express the propagation of diﬀerences in PRINCE and QARMA. In addition, we also use Mcla.input and Mcla.sec(B) to evaluate a minimum weight. These clauses play a role as follows: Mcla.input : Clauses to avoid a trivial diﬀerential propagation, such as all input diﬀerences being zero at the same time. Mcla.sec(B) : Clauses to count the total weight of a primitive. More speciﬁcally, j the constraint of i=0 pi ≤ B can be added, where pi is a Boolean variable to express a weight and j is the total number of pi . There are several methods to realize such a constraint in a Boolean formula [4,25]. Among these, we employ Sequential Encoding Method [25] that was used in numerous works.  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  415  
   
  Finding Diﬀerential Characteristics with Minimum Weight. With the clauses and variables introduced in this section, we construct a whole SAT model as follows: MSAT ← (Mcla.matrix , Mcla.sbox , Mcla.sec , Mcla.input ). Now, we are ready to ﬁnd a diﬀerential characteristic with the minimum weight by feeding MSAT and Mvar to a SAT solver. If a SAT solver returns “UNSAT”, there is no diﬀerential characteristic with a weight of ≤ B. In that case, we increment B and repeat it until a SAT solver returns “SAT”. This means that we obtain a diﬀerential characteristic with the minimum weight of B. Modeling for a Clustering Eﬀect. To take a clustering eﬀect into account, we must solve a SAT problem multiple times with the same input and output differences, while the identical internal diﬀerential propagation is deleted from the solution space of the initial SAT problem. To realize this procedure, we introduce the following clauses: Mcla.clust : Clauses to ﬁx the input and output diﬀerences to ﬁnd multiple differential characteristics with the same input and output diﬀerences. Mcla.clust : Clauses to remove the internal diﬀerential propagation from a SAT model. These will be repeatably added to a SAT model whenever another internal diﬀerential propagation is found. When evaluating a clustering eﬀect, we attempt to ﬁnd a diﬀerential characteristic with the weight of B, not the weight of ≤ B so as to calculate the exact probr·i−1 ability of a diﬀerential due to the same reason mentioned in [26]. j=0 pj = B r·i−1 r·i−1 can be obtained by applying both j=0 pj ≤ B and j=0 pj ≥ B. The ﬁrst constraint is already given above, and the second one can be easily obtained from r·i−1 p ≤ B with a small change. More information is provided in the previous j j=0 r·i−1 study [26]. Hereafter, Mcla.sec(B) denotes the clauses to express j=0 pj ≥ B. The detailed comprehensive algorithm for ﬁnding diﬀerential characteristics and evaluating the clustering eﬀect will be given in the following section.  
   
  3  
   
  A New SAT Framework to Find the Best Diﬀerential  
   
  In this section, we propose a new generic SAT-based automatic search framework to ﬁnd a diﬀerential with a higher probability under a speciﬁed condition (we refer to it as a good diﬀerential in this paper). Speciﬁcally, our framework can eﬃciently investigate the clustering eﬀect of all diﬀerential characteristics having diﬀerent (c0 , cr ) with a speciﬁed range of probability and identify a good diﬀerential. Our framework leverages a method to solve incremental SAT problems in parallel using a multi-threading technique, leading to an eﬃcient search for all diﬀerentials under the speciﬁed condition. Speciﬁcally, the unique features of our framework are listed as follows:  
   
  416  
   
  K. Sakamoto et al.  
   
  Speedy identiﬁcation of a good diﬀerential. Most of the existing studies on solver-aided search methods have focused on searching for the optimal differential characteristics as eﬃciently as possible. In contrast, our framework aims to identify a good diﬀerential among numerous diﬀerential characteristics having diﬀerent (c0 , cr ) by evaluating the clustering eﬀect of them within the practical time. This can be realized by taking a method to solve incremental SAT problems in parallel using a multi-threading technique into consideration. Thereby, our framework enables us to ﬁnd good diﬀerentials under the speciﬁed range of the weight that the corresponding diﬀerential characteristic has. Eﬃcient construction of a good truncated diﬀerential. Our framework also enables us to ﬁnd a good truncated diﬀerential. This can be realized by combining all the obtained diﬀerentials under the speciﬁed truncated diﬀerential. The truncated diﬀerential attack is more powerful than the ordinary diﬀerential attack; thus, our framework leads to a better diﬀerential attack on many symmetric-key primitives. Applicability to a wide class of the symmetric-key primitives. Our framework leverages the existing SAT-based automatic search method proposed by Sun et al. [27] and maintains its availability of applications; thus, our framework can be applied to a wide class of the symmetric-key primitives. Therefore, compared with existing solver-aided tools, our framework can be the best tool to construct the (truncated) diﬀerential distinguisher for a wide class of symmetric-key primitives. 3.1  
   
  Our Approach  
   
  Conventionally, when we attempt to obtain a good diﬀerential, we adopt a strategy of searching it based on the optimal diﬀerential characteristic. This strategy seems reasonable in many cases; therefore, most of the existing studies followed this strategy and improved the diﬀerential attacks based on the diﬀerentials obtained by this strategy. However, this strategy might overlook the better one because the non-optimal diﬀerential characteristic sometimes constructs the better diﬀerentials than that by the optimal diﬀerential characteristic, as the case on Simeck32 reported by K¨ olbl and Roy [19]. To investigate diﬀerentials in more detail, we need to evaluate a clustering eﬀect of numerous diﬀerential characteristics having diﬀerent (c0 , cr ). Since this requires a huge computational cost, it is a time-consuming task even with the state-of-the-art approach, such as a pure SAT-based automatic search method proposed by Sun et al. [27]. To tackle this task, we focus on a method to eﬃciently solve an incremental SAT problem and consider a new strategy to speedily obtain all diﬀerential characteristics having diﬀerent (c0 , cr ) with a speciﬁed range of weight to evaluate the clustering eﬀect of them. The essential idea of our search strategy is very simple; we ﬁrst enumerate all single diﬀerential characteristics having diﬀerent (c0 , cr ) with a relatively high probability and then investigate the clustering eﬀect of every obtained diﬀerential characteristic. Figure 1 illustrates the overview of our approach in comparison with the conventional one.  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  417  
   
  Fig. 1. Approaches to identifying a good diﬀerential. “# diﬀerential characteristics” denotes the total number of diﬀerential characteristics having diﬀerent (c0 , cr ) with the corresponding probability in horizontal axis. Popt denotes the probability of an optimal diﬀerential characteristic. The gray area depicts evaluated diﬀerentials.  
   
  3.2  
   
  Incremental SAT Problem  
   
  An incremental SAT problem is a kind of SAT problem, that solves a general SAT problem multiple times with a small modiﬁcation, which the bounded variable elimination method [16] can eﬃciently realize. Several SAT solvers support the function to eﬃciently solve the incremental SAT problem, such as CryptoMiniSAT which is the most popular SAT solver in the ﬁeld of symmetric-key cryptography. Figure 2 illustrates ﬂowcharts of solving the general and an incremental SAT problem. Some Insights About Solving an Incremental SAT Problem. According to the Erlacher et al.’s work [17], assigning multiple threads to solve a single general SAT problem has a positive impact on reducing the runtime, but does not obtain the same degree of gain as the degree of the parallelization. From this fact, our work starts at investigating whether the same phenomenon happens in the case of an incremental SAT problem. As a result, we ﬁnd that it happens in the case of an incremental SAT problem as well. Moreover, we also ﬁnd that assigning multiple threads to solve a single incremental SAT problem does not improve the eﬃciency of the evaluation at all (see Sect. 3.4). This means that solving multiple incremental SAT problems in parallel on each single thread is more eﬃcient than solving a single incremental SAT problem on multiple threads. We leverage this insight into our framework.  
   
  418  
   
  K. Sakamoto et al.  
   
  Fig. 2. Flowcharts of solving the general and an incremental SAT problem.  
   
  Good Solver for an Incremental SAT Problem. There are numerous excellent SAT solvers tending to solve a general SAT problem, while not so many of them support solving an incremental SAT problem. Since our framework requires to eﬃciently solve not a general SAT problem but an incremental SAT problem, we must employ a SAT solver suitable for solving an incremental SAT problem. To the best of our knowledge, CryptoMiniSat51 is the most eﬃcient SAT solver to solve an incremental SAT problem2 . Hence, we use CryptoMiniSat5 throughout all of our evaluations. 3.3  
   
  Finding a Good Diﬀerential  
   
  We present a new method to ﬁnd a good diﬀerential under a speciﬁed condition. Our method requires several basic algorithms to ﬁnd diﬀerential characteristics, such as the ones presented in [27]. Due to the page limitation, We leave a detailed explanation of them in the full version of this paper.3 The idea of our method is to investigate a clustering eﬀect about all diﬀerential characteristics having diﬀerent (c0 , cr ) with not only the minimum weight, but also a speciﬁed range of weight, and then identify a good diﬀerential. Before giving a detailed algorithm of our method, we explain the procedure of this method step by step as follows: Step 1: Identify the weight Wmin of the r-round optimal diﬀerential characteristic by SATdiff.min (). Step 2: Obtain all diﬀerential characteristics having diﬀerent (c0 , cr ) with the weight from Wmin to Wmin + α by SATdiff.all (). 1 2 3  
   
  https://www.msoos.org/cryptominisat5/. CryptoMiniSat5 is the winner of the incremental library track at SAT competition 2020. https://eprint.iacr.org/2023/1227.  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  419  
   
  Algorithm 1: Finding the best diﬀerential. input : Wmin , r, Tw , Tc output: D, N 1 2 3 4 5 6 7 8 9 10 11 12  
   
  begin D ← (D0 , D1 , . . . , DT w −1 ) N ← (N0 , N1 , . . . , NT w −1 ) for i = Wmin to Wmin + Tw − 1 do Di −W m i n ← SATdiff.all (i, r, 1, 1) Ni −W m i n ← ∅ j←0 for all pairs in Di −W m i n do (j ) add SATdiff.clust (i, i + Tc − 1, r, Di −W i n ) to Ni −W i n j ←j+1 /* j denotes the index of D i −W i n , i.e., M AX(j) = |D i −W i n |  
   
  */  
   
  return (D, N )  
   
  Step 3: Evaluate the clustering eﬀect of all diﬀerential characteristics obtained in Step 2, and then ﬁnd a good diﬀerential. As can be seen in the above steps, this method can investigate the probability of diﬀerentials in more detail than any other existing tools. We give the detailed algorithm of this method in Algorithm 1. As inputs to Algorithm 1, we provide the minimum weight Wmin , the number of target rounds r, and two thresholds Tw and Tc . We can obtain Wmin by SATdiff.min () and decide Tw as the range of weights taken into account in the whole evaluation. For example, suppose that we obtain Wmin = 60 by SATdiff.min () and set Tw = 3, Algorithm 1 searches a good diﬀerential in all diﬀerential characteristics having diﬀerent (c0 , cr ) with the weight of 60, 61, and 62. We can also decide Tc as the range of weight taken into account in a clustering eﬀect for each diﬀerential characteristic. After executing Algorithm 1, we obtain lists of D and N which store all diﬀerentials (c0 , cr ) and the number of the diﬀerential characteristics for each weight in each diﬀerential, respectively. Then, we can calculate the probability for each diﬀerential with D and N . The computational cost of Algorithm 1 highly depends on Tw and Tc , because these two thresholds highly inﬂuence the number of times to solve an incremental SAT problem in the whole procedure of Algorithm 1. Therefore, Tw and Tc must be set depending on the computational environment. It should be noted that the clustering eﬀect for each diﬀerential will be evaluated in parallel because of some observations discussed in Sect. 3.4.  
   
  420  
   
  3.4  
   
  K. Sakamoto et al.  
   
  Optimizing the Eﬃciency by a Multi-threading Technique  
   
  To optimize the eﬃciency of our algorithms, we investigate the feature of an incremental SAT problem, e.g., the most eﬃcient way to solve multiple incremental SAT problems. We show several experimental results on the 5- and 9round PRINCE and the 6-round QARMA64 in the single-key setting. Based on our results, we conclude that assigning a single incremental SAT problem to each thread is more advantageous than assigning many threads to a single incremental SAT problem. Due to the page limitation, we leave the detailed explanations of our investigation to the full version of this paper (see Footnote 3). 3.5  
   
  A More Eﬃcient Algorithm to Find a Good Diﬀerential  
   
  Algorithm 1 can ﬁnd a good diﬀerential under the speciﬁed condition, while a computational cost becomes vast along with increasing Tw and Tc . The downside of Algorithm 1 is that it never returns any result when all diﬀerentials cannot be found out, and this situation happens often along with a weight far from Wmin . To address this problem, we propose Algorithm 2, which can evaluate a clustering eﬀect whenever a diﬀerential characteristic having diﬀerent (c0 , cr ) is found. In Algorithm 2, it is not always possible to identify a good diﬀerential under a speciﬁed condition, as we discard some diﬀerentials (c0 , cr ) in the middle of the procedure. However, we place emphasis on evaluating a clustering eﬀect as eﬃciently as possible. To reduce the entire computational cost, we screen the differential (c0 , cr ) depending on its diﬀerential probability by a certain threshold whenever evaluating a clustering eﬀect. If it does not satisfy a certain threshold, the evaluation of a clustering eﬀect for this diﬀerential (c0 , cr ) halts, and this diﬀerential is discarded. In Algorithm 2, we assume to execute it in parallel on an environment with multiple threads based on the fact in Sect. 3.4. We explain the overview of the procedure step by step as follows: Step 1: Find the same number of diﬀerential characteristics having diﬀerent (c0 , cr ) with the weight Wmin as the degree of parallelization. Step 2: Evaluate the clustering eﬀect for each obtained diﬀerential characteristic in parallel. During this evaluation, we store or update the information of a diﬀerential (c0 , cr ) with the highest probability (speciﬁcally, the diﬀerential and its probability), and this information is used to specify the threshold. If the probability of a diﬀerential (c0 , cr ) in the middle of evaluating the clustering eﬀect does not surpass a certain threshold, this evaluation halts, and such a diﬀerential is discarded. Otherwise, the evaluation proceeds and the highest probability is updated if the probability of the resulting diﬀerential exceeds the previous highest one. Step 3: Repeat Step 1–2 until all diﬀerential characteristics having diﬀerent (c0 , cr ) with the weight Wmin are found. If it is infeasible to ﬁnd all differential characteristics having diﬀerent (c0 , cr ), we stop the evaluation and obtain the highest probability of a diﬀerential in this evaluation so far. Step 4: Increase Wmin and repeat Step 1–3 until Wmin reaches a speciﬁed weight.  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  421  
   
  Algorithm 2: Finding the (almost) good diﬀerential for a multi-thread programming technique input : Wmin , r, Tw , Tc , Ts , Tt , Nthr output: (co p t .i n , co p t .o u t ), Popt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  
   
  begin N −1 0 1 Popt ← 0, Pt h r ← (Pthr , Pthr , . . . , Pthrthr ) D ← (D0 , D1 , . . . , DN t h r −1 ) for i = Wmin to Wmin + Tw − 1 do (MSAT , Mvar ) ← SETmodel (i, r) add auxiliary Boolean variables of Mcla.sec(i) to Mvar add Mcla.sec(i) to MSAT count ← 0 /* incremental SAT problem while SATdiff.char (MSAT , Mvar ) = (“SAT”, Cr ) do Dc o u n t mod N t h r ← (c0 , cr ) count ← count + 1 if count mod Nthr = 0 then for each thread do thread Pthr ← Thread(i, r, Tc , Ts , Tt , Popt , Dt h r e a d )  
   
  */  
   
  if M AX(Pt h r ) > Popt then (Do p t , Popt ) ← M AX(D, Pt h r ) n−1 add k=0 (v0,k ⊕ c0,k ) ∨ (vr,k ⊕ cr,k ) to MSAT if count mod Nthr = 0 then for each thread do thread Pthr ← Thread(i, r, Tc , Ts , Tt , Popt , Dt h r e a d ) if M AX(Pt h r ) > Popt then (Do p t , Popt ) ← M AX(D, Pt h r ) return (Do p t , Popt ) Function Thread(W, r, Tc , Ts , Tt , Popt , D) // A multi-threading technique begin N ← (N0 , N1 , . . . , NTc −1 ) N ← SATdiff.clust (W, W + Tt − 1, r, D) W +Tt −1 Ptmp ← i=W (Ni−W · 2−i ) if Ts · Ptmp > Popt then N ← SATdiff.clust (W + Tt , W + Tc − 1, r, D)  +Tc −1 −i Ptmp ← Ptmp + W i=W +Tt (Ni−W · 2 ) return Ptmp  
   
  As inputs to Algorithm 2, we provide the same parameters in Algorithm 1 and the additional two thresholds Ts and Tt which are the bounding condition used to narrow down the search space. We specify Tt and Ts as a range of the  
   
  422  
   
  K. Sakamoto et al.  
   
  Table 2. Diﬀerential probabilities of (almost) good diﬀerentials of PRINCE. Wmin denotes the same parameter as in Algorithms 1 and 2. #diﬀerentials denotes the number of diﬀerent diﬀerentials with a particular weight. The minimum weight of a diﬀerential characteristic for each round is written in bold. The highest diﬀerential probability for each round is written in red. The probabilities in a white and gray cell are obtained by Algorithms 1 and 2, respectively. For all results, we set Tw = 1 and Tc = 10. PRINCE Rounds  
   
  4 (1+2+1)  
   
  5 (1+2+2/2+2+1)  
   
  Wmin  
   
  32  
   
  33  
   
  34  
   
  35  
   
  36  
   
  39  
   
  40  
   
  41  
   
  42  
   
  43  
   
  Prob.  
   
  2−30.868  
   
  2−31.861  
   
  2−32.587  
   
  2−33.333  
   
  2−32.979  
   
  2−38.810  
   
  2−39.385  
   
  2−40.017  
   
  2−40.607  
   
  2−40.837  
   
  477452  
   
  3792944  
   
  4929816  
   
  5537848  
   
  5547896  
   
  598592  
   
  2231756  
   
  # diﬀerentials Time  
   
  6h06m57s 48h48m43s 47h34m17s 47h35m06s 48h01m15s  
   
  Rounds  
   
  576  
   
  12512  
   
  113840  
   
  1m21s  
   
  26m09s  
   
  4h08m26s  
   
  6 (2+2+2)  
   
  23h14m24s 48h03m32s  
   
  7 (2+2+3/3+2+2)  
   
  Wmin  
   
  44  
   
  45  
   
  46  
   
  47  
   
  48  
   
  56  
   
  57  
   
  58  
   
  59  
   
  60  
   
  Prob.  
   
  2−43.907  
   
  2−44.907  
   
  2−45.195  
   
  2−46.111  
   
  2−46.374  
   
  2−55.771  
   
  2−55.887  
   
  2−56.810  
   
  2−57.37  
   
  2−57.990  
   
  25968  
   
  5632  
   
  100976  
   
  835456  
   
  205272  
   
  212280  
   
  # diﬀerentials  
   
  64  
   
  512  
   
  1984  
   
  6592  
   
  Time  
   
  51s  
   
  4m21s  
   
  17m57s  
   
  1h07m16s  
   
  Rounds  
   
  8 (3+2+3)  
   
  Wmin Prob. # diﬀerentials Time  
   
  4h46m53s 5h07m16s 90h40m16s 48h00m00s 73h03m01s 71h43m12s  
   
  66 2  
   
  −64.389  
   
  256  
   
  67 −65.384  
   
  2  
   
  3584  
   
  9 (3+2+4/4+2+3)  
   
  68 −66.303  
   
  2  
   
  46736  
   
  69 −66.970  
   
  70 −67.075  
   
  2  
   
  2  
   
  18352  
   
  24056  
   
  1h55m50s 24h34m09s 290h41m48s 47h32m37s 48h4m28s  
   
  74 −73.888  
   
  2  
   
  64 34m49s  
   
  75 −74.881  
   
  2  
   
  544  
   
  76  
   
  77  
   
  78  
   
  2−74.970  
   
  2−75.970  
   
  2−76.166  
   
  3400  
   
  26592  
   
  13968  
   
  5h11m49s 32h10m51s 235h42m42s 48h04m53s  
   
  evaluated weight in the clustering eﬀect before screening and a speciﬁc threshold of screening, respectively. Besides, we specify the degree of parallelization in Step 2 by Nthr . After executing Algorithm 2, we obtain a good diﬀerential Dopt with its probability Popt .  
   
  4  
   
  Applications to PRINCE and QARMA  
   
  We apply our framework to PRINCE and QARMA in some rounds. To make our results clear, we show the results on each Wmin with Tw = 1, i.e., we consistently set Tw = 1 for each Wmin . Furthermore, we set Tc = 10 unless noted otherwise. 4.1  
   
  Good Diﬀerentials for PRINCE  
   
  Table 2 shows the results of PRINCE, which are evaluated on Apple M1 MAX with 64 GB of main memory. In the case where the number of all diﬀerential characteristics having diﬀerent (c0 , cr ) is not so many, and the number of rounds is small, we can apply Algorithm 1, i.e., we can ﬁnd a good diﬀerential with Tc = 10. In other cases, the cost of the evaluation of a clustering eﬀect becomes so high that we apply Algorithm 2. For the results by Algorithm 1, the evaluation of a clustering eﬀect is parallelized on multiple threads to make the most of our computational environment, as described in Sect. 3.3 and 3.4. For the results by Algorithm 2, we pick up the best one among results on several combinations of Tt and Ts . Table 2 shows that the distinguishing attack can be applied up to seven rounds of PRINCE/PRINCEv2 that improves the previous best attack by one  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  423  
   
  round [2,12]. It must be mentioned that the previous best distinguishing attack by diﬀerential cryptanalysis is adjusted for the key recovery that restricts the space of the input and output diﬀerences. 4.2  
   
  Good Diﬀerentials for QARMA  
   
  Table 3 shows the results of QARMA64 and QARMA128, both of which are evaluated on Linux machine with Intel Xeon Gold 6258R CPU (2.70 GHz) and 256 GB of main memory. As with the case of PRINCE, we apply Algorithm 1 when the number of all diﬀerential characteristics having diﬀerent (c0 , cr ) is not so many, and the number of rounds is small. Otherwise, we apply Algorithm 2. Particularly, the computational cost becomes excessive in the evaluation of QARMA128, because the state length is 128 bits. Hence, we apply only Algorithm 2 in most cases of the evaluation of QARMA128. For the results by Algorithm 1, the evaluation of a clustering eﬀect is parallelized on multiple threads to make the most of our computational environment, as well as the evaluation of PRINCE. For the results by Algorithm 2, we pick up the best one among results on several combinations of Tt and Ts . As shown in Table 3, the distinguishing attack in the SK setting can be applied up to 7 and 10 rounds of QARMA64 and QARMA128, both of which improve the previous best attack [28] by 1 and 4 rounds, respectively. Further, the distinguishing attack in the RT setting can be applied up to 10 and 12 rounds of QARMA64 and QARMA128, both of which improve the previous best attacks [1,22] by 1 and 4 rounds, respectively. As with the case of PRINCE, we note that the previous best distinguishing attack may be adjusted for the key recovery. Besides, it must be mentioned that the same case reported by K¨ olbl and Roy [19] often happens in both QARMA64 and QARMA128, i.e., there are some better diﬀerentials corresponding to a diﬀerential characteristic with not the highest probability than that by the optimal diﬀerential characteristic. † These experiments were stopped before all diﬀerentials were obtained because the program took too long to run. 4.3  
   
  Discussion: Comparison with PRINCE and QARMA  
   
  We observe that the gaps in the probability between a diﬀerential characteristic and a diﬀerential can be large in QARMA64 and QARMA128 under the SK setting compared to that in PRINCE. When looking at each construction in detail, for the non-linear layer, the 4-bit S-boxes used in PRINCE and QARMA have the same property in terms of security, such as a full diﬀusion property and guaranteeing the maximum diﬀerential probability and the absolute linear bias of 2−2 . In contrast, their linear layers are designed with a diﬀerent strategy. The linear layer of PRINCE is designed to ensure 16 active S-boxes in consecutive four rounds, while that of QARMA is designed based on an almost MDS matrix suitable for hardware implementation. We summarize the diﬀerence in their matrices from the macro and micro perspectives as follows. Hereafter, we  
   
  424  
   
  K. Sakamoto et al.  
   
  Table 3. Diﬀerential probabilities of (almost) good diﬀerentials of QARMA. Wmin denotes the same parameter as in Algorithms 1 and 2. #diﬀerentials denotes the number of diﬀerent diﬀerentials with a particular weight. All notations and parameters are consistent with Table 2. QARMA64 under the SK setting Rounds  
   
  6 (2+2+2)  
   
  7 (2+2+3/3+2+2)  
   
  8 (3+2+3)  
   
  Wmin  
   
  52  
   
  53  
   
  54  
   
  64  
   
  65  
   
  66  
   
  72  
   
  73  
   
  74  
   
  Prob.  
   
  2−45.741  
   
  2−46.019  
   
  2−46.112  
   
  2−60.278  
   
  2−60.111  
   
  2−58.921  
   
  2−64.845  
   
  2−64.503  
   
  2−64.693  
   
  # diﬀerentials  
   
  1024  
   
  18048  
   
  315360  
   
  512  
   
  16896  
   
  313280  
   
  400  
   
  21904  
   
  333776  
   
  Time  
   
  35m15s  
   
  19h47m31s  
   
  109h51m44s  
   
  48m19s  
   
  39h48m41s  
   
  186h21m10s  
   
  15h47m58s  
   
  53h01m41s  
   
  508h11m56s  
   
  QARMA64 under the RT setting Rounds  
   
  6 (2+2+2)  
   
  Wmin Prob.  
   
  14  
   
  7 (2+2+3/3+2+2)  
   
  15  
   
  −14.000  
   
  16  
   
  −14.913  
   
  2  
   
  −15.193  
   
  2  
   
  2  
   
  28  
   
  8 (3+2+3)  
   
  29  
   
  −27.541  
   
  30  
   
  −28.000  
   
  2  
   
  −28.286  
   
  2  
   
  2  
   
  36 −36.000  
   
  2  
   
  37  
   
  38  
   
  2−36.679  
   
  2−36.679  
   
  # diﬀerentials  
   
  17  
   
  202  
   
  2571  
   
  84  
   
  3030  
   
  48840  
   
  20  
   
  840  
   
  18509  
   
  Time  
   
  36s  
   
  1m44s  
   
  13m33s  
   
  5m35s  
   
  1h15m24s  
   
  15h28m20s  
   
  11m16s  
   
  30m22s  
   
  10h18m25s  
   
  Rounds  
   
  9 (3+2+4/4+2+3)  
   
  Wmin Prob.  
   
  52  
   
  10 (4+2+4)  
   
  53  
   
  −51.415  
   
  54  
   
  −51.415  
   
  2  
   
  −52.246  
   
  2  
   
  2  
   
  62  
   
  11 (4+2+5/5+2+4)  
   
  63  
   
  −60.831  
   
  64  
   
  −60.831  
   
  2  
   
  −60.831  
   
  2  
   
  2  
   
  77 −77.000  
   
  2  
   
  # diﬀerentials  
   
  8  
   
  688  
   
  11290  
   
  273  
   
  4822  
   
  49585  
   
  64  
   
  Time  
   
  6h32m25s  
   
  10h27m32s  
   
  49h31m02s  
   
  96h12m59s  
   
  114h45m17s  
   
  303h33m25s  
   
  596h07m26s†  
   
  78  
   
  79  
   
  2−77.415  
   
  2−77.509  
   
  7616  
   
  18424  
   
  1317h17m08s† 1317h16m57s†  
   
  QARMA128 under the SK setting Rounds  
   
  6 (2+2+2)  
   
  7 (2+2+3/3+2+2)  
   
  8 (2+2+4/4+2+2)  
   
  Wmin  
   
  60  
   
  61  
   
  62  
   
  76  
   
  77  
   
  78  
   
  87  
   
  88  
   
  89  
   
  Prob.  
   
  2−54.494  
   
  2−54.521  
   
  2−54.581  
   
  2−71.930  
   
  2−72.321  
   
  2−72.614  
   
  2−84.850  
   
  2−85.093  
   
  2−85.539  
   
  # diﬀerentials  
   
  1312  
   
  98984  
   
  391352  
   
  516  
   
  32880  
   
  31960  
   
  16  
   
  708  
   
  14300  
   
  Time  
   
  15h27m17s  
   
  499h19m12s  
   
  1316h25m40s†  
   
  40h57m50s  
   
  530h05m58s  
   
  430h44m47s  
   
  57h59m37s  
   
  92h7m23s  
   
  693h25m04s  
   
  Rounds  
   
  9 (3+2+4/4+2+3)  
   
  10 (3+2+5/5+2+3)  
   
  Wmin  
   
  106  
   
  107  
   
  108  
   
  125  
   
  126  
   
  127  
   
  Prob.  
   
  2−104.285  
   
  2−103.616  
   
  2−103.255  
   
  2−121.549  
   
  2−121.667  
   
  2−122.304  
   
  240  
   
  561  
   
  1172  
   
  12  
   
  54  
   
  # diﬀerentials Time  
   
  249h25m14s† 1004h00m44s† 1004h00m32s† 794h25m35s† 794h25m23s†  
   
  31 794h25m13s†  
   
  QARMA128 under the RT setting Rounds  
   
  7 (2+2+3/3+2+2)  
   
  Wmin Prob.  
   
  28 −28.000  
   
  2  
   
  8 (3+2+3)  
   
  29 −27.415  
   
  2  
   
  30 −28.000  
   
  2  
   
  42  
   
  9 (3+2+4/4+2+3)  
   
  43  
   
  −42.000  
   
  44  
   
  −42.415  
   
  2  
   
  −42.187  
   
  2  
   
  2  
   
  # diﬀerentials  
   
  32  
   
  2144  
   
  64368  
   
  64  
   
  5248  
   
  203200  
   
  Time  
   
  38m43s  
   
  4h51m52s  
   
  48h32m23s  
   
  21h17m20s  
   
  52h32m19s  
   
  470h54m17s  
   
  Rounds  
   
  10 (4+2+4)  
   
  64 −63.679  
   
  2  
   
  1815  
   
  65  
   
  66  
   
  2−64.415  
   
  2−64.679  
   
  6870  
   
  26105  
   
  1154h39m26s† 1154h39m16s† 1154h39m05s†  
   
  11 (4+2+5/5+2+4)  
   
  12 (5+2+5)  
   
  Wmin  
   
  80  
   
  81  
   
  82  
   
  100  
   
  101  
   
  102  
   
  125  
   
  126  
   
  127  
   
  Prob.  
   
  2−78.005  
   
  2−79.005  
   
  2−78.408  
   
  2−96.466  
   
  2−97.929  
   
  2−96.521  
   
  2−120.024  
   
  2−123.499  
   
  2−124.084  
   
  2  
   
  72  
   
  51  
   
  9  
   
  6  
   
  2  
   
  3  
   
  2  
   
  # diﬀerentials Time  
   
  978h51m03s† 1316h34m33s† 1316h33m53s† 794h24m09s† 794h23m59s† 1036h39m39s†  
   
  3 794h16m56s†  
   
  1036h44m17s† 1036h44m02s†  
   
  mainly take a comparison between PRINCE and QARMA64 as an example for a better understanding. Macro perspective. When looking at the matrices of PRINCE and QARMA64 as a single 64 × 64 matrix, the matrix of PRINCE consists of two 16 × 16 (0) and M (1) while that of QARMA64 consists of only one 16 × 16 matrices M matrix M . Hence, the (forward and backward) round function of PRINCE can be seen as constructed on two super S-boxes, while that of QARMA64 can be seen as constructed on the one super S-box. Micro perspective. When focusing on output nibbles, each output nibble in the matrix of PRINCE comes from four input nibbles, while that of QARMA64  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  425  
   
  Table 4. Probability of diﬀerential characteristic and diﬀerential. PRINCE (6 (2+2+2) rounds) Tw = 1, Tc = 10 Matrix Wmin  
   
  Original  
   
  Me1  
   
  Me2  
   
  Me3  
   
  44  
   
  40  
   
  44  
   
  42  
   
  2−43.907 2−38.526 2−38.616 2−37.458  
   
  Prob. Gap (Prob./2−Wmin )  
   
  20.093  
   
  21.474  
   
  25.384  
   
  24.542  
   
  # diﬀerentials  
   
  64  
   
  256  
   
  8  
   
  272  
   
  Table 5. Distribution of diﬀerential characteristics. PRINCE (6 (2+2+2) rounds) Tw = 1, Tc = 10 Weight Matrix  
   
  # DC†  
   
  Original Me1 Me2 Me3  
   
  Wmin Wmin + 1 Wmin + 2 Wmin + 3 Wmin + 4 Wmin + 5 Wmin + 6 Wmin + 7 Wmin + 8 Wmin + 9 1 2 1 1  
   
  0 0 2 0  
   
  0 0 7 5  
   
  0 0 16 2  
   
  1 11 55 56  
   
  0 0 116 38  
   
  0 0 452 358  
   
  0 0 848 210  
   
  1 23 2152 1719  
   
  0 0 3498 1102  
   
  comes from three input nibbles. Thus, each output bit of the round function of PRINCE depends on 16 input bits of the round function, while that of QARMA64 depends on 12 input bits of the round function. To further investigate the impact of a matrix on a gap in the probability, we conduct three experiments with a change of the matrix in PRINCE focusing on the above perspectives. Hence, we change the matrix in PRINCE to: (0) , M (0) , M (0) , M (0) ); Me1 = diag(M 1 2 1 Me2 = diag(circ(0, ρ , ρ , ρ ), circ(0, 1, ρ2 , 1), circ(0, 1, ρ2 , 1), circ(0, ρ1 , ρ2 , ρ1 )); Me3 = diag(circ(0, ρ1 , ρ2 , ρ1 ), circ(0, ρ1 , ρ2 , ρ1 ), circ(0, ρ1 , ρ2 , ρ1 ), circ(0, ρ1 , ρ2 , ρ1 )). Notably, circ(0, 1, ρ2 , 1) in Me2 has the same diﬀusion property as circ(0, ρ1 , ρ2 , ρ1 ) given in [3]. With Me1 , the round function can be viewed as constructed on the one super S-box, but each output bit of the round function still depends on 16 input bits of the round function. With Me2 , the round function can be viewed as constructed on two super S-boxes like the original PRINCE, but each output bit of the round function depends on 12 input bits of the round function. With Me3 , the matrix in PRINCE changes to the same matrix as QARMA64 into PRINCE, that is, the round function can be viewed as constructed on the one super S-box and each output bit of the round function depends on 12 input bits of the round function. † DC: Diﬀerential Characteristic Tables 4 and 5 show the gap in the probability of the diﬀerential characteristic and diﬀerential on the six rounds of each variant of PRINCE and their distribution of the diﬀerential characteristics, respectively. From a macro perspective, the number of super S-boxes based on a primitive does not seem to have an impact on the gap as far as comparing the cases of the original matrix with Me1  
   
  426  
   
  K. Sakamoto et al.  
   
  and Me2 with Me3 . Meanwhile, the number of the input bits inﬂuencing each output bit seems to have a large impact on the gap as far as comparing the cases of the original matrix with Me2 and Me1 with Me3 . These observations can ﬁt into MIDORI64 [5] and SKINNY64 [6], both of which have the matrix with each output nibble depending on less than four input nibbles. Ankele and K¨ olbl showed that the probability of the optimal diﬀerential characteristic in MIDORI64 and SKINNY is dramatically increased by considering a clustering eﬀect [2]. When each output bit depends on 16 input bits, the number of the differential characteristics for each weight is curbed very few. Therefore, we predict that a cipher can have good resistance to a clustering eﬀect when each output bit of the round function depends on more input bits of the round function. In the RT setting, this gap of QARMA becomes small compared to that in the SK setting, i.e., the permutation-based tweak update function like that used in QARMA brings resistance to a clustering eﬀect. That is mainly because the transition of the diﬀerential propagation is uniquely ﬁxed in the tweak update function, and it contributes to making clustering diﬃcult in the whole cipher. Therefore, we expect that a tweakable block cipher with a linear tweak (tweakey) update function can have good resistance to the clustering eﬀect. Finally, the case reported by K¨ olbl and Roy [19] can occur in any cipher, as a clustering eﬀect in non-optimal weights can strongly aﬀect the probability of a diﬀerential, especially for a cipher like QARMA.  
   
  5  
   
  Conclusion  
   
  We provide a new generic SAT-based automatic search framework to ﬁnd a good diﬀerential under the speciﬁed conditions. Our framework introduces a method to solve incremental SAT problems in parallel using a multi-threading technique, and consequently, it allows us to evaluate diﬀerentials more comprehensively than any other previous methods. Our framework can be applied to a wide class of symmetric-key primitives. In this study, to demonstrate the eﬀectiveness of our framework, we apply it to PRINCE and QARMA from the aspect of distinguishing attacks. Our results are summarized as follows: – We specify the conditions for ﬁnding a good diﬀerential to build a distinguisher and conduct experiments using our framework. As a result, we improve previous diﬀerential bounds for all variants of the target ciphers. – We investigate the gap in the probability between a diﬀerential characteristic and a diﬀerential for PRINCE and QARMA and ﬁnd that diﬀerent design strategies for the linear layers have a signiﬁcant impact on this gap. For future direction, it would be interesting to expand the incremental SAT problem to more eﬃciently ﬁnd the optimal diﬀerential/linear characteristics and other kinds of distinguishers. Further, it would be useful for future designs to more comprehensively investigate the impact of the design construction on the gap in the probability between a diﬀerential characteristic and a diﬀerential.  
   
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
   
  427  
   
  Acknowledgments. Takanori Isobe is supported by JST, PRESTO Grant Number JPMJPR2031. These research results were also obtained from the commissioned research (No. 05801) by National Institute of Information and Communications Technology (NICT), Japan.  
   
  References 1. Ankele, R., Dobraunig, C., Guo, J., Lambooij, E., Leander, G., Todo, Y.: Zerocorrelation attacks on tweakable block ciphers with linear tweakey expansion. IACR Trans. Symmetric Cryptol. 2019(1), 192–235 (2019) 2. Ankele, R., K¨ olbl, S.: Mind the gap - a closer look at the security of block ciphers against diﬀerential cryptanalysis. In: Cid, C., Jacobson, M., Jr. (eds.) SAC 2018. LNCS, vol. 11349, pp. 163–190. Springer, Cham (2018). https://doi.org/10.1007/ 978-3-030-10970-7 8 3. Avanzi, R.: The QARMA block cipher family. Almost MDS matrices over rings with zero divisors, nearly symmetric even-mansour constructions with non-involutory central rounds, and search heuristics for low-latency s-boxes. IACR Trans. Symmetric Cryptol. 2017(1), 4–44 (2017) 4. Bailleux, O., Boufkhad, Y.: Eﬃcient CNF encoding of Boolean cardinality constraints. In: Rossi, F. (ed.) CP 2003. LNCS, vol. 2833, pp. 108–122. Springer, Cham (2003). https://doi.org/10.1007/978-3-540-45193-8 8 5. Banik, S., et al.: Midori: a block cipher for low energy. In: Iwata, T., Cheon, J. (eds.) ASIACRYPT 2015. LNSC, vol. 9453, pp. 411–436. Springer, Cham (2015). https://doi.org/10.1007/978-3-662-48800-3 17 6. Beierle, C., et al.: The SKINNY family of block ciphers and its low-latency variant MANTIS. In: Robshaw, M., Katz, J. (eds.) CRYPTO 2016. LNSC, vol. 9815, pp. 123–153. Springer, Cham (2016). https://doi.org/10.1007/978-3-662-53008-5 5 7. Biham, E., Shamir, A.: Diﬀerential cryptanalysis of des-like cryptosystems. In: Menezes, A.J., Vanstone, S.A. (eds.) CRYPTO 1990. LNCS, vol. 537, pp. 2–21. Springer, Cham (1990). https://doi.org/10.1007/3-540-38424-3 1 8. Biryukov, A., Roy, A., Velichkov, V.: Diﬀerential analysis of block ciphers SIMON and SPECK. In: Cid, C., Rechberger, C. (eds.) FSE 2014. LNSC, vol. 8540, pp. 546–570. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-467060 28 9. Borghoﬀ, J., et al.: PRINCE - a low-latency block cipher for pervasive computing applications - extended abstract. In: Wang, X., Sako, K. (eds.) ASIACRYPT 2012. LNSC, vol. 7658, pp. 208–225. Springer, Heidelberg (2012). https://doi.org/10. 1007/978-3-642-34961-4 14 10. Boura, C., David, N., Boissier, R.H., Naya-Plasencia, M.: Better steady than speedy: full break of SPEEDY-7-192. IACR Cryptology ePrint Archive, p. 1351 (2022) 11. Bozilov, D., et al.: PRINCEv2 - more security for (almost) no overhead. In: Dunkelman, O., Jacobson, M.J., Jr., O’Flynn, C. (eds.) SAC 2020. LNSC, vol. 12804, pp. 483–511. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-81652-0 19 12. Canteaut, A., Fuhr, T., Gilbert, H., Naya-Plasencia, M., Reinhard, J.: Multiple diﬀerential cryptanalysis of round-reduced PRINCE. In: Cid, C., Rechberger, C. (eds.) FSE 2014. LNSC, vol. 8540, pp. 591–610. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-46706-0 30 13. Cook, S.A.: The complexity of theorem-proving procedures. In: STOC, pp. 151– 158. ACM (1971)  
   
  428  
   
  K. Sakamoto et al.  
   
  14. Ding, Y., Zhao, J., Li, L., Yu, H.: Impossible diﬀerential analysis on round-reduced PRINCE. J. Inf. Sci. Eng. 33(4), 1041–1053 (2017) 15. Dobraunig, C., Eichlseder, M., Kales, D., Mendel, F.: Practical key-recovery attack on MANTIS5. IACR Trans. Symmetric Cryptol. 2016(2), 248–260 (2016) 16. E´en, N., Biere, A.: Eﬀective preprocessing in SAT through variable and clause elimination. In: Bacchus, F., Walsh, T. (eds.) SAT 2005. LNTCS, vol. 3569, pp. 61–75. Springer, Heidelberg (2005). https://doi.org/10.1007/11499107 5 17. Erlacher, J., Mendel, F., Eichlseder, M.: Bounds for the security of ascon against diﬀerential and linear cryptanalysis. IACR Trans. Symmetric Cryptol. 2022(1), 64–87 (2022) 18. K¨ olbl, S., Leander, G., Tiessen, T.: Observations on the SIMON block cipher family. In: Gennaro, R., Robshaw, M. (eds.) CRYPTO 2015. LNSC, vol. 9215, pp. 161–185. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-4798968 19. K¨ olbl, S., Roy, A.: A brief comparison of simon and simeck. In: Bogdanov, A. (ed.) LightSec 2016. LNSC, vol. 10098, pp. 69–88. Springer, Cham (2016). https://doi. org/10.1007/978-3-319-55714-4 6 20. Lai, X., Massey, J.L., Murphy, S.: Markov ciphers and diﬀerential cryptanalysis. In: Davies, D.W. (ed.) EUROCRYPT 1991. LNCS, vol. 547, pp. 17–38. Springer, Heidelberg (1991). https://doi.org/10.1007/3-540-46416-6 2 21. Leander, G., Moos, T., Moradi, A., Rasoolzadeh, S.: The SPEEDY family of block ciphers engineering an ultra low-latency cipher from gate level for secure processor architectures. IACR Trans. Cryptogr. Hardw. Embed. Syst. 2021(4), 510–545 (2021) 22. Li, M., Hu, K., Wang, M.: Related-tweak statistical saturation cryptanalysis and its application on QARMA. IACR Trans. Symmetric Cryptol. 2019(1), 236–263 (2019) 23. Liu, Y., Zang, T., Gu, D., Zhao, F., Li, W., Liu, Z.: Improved cryptanalysis of reduced-version QARMA-64/128. IEEE Access 8, 8361–8370 (2020) 24. Matsui, M.: On correlation between the order of S-boxes and the strength of DES. In: De Santis, A. (ed.) EUROCRYPT 1994. LNCS, vol. 950, pp. 366–375. Springer, Heidelberg (1994). https://doi.org/10.1007/BFb0053451 25. Sinz, C.: Towards an optimal CNF encoding of Boolean cardinality constraints. In: van Beek, P. (ed.) CP 2005. LNPSE, vol. 3709, pp. 827–831. Springer, Heidelberg (2005). https://doi.org/10.1007/11564751 73 26. Sun, L., Wang, W., Wang, M.: More accurate diﬀerential properties of LED64 and Midori64. IACR Trans. Symmetric Cryptol. 2018(3), 93–123 (2018) 27. Sun, L., Wang, W., Wang, M.: Accelerating the search of diﬀerential and linear characteristics with the SAT method. IACR Trans. Symmetric Cryptol. 2021(1), 269–315 (2021) 28. Yang, D., Qi, W., Chen, H.: Impossible diﬀerential attack on QARMA family of block ciphers. IACR Cryptology ePrint Archive, p. 334 (2018) 29. Yang, G., Zhu, B., Suder, V., Aagaard, M.D., Gong, G.: The Simeck family of lightweight block ciphers. In: G¨ uneysu, T., Handschuh, H. (eds.) CHES 2015. LNSC, vol. 9293, pp. 307–329. Springer, Heidelberg (2015). https://doi.org/10. 1007/978-3-662-48324-4 16 30. Zong, R., Dong, X.: MILP-aided related-tweak/key impossible diﬀerential attack and its applications to QARMA, Joltik-BC. IEEE Access 7, 153683–153693 (2019)  
   
  Deep Learning-Based Rotational-XOR Distinguishers for AND-RX Block Ciphers: Evaluations on Simeck and Simon Amirhossein Ebrahimi1(B) , David Gerault2 , and Paolo Palmieri1 1  
   
  2  
   
  School of Computer Science and IT, University College Cork, Cork, Ireland {a.ebrahimimodhaddam,p.palmieri}@cs.ucc.ie Cryptography Research Centre, Technology Innovation Institute, Abu Dhabi, UAE [email protected]   
   
  Abstract. The use of deep learning techniques in cryptanalysis has garnered considerable interest following Gohr’s seminal work in 2019. Subsequent studies have focused on training more eﬀective distinguishers and interpreting these models, primarily for diﬀerential attacks. In this paper, we shift our attention to deep learning-based distinguishers for rotational XOR (RX) cryptanalysis on AND-RX ciphers, an area that has received comparatively less attention. Our contributions include a detailed analysis of the state-of-the-art deep learning techniques for RX cryptanalysis and their applicability to AND-RX ciphers like Simeck and Simon. Our research proposes a novel approach to identify DL-based RX distinguishers, by adapting the evolutionary algorithm presented in the work of Bellini et al. to determine optimal values for translation (δ) and rotation oﬀset (γ) parameters for RX pairs. We successfully identify distinguishers using deep learning techniques for diﬀerent versions of Simon and Simeck, ﬁnding distinguishers for the classical related-key scenario, as opposed to the weak-key model used in related work. Additionally, our work contributes to the understanding of the diﬀusion layer’s impact in AND-RX block ciphers against RX cryptanalysis by focusing on determining the optimal rotation parameters using our evolutionary algorithm, thereby providing valuable insights for designing secure block ciphers and enhancing their resistance to RX cryptanalysis. Keywords: AND-RX ciphers Rotational-XOR cryptanalysis  
   
  1  
   
  · Deep Learning · Cryptanalysis ·  
   
  Introduction  
   
  Cryptography plays a crucial role in ensuring the security and privacy of information in modern communication systems. Block ciphers, in particular, are widely This publication has emanated from research supported in part by a Grant from Science Foundation Ireland under Grant number 18/CRT/6222. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2024  C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 429–450, 2024. https://doi.org/10.1007/978-3-031-53368-6_21  
   
  430  
   
  A. Ebrahimi et al.  
   
  used to provide encryption for data transmission, ensuring that the content remains conﬁdential and secure from unauthorized access. However, the eﬀectiveness of block ciphers is always being tested, and researchers are continually exploring new ways to improve their resilience against attacks like diﬀerential [7], linear [19], algebraic attacks [3], etc. Among several cryptanalysis techniques, Rotational-XOR (RX) cryptanalysis has emerged as a powerful method to evaluate the security of block ciphers, particularly ARX and AND-RX ciphers such as Speck, Simon, and Simeck [1]. In recent years, artiﬁcial intelligence (AI) and deep learning have shown great potential in a variety of applications, including cryptanalysis. Their ability to analyze complex patterns and relationships in large datasets has motivated researchers to explore new techniques for breaking cryptographic algorithms [6, 10,20]. This paper aims to investigate the application of deep learning in the RX cryptanalysis of AND-RX block ciphers, with a focus on Simon and Simeck, and proposes an approach to see the impact of diﬀusion layers in these ciphers. The conventional cryptographic analysis techniques utilized in RX cryptanalysis commonly depend on weak-key models, wherein statistical methods are utilized to detect distinguishers and possible vulnerabilities. Nevertheless, these methods are constrained, as achieving a good distinction with a limited weakkey model may not be feasible. In this context, deep learning has been proposed as an alternative technique, oﬀering the possibility of improved results in cryptanalysis tasks. Our proposed method has enabled us to acquire distinguishers for full-key classes concerning Simeck and Simon ciphers. In addition to assessing the security of ciphers, ﬁnding the best parameters for diﬀusion layers is a crucial aspect of cipher design. The diﬀusion layer plays a signiﬁcant role in ensuring that minimal alterations in plaintext or key inputs lead to substantial changes in the ciphertext output, making it challenging for adversaries to decipher the original data. In this paper, we propose a new approach that involves using a modiﬁed version of the optimizer in [5] to determine the best RX diﬀerential inputs and the optimal shift parameter for ﬁnding the longest round distinguisher with the aid of deep learning classiﬁers. Furthermore, we use this optimizer to identify the best set of rotations in the diﬀusion layer that works against deep learning optimizers, speciﬁcally for Simeck-like ciphers. Our approach ensures that deep learning distinguishers cannot ﬁnd the optimal distinguishers, thereby enhancing the overall security of the ciphers. Therefore, our method demonstrates the potential for improving the security of ciphers while also enhancing the eﬃciency of the design process by utilizing deep learning classiﬁers in combination with an optimizer. Our ﬁndings contribute to the ongoing eﬀorts to enhance the security of AND-RX block ciphers and highlight the potential of AI applications in Rotational-XOR cryptanalysis. Our Deep Learning (DL)-based distinguishers demonstrate superior performance on the Simeck cipher compared to the Simon cipher. In order to juxtapose our achieved Deep Learning (DL)-based distinguishers with other relatedkey DL-based distinguishers for the Simeck cipher, the results are presented in Table 1. It’s important to note that our distinguisher was trained exclusively on  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  431  
   
  a single pair, whereas the existing literature oﬀers distinguishers trained on eight pairs for the Simeck cipher. Consequently, we implemented the technique introduced in [11] to compute an amalgamated score for eight pairs. Furthermore, a comparison between our DL-based RX distinguishers and past RX distinguishers of the Simeck cipher can be found in Table 2. Our work introduces a superior related-key DL distinguisher for Simeck 64/128 cipher and marginally behind for Simeck 32/64, according to Table 1. Furthermore, our research introduces a novel distinguisher that is speciﬁcally designed for RX cryptanalysis and trained on the entire key space. This distinguisher can be further scrutinized to assess how the accuracy of these distinguishers is aﬀected by diﬀerent keys. Table 1. Comparison of related-key DL-based distinguishers for Simeck. RX: Rotational-Xor cryptanalysis, RD: Related-key Diﬀerential cryptanalysis. The Com1 bined Accuracy Score [11] for m pairs is m 1−pi 1+  
   
  Simeck 32/64  
   
  i=1  
   
  pi  
   
  Round Combined Accuracy Score Pairs Attack Type Ref. 13  
   
  0.9950  
   
  8  
   
  RD  
   
  [17]  
   
  14  
   
  0.6679  
   
  8  
   
  RD  
   
  [17]  
   
  15  
   
  0.5573  
   
  8  
   
  RD  
   
  [17]  
   
  15  
   
  0.5134  
   
  1  
   
  RX  
   
  This Work  
   
  15  
   
  0.5475  
   
  8  
   
  RX  
   
  This Work  
   
  Simeck 64/128 18  
   
  0.9066  
   
  8  
   
  RD  
   
  [17]  
   
  19  
   
  0.7558  
   
  8  
   
  RD  
   
  [17]  
   
  20  
   
  0.6229  
   
  8  
   
  RD  
   
  [17]  
   
  20  
   
  0.5212  
   
  1  
   
  RX  
   
  This Work  
   
  20  
   
  0.6338  
   
  8  
   
  RX  
   
  This Work  
   
  Table 2. Comparison of the RX distinguishers for diﬀerent versions of Simeck Cipher  
   
  Rounds Data Complexity Size of Weak Key Class DL-based Ref 15  
   
  220  
   
  Full  
   
  Yes  
   
  This Work  
   
  15  
   
  218  
   
  244  
   
  No  
   
  [18]  
   
  19  
   
  224  
   
  230  
   
  No  
   
  [18]  
   
  20  
   
  226  
   
  230  
   
  No  
   
  [18]  
   
  17  
   
  220  
   
  Full  
   
  Yes  
   
  This Work  
   
  16  
   
  218  
   
  268  
   
  No  
   
  [18]  
   
  18  
   
  222  
   
  266  
   
  No  
   
  [18]  
   
  19  
   
  224  
   
  262  
   
  No  
   
  [18]  
   
  27  
   
  244  
   
  246  
   
  No  
   
  [18]  
   
  Simeck64/128 20  
   
  220  
   
  Full  
   
  Yes  
   
  This Work  
   
  25  
   
  234  
   
  280  
   
  No  
   
  [18]  
   
  34  
   
  256  
   
  258  
   
  No  
   
  [18]  
   
  Simeck32/64  
   
  Simeck48/96  
   
  432  
   
  1.1  
   
  A. Ebrahimi et al.  
   
  Related Works  
   
  Simon [4] and Simeck [22] are lightweight block ciphers that have gained popularity due to their simplicity and eﬃciency. However, several attacks have been proposed against these ciphers, including related-key and weak-key attacks. Liu et al. [16] propose an automatic search algorithm to ﬁnd optimal differential trails in Simon and Simeck ciphers. The authors use Matsui’s branchand-bound algorithm to traverse input diﬀerences from low Hamming weight and break unnecessary branches. They also derive a more accurate upper bound on the diﬀerential probability of the Simon-like round function, which helps to improve the eﬃciency of the search algorithm. With this algorithm, they ﬁnd the provably optimal diﬀerential trails for all versions of Simon and Simeck ciphers. In [21] a detailed analysis of Simon-like block ciphers and their related-key differential trails is presented. The authors identify that not only the Hamming weight but also the positions of active bits in the input diﬀerence aﬀect the probability of diﬀerential trails. The authors proceed to reconstruct the Mixed Integer Linear Programming (MILP) model for Simon-like block ciphers, eliminating quadratic constraints, and introducing an accurate objective function that reduces its degree to one through the inclusion of auxiliary variants. Additionally, they investigate and identify the optimal diﬀerential trails for Simon and Simeck, utilizing this model, and they obtain related-key diﬀerential trails. Their core ﬁndings encompass the discovery of optimal related-key diﬀerential trails for various versions of Simon and Simeck (Simon32/64, Simon48/96, Simon64/128, Simeck32/64, Simeck48/96, and Simeck64/128), along with the identiﬁcation of impossible diﬀerentials for several iterations of Simon and Simeck. Rotational cryptanalysis is a technique that explores the propagation of rotational pairs, which consist of pairs (x, x ≪ γ) where γ is the rotational oﬀset. The success of this attack can be compromised when non-rotation-invariant constants are injected into the rotational pairs. Rotational-XOR (RX) cryptanalysis, a generalized attack method, accounts for these constants by incorporating their eﬀect into the analysis of the propagation probability. RX-cryptanalysis considers an RX-pair of the form (x, (x ≪ γ) ⊕ δ) where δ is known as the translation. Ashur and Liu [1] introduced the concept of an RX-diﬀerence, and demonstrated how RX-diﬀerences behave around modular addition. They presented a formula for computing the transition probability of RX-diﬀerences, which was veriﬁed experimentally using Speck32/64. Additionally, they provided guidance on the optimal choice of parameters and discussed two types of constants: round constants and constants that result from a ﬁxed key. Khovratovich et al. [13] provided theoretical and practical support for the security of modular addition, rotation, and XOR-based (ARX) systems. They used rotational cryptanalysis to illustrate the best-known attack on reduced versions of the Threeﬁsh block cipher. Lu et al. [18] extended RX-cryptanalysis to AND-RX ciphers that can be described using bitwise AND, XOR, and cyclic rotation operations. The authors formulated an equation for predicting the likelihood of RX-diﬀerences progressing through AND-RX rounds and established an SMT (Satisﬁability Modulo  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  433  
   
  Theories) model to investigate RX-characteristics in Simon and Simeck. They discovered RX-characteristics in Simeck across diverse block sizes, speciﬁcally for expansive groups of weak keys within the related-key model, and conducted an analysis of how the key schedule and the rotation quantities of the round function aﬀect the propagation of RX-characteristics in Simon-like ciphers. AI and ML methods have been utilized in various data security applications such as cryptographic algorithms, cryptanalysis, steganography, and others. At CRYPTO’19, Gohr introduced a novel cryptanalysis approach that harnessed the power of machine learning algorithms [10]. By employing deep neural networks, he successfully constructed a neural-based distinguisher, outperforming existing cryptanalysis on a version of the widely examined NSA block cipher Speck. This distinguisher could be incorporated into a broader key recovery attack scheme. He could perform an attack on 11 rounds of Speck with the help of the AIbased distinguishers. Subsequently, numerous other scholarly works have been published on the application and examination of AI and deep learning-based distinguishers for cryptanalysis, following Gohr’s initial contribution. Jaewoo So presents a novel approach to cryptanalysis in [20] wherein a generic model is established using deep learning (DL) to discover the key of block ciphers through analyzing known plaintext-ciphertext pairs. The author illustrates the eﬀectiveness of the DL-based cryptanalysis model through successful attacks on lightweight block ciphers, including simpliﬁed DES, Simon, and Speck. The experimental outcomes suggest that DL-based cryptanalysis is capable of accurately retrieving key bits when the keyspace is limited to 64 ASCII characters. Baksi et. al in [2] describe two innovative approaches that utilize machine learning to identify distinguishers in symmetric key primitives. The authors demonstrate that their techniques can signiﬁcantly reduce the complexity of diﬀerential cryptanalysis for round-reduced ciphers, resulting in an approximate cube root reduction in the claimed complexity. Through experiments on various nonMarkov ciphers, the authors demonstrate the eﬃcacy of their methods. The researchers also evaluate the selection of machine learning models and illustrate that even a shallow three-layer neural network can perform eﬀectively for their purposes. This study serves as a proof of concept for how machine learning may be utilized as a comprehensive tool in symmetric key cryptanalysis. Another research direction in AI-assisted cryptanalysis involves the interpretation of neural network distinguishers. Benamira et al. [6] provided a comprehensive analysis of a neural distinguisher proposed by Gohr. They analyzed classiﬁed sets of data to identify patterns and gain a better understanding of Gohr’s results. Their ﬁndings revealed that the neural distinguisher primarily depends on diﬀerential distribution in ciphertext pairs, as well as diﬀerential distribution in the penultimate and antepenultimate rounds. The researchers subsequently developed a distinguisher for the Speck cipher, independent of any neural network use, which matched the accuracy and eﬃciency levels of Gohr’s neural-based distinguisher. Furthermore, the researchers developed a machine learning-based distinguisher that utilized standard machine learning tools to approximate the Diﬀerential Distribution Table (DDT) of the cipher, similar to  
   
  434  
   
  A. Ebrahimi et al.  
   
  Gohr’s neural distinguisher. This allowed for full interpretability of the distinguisher and contributed towards the interpretability of deep neural networks. In [5], researchers presented a novel tool for neural cryptanalysis that comprises two components. Firstly, an evolutionary algorithm is proposed for the search of single-key and related-key input diﬀerences that are eﬀective with neural distinguishers, thereby enabling the search for larger ciphers while eliminating the dependence on machine learning and prioritizing cryptanalytic methods. Secondly, DBitNet, a neural distinguisher architecture independent of the cipher structure, is introduced and demonstrated to outperform current state-of-the-art architectures. Using their tool, the researchers improved upon the state-of-theart neural distinguishers for various ciphers and provided new neural distinguishers for others. The paper also provides a comparative review of the current state-of-the-art in neural cryptanalysis. 1.2  
   
  Our Contribution  
   
  In this paper, we present several contributions to the rotational-XOR cryptanalysis of AND-RX block ciphers such as Simon and Simeck. Our research advances the understanding of these ciphers and their resistance to attacks by incorporating deep learning techniques. Our main contributions are as follows: 1. We propose the ﬁrst study of neural-assisted RX cryptanalysis. We modiﬁed the evolutionary algorithm presented in the work of Bellini et al. [5] to determine the optimal values for the translation parameter, denoted as δ, and the rotation oﬀset parameter, represented as γ, for RX pairs, and by doing so we were able to ﬁnd new RX distinguishers for Simon and Simeck ciphers 2. Our research successfully identiﬁes RX distinguishers using deep learning techniques for diﬀerent versions of Simon and Simeck in the related-key scenario, as opposed to the traditional weak-key model. 3. Our work contributes to ﬁnding the best parameters for diﬀusion layer for Simeck-like ciphers. The practical implications of these ﬁndings oﬀer insights for designing secure AND-RX block ciphers and improving their resistance to RX cryptanalysis. 1.3  
   
  Outline  
   
  The current paper’s structure is outlined as follows. The background concepts relevant to AND-RX ciphers, RX cryptanalysis, and deep learning-based cryptanalysis are discussed in Sect. 2. The methodology employed, which includes a modiﬁed evolutionary algorithm, is presented in Sect. 3. In Sect. 4, we report new distinguishes that have been discovered for Simon and Simeck. Section 5 proposes a technique for identifying the optimal permutation parameters for the diﬀusion layer of AND-RX block ciphers against DL-based attacks. Finally, Sect. 6 provides a concluding remark for the paper.  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  2  
   
  435  
   
  Preliminaries  
   
  In this section, we provide an overview of the key concepts and terms related to AND-RX ciphers, Rotational-XOR (RX) cryptanalysis, and deep learning techniques for cryptanalysis. Understanding these foundational concepts is essential for comprehending the methods and results presented in this paper. 2.1  
   
  AND-RX Ciphers  
   
  Simon and Simeck are block ciphers intended for use in environments with limited resources, such as IoT devices. They utilize the AND-RX design paradigm, which employs only three basic operations: bitwise XOR (⊕), bitwise AND (∧), and left circular shift (≪ i) by i bits. The general round function, R, of AND-RX ciphers can be deﬁned by the following equation: R(x, y) = (y ⊕ f (x) ⊕ k, x), where f (x) = ((x ≪ a)∧(x ≪ b))⊕x ≪ c and k is the subkey for corresponding round. In 2013, the NSA designed a family of lightweight block ciphers called Simon [4]. Each cipher in the family employs a word size of n bits, represented as Simon2n where n ∈ {16, 24, 32, 48}. Simon2n with a key size of m ∈ 2, 3, 4 words (mn bits) is denoted as Simon2n/mn. For example, Simon32/64 operates on 32-bit plaintext blocks and utilizes a 64-bit key. In this paper, we focus on Simon2n/4n. The f (x) function for Simon2n encryption is f (x) = ((x ≪ 1) ∧ (x ≪ 8)) ⊕ x ≪ 2. Simon’s key schedule produces r key words k0 , . . . , kr−1 from a given key, where r is the number of rounds. This process also involves using a sequence of 1-bit round constants to remove slide properties and circular shift symmetries.  
   
  Fig. 1. The Simon and Simeck ciphers  
   
  In this paper, we assess another lightweight block cipher known as Simeck [22]. It is represented by Simeck2n/mn, where the word size n must be either 16, 24, or 32, and 2n is the block size, while mn represents the key size.  
   
  436  
   
  A. Ebrahimi et al.  
   
  The round function R used in Simeck is identical to the one used in the Simon cipher, as shown by the equation. However, Simeck’s function f is distinct from Simon’s and is deﬁned as f (x) = (x ∧ (x ≪ 5)) ⊕ (x ≪ 1). In Simeck cipher, the round key ki is generated from a given master key K by ﬁrst dividing the master key K into four words and using them as the initial states (t2 , t1 , t0 , k0 ) of a feedback shift register. To produce round keys and update the registers, the round function f is utilized as well as a 1-bit round constant cr . The number of rounds r for Simeck32/64, Simeck48/96, and Simeck64/128 are 32, 36, and 44, respectively. Figure 1 demonstrate the round function and key schedule of Simon and Simeck ciphers. 2.2  
   
  Rotational-XOR (RX) Cryptanalysis  
   
  Rotational cryptanalysis is a technique used to analyze the security of symmetric algorithms. Khovratovich and Nikoli´c introduced and formalized this approach for ARX structures in their work cited as [13], and subsequently applied it to scrutinize other ciphers like Skein [14]. In this technique, the attacker focuses on rotational pairs of plaintext and ciphertext, where the input values are related through a ﬁxed rotation. The attacker then looks for statistical biases or patterns in the ciphertexts of these pairs that can be exploited to recover the secret key. However, rotational cryptanalysis can be less eﬀective in the presence of constants, as these ﬁxed values can disrupt the rotational properties of the pairs. This is because the rotation operation alone does not account for the XOR operations that involve these constants, which can be present in many cryptographic algorithms. When constants are involved, the rotational relations between the input and output values might be obscured, making it harder to analyze the cipher using traditional rotational cryptanalysis techniques. To address the limitations of traditional rotational cryptanalysis, RotationalXOR cryptanalysis has been introduced as an extension. Ashur and Liu [1] have developed this technique to account for the XOR operations with constants that are commonly present in cryptographic algorithms. Rotational-XOR pairs and Rotational-XOR diﬀerences are deﬁned to provide a more comprehensive framework for analyzing cryptographic primitives that involve both rotation and XOR operations with constants. The following are the deﬁnitions for RX pairs, RX diﬀerence, and RX cryptanalysis, respectively Deﬁnition 1 (Rotational XOR Pair [1]). An RX-pair is a rotational pair with rotational oﬀset γ under translations δ1 and δ2 , deﬁned as the pair x0 ⊕ δ1 , (x0 ≪ γ)⊕δ2 . However, for the sake of simplicity, a slightly diﬀerent notation is used, where an RX-pair is represented by x0 and x1 = (x0 ≪ γ) ⊕ δ, or alternatively as x0 , (x0 ≪ γ) ⊕ δ, where δ = δ1 ⊕ δ2 . Deﬁnition 2 (Rotational XOR Diﬀerence [1]). An RX-diﬀerence of x0 and x1 , denoted by Δγ (x0 , x1 ), is formed by the rotational XOR of x0 with a constant  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  437  
   
  δ such that x1 = (x0 ≪ γ) ⊕ δ, where 0 < γ < n and δ ∈ Fn2 is a constant. In another word Δγ (x0 , x1 ) = x1 ⊕ (x0 ≪ γ) Deﬁnition 3 (Rotational XOR Cryptanalysis [1]). Rotational XOR Cryptanalysis is a cryptanalytic technique that extends traditional rotational cryptanalysis to handle XOR operations with non-rotational-invariant constants between input and output pairs of a cryptographic primitive. This method aims to estimate the transition probability with respect to non-linear operations in block ciphers (like modular addition or ∧ operation) of two input RX-diﬀerences to an output RX-diﬀerence. The technique introduces the concept of a (δ, γ)-Rotational-XORdiﬀerence (or RX-diﬀerence), which represents a rotational pair with rotation γ under translation δ, i.e., (x, (x ≪ γ) ⊕ δ) . The method seeks to analyze the propagation of RX-diﬀerences through the cryptographic primitive. The transmission of RX-diﬀerences through linear operations is known to be deterministic; however, this is not the case for nonlinear operations. Prior research conducted by Ashur et al. [1] and Lu et al. [18] delved into the investigation of the transmission of RX-diﬀerences through modular addition and AND (∧) operations, respectively. 2.3  
   
  Deep Learning and Its Application on Symmetric Cryptography  
   
  Deep learning has proven to be a game-changer in various challenging tasks, including image recognition, natural language processing, and speech recognition, to name a few. Although machine learning techniques have been applied to cryptography, much of the practical work has focused on side-channel analysis [12,20,23]. However, in 2019, Gohr explores the application of deep learning techniques for cryptanalysis [10], speciﬁcally for attacking the Speck [4] cipher. This approach aims to diﬀerentiate between real and random pairs of ciphertexts resulting from the encryption of plaintext pairs with ﬁxed and arbitrary input diﬀerences, respectively. While pure diﬀerential distinguishers have traditionally been used for this purpose, his research has shown that deep learning (DL) can outperform their traditional counterparts. Gohr’s study focused on Speck-32/64 and compared the accuracy of a pure diﬀerential distinguisher with a DL-based distinguisher for 5 to 8 rounds. The results demonstrated that the DL-based distinguisher achieved higher accuracy than the pure diﬀerential distinguisher, highlighting the potential of DL-based approaches in diﬀerential cryptanalysis. Algorithm 1 is the algorithm employed by Gohr in training a deep learning (DL)-based distinguisher for diﬀerential attack. The algorithm considers a pair of plaintexts P0 and P1 with a predetermined input diﬀerence Δ, i.e., P0 ⊕ P1 = Δ. Additionally, C0 = Ek (P0 ) and C1 = Ek (P1 ), where Ek signiﬁes encryption of plaintext P with key k. Furthermore, in the context of Feistel structured block ciphers, the left and right halves of a data block are typically referred to as L and R, respectively.  
   
  438  
   
  A. Ebrahimi et al.  
   
  Algorithm 1. DL-based Diﬀerential Distinguisher for r rounds of Speck32/64 1: Input: r (number of rounds), AI machine, (C0 , C1 ) 2: Output: Trained AI machine, diﬀerential distinguisher status 3: Generate 107 plaintext pairs (P0 , P1 ) with Δ = (L0 ⊕ L1 , R0 ⊕ R1 ) = (0x0040, 0x0000) 4: Randomly allocate 107 labels Y ∈r {0, 1} to the pairs 5: for each pair (P0 , P1 ) with label Y do 6: if Y = 0 then 7: P1 ← P1 ∈r {0, 1}32 8: Encrypt the pairs with r rounds of Speck32/64 to get ciphertext pairs (C0 , C1 ) 9: Store (C0 , C1 ) with corresponding labels in a dataset 10: Train DL-distinguisher using the dataset and their corresponding labels 11: Repeat steps 3-11 for another 106 pairs for testing 12: Measure the accuracy of the DL-based distinguisher 13: if accuracy > 50% then 14: The machine is a DL-based diﬀerential distinguisher  
   
  In deep learning (DL)-based distinguishers, determining the optimal input diﬀerence can signiﬁcantly improve their performance. Gohr [10] presented a novel algorithm for identifying appropriate input diﬀerences for neural network distinguishers, without requiring prior human knowledge. This algorithm employs few-shot learning, where a neural network learns features from a large dataset and a simpler machine learning algorithm is trained on a smaller set of samples. In [5], Bellini et al. presented an alternative approach that does not rely on neural networks for ﬁnding the best input diﬀerence for DL-based distinguishers. In order to ﬁnd the best input diﬀerence they had a bias score hypothesis which states that the optimal input diﬀerence for neural distinguishers cryptographically is the input diﬀerence that maximizes the bias of output diﬀerence bits. Computing the bias score for block ciphers is infeasible due to the requirement of enumerating all keys and plaintexts. However, we can use an approximation derived from a limited number of samples t: Deﬁnition 4 (Approximate Bias Score [5]). Let E : Fn2 ×Fk2 → Fn2 be a block cipher, and let Δ ∈ Fn2 be an input diﬀerence. The Approximate Bias Score for Δ, denoted by ˜bt (Δ), is deﬁned as the sum of the biases of each bit position j in the output diﬀerence, computed over t samples. Formally, we have:    n−1 t    i=0 (EKi (Xi ) ⊕ EKi (Xi ⊕ Δ))j ˜bt (Δ) =   − 1 2 ·   t   j=0 The authors’ hypothesis is conﬁrmed, and they propose an evolutionarybased search algorithm that leverages the approximate bias score to explore a larger set of candidate input diﬀerences. The algorithm starts with a population of randomly generated input diﬀerences, and an approximate bias score is computed for each of them. The top 32 input diﬀerences with the highest score are  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  439  
   
  retained for further evaluation. The algorithm then proceeds with 50 iterations, during which new individuals are derived and evaluated. To ensure the starting round’s inﬂuence on the bias score is accounted for, the number of rounds is incremented if the maximum bias score obtained surpasses a threshold limit. At the end of the algorithm, the authors obtain a list of 32 input diﬀerences for each round. The ﬁnal step involves computing a weighted cumulative bias score for all the obtained input diﬀerences from round 1 to round R. The authors’ search algorithm based on the biased score demonstrates improved performance in identifying input diﬀerences compared to other methods and can be useful for cryptographic applications. Algorithm 2 can show their method. The algorithm has several key parameters, including the initial population size for each generation (P ), the mutation probability (pm ), the approximate bias score sample size (t), and the relevance threshold (T ). The speciﬁc values of these parameters can be adjusted as needed to optimize the algorithm’s performance. Also curr populationi indicates the ith bit of the current population we have. Algorithm 2. Evolutionary optimizer [5] init population ← [RandomInt(0, 2n − 1) for 1024 times] Sort init population by ˜bt (·) in descending order curr population ← ﬁrst P elements of init population for iter ← 0 to 50 do cand ← [ ] for i ← 0 to P − 1 do for j ← i + 1 to P − 1 do if RandomFloat(0, 1) < pm then m←1 else m←0 Add curr populationi ⊕ curr populationj ⊕ (m ≪ RandomInt(0, n − 1)) to cand 13: Sort cand by ˜bt (·) in descending order 14: curr population ← ﬁrst P elements of cand return cand  
   
  1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12:  
   
  3  
   
  Identiﬁcation of Optimal RX Distinguishers in Cryptanalysis with Evolutionary Algorithm  
   
  In this section, we present a modiﬁed evolutionary algorithm that builds upon the algorithm of [5]. Our algorithm facilitates the discovery of novel RX diﬀerential pairs that can be leveraged to train deep-learning-based RX distinguishers. Initially, we discuss the artiﬁcial intelligence (AI) tools and deep learning model utilized in our study. Subsequently, we explore the relation between the rotational bias score and the accuracy of deep learning-based RX distinguishers. Drawing upon this insight, we introduce our evolutionary algorithm designed to identify the optimal RX input for training a deep learning-based distinguisher.  
   
  440  
   
  3.1  
   
  A. Ebrahimi et al.  
   
  AI Tools and Model Development  
   
  In this study, we employ the Keras [9] library to develop our deep learning model, which is inspired by the architecture proposed by Aron Gohr in his groundbreaking CRYPTO’19 paper [10]. Gohr’s model focuses on using a neural network to diﬀerentiate between pairs of Speck32/64 ciphertexts corresponding to ﬁxed diﬀerences (non-random) and random message pairs (random). His neural distinguisher is a residual network comprising four main components, achieving remarkable accuracy for varying rounds of Speck32/64 and enabling practical key recovery attacks. The input to Gohr’s neural distinguisher consists of a 64-bit ciphertext pair from Speck32/64, which is reshaped and permuted into a 16-bit wide tensor with four channels. This input reshaping takes into account the unique 16-bit word structure of Speck32/64. The second component of the architecture involves a one-dimensional convolution, denoted as Conv1D with kernel size 1 and 32 ﬁlters, that slices through the four-channel bits. Following the convolutional layer, batch normalization and ReLU activation function are applied as per conventional deep learning practices. The third component consists of residual blocks, with each block containing two convolutional layers, represented as Conv1D with kernel size 3 and 32 ﬁlters. The number of residual blocks in the network determines the depth of the neural distinguisher. Lastly, a densely connected prediction head with ReLU activations is employed, along with an output layer featuring a single neuron with sigmoid activation. L2 regularization with a value of 10−5 is used throughout the network to penalize large weights and reduce the likelihood of overﬁtting. Also, the Adam optimization method [15] is used for this architecture. The present research employs Gohr’s neural distinguisher architecture to train RX diﬀerential distinguishers for analyzing AND-RX ciphers such as Simon and Simeck. The rationale behind this choice is that Simon and Simeck, speciﬁcally the 32-bit version, share the same 16-bit structure as Speck32/64, which is extensively investigated in Gohr’s previous research. The aim of this approach is to harness the potential of deep learning to discover new RX distinguishers and enhance our comprehension of the security of these ciphers. We also employed a sequential training approach in which we increased the number of rounds in each iteration. Previous research has shown that this approach can improve model performance [5], as it utilizes knowledge learned in previous rounds. We trained our model using a dataset of 107 training samples and 106 validation samples, with a batch size of 1000. Our model had a depth of 1 and we used 5 epochs for training. 3.2  
   
  Training DL-Based RX Distinguishers  
   
  This section outlines the training process for our RX distinguishers, which is based on deep learning. Furthermore, it elaborates on the potential of DL-based RX distinguishers in gaining insights from pairs of ciphertext.  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  441  
   
  Training Phase. The training process involves data preparation, model conﬁguration, and evaluation of the trained model on AND-RX ciphers such as Simon and Simeck. The ﬁrst step in training the RX distinguishers is data preparation. We generate a dataset consisting of pairs RX of ciphertexts, labeled as either non-random (for ﬁxed (δ, γ)) or random (random message pairs). Since RX cryptanalysis is a related-key cryptanalysis, in addition to a translation δ that exists for the RX plaintext pairs, there may also be a translation for the keys used to encrypt each plaintext within the pair, which can be shown by δkey . According to the research presented in [18], the propagation of the RX diﬀerential in AND-RX ciphers primarily depends on the Hamming weight of the diﬀerence and their rotations. In this part, we chose to focus on an input diﬀerence zero for the initial training of the deep learning-based RX distinguisher for lightweight ciphers Simon32/64 and Simeck32/64. Speciﬁcally, we set both the input diﬀerence and key RX diﬀerence to 0x0000 and 0x00000000, respectively, using hexadecimal representation to represent the n-bit (n × m-bit) binary representation of plaintexts (keys). By doing so, we aimed to ﬁrst train the RX distinguisher as a starting point and subsequently analyze its behavior and performance in the context of Simon32/64 and Simeck32/64 ciphers, and then investigate possible improvements. For that, we ﬁxed δ = 0 for both plaintexts and keys and iterated through all possible γs to determine the γ that would produce the best distinguisher for the longest number of rounds r for both Simon and Simeck. Our results indicated that for the case of Simeck32/64, γ ∈ {1, 15} and for the case of Simon32/64, γ ∈ {4, 12} produced the best distinguishers for 14 and 10 rounds, respectively. The detailed training method for r rounds is presented in Algorithm 3. Algorithm 3 . DL-based RX Distinguisher for r rounds of a Cryptographic Primitive 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:  
   
  Input: r (number of rounds), AI machine, (C0 , C1 ) Output: Trained AI machine, RX distinguisher status Choose a rotational oﬀset γ and constants δ1 , δ2 , with δγ = δ1 ⊕ δ2 Generate 107 plaintext pairs (P0 , P1 ) with RX-diﬀerence δγ (P0 , P1 ) = (L1 ⊕(L0 ≪ γ), R1 ⊕ (R0 ≪ γ)) Randomly allocate 107 labels Y ∈r {0, 1} to the pairs for each pair (P0 , P1 ) with label Y do if Y = 0 then P1 ∈r {0, 1}32 Encrypt the pairs with r rounds of the cryptographic primitive to get ciphertext pairs (C0 , C1 ) Store (C0 , C1 ) with corresponding labels in a dataset Train DL-RX-distinguisher using the dataset and their corresponding labels Repeat steps 3-11 for another 106 pairs for testing Measure the accuracy of the DL-based RX distinguisher if accuracy ≥ 50% then The machine is a DL-based RX distinguisher  
   
  442  
   
  A. Ebrahimi et al.  
   
  Machine Interpretation. In this part, we present an interpretation of the deep learning-based RX distinguishers trained on AND-RX ciphers, specially on Simeck32/64. Our objective is to investigate the factors aﬀecting the accuracy of the distinguisher and its ability to analyze RX cryptanalysis. In prior studies on deep learning-based diﬀerential distinguishers, it has been shown that the bias of the output diﬀerential plays a crucial role in determining the accuracy and the number of rounds for which a distinguisher can be trained. To investigate the possible impact of bias on our RX distinguishers, we ﬁrst deﬁne the Approximate Bias Score for RX attack, inspired by the deﬁnition presented in [5]. Deﬁnition 5 (Approximate RX Bias Score). Let E : Fn2 × Fk2 → Fn2 be a block cipher, and let δ ∈ Fn2 be an input RX-diﬀerence with a given rotational oﬀset γ. The Approximate RX Bias Score for δ, denoted by ˜bt (δ, γ), is deﬁned as the sum of the biases of each bit position j in the output RX-diﬀerence, computed over t samples. Formally, we have:   t  n−1    ((E (X )) ⊕ E ((X ≪ γ) ⊕ δ))) K i i j (K ≪γ)⊕δ i i i=0 ˜bt (δ, γ) =  − 1 2·  t   j=0 In our study, we investigated the relationship between bias score, accuracy of the RX distinguisher, and number of rounds for the trained distinguisher. To do this, we trained distinguishers for a range of γ values while keeping δ ﬁxed at 0. For each distinguisher, we then calculated its bias score and accuracy across various rounds. This process was repeated for multiple iterations to ensure the robustness of our results. The Pearson correlation coeﬃcient and The resulting scatter plot (Fig. 2) maps bias scores (x-axis) against distinguisher accuracy (y-axis), with each dot representing a diﬀerent γ value for Simeck32/64. Diﬀerent colors are used to denote the number of rounds for each distinguisher: red for 11 rounds, green for 12 rounds, and blue for 13 rounds. Upon examining the plot, we observed a general trend: higher bias scores often corresponded to higher accuracy (or more rounds). There was one notable exception: γ = 11 produced the highest bias, but no distinguisher could be trained above 12 rounds with this γ value. In order to gain a deeper understanding of these patterns, we conducted a weighted correlation analysis. This analysis validated the presence of a positive connection between the bias score and accuracy. During our experiment, we utilized formula (1) for our analysis, and the resulting score was approximately 0.65. This score indicates that there is indeed a positive correlation between the bias and the accuracy of the trained machine. In the formula, ρ represents the Pearson correlation coeﬃcient, and a represents the accuracy of the DL-based distinguisher. We further scrutinized outliers, such as the aforementioned γ = 11 case, and hypothesize that these may be due to variations in the cipher structure or other unidentiﬁed factors that warrant further investigation.   correlation coeﬃcient = ρ ˜bt , a × e2r (1)  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  443  
   
  These results underpin our claim that the output RX diﬀerence bias score is a key determinant of the accuracy of a deep learning-based RX distinguisher. They also prompted us to introduce an adapted evolutionary algorithm aimed at identifying optimal (δ, γ) pairs for RX plaintext.  
   
  Fig. 2. Scatter plot of Bias Score vs Accuracy of RX Distinguisher (Colored by Rounds)  
   
  3.3  
   
  Evolutionary Optimization of Deep Learning RX Diﬀerential Distinguishers  
   
  Now, we propose an evolutionary optimization algorithm for ﬁnding the best RX input diﬀerences. The goal is to adapt the evolutionary-based search algorithm, leveraging the approximate RX bias score, to explore a more extensive set of candidate RX pairs. The modiﬁed algorithm will account for the rotational oﬀset γ and the XOR translation δ. The main modiﬁcation of this algorithm involves introducing a novel search strategy for the optimal shift parameter, γ, while also evaluating the impact of input diﬀerence. Notably, to the best of our knowledge, this approach is the ﬁrst to simultaneously search for both the optimal δ and γ parameters, instead of solely searching for the best δ for a ﬁxed γ value, which is commonly set to γ = 1 in the literature. To enable this search strategy, we have developed a methodology for generating a binary representation for the shift parameter based on the block size, with the ﬁnal bits appended to each member of the population. For example, for the Simeck32/64, γ and γkey represented by two 4-bit words. So, we increase the number of bits in the search by an additional 8, where the ﬁnal 8 bits represent the value of γ. The new algorithm starts with a population of randomly generated input differences and corresponding rotational oﬀsets. For each of them, an approximate RX bias score is computed. The top 32 input diﬀerences with the highest score are retained for further evaluation. The algorithm proceeds with 50 iterations,  
   
  444  
   
  A. Ebrahimi et al.  
   
  during which new individuals are derived and evaluated. If the highest bias score returned is greater than a threshold, the number of rounds is incremented by one. At the end of the algorithm, a list of 32 input diﬀerences for each round is obtained. The ﬁnal step involves computing a weighted cumulative RX bias score for all the obtained input diﬀerences from round 1 to round R. Algorithm 4 shows the modiﬁed optimizer for RX attack. The algorithm has several key parameters, including the initial population size for each generation (P ), the mutation probability (pm ), the approximate RX bias score sample size (t), and the threshold (T ). The speciﬁc values of these parameters can be adjusted as needed to enhance the algorithm’s performance. Also, curr populationi,δ and curr populationi,γ indicate the value of ith bit of δ and γ, respectively.  
   
  Algorithm 4. Evolutionary optimizer for RX diﬀerential distinguishers init population ← [RandomInt(0, 2n − 1)||RandomInt(1, n − 1) for 1024 times] Sort init population by ˜bt(δ,γ) (·) in descending order curr population ← ﬁrst P elements of init population for iter ← 0 to 50 do cand ← [ ] for i ← 0 to P − 1 do for j ← i + 1 to P − 1 do mγ ← 1 if RandomFloat(0, 1) < pm then mδ ← 1 else mδ ← 0 Add (((curr populationi,δ ≪ RandomInt(0, n−1))⊕curr populationj,δ ⊕ mδ ) || (curr populationi,γ ⊕ mγ ) to cand 14: Sort cand by ˜bt (·) in descending order 15: curr population ← ﬁrst P elements of cand return cand 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13:  
   
  The modiﬁed algorithm presented in Algorithm 4 is speciﬁcally designed for optimizing RX diﬀerential distinguishers for cryptographic primitives such as Simon32/64 and Simeck32/64. By incorporating the rotational oﬀset γ and the XOR translation δ, the search space for potential input diﬀerences is expanded, increasing the likelihood of discovering more eﬀective RX input pairs for deep learning-based RX distinguishers. One signiﬁcant improvement aﬀorded by this method is the capability to identify eﬀective RX distinguishers for full-key classes, which represents a marked advancement over prior research that only succeeded in identifying such distinguishers for classes that were not full-key.  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  4  
   
  445  
   
  Results and Discussion  
   
  In this section, we present the results of applying our evolutionary optimization method to diﬀerent versions of Simon and Simeck block ciphers. Our goal is to determine the most eﬀective RX input diﬀerences (δ) and rotational oﬀsets (γ) for each cipher version, allowing us to train deep learning-based RX distinguishers. In this study, we utilized an Intel(R) Core(TM) i7-6700HQ CPU @ 2.60 GHz to run the evolutionary algorithm and employed Colab [8] for training the distinguishers. The experiments for the highest weight version of Simon and Simeck lasted approximately 3 h each. We used an evolutionary optimization approach, applying Algorithm 4 to the various versions of Simon and Simeck. The optimization was run 10 times, with each trial randomly initializing the values of δ and γ. The evolutionary algorithm subsequently updated these values to achieve higher distinguisher accuracy. Each trial was trained on a dataset of 106 cipher text pairs, which were generated using diﬀerent random keys. The key space was varied over the course of the trials to ensure thorough testing. In the following subsections, we discuss our ﬁndings for each cipher version, highlighting the speciﬁc δ and γ values that led to the most eﬀective RX distinguishers. 4.1  
   
  Simeck Cipher  
   
  For the Simeck cipher family, our evolutionary optimization method was successful in identifying eﬀective RX distinguishers for 15, 17, 20 rounds of Simeck32/64, Simeck48/96, and Simeck/128, respectively. The results for each version of the Simeck cipher are detailed in Table 3, which includes the optimal RX input differences (δ), rotational oﬀsets (γ), the number of rounds, and the corresponding distinguisher accuracy. Our proposed method for training deep learning-based RX distinguishers demonstrates notable advantages despite not necessarily achieving the best possible distinguisher performance for the Simeck cipher family. Although there exist distinguishers with higher round coverage, As shown in Table 2, such as 20 rounds for Simeck32/64, 27 rounds for Simeck48/96, and 34 rounds for Simeck64/128, these distinguishers operate under signiﬁcantly smaller weak key classes, specifically of size 230 , 246 , and 258 , respectively, while our distinguishers cover the entire key space. In our investigation, it was found that the Simeck cipher is more susceptible to RX cryptanalysis compared to Simon. While searching for vulnerabilities in Simon, we did not ﬁnd any eﬀective deep learning-based distinguishers with γ = 0 that perform better than conventional DL-diﬀerential distinguishers. However, this is not the case for Simeck, as shown by the results presented in Table 1, where DL-RX distinguishers can almost match the performance compared to related key distinguishers reported in the literature for the same round.  
   
  446  
   
  A. Ebrahimi et al.  
   
  Table 3. Summary of the optimal RX input diﬀerences (δ), key diﬀerences (δkey ), rotational oﬀsets (γ), the number of rounds, and distinguisher accuracy for diﬀerent versions of Simeck block ciphers.  
   
  4.2  
   
  Cipher Version δ  
   
  δkey  
   
  Simeck32/64  
   
  (0, 0x0002)  
   
  0002 1 15 14 13  
   
  γ Number of Rounds Accuracy 51.34 57.08 70.57  
   
  Simeck48/96  
   
  (0, 0x000002)  
   
  0002 1 17 16 15  
   
  52.06 57.67 69.85  
   
  Simeck64/128  
   
  (0, 0x00000002) 0002 1 20 19 18  
   
  52.12 57.01 70.15  
   
  Simon Cipher  
   
  The results of applying our proposed method to the Simon cipher family, specifically Simon32/64, Simon64/128, and Simon128/256 is shown in Table 4. We obtained deep learning-based RX distinguishers for 11 rounds for Simon32/64, 13 rounds for Simon64/128, and 16 rounds for Simon128/256, respectively. It should be noted that these results exhibit worse performance in terms of round coverage when compared to existing distinguishers from the literature that also cover the full key space. Additionally, we introduce RX distinguishers with rotational oﬀsets γ other than 1, which, to the best of our knowledge, has not been previously explored. This highlights the potential of our proposed method to uncover new insights in the realm of RX cryptanalysis and contribute to the development of more secure cryptographic primitives. In our study, we observed that the performance of the proposed method was worse for the Simon cipher family compared to the Simeck cipher family, even though both ciphers share the AND-RX design paradigm. One possible explanation could be the diﬀerent structures of the diﬀusion layer in the round functions of Simon and Simeck ciphers. The diﬀerent choices of shift parameters in these functions could result in diﬀerent resistance to RX cryptanalysis. The rotation oﬀsets in the f (x) functions might interact diﬀerently with the proposed distinguishers, making it harder for the evolutionary search algorithm to ﬁnd strong RX distinguishers for Simon compared to Simeck.  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  447  
   
  Table 4. Summary of the optimal RX input diﬀerences (δ), key diﬀerences (δk ), rotational oﬀsets (γ), the number of rounds, and distinguisher accuracy for diﬀerent versions of Simon block ciphers. Cipher Version δ  
   
  5  
   
  δk  
   
  γ  
   
  Simon32/64  
   
  (0x0, 0x0002) 0002 3  
   
  Simon64/128  
   
  Simon128/256  
   
  Number of Rounds Accuracy 11 10 9  
   
  54.45 74.11 98.48  
   
  (0x0, 0x0)  
   
  0000 30 13 12 11  
   
  51.51 73.15 98.5  
   
  (0x0, 0x0)  
   
  0000 60 16 15 14  
   
  50.62 72.26 96.87  
   
  Impact of the Diﬀusion Layer and Optimal Rotation Parameters  
   
  In the design of AND-RX ciphers, the choice of round constants and shift parameters are crucial in improving the security against RX cryptanalysis. While Lu et al. investigated the impact of round constants on RX cryptanalysis [18], the present study aims to extend this line of inquiry by examining the inﬂuence of shift parameters in AND-RX ciphers. Our primary focus is on identifying the ideal rotation parameters, (a, b, c) for f (x) function of AND-RX ciphers that can be deﬁned as below: f (x) = ((x ≪ a) ∧ (x ≪ b)) ⊕ x ≪ c In this section, our exploration of optimal parameters is centered on AND-RX ciphers with non-linear key schedules. The rationale behind this choice is based on our observation that Simon outperforms Simeck in resisting RX cryptanalysis. Consequently, our aim is to ascertain whether it is feasible to devise a variant of the Simeck-like cipher that could rival Simon in its defense against deep learningbased RX and diﬀerential attacks, or ﬁnd other parameters that can enhance the security of Simeck-like ciphers. In our pursuit of the optimal rotation parameters, we employed our previously discussed evolutionary algorithm (see Algorithm 4). This involved an iterative process where we tested various combinations of a, b, and c parameters for Simeck32/64. For each combination, we identiﬁed the highest bias score and the maximum number of rounds for which our algorithm could determine an appropriate input for the DL distinguisher. Notably, as our algorithm eﬀectively searches for the best inputs with any γ values, even γ = 0, the optimal shift parameters identiﬁed also enhance resistance against both diﬀerential and RX diﬀerential attacks.  
   
  448  
   
  A. Ebrahimi et al.  
   
  Among the shift sets found during our comprehensive exploration, (4, 6, 3) stood out due to its superior cumulative bias score, indicating enhanced resistance to these types of attacks. We could not ﬁnd any distinguisher for more than 13 round for a Simeck-like cipher with these parameters as their shift parameter based on our optimizer. Notably, for other optimal parameter sets, we were successful in training DL-based distinguishers for up to 14 rounds. These ﬁndings, including the six shift parameter sets that performed optimally in our experiments, are detailed in Table 5. Table 5. Optimal rotation sets for AND-RX ciphers with non-linear key schedule and n = 32 determined by the evolutionary algorithm Rotation Set Highest Cumulative Bias Highest Round Distinguisher (4, (4, (6, (3, (3, (3,  
   
  6, 5, 7, 7, 5, 6,  
   
  3) 7) 4) 2) 6) 1)  
   
  14.32 17.28 17.99 18.25 18.67 18.95  
   
  13 14 14 14 14 14  
   
  Our ﬁndings provide useful considerations for the design of block ciphers. The optimal rotation parameters we identiﬁed demonstrate how speciﬁc conﬁguration choices can inﬂuence a cipher’s behavior against RX and related-key diﬀerential attacks. It should be noted, however, that the parameters we found optimal for security may not directly apply to practical cipher design, as designers also have to consider other factors such as hardware eﬃciency. Therefore, our insights should be considered along with other factors like hardware eﬃciency during the selection of shift parameters in the development of AND-RX ciphers. This balanced approach could yield designs that optimize both security and eﬃciency, aligning with the core goals of ARX/AND-RX cipher designers.  
   
  6  
   
  Conclusion  
   
  This paper has investigated the application of deep learning techniques to the RX cryptanalysis of AND-RX block ciphers, with a particular focus on the Simon and Simeck families. We have uncovered distinguishers in the related-key model, which is opposed to the conventional weak-key models found for these ciphers. Our deep learning models have shown a promising ability to identify eﬀective RX distinguishers for various rounds of the Simeck cipher. Speciﬁcally, a combined accuracy score of 0.5475 was achieved for 15 rounds of Simeck32/64 and 0.6429 for 18 rounds of Simeck64/128. Moreover, optimal RX input diﬀerences, key diﬀerences, and rotational oﬀsets for diﬀerent versions of Simeck and Simon block ciphers were identiﬁed.  
   
  Deep Learning-Based Rotational-XOR Cryptanalysis  
   
  449  
   
  In addition, this study presented a novel approach to optimizing diﬀusion layers in AND-RX block ciphers. As a result, several optimal rotation sets for Simeck-like ciphers were identiﬁed.  
   
  References 1. Ashur, T., Liu, Y.: Rotational cryptanalysis in the presence of constants. IACR Trans. Symmetric Cryptol. 2016, 57–70 (2016) 2. Baksi, A.: Machine learning-assisted diﬀerential distinguishers for lightweight ciphers. In: Baksi, A. (ed.) Classical and Physical Security of Symmetric Key Cryptographic Algorithms. CADM, pp. 141–162. Springer, Singapore (2022). https:// doi.org/10.1007/978-981-16-6522-6 6 3. Bard, G.: Algebraic Cryptanalysis. Springer, New York (2009). https://doi.org/10. 1007/978-0-387-88757-9 4. Beaulieu, R., Shors, D., Smith, J., Treatman-Clark, S., Weeks, B., Wingers, L.: The SIMON and SPECK families of lightweight block ciphers. Cryptology ePrint Archive (2013) 5. Bellini, E., Gerault, D., Hambitzer, A., Rossi, M.: A cipher-agnostic neural training pipeline with automated ﬁnding of good input diﬀerences. Cryptology ePrint Archive (2022) 6. Benamira, A., Gerault, D., Peyrin, T., Tan, Q.Q.: A deeper look at machine learning-based cryptanalysis. In: Canteaut, A., Standaert, F.X. (eds.) EUROCRYPT 2021. LNSC, vol. 12696, pp. 805–835. Springer, Cham (2021). https:// doi.org/10.1007/978-3-030-77870-5 28 7. Biham, E., Shamir, A.: Diﬀerential cryptanalysis of DES-like cryptosystems. J. Cryptol. 4, 3–72 (1991). https://doi.org/10.1007/BF00630563 8. Bisong, E.: Google colaboratory, pp. 59–64. Apress, Berkeley (2019). https://doi. org/10.1007/978-1-4842-4470-8 7 9. Chollet, F.: Keras (2015). https://github.com/fchollet/keras 10. Gohr, A.: Improving attacks on round-reduced speck32/64 using deep learning. In: Boldyreva, A., Micciancio, D. (eds.) CRYPTO 2019. LNSC, vol. 11693, pp. 150–179. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-26951-7 6 11. Gohr, A., Leander, G., Neumann, P.: An assessment of diﬀerential-neural distinguishers. Cryptology ePrint Archive (2022) 12. Hu, F., Wang, H., Wang, J.: Multi-leak deep-learning side-channel analysis. IEEE Access 10, 22610–22621 (2022) 13. Khovratovich, D., Nikoli´c, I.: Rotational cryptanalysis of ARX. In: Hong, S., Iwata, T. (eds.) FSE 2010. LNSC, vol. 6147, pp. 333–346. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-13858-4 19 14. Khovratovich, D., Nikoli´c, I., Rechberger, C.: Rotational rebound attacks on reduced Skein. J. Cryptol. 27, 452–479 (2014). https://doi.org/10.1007/s00145013-9150-0 15. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 16. Liu, Z., Li, Y., Wang, M.: Optimal diﬀerential trails in SIMON-like ciphers. IACR Trans. Symmetric Cryptol. 358–379 (2017) 17. Lu, J., Liu, G., Sun, B., Li, C., Liu, L.: Improved (related-key) diﬀerential-based neural distinguishers for SIMON and SIMECK block ciphers. Cryptology ePrint Archive (2022)  
   
  450  
   
  A. Ebrahimi et al.  
   
  18. Lu, J., Liu, Y., Ashur, T., Sun, B., Li, C.: Improved rotational-XOR cryptanalysis of Simon-like block ciphers. IET Inf. Secur. 16(4), 282–300 (2022) 19. Matsui, M.: Linear cryptanalysis method for DES cipher. In: Helleseth, T. (ed.) EUROCRYPT 1993. LNCS, vol. 765, pp. 386–397. Springer, Heidelberg (1994). https://doi.org/10.1007/3-540-48285-7 33 20. So, J.: Deep learning-based cryptanalysis of lightweight block ciphers. Secur. Commun. Netw. 2020, 1–11 (2020) 21. Wang, X., Wu, B., Hou, L., Lin, D.: Automatic search for related-key diﬀerential trails in SIMON-like block ciphers based on MILP. In: Chen, L., Manulis, M., Schneider, S. (eds.) ISC 2018. LNSC, vol. 11060, pp. 116–131. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-99136-8 7 22. Yang, G., Zhu, B., Suder, V., Aagaard, M.D., Gong, G.: The Simeck family of lightweight block ciphers. In: G¨ uneysu, T., Handschuh, H. (eds.) CHES 2015. LNSC, vol. 9293, pp. 307–329. Springer, Heidelberg (2015). https://doi.org/10. 1007/978-3-662-48324-4 16 23. Zhang, L., Xing, X., Fan, J., Wang, Z., Wang, S.: Multilabel deep learning-based side-channel attack. IEEE Trans. Comput. Aided Des. Integr. Circ. Syst. 40(6), 1207–1216 (2020)  
   
  Author Index  
   
  B Bao, Zijian 169 Basso, Andrea 147 Bellini, Emanuele 387 C Chen, Yincen 43 Cheon, Jung Hee 127 Choe, Hyeongmin 127 Costache, Anamaria 325 Curtis, Benjamin R. 325 D Deshpande, Sanjay 297 Dhooghe, Siemen 97, 191 E Ebrahimi, Amirhossein 429 Essex, Aleksander 346 F Feng, Yansong 369 Feng, Zhuohui 43 G Gerault, David 387, 429 Grados, Juan 387 H Hales, Erin 325 He, Debiao 169 He, Jiahui 3 Hirose, Shoichi 233 Hong, Dongyeon 127 Hu, Kai 3 Huang, Yun Ju 387 I Isobe, Takanori 409 Ito, Ryoma 409  
   
  J Jauch, Melanie 275 Jiang, Haodong 255 L Lei, Hao 3 Li, Muzhou 213 Liang, Xuanyu 43 Liu, Fukang 22 Luo, Min 169 M Makarim, Rusydi 387 Maram, Varun 275 Meier, Willi 22 Mi, Ruiqi 255 Minematsu, Kazuhiko 233 Mouha, Nicky 213 Murphy, Sean 325 N Nawan, Mamuri 297 Nawaz, Kashif 297 Nikova, Svetla 191 Nitaj, Abderrahmane 369 O Ogilvie, Tabitha 325 Ovchinnikov, Artemii 97 P Palmieri, Paolo 429 Pan, Yanbin 369 Peng, Cong 169 Perin, Guilherme 82 Picek, Stjepan 82 Player, Rachel 325 Pratapa, Mounika 346  
   
  © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 C. Carlet et al. (Eds.): SAC 2023, LNCS 14201, pp. 451–452, 2024. https://doi.org/10.1007/978-3-031-53368-6  
   
  452  
   
  Author Index  
   
  R Rachidi, Mohamed 387 Rossi, Mélissa 65  
   
  S Saarinen, Markku-Juhani O. Sakamoto, Kosei 409 Song, Ling 43 Sun, Ling 213 Sun, Siwei 22 Szefer, Jakub 297  
   
  T Tiwari, Sharwan  
   
  387  
   
  W Wang, Gaoli 22 Wang, Meiqin 3, 213 Wei, Wei 169 Wu, Lichao 82 65  
   
  X Xu, Chuanqi 297 Y Yang, Qianqian 43 Yi, MinJune 127 Yu, Xiaorui 22 Z Zhang, Nana 43 Zhang, Zhenfeng 255  

 Report "Selected Areas in Cryptography – SAC 2023. 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers 9783031533679, 9783031533686"  
 ×    

 --- Select Reason ---  Pornographic  Defamatory  Illegal/Unlawful  Spam  Other Terms Of Service Violation  File a copyright complaint     

 Close  Submit    

    Contact information  
 Michael Browner   
   [email protected]    
   
   Address:   
 1918 St.Regis, Dorval, Quebec, H9P 1H6, Canada.   
   
 Support & Legal  
  O nas 
  Skontaktuj się z nami 
  Prawo autorskie 
  Polityka prywatności 
  Warunki 
  FAQs 
  Cookie Policy 
    
 Subscribe to our newsletter  
  Be the first to receive exclusive offers and the latest news on our products and services directly in your inbox.  
   Subscribe     

 Copyright © 2024 DOKUMEN.PUB. All rights reserved.        

 Unsere Partner sammeln Daten und verwenden Cookies zur Personalisierung und Messung von Anzeigen. Erfahren Sie, wie wir und unser Anzeigenpartner Google Daten sammeln und verwenden  .   Cookies zulassen