 This book constitutes the revised selected papers of the 9th International conference, ICOCI 2023, held in Kuala Lumpur,   
 This book constitutes the revised selected papers of the 9th International conference, ICOCI 2023, held in Kuala Lumpur,  
 This book constitutes the refereed proceedings of the 8th International Conference on Advances in Visual Informatics, IV  
 Author / Uploaded 
  Nur Haryani Zakaria (editor) 
  Nur Suhaili Mansor (editor) 
 Table of contents :  
  Preface  
  ICOCI 2023 Committee  
  Contents – Part I  
  Contents – Part II  
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  Nur Haryani Zakaria Nur Suhaili Mansor Husniza Husni Fathey Mohammed (Eds.)  
  Communications in Computer and Information Science  
  Computing and Informatics 9th International Conference, ICOCI 2023 Kuala Lumpur, Malaysia, September 13–14, 2023 Revised Selected Papers, Part I  
  Nur Haryani Zakaria · Nur Suhaili Mansor · Husniza Husni · Fathey Mohammed Editors  
  Computing and Informatics 9th International Conference, ICOCI 2023 Kuala Lumpur, Malaysia, September 13–14, 2023 Revised Selected Papers, Part I  
  Editors Nur Haryani Zakaria Universiti Utara Malaysia Sintok, Malaysia  
  ICOCI 2023 Committee  
  Patron Mohd. Foad Sakdan  
  viii  
  ICOCI 2023 Committee  
  Secretariat Alawiyah Abd Wahab Nur Azzah Abu Bakar  
  Intel, Argentina Universiti Kebangsaan Malaysia, Malaysia University Ferhat Abbas Setif 1, Algeria University Ferhat Abbas Setif 1, Algeria University Ferhat Abbas Setif 1, Algeria Flextronics International (Flex), Austria City University, Bangladesh Institute of Applied Physics and Computational Mathematics, China University of Hong Kong, China Amity University, Noida, India Chitkara University, India Chitkara University, India Chitkara University, India Chitkara University, India  
  ICOCI 2023 Committee  
  Nazeer Unnisa Qurishi Ali M. Abdulshahed Pooja Gupta Prateek Agrawal Gulfam Ahamad Prashant Johri M. A. Ansari Swagata Dey Venkatesh Gauri Shankar Bali Devi Vikas Kamra Lalit Kumar R. Raja Subramanian Shrddha Sagar Vikram Kumar Susama Bagchi P. Sardar Maran Amit Kumar Mishra Ade Novia Maulana Apri Siswanto Abdullah Tito Sugiharto Rio Andriyat Krisdiawan Erlan Darmawan Evizal Abdul Kadir Yeffry Handoko Putra Waleed Khalid Al-Hadban Athraa Jasim Mohammed Suhaib Kh. Hamed Mohammed Rashad Baker Firas Mahmood Mustafa Zakho Khalid Shaker Arwa Alqudsi Ramadi Hussein K. Almulla Roberto Vergallo Mohd Nor Akmal Khalid  
  x  
  ICOCI 2023 Committee  
  Mustafa Ali Abuzaraida Bhagyashree S. R. Mohd Hasbullah Omar Rubijesmin Abdul Latif Mohd Helmy Abd Wahab Husna Sarirah Husin Aida Zamnah Zainal Abidin Mohammed Gamal Alsamman Quah Wei Boon Ihsan Ali Abdulrazak Yahya Saleh Rajina R. Mohamed Dalilah Binti Abdullah Shahrinaz Ismail Ruhaya Ab. Aziz Syahrul Fahmy Nooraida Samsudin Norhafizah Ismail Noormadinah Allias Zainab Attar Bashi Ashikin Ali Roziyani Setik Siti Fairuz Nurr Sadikan Safyzan Salim Marwan Nafea Irny Suzila Ishak Abdul Majid Soomro Nor Masharah Husain Nur Intan Raihana Ruhaiyem Nik Zulkarnaen Khidzir Nadilah Mohd Ralim Kavikumar Jacob Fawad Salam Khan Muhammad Abdulrazaaq Thanoon Mohammad Jassim Mohammad Muazam Ali  
  Misurata University, Libya ATME College of Engineering, Libya Universiti Utara Malaysia, Malaysia Universiti Tenaga Nasional, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Kuala Lumpur Malaysian Institute of Information Technology, Malaysia Asia Pacific University of Technology & Innovation, Malaysia Universiti Utara Malaysia, Malaysia Ministry of Higher Education, Malaysia University of Malaya, Malaysia Universiti Malaysia Sarawak, Malaysia Universiti Tenaga Nasional, Malaysia Universiti Kuala Lumpur, Malaysia Albukhary International University, Malaysia Universiti Tun Hussain Onn Malaysia, Malaysia University College TATI, Malaysia University College TATI, Malaysia Politeknik Mersing, Malaysia Tunku Abdul Rahman University of Management and Technology, Malaysia International Islamic University Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Selangor, Malaysia Universiti Teknologi MARA, Malaysia Universiti Kuala Lumpur British Malaysian Institute, Malaysia University of Nottingham Malaysia, Malaysia Universiti Selangor, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Pendidikan Sultan Idris, Malaysia Universiti Sains Malaysia, Malaysia Universiti Malaysia Kelantan, Malaysia Universiti Kuala Lumpur, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia Universiti Kebangsaan Malaysia, Malaysia Universiti Kebangsaan Malaysia, Malaysia Universiti Tun Hussein Onn Malaysia, Malaysia  
  ICOCI 2023 Committee  
  Khairol Amali Ahmad Juliana Aida Abu Bakar Mohd Nizam Omar Waqas Ahmed Shakiroh Khamis Habiba Akter Noris Mohd Norowi Siti Munirah Mohd Sulaiman Mahzan Shahidatul Arfah Baharudin Pantea Keikhosrokiani Renugah Rengasamy Khalid Hussain Massudi Mahmuddin Mahmood Abdullah Bazel Masitah Ghazali Norhanisha Yusof Saiful Bakhtiar Osman Azliza Mohd Ali Norhasyimatul Naquiah Ghazali Yusmadi Yah Jusoh Jasni Ahmad Azlin Nordin Abdullah Al-Sakkaf Kamsiah Mohamed Mudiana Mokhsin Suhaimi Abd-Latif Sani Salisu Ijaz Ahmad Ghaith Abdulsattar Al-Kubaisi Qamar Ul Islam Abdulrazak F. Shahatha Al-Mashhadani Muhammad Kashif Shaikh Mir Jamal Ud Din Najia Saher Tasneem Mohammad Ameen Duridi  
  xii  
  ICOCI 2023 Committee  
  Krzysztof Marian Tomiczek Abdullah Hussein Al-Ghushami Abayomi Abdultaofeek Fathima Musfira Ameer Mohammed Ahmed Taiye Sasalak Tongkaw Abdulfattah Esmail Hasan Abdullah Ba Alawi Mehmet Nergiz Huseyin First Ismail Rakip Karas Mehmet Sirac Ozerdem Evi Indriasari Mansor Hamzah Alaidaros Munya Saleh Ba Matraf Abdullah Almogahed Abdulaziz Yahya Yahya Al-Nahari Ridhima Rani Hani Mizhir Magid Rohaida Romli Shaymah Akram Yasear Mohd Hafizul Afifi Abdullah  
  duties. It must comply with the leadership’s, rules, regulations, and direction. Finally, if all the actors can overcome the obstacles, they will benefit as determined, and the actor-network mission will be complete. This OPP cannot perfectly unite all actors in the translation process, which results in the actor network’s mission not being fully completed. The imperfections in completing the mission of the actor-network will be analyzed to find the factors that support and hinder the completion of the mission of the actor-network.  
  7 Interessement Moment In the problematization process, the actors involved agreed to be included in the network. In practice, it is not guaranteed that they are all consistent in the network of actors, some of whom may think they are not worthy of joining because of their identity. Therefore, it takes a second moment, namely interressement. Several tools were used to establish a balance of power that favored each actor in overcoming obstacles to passing the OPP. Referring to the network of cyber security actors at JSC, the instruments needed for the moment of interest are standardizations, Standard Operating Procedures, Laws and regulations, policies, and leader instructions. Standardization is an instrument needed by non-human actors, especially in electronic systems for public services. It acts as a liaison between the focus actor and other actors and helps the focus actor in coercing and controlling other actors to perform the required activities in the actor network. Standard Operating Procedures are necessary tools for human and non-human actors, especially in software and hardware maintenance. If the network actor does not have this device, there is no good communication between the security system, hardware, and users. In other words, this device can help non-human actors to have a universal language to interact or communicate in the actor-network. Laws and regulations are other significant instruments needed by a network of actors. Laws and regulations are the main handle of the focus actor (JSC Director) to lock all human actors in position. Like laws and regulations, policies are another crucial instrument a network of actors needs. Policies are very effective to use in imposing the interests of various actors in the network. Lead instruction is another tool used to conduct the interessement of several actors in the network.  
  8 Enrollment Moment Enrollment is the third moment of translation, which refers to a set of strategies as actorfocused efforts to define and link various roles that allow other actors to be enrolled. This process relates to the transforming or moving intangible things such as ideas, concepts, and plans into something tangible in the real world [17]. The analysis of enrollment moments in this study is summarized in Table 2.  
  Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh {asadullah.al.galib,humaion.kabir.mehedi, ehsanur.rahman.rhyth}@g.bracu.ac.bd  
  Abstract. The heart of any substantial search engine is a crawler. A crawler is a program that collects web pages by following links from one web page to the next. Due to our complete dependence on search engines for finding information and insights into every aspect of human endeavors, from finding cat videos to the deep mysteries of the universe, we tend to overlook the enormous complexities of today’s search engines powered by the web crawlers to index and aggregate everything found on the internet. The sheer scale and technological innovation that enabled the vast body of knowledge on the internet to be indexed and easily accessible upon queries is constantly evolving. In this paper, we look at the current state of the massive apparatus of crawling the internet, specifically focusing on deep web crawling, given the explosion of information behind an interface that cannot be extracted from raw text. We also explore distributed search engines and the way forward for finding information in the age of large language models like ChatGPT or Bard. Our primary goal is to explore the junction of large-scale web crawling and search engines in an integrative approach to identify the emerging challenges and scopes in massive data where recent advancements in AI upend traditional means of information retrieval. Finally, we present the design of a new asynchronous crawler that can extract information from any domain into a structured format. Keywords: Web Crawling · Crawler · Distributed Systems · Search Engines · Deep Web Crawling · Large Language Models · Asynchronous Crawler  
  A. Al Galib et al.  
  Even though crawling and related technologies have gone through tremendous innovations in the past decade, the ever-expanding and fluid nature of the internet means crawling tools and techniques need to stay on top of the current trends of the web to collect and aggregate information. Due to the explosion of various web technologies, most of the data on the internet now sits behind various search forms that can only be accessed using search queries. This adds another layer of complexity for the crawlers. Along with the current technological advancements in the field of large-scale web crawling, we also focus on deep web crawling, where crawlers need to interact with various web forms to access information. We explore and analyze research materials with a primary focus on old and new crawling techniques capable of withstanding the ever-growing data space. Distributed crawling techniques built on open-source tools show great promise for a future of decentralized indexing of vast amounts of data. We look at the current status of distributed search engines and the challenges it faces in the era of big data. To consider the disruptive AI conversational agents like ChatGPT and Bard and to understand their implications on how people search for information on the web, we explore how these agents can upend the status quo of current search engines. Finally, we are proposing a new asynchronous web crawling technique that can be employed by small to medium organizations for domain-agnostic crawling needs that require data to be extracted from web pages with similar content architectures and stored in specific formats. The Introduction describes the motivation and overall targets of the paper. In the Related Work section, we will explore previous survey works related to web crawling techniques and various architectures of distributed search engines. In the next section, Crawling, we will take a detailed look at various current crawling techniques and different scaling issues that crawlers need to deal with. In the Distributed Search Engine section, we will go through various architectural challenges and prospects of distributed search engines. In the section, AI Conversational Agents, we will discuss the implications of ChatGPT and other conversational AI agents for the domain of search engines and how we acquire information from the web. The section, Domain-Agnostic Asynchronous Crawler, proposes a new asynchronous crawler suitable for crawling sites of any domain, all of which share a common architecture. Finally, we will conclude with the utility and prospects of crawling in the age of AI and generated content.  
  19  
  perform against traditional graph crawling techniques. The authors introduce a type of preferential crawler, Board Forum Crawling, designed explicitly for crawling forum sites efficiently. Traditional breadth-first crawling for forum sites struggles with spider-trap due to duplicate pages and noisy links for various user actions. In their preferential or topical crawler, authors describe a structured approach to crawling all the post pages on a forum site. The authors of [7] describe the challenges and learnings of a highperformance web crawler, Mercator, that is distributed, scalable, and extensible. In, the authors propose a distributed search engine based on a cooperative model, where local search engines reduce the update interval time as compared to a centralized search engine. Each local search engine maintains a local index by accessing files locally. A Hadoop-based distributed search engine is proposed in [4]. Due to the ever evolving and constantly growing internet size, a distributed search engine is proposed to reduce query time. Hadoop consists of two dimensions, the storage dimension and the processing dimension. The proposed system uses Hadoop Distributed File System or HDFS for storage, and for indexing and processing users’ requests, MapReduce is used. This personalized distributed search engine presents an end-to-end model where crawling, extracting, indexing, and fetching results for users’ queries are implemented in Hadoop. A detailed explanation of various crawlers as well as a comparison among crawlers in terms of applicability, usability, and scalability, can be found in [2].  
  3 Crawling The essential components of a web crawler are - a list of URLs to start crawling from, an aggregator that collects web pages using the URLs from the initial list, and a parser that parses the content of web pages and adds new URLs in the initial list. The crawler keeps repeating this process until the crawling list is empty or some other thresholds are achieved [4]. In this section, we will focus on the recent advancements in the crawling field along with the challenges that arise as the size of the data grows. 3.1 Types of Crawlers Based on the scope and type of information collected during crawling, crawlers can be divided into multiple categories, such as universal crawlers, topical crawlers, forum crawlers, and hidden or deep web crawlers [4]. As the internet grows rapidly, the vast majority of the world’s accumulated information can be found hidden behind millions of databases that are only accessible through search panels or forms. To collect data from these hidden parts of the internet, a new type of crawler has emerged, called hidden or deep web crawlers. These crawlers need to generate queries for the search interfaces to acquire information buried in various databases and storage systems. For the indexed data to be useful, it needs to stay updated as time passes. Instead of visiting new web pages, Incremental crawlers update existing and already crawled sites and remove various redundancies to increase storage efficiencies [8]. Running multiple instances of the crawler in a distributed architecture to minimize traffic load and scale horizontally is a required attribute of modern web crawlers [1].  
  A. Al Galib et al.  
  3.2 Techniques This section will explore emerging technologies and approaches for distributed and hidden web crawlers. In a stand-alone crawler module, also known as centralized crawling, a single instance performs all required steps of crawling. While this approach is easy to implement and maintain, given the sheer scale of data, this approach is only suitable for simple use cases [9]. In a distributed system, each crawler instance is a complete crawling module equipped with the necessary tools to perform the end-to-end crawling task. Depending on how these individual modules are managed and run, distributed crawling can be divided into master-worker and peer-to-peer crawling. Distributed crawling is slightly different from parallel crawling. In parallel crawling, individual crawler instances reside in the same LAN, whereas crawler instances in the distributed architecture are scattered across different geographical locations [9]. The hybrid crawling architecture tries to combine the simplicity of centralized crawling with the scalability of distributed crawling. In this approach, URL queue management is centralized, and content fetching from the URLs is distributed across many instances [9]. In the master-worker mode of distributed crawling, a master node performs an orchestrator’s job, where crawling tasks are managed and assigned to worker nodes to perform the actual crawling tasks. The master node manages the global URL list of pages to visit and assigns URLs to each crawler instance [10]. The master node can perform load balancing to avoid overwhelming crawler instances. In a peer-to-peer architecture, crawler instances independently discover and visit web pages without a master node. Some disadvantages of this approach include - a lack of load balancing where one crawler may be downloading a huge number of pages, whereas other crawlers may not have a sufficient number of URLs to crawl data from. 3.3 Distributed Crawling Architectures Some recent implementations of distributed crawling use various open-source crawling frameworks and combine those with the power of cloud services to build robust and scalable systems. Some of these approaches are described below: Scrapy and Redis: Scrapy is an open-source crawling and scraping framework. Redis is an efficient, in-memory, and key-value data store that is used to manage the message queues used to assign tasks to crawler instances. The Scrapy-Redis distributed component can be used to crawl sites with semi-structured information efficiently [10, 11]. Container Clustering: This approach uses a docker container cluster that hosts the crawler instances. Kubernetes is used to orchestrate the clusters of distributed containers. Apache Kafka is used as the communication medium for the crawling instances [10]. Apache Nutch: Apache Nutch is an open-source, powerful, highly scalable, and configurable web crawler that can be customized in various ways to handle all sorts of web pages found on the internet. It uses Hadoop for data processing [12].  
  Large Scale Web Crawling and Distributed Search Engines  
  A. Al Galib et al.  
  online users’ privacy and data protection are of major concern. It is now possible to deanonymize people using sophisticated crawling techniques even if their real identity is hidden behind some authentication protocols [18]. As crawling tools get more efficient and competent in discovering data in the deep web, users’ privacy and, in some cases, their safety due to their political or social activism can be threatened using mass surveillance tools powered by large-scale invasive crawling. Despite these major challenges, many optimization techniques can be applied to increase the efficiency of the crawlers. To identify more effective search panels or user interfaces for deep web crawling, advanced machine learning models can be applied to infer the usability of search interfaces or forms. In deep web crawling, AI can generate better queries to extract more data with few queries. Emerging cloud services could be used to make distributed crawling more efficient and cost-effective. To counter the unfair use of personal data and protect people’s privacy, there should be proper regulations to prevent both private and public sectors from aggressive and indiscriminate crawling without respecting Robots Exclusion Protocols.  
  23  
  Apache Lucene: Lucene is a high-performance indexing and search engine library. Many websites use Lucene to implement their internal search engines. It indexes text documents and then, upon query, generates ranked search results from the indexed content [22]. For the data structure, Lucene uses an inverted index. To provide the auto-complete feature, Lucene uses an n-gram tokenizer. Elasticsearch: Elasticsearch is another open-source distributed analytics and search engine built on Lucene [23]. It provides REST APIs to index and search relevant documents using highly configurable and advanced queries. Due to its distributed architecture using clustering of nodes, Elasticsearch can scale horizontally and rebalance indexes as necessary. It uses JSON as its document storage type [23].  
  4.3 Challenges and Prospects Even though the idea of an open and censor-free web is very promising, there are major technical and practical challenges while running a highly scalable, distributed, and faulttolerant search engine at the internet scale. Current implementations of P2P text-based searching using DHT works efficiently for a fraction of the actual size of the web. Two main issues regarding decentralized search engines are available storage on individual nodes considering the ever-expanding nature of the internet, and constraints on network bandwidth used during full-text searching on a P2P network [21]. Another challenge is reducing search queries’ response time, given that multiple nodes with indexed data need to be queried before a result can be sent to the users. Since no central server controls the addition of new nodes, some adversarial entities can manipulate the crawled index and ranking of search results [19]. Recent advancements in blockchain technologies can be used to optimize different aspects of a distributed search engine, such as crawling, indexing, and storage [19]. Extensive research is also needed to prevent attacks on the P2P system from adversarial players.  
  5 AI Conversational Agents The emergence of ChatGPT has taken the internet by storm. Since it was published in November 2022, ChatGPT has become one of the most popular sites on the web in just a couple of months [24]. ChatGPT is based on GPT-3, a large language model [17] trained on petabytes of data to produce human-like text. 5.1 Challenges to Traditional Search Engines People have been using ChatGPT to generate text and as an interactive search engine to find information. While search engines return a list of web pages, these AI conversational agents provide information in a way that humans are more comfortable with. We need to keep in mind that the purpose of search engines is to parse users’ queries, find relevant pages or documents, and finally rank the search results to provide high-quality responses  
  24  
  A. Al Galib et al.  
  that are beneficial to the users. Large language models generate a sequence of words with a starting prompt. Moreover, the models cannot access the most recent data since their training and cannot provide all sources that played a role in generating certain content. The appeal of these conversational agents in finding information on the web stems from the fact that search engines cannot combine information from multiple sources and then aggregate it to produce a coherent and factually correct answer. Whereas these agents answer questions like another human expert would [25]. Due to the lack of reference materials for generated content, it is more difficult to ascertain the authenticity of generated content by these models. 5.2 Societal and Ethical Impacts The world is already plagued by propaganda and disinformation abundant on social media sites. Since these models are trained on human-generated text in the first place, they may have inherent biases due to the training datasets. Producing disinformation will be much easier with human-like text and will exacerbate the already fragile social and political divides worldwide. Any task that is about generating text on a given topic now needs to be re-examined and re-evaluated. Various professional roles in the domain of content generation, whether article writers or programmers, will be transformed significantly. Prompt engineering, in other words, providing the correct starting sequence of words to generate the best possible output, will be a key skill in the coming days. Exams and evaluation criteria of all sorts need to be rethought in light of the ubiquity of these language models. 5.3 Future Prospects A key area of research that needs to take place is to retain the sources of information generated by the language models and provide them as references to the users. Since people will be using these AI agents to find information on the web, further improvements can be introduced to incorporate recent events in the generated content and references. Much more attention should be given to de-bias the training datasets and shielding the models from being tricked into generating harmful and dangerous content.  
  N. Katuk et al.  
  performed using a hash function that is embedded in the second component of the model. It guarantees that data is safe from alteration or corruption and that access and analysis may be done in a secure manner. It is essential in deepfake films, which may be used to edit video to fabricate scenarios [9]. The integrity of the footage is protected by utilizing a hash function to detect any data manipulation or alteration. Additionally, it ensures that the data can be securely accessed and analyzed without being altered or corrupted. The approach is anticipated to assist organizations by enhancing data retrieval abilities and archiving processes, which will lead to increased security and better incident reaction times.  
  N. Katuk et al.  
  demonstrates the basic need for a system architecture to ensure critical surveillance data is stored securely and efficiently accessible when needed. The second component of the data archiving model focuses on the archiving algorithm, which performs several critical tasks to ensure that only relevant data are stored in the cloud. The algorithm reads the video feed, identifies humans, captures the scene, cuts and stores it in a new image file, creates metadata about the individual, and stores it along with the image file, with an index table created to enable quick and easy retrieval of the archived data. By incorporating integrity checks using hash functions, which produce a distinct digital fingerprint, or hash value, for each data file, the third component addresses the security issue and ensures that the highest levels of data security and reliability are maintained. Last but not least, the data schema is a crucial part of the data archiving architecture for cloud-based video surveillance systems. It outlines the metadata and other pertinent information, as well as the structure of the stored data. The efficiency and efficacy of the surveillance system are increased when the archived data is readily available and retrievable for authorized users thanks to a well-designed data structure. Modern video surveillance operations require a system architecture for video archiving of surveillance equipment on the cloud. Different devices, front-end and back-end applications, communication routes, storage options, and system modules must all be supported by the architecture. The gadgets often consist of cameras, sensors, and other monitoring tools that record and provide data to the system, including video. The incoming data must be captured, processed, and sent to the back end for archiving by the frontend application. The back-end program is in charge of archiving the data in the cloud and giving authorized users access to search and retrieval capability. The data transmission requires channels across a virtual private network (VPN) between the devices, the back end, and the cloud. Data preservation and retrieval solutions are essential for system modules including person detection, scene cropping, indexing, and metadata development. Critical surveillance data may be saved safely and made easily accessible when needed with the aid of a well-designed system architecture for video archiving surveillance systems on the cloud. The system architecture for video archiving of surveillance systems in the cloud is shown in Fig. 1.  
  Fig. 1. System architecture for cloud-based video archiving of surveillance systems  
  N. Katuk et al.  
  hash functions into the data archiving strategy, organizations may guarantee that their crucial surveillance data is accurate and safe while enabling quick and easy retrieval. Algorithm 2 Data integrity checking using a hash function. 1. Retrieve the information and picture data for each image file kept in the cloud. 2. Utilizing a hash function to determine the image data's hash value. 3. Verify that the hash value generated matches the hash value recorded in the metadata. 4. The data is still intact if the hash values line up. Transfer to the following file. 5. If the hash values are different, the data has been tampered with or corrupted. Notify the system administrator and place the file in quarantine. 6. Repeat steps 1-5 for all cloud-stored image files.  
  The data structure, which includes tables to store the information produced by the algorithm, is the fourth component. For example, a table is needed to store information about the video feeds, like the original file name, time, and frame number. Next, a table is needed to store information about the humans in the video, like their physical characteristics, movements, and scene. Next, the metadata requires a separate table to capture information about the new image file, including about the humans in the scene. This table would also include fields for storing the hash values of both the metadata and the image file. Finally, an index table is necessary to retrieve the archived data quickly, with a searchable database of the archived data. Each table in the data schema would have a primary key field to identify each record uniquely. Additionally, there would be foreign key relationships between the tables to ensure that data is appropriately linked and can be accessed and analysed meaningfully. Tables 1, 2, 3 and 4 list the four tables that make up the proposed data schema: Video_Feed, Human_Identification, Metadata, and Index_Table. The original file name, frame number, and time of the video feed are all stored in the Video_Feed table. The Human_Identification database keeps track of details about the people in the video, such as their appearance, how they move, and the scene in which they are present. The new image file’s metadata, including details about the people in the scene and the hash values of the metadata and image file, are stored in the metadata table. The Index_Table, which offers a searchable database of the saved data and enables simple retrieval, is the final component. Table 1. Video_Feed. Column Name  
  Data Type  
  Fig. 4. The data in the table for the humans’ identification with a hash value.  
  the retrieval module is beyond the scope of this paper, as it could involve data mining and machine learning techniques, integrating the integrity check module with it ensures a robust system for data verification. The process of the integrity check module consists of several steps. First, the hash value is calculated using SHA-1 for the given image files, representing the selected frame of the video footage. Next, the stored hash value in the Human_Identification table, calculated during the archiving process, is retrieved. These two hash values are then compared to determine the integrity of the data. If the hash values match, the data is confirmed intact, ensuring the system’s reliability. However, if the hash values do not match, the data has likely been corrupted or tampered with. When combined with the retrieval module, this process fortifies the security and trustworthiness of video surveillance systems, paving the way for a more secure digital landscape. Figure 5 shows the function for calculating the hash value using SHA-1.  
  Fig. 5. The function for calculating hash value using SHA-1.  
  Abstract. Blockchain technology is a distributed digital ledger in a decentralized network that offers immutability, security, and transparency in various applications among digital societies. The consensus mechanism is the defining technology behind the security and performance of the Blockchain system. Under the Industrial Revolution 4.0, blockchain has been considered for integration into supply chain business as an innovative solution to tackle the challenges of traceability, transparency, lack of trust, and data counterfeiting in digital supply chain management. A private permissioned Blockchain is the most suitable type of Blockchain for Supply Chain Management (SCM) as it promises better performance with high throughput and low latency. However, private Blockchains that use the Byzantine Fault Tolerance (BFT) consensus mechanism have low-security capabilities and are more vulnerable to cyber-attacks triggered by malicious nodes. In this paper, we outline the research challenges from the security aspect towards the integration of Blockchain with SCM. Then we design an approach for a private Blockchain-based Supply Chain with security capabilities by proposing an enhancement consensus model to the BFT consensus mechanism for identifying and terminating malicious nodes in the consensus process. The performance of the proposed approach will be validated experimentally and compared against Practical Byzantine Fault Tolerance (PBFT). The proposed approach is expected to prevent security attacks on the consensus mechanism, thereby improving the security and performance of the Blockchain system. Keywords: Byzantine Fault Tolerance · Consensus Algorithm · Supply Chain  
  Kota Samarahan, Sarawak, Malaysia  
  Abstract. Phishing attacks have emerged as a major problem in the digital world due to a rising trend in their frequency. While various approaches have been developed to detect and prevent phishing attacks, a definitive solution to the problem has yet to be discovered. This study discusses automated anti-phishing systems while analyzing and comparing various anti-phishing strategies using exploratory research. Traditional, machine learning, and deep learning-based anti-phishing systems are discussed in the article. The study highlights the use of Artificial Intelligence (AI) based systems, particularly utilizing methods such as Convolutional Neural Networks, Support Vector Machines, and Recurrent Neural Networks. These AI-based approaches dominate the current trend in the field. This study could potentially be helpful for researchers who wish to delve deeper into the topic of automated phishing detection and prevention systems with a comprehensive review. It is advised to carry out further research to investigate the strengths and limitations of different methods and algorithms used in automated anti-phishing systems to understand their performance and effectiveness better. Keywords: Phishing · Anti-phishing · Machine Learning · Deep Learning  
  M. A. A. Aziz et al.  
  As the advancements in internet technology progresses, so does the sophistication of phishing attacks that malicious actors carry out. These attacks pose a growing threat to individuals and organizations; as they evolve, they become progressively more challenging to detect and employ diverse approaches. In order to effectively counter phishing threats, the development of automated anti-phishing systems must keep pace with the advancements in current technologies. Therefore, it becomes crucial to identify the effectiveness and efficiency of advanced automated anti-phishing. This study aims to identify common methods for automated phishing detection and prevention and comprehensively review the related literature. In order to do this, two research questions were constructed to drive the study, which are; (1) what are the available automated methods for detecting and preventing phishing, and (2) how efficient are these common detection and prevention methods. The research aims to investigate the use of automated detection and prevention systems in combating phishing attacks. It will thoroughly review existing literature and studies in the field, including scientific papers, articles, and reports. The objective of the review is to understand the various technologies and algorithms used in these systems and evaluate their effectiveness in detecting and preventing phishing attacks. The research will also identify current systems’ limitations and challenges and determine areas for future research. It is important to note that the research is limited to a literature review and does not involve primary research or experimentation. The information available in the literature may be influenced by the current state of research on the subject and may affect the scope of the research. This research is significant as it addresses the growing need for effective solutions to combat the increasing progression of phishing attacks. The research findings can contribute to the community by identifying suitable anti-phishing systems and providing insights to the limitations and challenges faced by current systems. It can also serve as a reference for future studies in the field and benefit other researchers who are interested in the topic. The significance of the research lies in its support to the development of methods for countering phishing attempts and provides information on their challenges and limitations. Additionally, it contributes to the body of information regarding the development of automated systems that can successfully detect and stop phishing attacks. This article is organized as follows; the next section will discuss the literature review, the methodology, the results and findings will be presented, and a discussion and conclusion.  
  M. A. A. Aziz et al.  
  4.2 Various Methods of Implementing Anti-phishing Systems According to the second objective of the research, this section examines the various methods of implementing anti-phishing systems and presents a brief evaluation of their results. [21] make use of both random forest and naive bayes machine learning algorithms and deep learning-based methods such as convolutional neural networks and long-short term memory networks. Similarly, [22] discusses an automated anti-phishing system that uses convolutional neural networks and recurrent neural networks. Other studies such as [23–30] emphasize the use of multiple machine learning algorithms including decision tree-based algorithms, support vector machines, and natural language processing to classify and identify phishing content in emails and URLs of websites. [30] proposed an anti-phishing system that leverages deep learning to classify the phishing content, and machine learning algorithms to identify phishing content based on the homographs of the website domain name. These studies’ results suggest that using machine learning and deep learning algorithms is a promising approach for anti-phishing systems. Of the 22 studies reviewed, only one proposed a non-machine and deep learning-based method. The advantage of using AI-based anti-phishing systems is the ability to identify new and unknown threats, a significant challenge for traditional detection methods. However, the performance of these methods varies depending on the specific models and techniques used. Some studies have found that deep learning algorithms outperform traditional machine learning methods in terms of malware detection rate and false positive rate, while others have found that a combination of machine learning and deep learning methods is more effective. As a result, the use of a combination of machine learning and deep learning has become widely adopted in the development of anti-phishing systems. It is important to note that the results of these studies may vary depending on the specific dataset and evaluation metrics used. Furthermore, the constantly evolving nature of phishing attacks requires the development of new and innovative anti-phishing systems that can adapt to changing threats. As a result, further research is needed to improve anti-phishing systems’ performance and accuracy.  
  Abstract. Cloud computing is a pay-as-you-go business model that offers elastic remote data storage, and computing resources have become necessary due to the emergence of big data. After data outsourcing to the cloud, cloud users lose control over data and are always concerned about data privacy and security in adopting the cloud service model. So, to ensure remote data integrity, a trusted auditor can make auditing tasks according to the users’ request, which is helpful to release auditing overheads on a user device and meaningfully improve the scalability of cloud services. Although numerous data auditing techniques have been designed with TPA so far, these techniques need to improve on data security and efficiency issues. First, these techniques cannot authenticate block indices, so the server can produce valid proof without an original data block to pass the audit process. Second, existing approaches do not include position fields, so the server can replace the tampered data block with a healthy one to pass the audit phase. To overcome these issues, this paper introduces a new public data authentication scheme, ERPDA. The proposed technique incorporated a newly designed Merkle Tree (MT) based structure, Sequence and Position-based Tree (SPT) that minimises computation complexity to find nodes in data audit and avoid data replacement attacks. The experimental outputs showed that our suggested technique is effective with the comparative data auditing techniques in computation overheads, and the security is proved under the random model. Keywords: Cloud Computing · Third-party Auditing · Proof of Data Possession  
  2 Literature Review Juels and Kaliske [16] presented the first evidence of the data retrievability technique. The sentinel (block masking) approach was used to hide the value in the standard data block so that the server cannot differentiate hashed data blocks. The users may download and confirm the accuracy of data simultaneously. They use symmetric encryption methodology to secure user data, which imposes little computation overhead. However, it is limited to the number of requested blocks as the pre-processed Message Authentication Codes (MACs) are used in the audit phase. Also, these schemes support static data and cannot support public audits.  
  Abstract. In today’s world, cybersecurity is critical in the field of information technology. With the rise of cyber-attacks, including ransomware attacks, protecting user data has become a top priority. Despite the various strategies employed by governments and companies to counteract cybercrime, ransomware continues to be a major concern. Therefore, there is a need to detect and obfuscate viruses in a better way. This immutable impact on the target is what recognizes ransomware attacks from traditional malware. Ransomware attacks are expected to become more problematic in the future. Attackers might use new encryption methods or obfuscation techniques to make ransomware detection and analysis a difficult job. To protect against such attacks, organizations and users employ various tools, guidelines, security guards, and best practices. However, despite these efforts, cyber-attacks have increased exponentially in recent years. Among the most devastating of these attacks is ransomware, which can encrypt user files or lock their devices’ interfaces, rendering them unusable. This research paper provides a valuable resource for researchers, practitioners, and policymakers seeking to enhance their understanding of ransomware detection and mitigation. It also examines defense tactics, such as system backups and network breakdowns, which can help mitigate the impact of an attack. Finally, the paper considers upcoming challenges in the field of cybersecurity and the importance of staying vigilant in protecting against cyber threats. Keywords: Cyberattack · Cybersecurity · Ransomware detection · Ransomware mitigation  
  M. u. Rehman et al.  
  1.3 Major Problem Previous systematic reviews of ransomware have mainly focused on its impact in specialized industries such as healthcare, and government organizations neglecting the fact that ransomware is not limited to specific domains. To address this limitation, this paper proposes a comprehensive evaluation of the detecting and mitigating of ransomware, serving as a starting point for further research. Furthermore, the paper discusses existing methods for detecting ransomware, analyzing their pros and cons. Lastly, prevention tools for ransomware attacks are discussed, providing valuable insights for organizations looking to enhance their security measures against ransomware threats. 1.4 Study Objectives The aim of this study is to examine prior research, consolidate its findings, and concentrate on analyzing ransomware attacks, risks, mitigation, and prevention methods to control ransomware attacks. The study also aims to provide recommendations for the use of these techniques and tools, as well as identify areas for future research in this field. Ultimately, the objective would be to contribute to the development of more effective strategies for mitigating the impact of ransomware attacks. To achieve this goal, three research questions have been formulated, as shown in Table 1. Table 1. Formulated Questions and discussion Research Question  
  Discussion  
  1.5 Contribution and Structure This systematic literature review provides a valuable resource for individuals seeking to advance their knowledge in ransomware attacks and cyber security. By synthesizing previous research, it builds upon existing knowledge and makes new research, as discussed in Table 1. • Our review identified 31 papers that are relevant to the topics of cyber security and ransomware threats and detection. This set of studies can serve as a resource for other researchers who seek to further investigate these areas. • Organize and classify different methods of ransomware attacks into a specific taxonomy. • We investigated the conditions utilized for evaluating defense, detection, mitigation, and prevention techniques against ransomware attacks. • We identified available research data for a future analysis of ransomware and provided guidelines to assist in further research in this field. The structure of this paper unfolds as follows: Sect. 2 explains the methodology employed to systematically select primary studies for our comprehensive analysis. In Sect. 3, we present the outcomes derived from our scrutiny of the selected primary research studies. Finally, Sect. 4 serves as the result of our research efforts, offering conclusions drawn from our findings and suggesting recommendations for future investigations.  
  2 Methodology The research methodology section of this paper describes the systematic approach taken to look at previous studies about prospective ransomware attacks and their corresponding detection systems. Article offer details on the inclusion and exclusion criteria used to choose relevant research, also describe how we locate articles, papers, books, and journals about ransomware attacks. 2.1 Source Material The study utilized a specific search engine and focused on entering relevant keywords to ensure the retrieval of primary research that would address the research questions. The selected keywords were carefully chosen to optimize the development of relevant findings. Boolean operators were limited to AND and OR. The search terms used were: (insert the specific keywords used). (“ransom” OR “ransom-ware” OR “ransomware” OR “Mal-ware” OR “Malware” OR “ransomware attacks”) AND “information security” (“ransomware” OR “ransom” OR “Malware AND (“security” OR “cybersecurity” OR “cyber-security”). In the first phase, the task to be performed for the quality of research is to undertake an exhaustive literature search. Therefore, a search was conducted using six different electronic libraries namely IEEE Xplore, Science Direct, ACM, Springer, Web of Science, and Google Scholar to search for the relevant materials.  
  the “anywhere in the article” option was used. For Web of Science, the search was limited to the “subject” parameter. The search included a variety of publication types, such as journal articles, book sections, working papers, conference papers, dissertations, and reports. Advanced search filters were used to refine search results, including past 13 years, document types, and English language. New keywords like “cyber risk” and “challenges and analysis” were added. A slimming approach was used to analyze articles, removing duplicates, and considering only English-language textual sources. 30 journal articles were selected for the literature study, as shown in Fig. 4. 2.2 Inclusion and Exclusion Criteria A systematic literature review requires empirical evidence from case studies, new ransomware attacks, and advancements in ransomware mitigation technologies. Englishwritten, peer-reviewed studies must meet standards, and only updated ones within recent years are considered. Google Scholar results may not meet standards, so all results are evaluated for compliance (Table 2). Table 2. Inclusion and exclusion criteria for primary studies Inclusion Criteria  
  Exclusion Criteria  
  Governmental documents and blogs should not be included in the article  
  The article must be a peer-reviewed paper published in a journal or conference proceedings  
  non-English publications  
  M. u. Rehman et al.  
  The evaluation process was modeled after similar literature reviews. To evaluate the effectiveness of randomly selected papers, a specific quality assessment procedure was implemented. Step 1: Ransomware: The article should discuss multiple forms of ransomware attacks or security breaches and offer insightful commentary on a specific issue. Step 2: Perspective: The research’s objectives and conclusions should be properly contextualized to ensure a comprehensive understanding of the study. Step 3: Ransomware detection Strategy: Study must provide enough information to show how technology is used to detect attacks and answer research questions, including specific tools and techniques used for detection and mitigation. Step 4: Defense context: The document should explain the security issue to help answer research questions, including its nature, potential consequences, and challenges in addressing it. Step 5: Security measures: The application of diverse security measures to alleviate several types of ransomware attacks. Step 6: Data Recovery: Specifics on data collection, measurement, and reporting must be provided to assess accuracy. 2.5 Data Extraction The data completeness and accuracy of articles were assessed by extracting data from quality-approved papers. The technique was tested on a preliminary investigation before being applied to the full set of research. Data was categorized and entered into a spreadsheet using the following categories. Context Data: Information involving the study’s performed objectives. Qualitative Data: The author’s findings and opinions. Quantitative Data: Information collected through tests and research has been used in the study.  
  Key Qualitative  
  [26] The article covers the methodology and threats of Petya ransomware, as well as strategies for awareness and mitigation [27] Healthcare companies can improve system defense through user-focused tactics like simulation and training on proper computer and network application usage [19] [25] The paper covers the impact of ransomware attacks on cloud service users and providers and proposes mitigating tactics.[28] [29] To provide the decryption key for encrypted user data, hackers often demand a ransom or payment, typically in the form of digital currencies [19] The paper stresses the importance of a written information security program mandated by Massachusetts law or other security frameworks [30] Memory forensics was conducted on volatile memory dumps of virtual machines using the Volatility framework for analysis [9] The report introduces Net Converse, a machine learning study for detecting ransomware network traffic reliably [18] The article proposes DNA act-Ran, a digital DNA sequencing engine that uses machine learning to detect ransomware, utilizing frequency vectors and design limitations for digital sequencing  
  Type of research effects Mitigation  
  3.3 What Are the Most Common Tactics and Techniques Used by Ransomware Attackers and How Can These Be Thwarted? Ransomware attackers commonly use social engineering, phishing, and software vulnerabilities to gain access to systems and demand payment [30]. To thwart these attacks, user education, software patching, data backups, network segmentation, and access controls can help prevent these attacks and limit their impact.  
  4 Mitigation and Prevention Techniques of Ransomware Preventing ransomware is crucial to protect against its damaging effects on individuals and corporations. In case of infection, data recovery can be challenging and may require the help of a trusted specialist. Pre-encryption mitigation refers to the security measures taken before the encryption process to minimize the risk of security breaches.  
  Identify the simulator or programming language used  
  Fig. 1. Paper Selection Process  
  G. Guntoro and M. N. B. Omar  
  3.4 Study Selection Specific inclusion and exclusion criteria are used to select the primary studies [25]. The inclusion criteria are as follows: 1) articles must be published in journals. 2) articles are selected based on the journal’s impact factor, limited to Q1-Q3. 3) papers on the subject of IDS primarily compare algorithms or techniques. On the other hand, the exclusion criteria are as follows: 1) Research not written in English. 2) Literature review studies. 3) Studies without substantial IDS validation. 4) Research addressing intrusion methods and datasets unrelated to IDS contexts. 5) Studies not centered on relevant subjects. 3.5 Data Extraction To address the research questions, data collection from primary studies is necessary. Following that, we will perform data extraction using the collected data. We will extract the IDS research area (to address RQ1), IDS techniques (to address RQ2), IDS datasets (to address RQ3), IDS methodologies (to address RQ3, RQ4, RQ5), and IDS simulators (to address RQ6). 3.6 Study Quality Assessment and Data Synthesis Assessing the quality of studies is essential to enhance the understanding of synthesized findings and solidify conclusions. The primary aim of data synthesis is to provide comprehensive responses to all research inquiries. This data is organized based on the research question. It is then visualized using pie charts, bar graphs, and tables. 3.7 Threat Validation There is a potential threat to this review’s reliability. This occurs because the paper search solely entails manually reviewing the titles of all journal articles. Therefore, specific papers might have yet to undergo comprehensive screening for inclusion in this study.  
  March 2023, as indicated by the selected primary research, include Python (20), Matlab (20), Weka (4), Sucirata (1), Rapidminer (1), C# (1), and Java (1). This information is depicted in Fig. 4.  
  Fig. 4. Simulators Used  
  (de Carvalho Bertoli et al. 2023a)  
  2023  
  (Abu Alghanam et al. 2023)  
  2023  
  Abstract. Wireless Sensor Networks (WSNs) consist of numerous affordable, energy-efficient, compact wireless sensors. These sensors are designed to collect, process, and communicate data from their surrounding environment. Several energy-efficient protocols have been created specifically for WSNs to optimize data transfer rates and prolong network lifespan. Multi-channel protocols in WSN are one of the ways to optimize efficiency and enable seamless communication between nodes, thereby reducing interference and minimizing packet loss through multiple channels. Despite their numerous advantages in data sensing and monitoring, various attacks can pose a threat to a WSN. There are several types of attacks that a WSN may encounter, including spoofing, eavesdropping, jamming, sinkhole attacks, wormhole attacks, black hole attacks, Sybil attacks, and DoS attacks. One of the strategies for enhancing security in WSNs is implementing a cross-layer intrusion detection system (IDS) that can detect initial indicators of attacks that target vulnerabilities across multiple WSN layers. This paper reviews the existing IDS at each layer and the challenges in an energy-efficient cross-layer IDS for WSN in terms of the attacks and IDS approaches. Keywords: Cross-layer IDS · Wireless Sensor Network · Multi-channel protocol  
  can lessen the impacts of interference, enhancing network effectiveness, stability, and link dependability, minimizing latency, and reducing total energy usage. This, however, creates another issue. A wireless sensor network is susceptible to several various attacks. Due to several flaws and, most crucially, the data involved, wireless sensor networks are continually vulnerable to serious attacks. Typically, the nodes in a WSN are tiny, battery-operated gadgets containing sensors, microcontrollers, and communication transcribers. Due to the node’s limited resources, wireless sensor networks are susceptible to various threats that may jeopardize the security and integrity of the data. Nevertheless, WSNs are susceptible to risks despite the various benefits they offer regarding data sensing and monitoring. These risk factors include those caused by memory limitations, unreliable communication, higher communication latency, unattended network operation, deployment in an environment prone to attacks and scalability. Some of these attacks, such as random multi-channel jamming attacks that interfere with radio frequencies on wireless communication channels and cause channel congestion, are intended to take down the network. The challenge may be that random multi-channel jamming attacks are difficult to detect and eliminate due to their random jamming behaviors. Attackers have complete discretion over the time and the specific channels to jam. Other attacks aim to eavesdrop on communications. Others are made to introduce erroneous data into the network. This poses a danger to real-time, reliable WSNs. Security in WSNs is, therefore a difficult problem since it depends on the way to evaluate the reliability of sensor data. Numerous studies on intrusion detection in WSNs have been done in recent years [1–5]. Intrusion detection is used to detect unauthorized activity in a system. It works well as a security measure to defend WSNs against intrusion. There have been a few studies on the security of WSNs. However, they have mostly emphasized attack prevention instead of attack detection. This is an important study area since an attacker who can go undetected might cause significant damage or disruption. Although several intrusion detection systems have been developed to support WSNs, the majority of these systems only work at one layer of the Open Systems Interconnection (OSI) model. Several proposed intrusion detection systems are based on a cross-layer approach. They comprise the physical, data link, and network layers that contribute to cross-layer intrusion detection systems (IDS) design. By detecting the attackers across multiple layers, cross-layer IDS secures the WSN. The rest of the paper is organized as follows. Section 2 highlights various attacks and challenges associated with WSN at each layer. Section 3 presents and compares recent existing work in cross-layer IDS in WSN. Section 4 discusses the challenges and future directions on cross-layer IDS, and Sect. 5 concludes the paper.  
  2 Related Work 2.1 Wireless Sensor Network Cross-layer Protocols WSNs are networks of many inexpensive, low-power, small wireless sensors. The sensors can gather, analyze, and transmit data from their environment. WSNs have gotten a lot of attention from several application sectors because of their capabilities, including  
  N. Nordin and M. S. Mohd Pozi  
  military surveillance, industrial monitoring, target tracking, and environment monitoring. Numerous energy-efficient protocols have been developed for WSNs to maximize throughputs while extending the lifetime of the networks through the Medium Access Control (MAC) and routing protocols, power consumption, and energy harvesting. The protocols are a vital aspect of WSN communication. The protocols determine the allocation of channel resources among the network’s nodes in a way that maximizes efficiency, manages channel constraint, and ensures that nodes communicate simultaneously in single or multiple channels effectively to reduce interference which leads to packet drop. The WSNs are susceptible to attacks due to the extensive nature of node dispersion and the hardware limitation of the nodes. Numerous studies on single-channel WSN protocols such as LEACH [6], RPL [7] and multi-channel protocols such as Chrysso [8] and MiCMAC [9] that interface to the MAC and the network layers, as well as MCRP [10], that interfaces to the MAC, network, and application layers, have been conducted. The real-time nature of MCRP’s multichannel processing enables it to adjust to any location’s local interference. MCRP is a cross-layer protocol that is decentralized and centrally controlled to reduce interference without knowing where the channels are occupied in advance. In order to effectively use the spectrum, MCRP considers all channels that are accessible and transmits on a number of them. This generality makes it possible for better channels to be selected based on the location the sensor nodes are deployed. As a result, the protocol reduces the impact of interference, improving network efficiency, stability, and link reliability. While MCRP exhibits promising results in terms of improved resilience to interference, significantly higher throughput, and link stability, extending the lifetime of WSNs, it is vulnerable to numerous attacks because security was not considered. The protocol is more susceptible to attacks due to the cross-layer attributes and usage of several channels which are necessary for proper data transmission and reception. Thus, the intrusion detection system is a potential approach to detect attacks. 2.2 Intrusion Detection Systems The limitations of sensor nodes in WSNs prevent traditional IDSs from being directly implemented in WSNs. To resolve this issue, various IDSs have been proposed for WSNs. Due to its IDS mechanism and the high processing demands of the algorithms of the IDs, several extended protocols have negatively impacted the network’s energy. An IDS tracks traffic data that may be used to spot and prevent intrusions that compromise the privacy, integrity, and accessibility of an information system. An IDS is a term for software or hardware devices that monitor networks for cyberattacks from inside or outside and trigger an alert.  
  Fig. 1. Fundamental IDS architecture  
  Application Eavesdropping, false data injection, spoofing and altering routing attack, malicious code attack, repudiation attack, DoS attack  
  Other attacks on all the layers are listed in Table 1 [1–5]. These cyber-attacks have a variety of objectives, including stealing, altering, hacking, and flooding the targeted nodes with excessive packets to deplete the sensors’ battery power and disconnect them from the network, making them unusable and hindering them from sensing or routing traffic. The performance, effectiveness, and reliability of communication may suffer as a result of these attacks. To overcome these problems, effective security mechanisms, such as well-defined detection and mitigation procedures, must be put in place. As a result, intrusion detection methods to protect against such attacks are becoming increasingly important. An intrusion detection system (IDS) is a promising solution to identify intrusions in WSNs. However, the IDSs in WSNs face new challenges due to the characteristics of WSNs, thus, there is a need for an IDS to work interoperability across the layers.  
  3.2 Cross-layer Intrusion Detection Systems Due to the numerous characteristics of sensor networks, such as their limited battery power supply, poor bandwidth support, self-organizing nature, and dependence on other nodes, there is a significant risk of security attacks in all OSI model layers. A single or a series of attacks may be made. Several specific attacks occur at regular intervals, such as blackhole attacks, rushing attacks, and flooding attacks. It has been noticed that circumstances may result in several attacks rather than a single attack. As a result, it is  
  Packet delivery ratio, energy consumption, RSSI, bad packet ratios  
  Bengag et al. (2023) [23]  
  Fuzzy logic system  
  Trust value  
  Kumar et al. (2023) [30]  
  Trust model and verification  
  routing mechanism and make it resistant to both insider and external attackers, it might be expanded to add security features such as with an IDS. WSNs use energy to gather information about their surroundings, process it, and send the resulting data. The IDSs must therefore use the least amount of energy feasible to leave enough for the WSN’s vital operations. IDSs are crucial for the security of WSNs, and those created for them need to have specific features like low power usage. The success of an IDS in a WSN depends on the way it affects the network’s energy usage as a WSN is resource constrained. Maintaining a network over its lifespan is one of the biggest issues in WSNs, so energy efficiency in IDSs is equally important. WSN sensor nodes have limited storage capacity. Therefore, it is challenging to meet the need to store attack signatures in sensor nodes. In order to create an IDS in the WSN to identify various sorts of attacks, machine learning techniques were mostly utilized. The drawback of those techniques is that they require more memory to deploy a model to a sensor node and take longer for machine learning algorithms to build and evaluate data sets for WSN. It could be conceivable to develop a hybrid or cloud-based machine learning prototype for carrying out intrusion detection in the WSN to reduce the amount of memory required in the detection techniques. Another point to consider is many of the IDS schemes available do not provide self-defense. It is crucial because certain attackers may frequently generate false alarms by flooding the IDS host with irrelevant traffic. The host can run out of resources as a result, leaving the system open to intrusions. IDS’s ability to protect itself is thus desirable.  
  5 Conclusions WSNs face numerous cyberattacks that pose risks to the network’s availability, privacy, control, and reliability. These attacks exploit the vulnerable nature of nodes deployed in hazardous and remote environments, where they often remain unattended, unable to protect the information flow physically. As a result, there is an increased likelihood of node compromise, leading to decreased network security and protection. It is crucial to implement robust security measures to safeguard these networks against breaches and assaults. One effective approach is the adoption of a cross-layer intrusion detection system, which provides comprehensive protection across multiple WSN layers. This paper reviews the existing IDS at each of the layers and cross-layers for WSN in terms of the attacks and approaches. Cross-layer IDS can detect early signs of advanced attacks exploiting multiple layers’ vulnerabilities. They reduce evasion techniques by analyzing data from multiple layers, making it harder for attackers to evade detection. However, it’s important to consider WSN’s limited resources and constraints when designing and implementing cross-layer IDS. Thus, a more energy-efficient cross-layer IDS for WSN needs to be developed and improved from the existing IDS.  
  Cross-layer Based Intrusion Detection System  
  Kedah, Malaysia [email protected]  2 Department of Computer Science, College of Computer Science and Engineering, Al-Ahgaff University, Hadhramaut, Yemen  
  Abstract. The social networks and news ecosystem provide valuable social information, however, the rise of deceptive content such as fake news generated by social media users, poses an increasing threat to the propagation and diffusion of fake news over the social network and among users. Low-quality news and misinformation spread on social media had negative impacts on individuals and society. Hence, it is essential to detect fake news to ensure the spread of accurate and truthful information. To address this problem, a new approach using Binary Bat Algorithm (BBA) for fake news detection (FND) on Twitter data is proposed in this paper. Twitter data usually generates massive feature space which might consist of irrelevant features that could jeopardize the subsequent process. The proposed FND approach involves four stages, namely data collection, pre-processing, feature extraction, and fake news detection. The proposed techniques are tested on PHEME dataset, and the experimental results are measured in term average of Precision (PR), Recall (R), F-measure (F), and Accuracy (ACC). The experimental results show that the BBA algorithm has outperformed the Social Spider Optimization (SSO) algorithm. Thus, BBA is a promising solution for solving high-dimensionality feature space in fake news Twitter data. Keywords: Fake news detection · Binary Bat Algorithm · Social Spider Optimization · Feature Selection · Text Mining  
  can be used to identify patterns in language use that may be associated with fake news. Like NER, POS is dependent on existing lexicons. Besides the above studies, in some fake news detection models, several researchers have omitted the feature selection phase that caused poor performance since the training data can be biased and overwhelmed which ultimately hinders the subsequent process. Hence, this paper proposes a feature-based optimization approach for fake news detection on social media using BBA and SSO with K-Means clustering. The approach involves extracting relevant features from Twitter data, and optimizing these features before clustering them with K-Means. The resulting optimized features are to train a machine learning model for fake news detection. The proposed approach aims to achieve high accuracy in fake news detection. The remainder of this paper is organized as follows. Section two provides related works in clustering analysis. The proposed feature-based optimization approach for fake news detection on social media using K-Means clustering is presented in section three. Section four describes the experimental setup and the results obtained. Finally, section five presents the conclusion and future work.  
  2 Related Works in Clustering Analysis Cluster analysis is a technique for finding regions in n-dimensional space with large concentrations of data. These regions are called “clusters”. Data are sorted into groups in clustering analysis based on predetermined principles. Its function is to categorize the data so that there is a significant degree of similarity within classes and a minor degree of similarity between classes. Although there are many different types of clustering algorithms available right now, each one has unique properties and applications. In general, there are different perspectives in categorizing the data, such as: a) Partition method: The partition method is an iterative relocation algorithm that reduce the clustering criteria by reallocating the data points between clusters until convergence occurred. One of the common algorithms under this method is K- Means [4]. b) Hierarchical method: Hierarchical clustering is an algorithm that builds a hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left. The results of hierarchical clustering can be shown using a dendrogram. The issue with this method is the emerging and splitting of clusters are complex and errors generated cannot be revised. Moreover, this method is difficult to handle complex datasets. c) Density-based method: Density-based Clustering method is based on determining regions where points are concentrated and those regions where the data points are separated by vacant or sparse regions [5]. Points that do not belong to any cluster are assigned as noise/outliers. One of the disadvantages of density-based clustering is it unable to handle high-dimensional datasets. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is representational of the density-based clustering method.  
  F. K. Ahmad et al.  
  3.2 Phase II Data Pre-processing Data pre-processing is a crucial step in text mining, where the raw data is prepared for subsequent processes. In general, there are some common techniques used in data preprocessing such as (1) tokenization, (2) stop word removal, (3) stemming and lemmatization, (4) lowercasing, removing special characters and punctuation and (5) spell checking and correction. Some of these techniques can be applied to the raw dataset depending on its need and the requirement of the task at hand. At the stage of data pre-processing, several techniques have been applied and some parameters are adjusted to obtain optimal results. In this study, the TextBlob library is used to tokenize the words into features. TextBlob is a Python library for processing textual data. 3.3 Phase III Feature Extraction Once the data is cleaned, the next phase is text feature extraction. Text feature extraction is the process to transform the pre-processed data into vectorization space which can be used as input for the machine learning model. The main aim of this phase is to reduce the high dimensionality features to more efficient and meaningful vector space. Generally, there are three common feature extraction techniques as explained below: 1. Bag of words: Bags of words is a representation of text according to its occurrence in the document. This technique counts the words and represents them based on their frequencies in a given document or corpus. The matrix created represents unique features however, this technique may lead to a spare matrix that often hinders the downstream process. 2. Term frequency-inverse document frequency (TF-IDF): TF-IDF is a technique based on a weighting mechanism that calculates the importance of words in a document or corpus. This technique measures the frequency of words and inverse document frequency of words across the documents [6]. 3. Word embedding: Word embedding is a technique with a dense vector representation that captures the semantic relationships between words. Word embedding is usually trained with a large corpus and produces a small number of feature dimensionality in comparison to a bag-of-words or TF-IDF. In this study, the TF-IDF technique is used to extract feature vectors. Two factors are measured, which are frequency (TF) and inverse document frequency (IDF). TF measures how many times the term appears in the document while the IDF indicates the scarcity of words across the documents. Formulas to calculate TF is given in Eq. (1) while Eq. (2) shows the IDF formula. (1)  
  tf i,j= ni,j  
  Fig. 2. Feature space generated using the TF-IDF  
  This paper aims to automate the discovery of the number of clusters and their respective centroids using BBA. The following section describes the K-Means clustering and BBA proposed in this study. Binary Bat Algorithm (BBA). BBA is a meta-heuristic technique that is inspired by the behaviors of bats. This technique used the echo property of bats as a medium of communication in determining its prey [7–9]. Generally, BBA involves four phases as given below:  
  Description  
  Step 3 Recalculation: Recalculate the mean of all data points to assure all data points are assigned to the respective centroid Step 4 Repetition of Step 2 and Step 3 until convergence occurred: Reallocating data points to their nearest centroid and recalculate the centroids until no data point changes its assigned cluster Step 5 The final k centroids and the cluster assignments for each data point are determined  
  The k-means algorithm intends to reduce the within-cluster sum of squares, by measuring the distance between the data points to their assigned centroids. Data points that are close to assigned centroids will be grouped in the same cluster. In this study, we have used the elbow method or silhouette in determining the number of clusters, k.  
  Abstract. Arabic people use Arabic dialects on social media platforms to express their opinions and connect. Due to the absence of standard rules or grammar, Arabic dialects are more challenging for NLP tools to analyze than standard Arabic. While most review studies in this field have focused on highly indexed databases such as Scopus, Web of Science, and IEEE, these databases are not accessible to many Arabic researchers in Arabic countries due to financial constraints. This review study explores recent research and studies published in different databases to address this gap. The study identifies the most common sentiment analysis approaches, preprocessing and feature extraction techniques, and classification and evaluation techniques used in this field. The authors found that Twitter is the most commonly utilized source for researchers to collect their datasets, and machine learning approaches are the most commonly used for sentiment analysis in Arabic dialects. Overall, this study provides valuable insights into the challenges and opportunities for sentiment analysis in Arabic dialects. Keywords: Sentiment Analysis · Arabic Language · Arabic Dialects · Machine Learning · Classification  
  A. Habberrih and M. A. Abuzaraida  
  and education, whereas DA is an informal version of Arabic [2]. DA is used in daily life for communication. DA is different from country to country and even from city to city. However, there are six popular types of dialectal Arabic, namely Maghrebi, spoken in northern Africa, Khaliji spoken in the Arab Gulf area, Shami spoken in Jordan, Lebanon, Palestine, and Syria, Egyptian spoken in Egypt, Sudanese, and Iraqi spoken in Sudan and Iraq [4]. The Arabic language poses several challenges in SA, are addressed in [6, 11] and a study that focused on Saudi dialects, as described in [7, 10, 20]. One of the key complexities in dealing with dialectical Arabic is the absence of standard rules or grammar. Regarding sentence structure and morphology, Arabic sentences can start with a noun phrase, verb, or nominal phrase. Moreover, Arabic exhibits numerous syntactic variations within all sentence types. Additionally, dialect natives may express their opinions using different dialects and slang words and abbreviations. Furthermore, repetition of letters may be used to show emotion and emphasis, leading to spelling errors. Many Arabic individuals use DA to convey their opinions on social media platforms like Twitter and Facebook. Consequently, many researchers have focused on studying DA rather than MSA [8, 20]. The main objective of this study is to examine the most notable research conducted on DA and emphasize the available datasets, common preprocessing techniques, and Machine Learning (ML) approaches commonly used to classify DA sentiment.  
  2 Methodology This study conducts a comprehensive survey for the previous studies in Arabic dialect sentiment analysis. Here, the techniques used by the past studies in each phase of processing the Arabic sentences are highlighted. These phases are listed: Arabic dialect datasets, preprocess, feature extraction, and classification techniques. The search strategy of this study is explained in Sect. 2.1. 2.1 Search Strategy Many recent studies have been carried out to present a systematic review in this field, as evidenced by the works of [2, 4, 7, 9]. However, these studies have exclusively employed highly indexed databases such as Scopus, Web of Science, and IEEE. Notably, publishing in these indexed databases is not preferred for many Arabic researchers in Arabic countries, mainly due to financial constraints. The primary reason for this reluctance is the lack of financial support, discouraging researchers from investing heavily in publication costs. Consequently, many researchers publish their work in local or non-indexed journals, even if their papers represent high-quality research. Against this backdrop, this study aims to conduct a comprehensive survey to review as many studies as possible instead of solely focusing on limited databases. The scope of this study is limited to cover the ML and Hybrid approaches of SA.  
  Sentiment Analysis of Arabic Dialects: A Review Study  
  A. Habberrih and M. A. Abuzaraida  
  After the preprocessing process, the next step is feature extraction, which involves identifying the most effective features for the sentiment analysis process and removing irrelevant, redundant, and noisy data. This step reduces the dimensionality of the feature space and the processing time, ultimately improving the efficiency and effectiveness of the analysis [17]. Feature frequency (FF), Term Frequency-Inverse Document Frequency (TF-IDF), feature presence (FP), Word Embedding (WE), Part of Speech (POS), and N-grams methods are the most commonly used methods for the features extraction phase [16]. 2.4 Classification Techniques The process of SA can be achieved through three primary approaches: Lexicon-Based (LB), Machine Learning (ML), and Hybrid approaches. The Lexicon-Based approach can be divided into two techniques: Corpus-based and Dictionary-based. Lexicon-Based techniques analyze a sentiment lexicon, a compilation of words with corresponding positive, negative, or neutral polarity labels [20]. However, this approach is excluded from this study due to its structure which is completely different from the ML and Hybrid approaches. The ML approach involves the training of models on pre-labeled data, and it is categorized into three approaches: Supervised, Unsupervised, and Semi-supervised learning. The Supervised learning technique, also known as Classification or Regression, requires two subsets of data: a training set of labeled data and a testing set. The accuracy of this technique is dependent on the training set and the algorithm used. Unsupervised learning, also known as Clustering, is used for unlabeled data and aims to create clusters of data points, with similar points grouped in the same cluster and dissimilar points in different clusters. Semi-supervised techniques combine the advantages of both Supervised and Unsupervised techniques. The third approach used in SA is the Hybrid approach, which combines the Lexicon-Based and Machine Learning approaches [21]. Furthermore, a comparative study of some of the works invested in Arabic dialect sentiment analysis is presented in Table 1. Note that N/A means Not Available; DC: Data Cleaning; SWR: Stop-words removal; W2V: Word2Vec; NER: Named Entity Recognition; E: Evaluation; BR: Best Results; SR: Some Results.  
  4 Conclusion and Future Work Sentiment analysis is a significant application of NLP due to the rich source of information provided by textual data. However, developing NLP models and tools for Arabic dialects is challenging, given the limited written resources for many of these dialects. Unlike MSA, which has a rich corpus of written resources such as news articles, books, and academic papers, Arabic dialects often lack standard written forms and are primarily used in spoken communication. This study has investigated the studies that have been recently published and revealed that most researchers collect their datasets from Twitter, Facebook, and YouTube. Furthermore, studies were predominantly applied to dialects spoken in Saudi Arabia, Tunisia, Iraq, Algeria, and Sudan, with fewer studies applied to Libyan, Syrian, and Yemeni dialects. Machine learning approaches were the most utilized in sentiment analysis of Arabic dialects, with the best results obtained using this approach—however, most papers employed Arabic NLP tools rather than building specific tools for the studied dialect. Future work will aim to build a sentiment analysis model and compare different machine learning classifiers for the Libyan dialect.  
  Total Publications (TP)  
  Conference Paper  
  Percentage (%)  
  Citations per Paper  
  12.08  
  Figure 3 depicts a visualization map that demonstrates the connections among author keywords, citations by documents, and bibliographic coupling by authors. The varying colors, font sizes, and thickness of the connecting lines signify the strength of the relationships between the keywords. Keywords that share the same color are frequently listed together, indicating their close association and tendency to co-occur. For example, the diagram shows that E-Participation, Social Media, Social Networking (online), and Electronic Participation are highly related. Additionally, after excluding the core keyword (E-Participation) specified in the search query, the keywords with the highest occurrences are “Government Data Processing”, “E-government”, “Decision-Making”, “Public Policy”, and “Electronic Participation”. This figure is helpful for researchers to visualize the relationships between different keywords in the field of E-Participation research and to identify potential research topics or themes.  
  H. Awang et al.  
  Fig. 3. Network visualization map of the author keywords.  
  S. O. Haroon-Sulyman et al.  
  2 Review of Related Literature 2.1 Algorithmic Features In recent times, researchers have examined the simultaneous use of models such as Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) [3], Convolutional Neural Network (CNN) with Bidirectional Long Short-term Memory (BiLSTM) [10], Long Short-term Memory (LSTM) with Bidirectional Long Short-term Memory (BiLSTM) [11, 12] to mitigate text data vanishing gradient issues. CNNs commonly used in image data contain spatial features that, when combined with LSTM, are effective in handling sequential and temporal text data; this sometimes results in a potential model for the vanishing gradient. CNNs can extract local text features, while LSTMs capture contextual features and long-term dependencies. However, as much as this approach has shown promising results in research, its limitations still lie in unsuitability for large-scale datasets as it requires time and multiple network layers, leading to model complexity and lack of interpretability. [3] used CNN and LSTM for sentence classification, which is an NLP approach. The study observed that even though the model could learn longterm dependencies, their experiments show that the model performance is affected by dataset size, classifiers, and gradients vanishing. A similar study [10] used CNN and BiLSTM deep neural networks for a text sequence classification task. The model was able to extract semantics, which is an essential feature in long text sequences, though the delay was encountered in the processing time due to the sequential process in the BiLSTM architecture. Similarly, the sentiment analysis task was performed using deep networks. Authors observed that the proposed framework is prone to limitations in cases of imbalanced or text bias in capturing diversity [11]. To improve performance, especially in the case of imbalanced datasets, some studies used multiple embedding techniques in addition to the network layers. [13] proposed a model that combines CNN and LSTM with word2vec and GloVe word embedding representations for sentence-level classification. Their model was able to capture different various representations of word embedding in the input layer. The results showed that it outperformed the models with a single embedding technique though it is timeconsuming and may not be efficient for large datasets. Also, [14] proposed a model on CNN-LSTM with an attention mechanism, where the attention mechanism is useful to focus and capture relevant information. However, its use is dedicated to the specific study, which may not be suitable in all cases. Other algorithmic features that have been commonly used in research to mitigate the vanishing gradient problem is the combination of activation functions [15–24] such as Rectified Linear Unit (ReLU), sigmoid, tanh, and SoftMax function, which has been used to improve the deep learning training by involving them in one or more of the network layers. A commonly used activation function is ReLU which has been used in various NLP tasks [15–18, 25]. Studies show that its major challenge is the dying ReLU which causes neurons to stop learning and eventually die. 2.2 Regularization Technique This has been widely used as a means of mitigating the vanishing gradient challenge with techniques such as dropout [13, 26, 27], early stopping [13, 15, 28, 29], batch  
  Systematic Literature Review and Bibliometric Analysis  
  Fig. 1. PRISMA flow diagram  
  Step 2 VOSviewer is a valuable tool for researchers in bibliometric analysis. It helps visualize and understand complex networks from scholarly literature and identify research trends and gaps, thereby making informed decisions regarding collaboration, resource allocation, and future research directions surrounding the DNN and vanishing gradient issue. Combining these analytical approaches aims to ensure quality information can be reproduced from the study. Furthermore, the Scopus database was chosen to gather relevant bibliographical data due to its functionality in generating a wide range of results.  
  Systematic Literature Review and Bibliometric Analysis  
  Microsoft Excel was also used to collect data in structured form for a rigorous review of literature related to “vanishing gradients” and “text data.” This has enabled easy identification of patterns from the existing studies and suggests new ideas to overcome the challenges for further research. 3.2 Data Analysis Process This is important to achieve the aim of this study and guide further research. The data analysis includes literature selection, information extraction, information synthesis, and bibliometric analysis. Table 1 further shows the top 10 most cited documents related to DNN and their various NLP applications. It is observed that the most common NLP applications focus on short-text sentence tasks such as sentiment analysis and text classification. It also provided insights into the various techniques for handling the vanishing gradient problem. As a result, the information provided in the table should be valuable to researchers in this domain and serve as a reference point for future work. Table 1. Top 10 highly cited documents No  
  Author/Title  
  Task performed  
  No  
  Author/Title  
  4  
  Table 1. (continued) No  
  Author/Title  
  Task performed  
  Figures 2, 3, 4 give insights into exploring the selected text data. The next section will discuss observations and trends obtained from this data. Cluster 1 centers on text data, classification, and sentiment analysis. It provides potential insights into methodologies for analyzing and classifying textual data and provides insights into exploring the performance evaluation, methodologies for exploring transfer learning, and experimental outcomes analyses within the dataset. Cluster 2 This cluster highlights deep learning, recurrent neural networks RNNs, and NLP. It offers insights into the practical application of deep learning models, RNN architectures, and NLP techniques relevant to the dataset.  
  4 Findings The clusters and their associated terms provide an overview of the various approaches and techniques employed in research to address the vanishing gradient issue in text data. It is, however, important to note that this review paper serves as only a starting point in the analysis of this challenge. There is a need for further research in terms of other domain–specific contextual analysis for a more explicit solution to the vanishing gradient issue in text data.  
  Systematic Literature Review and Bibliometric Analysis  
  2 Offensive Words in the Malay Language English language datasets are the focus of most current research compared to other languages [11]. English, Spanish and French is recognized as high-resource languages [12]. In contrast, Malay is one of the low-resource languages [14]. High-resource language refers to numerous datasets that are currently available, and low-resource language refers to limited datasets [13]. Malay is spoken in Malaysia, Indonesia, Singapore, and Brunei. Despite their shared basis and incredible similarities, Malay dialects vary in each country [14]. Due to its low-resource nature, there are very few datasets related to cyberbullying in the Malay language [15]. Most recent studies on cyberbullying detection utilize Twitter datasets in the Indonesian language [2, 16–18].  
  N. Ismail et al.  
  Due to the close similarity of the Malay language used in Malaysia and Indonesia, this research adopted the guideline of determining offensive words from the research conducted by Ibrohim and Budi [18]. Offensive words are usually associated with conditions, animals, astral beings, objects, parts of a body, family members, activities, or professions. Table 1 explains further about the categories of offensive words referenced in the Indonesian language, which are also relevant to the Malay language in Malaysia. Table 1. Categories of offensive words in Indonesian and Malay languages. Category  
  Conditions  
  4 Conclusions This paper presented a new test collection for cyberbullying and language in the Malaysian language. Additionally, this paper outlined the methodology to build a test collection that includes a series of offensive language written by Instagram users. A series of baseline experiments were performed to evaluate the model performance and provided a report.  
  A Test Dataset of Offensive Malay Language  
  This paper also discussed the types of offensive language identified in the Malay language used in Malaysia. Due to the limited number of cyberbullying datasets for the Malaysian language, this new collection can be used as a reference to foster research related to cyberbullying in Malaysia. Future works aims to extend this research by exploring cyberbullying behaviour in the bystanders’ comments and building a corpus specifically for Instagram in the Malay language. This research would be beneficial for facilitating cyberbullying research in Malaysia by providing a complete corpus in the Malay language. Furthermore, it is also important in supporting society’s awareness of their roles in the effort to curb cyberbullying as well as for the authorities to witness the cyberbullying incidents in Malaysia.  
  2 Related Work A topic modelling experiment based on user comments on social media is shown in the study [3]. The author conducted an experiment using two datasets from Yahoo and Tokyo Electric Power Company (TEPCO), which covered the most popular news stories and video streaming comments, respectively. LDA was used to implement topic clustering based on topic modelling. This experiment received 15,000 comments throughout the same period. The results of the modelling are displayed in Fig. 1.  
  Fig. 1. Modelling’s outcome  
  In the [4] study, Twitter social media was searched for information about the Covid19 epidemic from March 3rd to March 31st. This author copied ten thousand tweets. The outcomes of clustering based on latent semantic analysis produced more clusters than clustering based on LDA. Because the largest cluster would show the day of trend, it was used as a comparison. The total number of confirmed cases in each nation and worldwide are shown in Fig. 2 [4].  
  Fig. 2. Number of confirmed cases  
  This study [5] focuses on the two-step problem of extracting semantically relevant themes and trend analysis of these subjects from a large temporal text corpus utilising an end-to-end unsupervised technique. The author first created word clouds based on the frequency of terms in each cluster of abstract text. As a result, terms that were less prevalent and significant to the cluster were deleted from word clouds. The author generated word clouds based on the TF-IDF scores of phrases belonging to a cluster. The TF-IDF based on a word cloud of four distinct clusters is displayed in Fig. 3 [5].  
  Fig. 3. TF-IDF based on a word cloud of 4 different clusters  
  Fig. 4. Process of LDA  
  In [7], a new spammer classification method is based on the LDA topic model. This technique captures the spamming essence by retrieving global and local data regarding topic distribution patterns. Clustering based on online spam detection is proposed to discover spammers that appear to be posting legal tweets but are difficult to identify using existing spammer categorization methods. The examination of the K-means technique produces an accurate result, and it also identifies spammers on social media. A past study from [8] explained the process of document clustering by using Kmeans and K-medoids algorithm (see Table 1.). This study focuses on hundreds of documents collected from Entertainment, Literature, Sports, Political, and Zoology. The authors implement the K-means algorithm on the WEKA tool and K-medoids on the Java platform. Both results of each algorithm are compared to get the best cluster. Based on Table 1, each cluster defines documents of a particular domain topic. According to the result, the authors conclude that the K-means algorithm is more efficient than the clusters obtained from the K-medoids algorithm [8]. The following word clustering experiment, carried out by [9], focuses on grouping Chinese words using LDA and K-means in accordance with five categories: politics, economics, culture, people’s livelihood, and science and technology. The Latent Dirichlet Allocation (LDA) algorithm and the k-means clustering algorithm are combined in a novel approach that is put forth in this work. The highest probability of each topic is picked as the centroids of k-means after some topics are retrieved using LDA. In the final stage, the K-means algorithm is employed to group every word in the text [9]. The authors calculate the K-means centroids using the LDA algorithm findings and utilise the Chinese word similarity calculation method to calculate the distance between the  
  TikTok Video Cluster Analysis Based on Trending Topic  
  Abstract. We live in an information world where visual data undergo exponential growth within a very short time window. With diverging content diversity, we simply have no capacity to keep track of those data. While short video platforms (such as TikTok™ or YouTube Shorts™) can helped users viewing relevant videos within the shortest time possible, those videos might have misleading information, primarily if it is derived from long videos. Here, we analyzed several short videos (in terms of movie trailers) from YouTube and established a correlation between one movie trailer and the classified movie genre based on the emotion found in the trailer. This paper contributes to (1) an efficient framework to process the movie trailer and (2) a correlation analysis between the movie trailer and movie genre. We found that every movie genre can be represented by two unique emotions. Keywords: movie analytic · face detection · emotion recognition · video summarization  
  2 Related Works The use of electronic nose machine learning for identifying the various characteristics of foods or drinks was covered in a number of literature studies. Conventional techniques like high performance liquid chromatography (HPLC), microbiological cell counting, mass spectrometry, and gas and liquid chromatography are cumbersome because they require time-consuming and laborious sample processing [7]. To run these conventional methods, skilled personnel were also required. Therefore, e-nose technology has been used in the food industry due to its ease, cost effectiveness, and close connection to sensory panels. In a number of areas of food safety assessment, electronic noses have recently become known as potential tools for quick, early detection of contamination and defects in the food production chain. By identifying its distinctive components and studying its chemical components, an e-nose may recognize an odour [8, 9]. The electronic nose (E-nose) is a group of electronic gas sensors that can detect volatile compounds in the headspaces of food product samples with high sensitivity and selectivity [10]. It resembles the human sense of smell to some extent [10–12]. Figure 1 shows the correlation of human nose with electronic nose. According to Fig. 1 above, gas sensors and sensing materials were used in place of the odour receptor cells to mimic a biological olfactory system. A computational algorithm, an artificial neural network, and data analysis software are used in place  
  E-Nose: Spoiled Food Detection Embedded Device Using Machine Learning  
  Fig. 2. Food Samples in Fresh Condition: (a) Sup Daging, (b) Nasi Goreng Daging, (c) Ayam Masak Kunyit, (d) Ayam Masak Lemak, (e) Ayam Goreng Berempah and (f) Sawi Masak Air.  
  3.2 Experimental Setup The food’s emissions of gases can be detected using four (4) gas sensors. The data from the sensor to the computer in this study is processed and converted using an Arduino microcontroller and a data collection programme called PLX-DAQ. The list of sensors utilized in this investigation was provided in Table 1 below, along with the types of gases the sensors detected. 3.3 Data Analysis Methods The present study involved the utilization of Support Vector Machines (SVM) and kNearest Neighbors (k-NN) in the analysis of data gathered from gas sensors, as well as the classification of the degree of food contamination. The datasets utilized in this study were characterized by an imbalance in the amounts of dependent variables, which may result in inaccurate classification algorithm performance due to unevenly distributed class label characteristics. To address this issue, oversampling and undersampling techniques were  
  E-Nose: Spoiled Food Detection Embedded Device Using Machine Learning  
  Based on the summarized outcomes presented in Table 2, it is apparent that the level of accuracy in food classification varies between different food samples. This difference is due to the various ways of cooking the food samples such as frying, cooking proteins or vegetables in soups and cooking proteins or vegetables using coconut milk. These different cooking methods can also influence the spoilage rate of the food samples. Hence, it can be inferred that the accuracy of classification outcomes is impacted by the techniques employed for preparing the food samples.  
  Education University, Amman, Jordan [email protected]  2 InterNetWorks Research Lab, School of Computing, Universiti Utara Malaysia, Kedah, Malaysia 3 Data Management and Software Solution Research Lab, School of Computing, Universiti Utara Malaysia, Kedah, Malaysia  
  Abstract. The routing protocol known as RPL is employed in low power and lossy networks. It makes use of an objective function (OF) to establish a Destination Oriented Directed Acyclic Graph (DODAG) and ascertain the most suitable parental candidate or trip route. Nevertheless, the task of identifying a suitable OF in Low Power and Lossy Networks (LLN) is a significant challenge. The RPL was intentionally designed to possess a high degree of flexibility, allowing for the construction of routing topologies without imposing any specific routing metric or constraint. This design choice was made to accommodate the diverse range of LLN that exist. This study provides a critical overview of recent literature pertaining to the topic of RPL and specifically focuses on the many strategies aimed at enhancing OF inside the RPL protocol. The objective of this study is to provide an analysis of relevant endeavors, including the development of innovative metrics and the application of fuzzy logic techniques in the combination of OF metrics. Furthermore, this paper discusses the recommended augmentation strategies, as well as constraints and future development directions. The research community can employ the findings to gain a deeper comprehension of objective functions and improve the performance of RPL in the face of security problems. Keywords: RPL · OF enhancement · IoT · RPL performance · Fuzzy Logic  
  L. Al-Qaisi et al.  
  devices utilized on IoT [2]. Hence, an efficient routing protocol in this setting is both a requirement and a difficult problem to study [3]. A new IPv6 protocol, termed Routing Protocol for Low Power and Lossy Networks (RPL), has been suggested by the IETF ROLL working group to address the needs of low-power and lossy networks (LLN) [4]. As a promising protocol, RPL offers various benefits for tiny devices, that it can adapt to and manage shifts in network architecture and implementation, which is one of its primary strengths. It is also employed to address issues unique to LLN, such as traffic congestion [5], imbalanced consumed energy [6], and load balancing [7]. Yet, choosing the best route to the destination still represents a significant challenge for RPL [8]. Consequently, numerous research studies have addressed this concern and recommended various enhancements for the best parent/best path [9]. For advanced IoT applications to succeed, it’s essential to select a path between sensors that is both fast and has minimal data loss [10]. The routing quality can be enhanced by employing a suitable objective function (OF). Yet, several scenarios, including network scalability, mobility [11], security [12, 13], and topology changes, might pose challenges for applications built on LLNs. More packet loss, shorter network life, higher overhead, and higher energy consumption are all possible outcomes of certain worst-case scenarios [10–13]. This article will focus on OF enhancements. The OF in routing protocols finds the optimum path to a destination. A good route meets power consumption, network durability, convergence speed, and connection quality parameters like ETX and PDR. The rapid development of OF attracts LLN researchers. This paper discusses the most important efforts to assess and enhance the objective function, prompted by the lack of earlier RPL objective function surveys. This research critically evaluates OF modification methods for RPL routing service improvement. RPL should be explained thoroughly. Discussing OF modification approaches. Then fuzzy logic is discussed. The rest of the paper is organized: Prelims are explained in Sect. 2. The paper’s methodology is in Sect. 3. Modification procedures are in Sect. 4. The results are thoroughly discussed in Sect. 5. Section 6 discusses directions and opportunities.  
  2 The Objective Function in RPL It’s one of the most important parts of RPL because it helps build the topology using the Directed Acyclic Graphs (DAG) concept and the distance vector technique to create a tree-like structure, or Destination Oriented Directed Acyclic Graph (DODAG), which regulates links between reachable nodes [14]. DODAG root and RPL routers exchange data between source and destination nodes [15]. RPL’s DODAG construction process depends heavily on the networks OF. Route selection also affects network efficiency. The metrics and limitations used to establish the ideal path from a node to the root can improve or damage network performance [16]. 2.1 Standard Objective Functions (oF) RPL defines two standards OF, Objective Function zero (OF0) and Minimum Rank with Hysteresis Objective Function (MRHOF).  
  L. Al-Qaisi et al.  
  network-condition-based routing flexibility. Metrics give RPL extensions a complete network picture and enable intelligent routing decisions that improve energy efficiency, dependability, latency, and service quality. RPL routes data in resource constrained and lossy IoT environments using many variables. 2.2 Routing Metrics RPL was ratified by the IETF ROLL working group as an IPv6 routing protocol to meet LLN lossy link and limited node requirements. RPL supports point-to-point, multipointto-point, and point-to-multipoint topologies [9]. RPL updates routing topology and information using four ICMPv6 control packets. DODAG Information Object (DIO) preserves node rating and root distance before finding best parent. Second, send DAO-containing up-ward traffic to parents. Third, joinable nodes receive a DODAG Information Solicitation for DIO messages. Finally, the DAO receiver confirms receipt with a DAO-ACK message [13]. LLN have distinct behavioral characteristics compared to wired and ad hoc networks. The most notable is that RPL is being used as the major routing protocol in these networks. Because of the OF, this protocol offers tremendous freedom in choosing routing metrics [25]. Routing metrics ensure path cost evaluation and the least restrictive path selection. Certain RPL implementations require multiple routing metrics and limitations, whereas others require only one [26]. Routing metrics can be static or dynamic, focus on the link or the node, emphasize quality or quantity, etc. However, routing metrics and constraints are different. The routing protocol may consider Both of these factors when determining the best route to take. In order to avoid potentially problematic links, a routing protocol may take advantage of a routing constraint. A routing metric selects its path according to the links that guarantee a certain level of reliability. The requirements for RPL implementation will determine which metrics or constraints will be imposed. The routing metrics also need to consider the network’s dynamic nature. LLN networks’ link or node metrics are dynamic and subject to change as the network functions. Here, we’ll look at the residual energy as a node metric for choosing a route. The network nodes’ inability to maintain their energy reserves gradually decreases their remaining power. Thus, the path calculation using this metric shift as the measure itself shifts and evolves. Both node and link metrics are explained as per [27, 28] and [29]. 1. Node Metrics: Hope Count: a common wireless network routing metric It is deployed for measuring network path length. Energy: reports network node power consumption. Location and distance from the sink may cause some nodes to lose energy faster than others. 2. Link Metrics: Throughput: amount of data to be exchanged between nodes over the network in a certain timeframe. More throughput means better performance. Latency: time to transport data across the network. The latency of a network is measured in milliseconds, and lower latency indicates a quicker reaction time.  
  An Analysis of Objective Function Modification Approaches  
  Expected Transmission Count (ETX): checks network reliability. It shows how many transmissions the destination needs to confirm data receipt. The root is best reached via the lowest ETX path. ETX’s high value shows the network’s instability. RSSI/LQI: The physical layer may precisely set a network’s signal, frequency, voltage, etc. RSSI and LQI are the most common radio link estimators. RSSI checks received frequency signals as a radio transceiver. Thus, a greater RSSI indicates a stronger radio signal and closer destination. LQI rates link reliability from 0 to 7.  
  2.2 Related Work The research by Shamantha, Shetty, and Rai [15] focuses on conducting sentiment analysis of tweets and reviews shared on Twitter. Its objective is to categorize opinions expressed in the text as positive, negative, or neutral. ML classifiers, including Naïve Bayes, Random Forest, and Support Vector Machine (SVM), are employed to assess sentiments using specific keywords. The theoretical framework of this work is rooted in sentiment analysis, a subset of natural language processing (NLP). The methodology blends techniques from NLP and ML to carry out sentiment analysis, involving the identification of particular keywords in tweets and reviews to discern expressed sentiment. Classifier performance is measured in terms of accuracy, precision, and processing time. Additionally, feature selection is applied to pinpoint the most relevant aspects for sentiment analysis. The results indicate that the NB Classifier outperforms the other two classifiers in both accuracy and speed. Moreover, concerning sentiment model performance, the study suggests that employing binary labels for sentiment analysis of tweets or reviews can be a beneficial approach for gauging overall sentiment as it simplifies classification and reduces complexity. Similarly, with binary labels, text sentiment can be categorized as either positive or negative, facilitating result interpretation. Additionally, binary labels prove valuable when text sentiment isn’t distinctly positive or negative, enabling a more nuanced sentiment analysis. As well, Wang and Wang [16] emphasize the need to safeguard plant biodiversity, which necessitates the identification of plant species. However, traditional plant species identification is challenging for the general public and even experts. In response, the study presents a few-shot learning method for leaf classification with a small sample size based on the Siamese network framework. The neural network architecture in this study consists of two identical subnetworks that share weights and distinguish between similar and dissimilar inputs. To extract features from two distinct images, a Siamese network structure is used, involving a parallel two-way Convolutional Neural Network (CNN). The learned metric space is then used for leaf classification with a K-Nearest Neighbor (KNN) classifier. The loss function used to generate this metric space aims to place analogous leaf samples close together and different leaf samples far apart. Additionally, the study proposes a Spatial Structure Optimizer (SSO) method to enhance leaf classification accuracy. The proposed method is evaluated on three datasets (Flavia, Swedish, and Leafsnap). Despite the limited size of supervised samples, it achieves high classification accuracy. The proposed method’s effectiveness is evaluated using the average classification accuracy as a performance metric and it achieves high accuracy with a small size of supervised samples. The use of the SSO method further enhances the accuracy of leaf classification. However, the study does not provide a comparison of the proposed method with other state-of-the-art methods for leaf classification, which may limit its applicability. Despite this limitation, the proposed method shows promise as a solution for leaf classification with a small sample size. The use of binary labels in the Siamese network suggests a recommendation for future studies.  
  Abstract. Mooring (Thin) lines are fabricated of polyester ropes, steel wire ropes, and chains. These are considered the essential components which are used to secure offshore marine vessels and floating facilities by keeping them in a fixed place and resisting external loads. However, the failure of any mooring lines because of anomalies can cause severe consequences including financial losses, loss of life, and harm to the environment. Thus, it is essential to determine the anomalies in mooring lines beforehand to ascertain reliable and safe offshore mooring operations. This paper furnishes a comprehensive review of various types of anomalies in mooring lines with their underlying causes, and risk mitigation tactics. Furthermore, the types of mooring lines including polyester ropes, chain, and steel wire ropes have been discussed with their advantages and disadvantages. Additionally, the real-time consequences of failure in mooring lines are explored which occur due to the anomalies in the mooring lines including but not limited to environmental damage, vessel drift, and collision. In order to reduce the risks associated with mooring line anomalies, this review concludes by summarizing the major findings and emphasizing the significance of proactive monitoring and maintenance. Keywords: Anomalies in Mooring Lines · Mooring Systems · Mooring (Thin) Line Failure  
  1 Introduction Mooring systems consist of mooring (Thin) lines that are used to keep floating structures and the offshore vessel stationary in deep water during the unloading of the hydrocarbon production. These offshore vessels shape like ships or boats. The mooring lines are disseminated into polyester or fiber ropes, chain, and steel wire ropes, and such types are utilized to fix a floating vessel in one place by connecting the mooring lines to the vessel which is further anchored to the seafloor [1]. However, mooring systems are used to prevent the offshore floating structure from drifting and to keep the vessels fixed from being affected due to the external force that offshore waves, currents, and strong winds may cause. Mooring systems fall under numerous categories which are considered based on the length, floating structure type, offshore water depth, and external sea conditions. Besides, many factors are involved in the design of the mooring system such as seabed constitution, length of the mooring ropes, and the strength of the anchors and weight [2]. However, the maintenance and the proper installation of the mooring systems are critical for protecting the marine environment and keeping the vessel safe [3]. Mooring lines have great importance in the offshore marine environment and failure of any line in the mooring system due to anomalies in the mooring lines can cause severe consequences. The anomalies in the mooring lines induce a substantial risk to the safety of the floating facilities. These anomalies are posed by numerous factors that may include poor maintenance, broken wires or chains, corrosion, and loose connections [4]. These issues can lead to a loss of stability or position of the vessel or structure, increasing the risk of collisions or damage to equipment, and posing a threat to the safety of personnel onboard. Besides, if a mooring line fails due to the existence of an anomaly in the mooring systems, it can cause the vessel or offshore structure to drift, potentially colliding with other vessels, structures, or even shorelines, resulting in severe damage or loss of life [4, 5]. Additionally, failure in the mooring system can cause hydrocarbon spills, leading to environmental damage and financial losses. Furthermore, the failure of a single mooring line can result in increased tension on the remaining mooring lines, potentially causing them to fail as well, leading to a catastrophic situation. Therefore, it is essential to regularly inspect, monitor and maintain mooring lines and address anomalies promptly to prevent any potential safety or environmental risks caused by the failure of the mooring system [6]. Before monitoring and addressing the anomalies in mooring lines, it is crucial to identify the different types of anomalies in mooring lines with their causes and risk mitigation strategies [4]. However, no comprehensive review paper defines the various anomalies in different types of mooring lines except [1, 7], along with their underlying causes and prevention. Besides, no real-time consequences of failure in mooring lines have been discussed in the literature except for a few instances in [8]. Therefore, this paper comprehensively reviews anomalies in different types of mooring lines, including polyester ropes, chains, and steel wire ropes, along with their underlying causes and risk mitigation strategies, as part of mooring systems. Additionally, the paper explores the real-time consequences of mooring line failure due to anomalies in the offshore marine environment, including vessel drift, collision, and environmental damage.  
  T. K. Khatri et al.  
  Furthermore, these objectives have been accomplished by an extensive analysis of research articles, existing studies, publications, and reports associated with the various types of mooring lines and their anomalies, causes, risk tactics, and real-life consequences of line failure. However, the said comprehensive information has been gathered by searching prestigious databases, conference proceedings, academic journals, and industry reports which have been found available through online sources and other technical websites. To the best of our knowledge, this paper represents the first attempt to comprehensively review the different anomalies in various types of mooring lines with their underlying causes and risk mitigation strategies while also discussing their real-time reported consequences of failure. This review paper is structured into the following sections. Section 2 describes the anomalies in mooring systems, their underlying causes, and risk mitigation strategies. A comparative analysis is also depicted in this section based on the advantages and drawbacks of each type of mooring line. The real-time reported consequences of failure in mooring lines are demonstrated in Sect. 3 with some examples. Finally, the entire study is concluded in Sect. 4.  
  2 Anomalies in Mooring Lines Mooring (Thin) lines normally comprise polyester or fiber ropes, steel wire ropes, and the chain. Each form of these mooring lines is thoroughly discussed in the following subsections with respect to the different types of anomalies in various sorts of mooring lines, their underlying causes, and the risk prevention tactics. Properly implementing the risk mitigation tactics can prevent the mooring lines from failing and may also assure the marine vessels’ reliable and safe mooring operation. Furthermore, Table 1 presents a comparative analysis aimed at selecting the most suitable mooring lines solution from among all available types, based on their respective advantages, and disadvantages in the context of mooring systems. 2.1 Anomalies in Polyester Mooring Ropes Fiber ropes are utilized in mooring operations in a wide range because of their durability, ability to withstand abrasion, and great strength. Even so, these ropes may still fail in a case when not maintained in the right manner or if they are experienced with specific anomalies. The following subsections discuss typical anomalies by which the polyester ropes are caused to fail when the mooring operations are carried out. Cut or Abrasion Damage. Abrasion takes place when the polyester ropes are rubbed in contact with the seafloor, other fiber ropes, or hull. It induces the fibers in the ropes to undergo scratches and wear which may result in undermining the structure of the ropes and can be susceptible to failure or breakage in the mooring rope. Abrasion in ropes can be caused by various factors, including but not limited to the rough sea, strong current, and acute edges of the surface by which the rope is rubbed [9]. Furthermore, some examples of damage in the fiber ropes are shown in Fig. 1. To mitigate the risk of such failure, obviating contact with abrasive surfaces or rough seas is essential. Besides the ropes need to be cautiously inspected to identify the damage  
  Anomalies in Mooring (Thin) Lines  
  or wear signs. In case of abrasion is observed, it must be either replaced with new material or completely removed to assure the unity of the rope. Thorough storage of the rope aids in precluding abrasion by storing the rope on a reel or drum from being tangled and avoiding rubbing the rope in contact with other surfaces [10]. UV Degradation. The exposure of sunlight to the polyester rope causes the fibers of the mooring rope to collapse overtime because of ultraviolet (UV) radiation in the sunlight that breaks down the fibers of the mooring rope which result in deterioration of the rope strength and may be susceptible to fail [11]. To mitigate the risk of such type of failure, placing the mooring ropes away from sunlight is significant when these are not in function. Chemical Damage. The damage to polyester mooring ropes is done when the ropes are in contact with solvent and oil chemicals which causes the fibers of the ropes to become weak and increases the chances of the rope failing [12]. The risk of such failure can be mitigated by obviating the leakage and spilling of oil close to mooring ropes. Besides, the ropes should be stored in a dry and well-ventilated area when they are not functioning.  
  Fig. 1. Polyester rope cut or abrasion damages [13].  
  Knotting. The fibers in the polyester ropes are compressed and deformed when the rope is knotted. Over time, this anomaly weakens the rope and makes it more susceptible to failure [14]. The risk of such an anomaly can be mitigated by avoiding the knot that may create unneeded stress on the rope [15]. Manufacturing Defects. The structure of the rope can be weakened due to the manufacturing defects in the polyester ropes. These defects may contain wear spots, defects  
  T. K. Khatri et al.  
  in the construction of the rope, or incompatibility in the fibers [16]. The risk of these defects can be mitigated by utilizing good quality mooring ropes that are constructed and produced by good manufacturers [15, 16]. Heat Damage. Polyester mooring lines are caused by heat damage which fails the mooring rope. When a rope is subjected to extreme temperatures, heat damage happens. Hot surfaces induce this anomaly, the friction generated by the motion of the fiber rope through equipping and the exposure to flames [7, 17]. It is crucial to place the ropes away from the heat origins to mitigate the heat damage anomaly in the fiber mooring rope. Besides, the ropes must be avoided in close contact with the hot surfaces and flames. In addition, it is very important to inspect the rope regularly to determine the signs of heat damage which may include melting of the fibers. To reduce the risk of failure, the rope must be replaced immediately if heat dam-age is suspected [18].  
  2.2 Anomalies in Steel Wire Mooring Ropes Steel wire ropes are utilized in mooring operations in a wide range because of their durability and great strength. Even so, these ropes may still fail in a case when not maintained in the right manner or if they are experienced with specific anomalies. The following subsections discuss typical anomalies by which the steel wire ropes are caused to fail when the mooring operations are carried out. Corrosion Fatigue. Coronary fatigue happens when steel wire is brought out to a corrosive environment and undergoes cyclic loading during mooring operations. Multiple factors can spread the small cracks in the wires and lead to the wire rope failure [1, 7]. The risk of corrosion fatigue can be mitigated by regularly inspecting the steel wire ropes to get the sign of corrosion and obviate revealing the mooring lines to extravagant loads. Steel Corrosion Cracking. Steel corrosion cracking happens when the steel wire ropes are subjected to high tension. Steel corrosion cracking causes cracks in the steel wire ropes when the mooring lines are exposed to high loads for an extensive delay [19]. To mitigate the risk of steel corrosion cracking, it is significant to determine the corrosion signs by regular inspection and avoid disclosing the line under high tension [20]. Human Error. Humans can also be part of the failure of steel wire mooring ropes. Wrong storage and handling of the mooring lines, absence of regular inspection of the mooring lines, and improper training of the staff are the factors that may lead to problems that enhance the chances of failure in the mooring line [1]. Establishing accurate processes, inspection, and monitoring procedures is crucial to reduce the risk of human error. Furthermore, proper training and supervision are necessary for the staff who perform mooring operations [20]. Vibration. When the frequency of vibration coincides with the natural frequency of the steel wire then the steel wire is more likely to become worn out and break. Over time, the lines are caused to fail [20]. The risk of such anomaly can be mitigated by measuring the length and the load of the mooring lines properly and obviating the reveal of mooring lines to the high frequency of vibrations [21].  
  Anomalies in Mooring (Thin) Lines  
  Environmental Factors. Exposure to sunlight that causes to generate ultraviolet radiation may weaken the steel wire mooring ropes. Over time, it may enhance the chances of failure in steel wire rope [7, 20]. To prevent environmental degradation, inspecting the mooring lines for signs of damage regularly and storing the lines in a cool, dry, and protected environment when not in use [20]. Creep. Creep occurs when the wire is under a constant load for an extended period, which can cause slow, permanent deformation of the wire [21]. To prevent creep, it is important to properly size the mooring lines and avoid exposing them to excessive loads for extended periods [1]. Manufacturing Defects. Manufacturing defects can occur during the wire drawing process, heat treatment, or quality control procedures. These defects can cause weaknesses in the wire, leading to failure over time [7]. To prevent manufacturing defects, it is important to work with reputable manufacturers and inspect the mooring lines for any signs of defects before using them [20]. Overloading and Broken Wires. Overloading occurs when the mooring lines are subjected to loads that exceed their capacity. This can cause the wire to stretch, deform, or even break. Very few amounts of broken wires in a steel rope at termination show high tension. This may be because of inaccurate fixing of the steel rope at the endpoints, managed badly at the time of recovery and deployment, some fatigue, or the due to overloading [22]. The local damage is caused by broken wires which are grouped in a neighboring strand or one strand. This is considered the worst situation when finding such a breakage of steel wire rope and this constraint can disturb the load balance that is conveyed by steel rope strands as shown in Fig. 2. To prevent overloading, it is important to properly size the mooring lines for the vessel and operating conditions and to avoid exposing the lines to extreme weather conditions or other sources of excessive loading [22].  
  Fig. 2. Broken wires at wire rope endpoint [1].  
  2.3 Anomalies in Mooring Chains Mooring chains are commonly used in mooring operations due to their high strength and durability. However, these chains can still fail if they are not properly maintained or if they are subjected to certain anomalies. Some of the common anomalies that can cause mooring chains to fail during mooring operations include:  
  T. K. Khatri et al.  
  Corrosion. Corrosion is a chemical reaction between metal and its environments, such as saltwater or moisture. Corrosion can lead to the loss of metal mass, weakening the chain’s structural integrity, and eventually causing it to fail. Corrosion can be accelerated by factors such as high humidity, exposure to saltwater, and extreme temperatures. Regular inspection and maintenance of the mooring chains are essential to detect and address any corrosion promptly [23]. Chemical reactions between the material and the environment can cause rust and corrosion. Expanding marine life may also increase the need for new mooring lines to pre-vent failure [7]. The mooring chains are typically found with corrosion in the splash zone as demonstrated in Fig. 3 (a and b). It is very belligerent to have a corrosion rate greater than 1 mm per year by relying on the temperate of the seafloor and the quality [1].  
  Fig. 3. Mooring chain with dense corrosion [1].  
  Fatigue. Fatigue failure occurs when a material is subjected to repeated loading and unloading. This repeated stress can cause small cracks to develop in the material, which can eventually grow into larger cracks that weaken the chain and cause it to fail. The risk of fatigue failure can be reduced by ensuring that the mooring chains are designed and manufactured to withstand repeated stress cycles and that the loading is distributed evenly across all chain links [23]. Overloading. Overloading occurs when the mooring chain is subjected to loads that exceed its maximum capacity. This can cause the chain to deform, stretch, or even break. Overloading can occur due to factors such as high winds, waves, or improper mooring techniques. To mitigate the risk of such anomaly, mooring chains are properly managed for the expected tension concerning their design, installation, and size to function within their great capacity [24]. Wear and Tear. Another anomaly known as wear and tear contributes to failure in the mooring chain. This type of anomaly occurs due to the friction of the chain tied to the seabed, other chain links, or the mooring chain connected to the Catenary Anchor Leg Mooring (CALM) mooring buoy [1]. It causes the materials to wear down, which reduces the diameter of the chain, loss of the metal, and finally leads to failure in the mooring chain [7]. Such anomaly can be mitigated by properly classifying the chain loads and it can be done by regularly inspecting the mooring chain to identify the signs of wear and tear in the form of reduction in the chain diameter and distortion. Furthermore, the proper maintenance and lubrication of the chain can also help to reduce friction and wear and tear [23]. In addition, it is important to monitor the mooring environment and take appropriate measures to reduce the risk of wear and tear, such as using a protective covering over the chain in areas where it is likely to come into contact with other surfaces or reducing the intensity of loading on the chain by using multiple mooring points [24]. Abrasions. Abrasion can occur when the mooring chain is subjected to contact with other surfaces or when the chain is bent and flexed. Over time, abrasion can weaken the chain and make it more susceptible to failure [7, 25]. Besides, Sediments on the seafloor can be abrasive, and friction can erode the chain when it encounters the bottom of the ocean [1].  
  Fig. 5. Ground touching mooring chain with one side material loss [1].  
  The mooring chain can also fail due to ground touching region as shown in Fig. 5 and such causes are considered seafloor abrasion [1]. To prevent abrasion, it is important  
  T. K. Khatri et al.  
  to ensure that the mooring lines are properly sized, positioned, and secured to avoid contact with other surfaces [24]. Deformation. Deformation occurs when the mooring chain is subjected to excessive bending or torsional stress, causing it to deform or bend out of shape. This can weaken the chain and eventually lead to failure [22]. Deformation is caused by overloading, and environmental conditions (waves, strong wind, and current). It can also be due to not properly installing the mooring system during the operation [1, 7]. The deformation risk can be reduced by checking the proper installation of the mooring chains that will be utilized for bearing the high loads, which is limited to their design capacity. Besides, causes of deformation can also be addressed promptly by regularly monitoring the mooring chains to diagnose the deformation-affected factors and then perform the required maintenance to fix them [4]. Chain Links. Chain links anomaly concerned with the failure in mooring chains which is caused by wear and tear, overloading, wear and tear and most often it can be found due to manufacturing faults. A failure of a chain link causes the adjacent links of the chains to fail and finally result in the failure of the entire mooring chain [23]. These anomalies can be mitigated by monitoring the mooring chain regularly to confront the signs of the above-mentioned factors and fixing them appropriately through proper maintenance of the mooring chains where the faults actually exist. Additionally, assurance of the designed mooring chain, manufacturing materials, and knowing the installation base for the expected load to be borne by mooring chains can aid in minimizing the risk of failure of chain links and deformation and results in ensuring secured and reliable mooring operations [4]. Improper Handling. The failure of morning chains also happens due to improper installation and handling them inappropriately. Several examples lead to improper handling anomalies. These include distortion or weakening of the chain [23], letting the chain fall to the surface can induce cracks in the chains, and improper handling of the mooring chain throughout the storage, installation, and transportation. All of these contribute to damage in the chain and finally cause mooring chains to fail [24]. The risk of such anomaly can be reduced by following the appropriate procedures and thumb rules, including properly giving up the components, keeping the mooring chain secure, and obviating the high tension and load that causes the chains to bend over time [24]. Additionally, the mooring chains must be inspected and monitored before and during the commencement of the mooring operations to eliminate the damages through repairing or completely replacing the chains that may turn to fail the mooring chains after a long [4, 26].  
  Anomalies in Mooring (Thin) Lines  
  Can withstand high loads and tension  
  Can be prone to deformation and failure due to repeated bending and straightening  
  Resistant to UV radiation and chemical degradation  
  4 Conclusion Mooring (Thin) lines are considered essential to secure offshore marine vessels and floating facilities by keeping them in a fixed place and resisting external loads. However, the failure of any mooring line because of anomalies can cause severe consequences including financial losses, loss of life, and harm to the environment. This review paper has comprehensively reviewed various types of anomalies in mooring lines with their underlying causes and risk mitigation tactics. Besides, the real-time consequences of  
  Anomalies in Mooring (Thin) Lines  
  Malaysia [email protected]   
  Abstract. Community College adhered to the Ministry of Higher Education is well known for its lifelong learning prospects and distinctively offers a short course about the community surrounding the institute. To date, the primary focus in the academic and industrial realms is on descriptive and predictive analytics. Nevertheless, prescriptive analytics, which seeks to find the best course of action for the future, has increasingly garnered research interest. Meanwhile, the analysis will be used to implement actionable plans to help in decision-making that can benefit the institution as well as the officers concerned. This paper investigates the problem arising by using analytical methods in elevating short course enrolment in Seberang Jaya Community College. Upon completion with the usage of Market Basket Analysis (MBA) techniques integrating the descriptive and predictive analysis, results obtained are established thoroughly with specific details that were to attain cluster insights based on the participant’s interest that leads to non-mainstream courses related to the college credential-expertise program. Course modelling proposal for participants’ enrolment through MBA that leads to output produced for Lift Parameter, uses specific rules that have higher lift and confidence that participants tend to join Kursus Penyelenggaraan Komputer (consequents) when they joined Kursus Rangkaian Komputer (Antecedents). In looking at the association rules, it seems that both these courses are highly considered to be enrolled. Keywords: Community College · data analytics · lifelong learning · short courses  
  Fig. 1. The relationship between confidence at 0.9 and lift for Unsupervised rules  
  7 Conclusion This paper introduces association rule mining (ARM) and its application in developer turnover, with a focus on OSS projects like blockchain. It covers ARM’s key concepts and the Apriori algorithm for generating rules. Past ARM applications are mentioned, along with a counseling example. However, the paper stresses the need for caution in interpreting ARM’s discovered rules, as they serve as exploratory findings requiring validation by domain experts. Its primary objective is to demonstrate ARM’s value as a potent data analysis tool for researchers in various fields.  
  N. S. Mansor et al.  
  1 Introduction Geospatial technologies is a field of study in which multispectral refers to using multiple electromagnetic radiation wavelengths or spectral bands for analysis and mapping [1]. Satellite image classification is a technique that involves grouping pixels with similar radiance or digital number values across various image bands or data channels [2]. In addition, applying different statistical learning techniques has become instrumental in extracting valuable information from remote sensing data [3]. Multispectral data in geospatial typically involves the integration of remote sensing imagery with spatial data layers, allowing for a more comprehensive understanding of geographic phenomena [4]. Multispectral remote sensing images are crucial in various fields, including environmental monitoring, agriculture, forestry, urban planning, disaster assessment, and resource exploration [5]. They enable researchers, scientists, and decision-makers to gain insights into the Earth’s features, changes over time, and environmental conditions by leveraging different materials’ distinct spectral signatures and interactions of other materials with electromagnetic radiation. Prior studies used traditional methods of handling multispectral remote sensing images, such as pixel-based methods [6] and object-based methods [7]. However, the conventional method faces challenges in complex applications due to distributional assumptions and the nature of the input data image [8]. Older approaches also have limitations in accurately classifying and interpreting multispectral data due to difficulties extracting complex characteristics [9]. Recent studies have shown that intelligent computing systems, such as machine learning (ML) tools like the Random Forests, K-Nearest Neighbors, and Neural Networks, offer interesting classification task results but face challenges capturing intricate relationships within images and requiring substantial training data, leading to higher computational cost [10, 11]. Support vector machine (SVM) with polynomial kernels can effectively replace conventional statistical methods in handling multispectral remote sensing image classification problems [2]. Although this method requires more processing resources, it has been found to have potential overfitting and inefficiency for large datasets. This study aims to improve the existing methods in handling multispectral remote sensing image classification problems; perhaps a new approach is needed, one that requires less computing power and provides more accurate results. This study utilizes Python libraries and data science tools such as Jupyter Notebook to perform SVM classification with a radial basis function kernel approach, addressing a gap in the existing literature. Figures 1 and 2 depict satellite images of the study area, while Figs. 3 and 4 illustrate the ground truth data for training samples and compare them with the data. Generating supervised training sample datasets necessitates carefully collecting ground truth data through surveys using georeferenced satellite image data.  
  Support Vector Machine for Satellite Images Classification  
  Abstract. In software development, test cases are stored for later use, such as retesting or regression testing. Optimization is one of the approaches used in regression testing, particularly test case prioritization (TCP). TCP aims to rapidly uncover defects during software development. Existing TCP methods lack reliability and suffer from efficiency and effectiveness due to insufficient evaluation, reproducibility, and benchmarking. Currently, no existing TCP framework is integrated with the hybrid PSO-ABC optimization method. This paper aims to introduce a TCP framework that includes five factors, namely fault detection and severity, test case dependency, clustered test cases, and test input, which are used to prioritize test cases. The process starts by determining three factors from the literature (i.e., fault detection and severity, as well as clustered test cases) and contributing the other two (i.e., test case dependency and test input) to seek better optimization. Historical data from previous runs regarding these TCP factors were extracted and stored for analysis. The proposed TCP framework was verified by ten experts, and it was learned that this framework received positive feedback. TCP is closely related to the longevity of software since it can ensure that systems remain reliable, dependable, and maintainable over time. By identifying and prioritizing essential test cases, developers can focus their testing efforts on the areas of the system that are most likely to be affected by changes. Keywords: Test Case Prioritization · Swarm Intelligence · Multi-objective optimization  
  To find faults by gaining coverage of other aspects related to [16–18] software criteria  
  Table 1 summarizes the current TCP methods in the literature, their aims, and related studies. The code-based TCP methods are broadly popular, in which their focus is concentrated on achieving maximum code coverage for revealing defects. Fault-based TCP methods mainly focus on fault-proneness, where fault history, impacts, and probability are utilized. Requirement-based TCP techniques attain coverage of stakeholder requirements to build confidence in software functionality. On the contrary, employing multiple factors in optimizing test cases is gaining popularity due to their performance. Finally, several TCP methods apply different aspects of the information related to software development for test optimization. With the increasing use of multi-objective optimization to solve NP-hard problems, TCP has been treated as a multi-factor problem in the research community [19, 20]. On the contrary, using a single factor for optimizing test cases restricts the ability of the optimized test cases to locate faults and minimizes the flexibility of the technique against the increased complexity of regression testing and other practical considerations [2, 22]. So, multi-objective optimization methods in TCP received wide acceptance and popularity in the research society because they surpassed single optimization methods  
  A Regression Test Case Prioritization Framework  
  in several aspects, such as their capability to cope with the increasing complexity of test case optimization and tackle two or more objectives for optimization [22–25]. Also, their capability to accelerate fault detection ability, maximize coverage criteria, minimize cost, provide better performance in industrial case studies, and provide better distributions of the weighted average percent of faults detected (APFD) metric values makes them widely acceptable [22, 26, 27]. This paper aims to contribute four aspects to the body of knowledge, as follows: • A new method for optimizing test cases through multiple factors mainly focuses on testware without considering the source code in optimizing test cases. • New optimizing factors were introduced to the literature in the prioritization activity. • A new implementation of the hybrid swarm algorithm in TCP coped with tackling the optimization process. • A new weighted objective (fitness) function was formulated to handle these factors for the optimization process. This paper proposes a new TCP framework in which five factors, namely fault detection and severity, test case dependency, clustered test cases, and test input, are considered for optimizing test cases. The framework uses a hybrid swarm algorithm to tackle the multi-objective optimization process. This proposed framework aims to enhance the efficiency and effectiveness of the testing process by involving different factors and focusing on the testware artifacts and their historical data. The rest of this paper is organized as follows: Sect. 2 discusses the related works of literature. Section 3 elaborates on the proposed work, and Sect. 5 concludes this paper and highlights directions for future research.  
  Abstract. The increasing proliferation of intelligent mobile devices and the subsequent surge in data traffic have placed a burden on the current Internet infrastructure. To address this challenge, Named Data Networking (NDN) has emerged as a promising future Internet architecture. NDN aims to address the evolving patterns of Internet traffic by providing inherent support for consumer mobility through innetwork caching. This approach enhances content availability while minimizing delays. However, producer mobility in NDN raises numerous challenges, including Interest packet loss, Interest retransmission, high signalling costs, and unnecessary bandwidth consumption. This research explores and critically analyses the most widely used approaches for managing producer mobility in NDN. This paper introduces an innovative immobile anchor-based mobility mechanism designed to address the challenges associated with producer mobility in NDN. The immobile refers to the fixed nature of the anchor router, which is strategically placed within the network topology to facilitate the management of producer mobility. This immobile anchor router serves as a centralized control point for caching and redirecting Interest packets during producer handoff processes, thereby mitigating packet loss and optimizing bandwidth usage. The focal point of this novel approach is to reduce the repercussions of producer mobility on network performance. Its aim is to minimize factors like packet loss, signalling overhead, and bandwidth usage, with the ultimate goal of enhancing the overall efficiency of NDN-based networks. Keywords: Handoff · Mobility Management · Producer Mobility · Information-centric networking · Named data networking  
  Fig. 2. Assistance of consumer mobility in NDN.  
  The producer sends the requested content in the form of data packets to the consumer via the Interest reverse path. The consumer disconnects from CR3 and connects to CR4 during the packet exchange, which is known as handoff. The consumer handoff process disrupts ongoing communication. Surprisingly, due to the in-router caching ability the remaining contents already caches at the junctional router CR2. When a consumer relocates to a new location CR4, it sends the Interest for remaining content to the junctional router CR2 and retrieves it. As a result of the in-router caching feature, the NDN inherently supports consumer mobility and improves the content availability [12].  
  5 Producer Mobility Assistance In NDN paradigm, a producer implies to a content source that supplies content in response to consumer data demand. Every producer has its a unique name prefix to distinguishes it from others, and the FIB in the NDN data structure maintains the list of these different name prefixes. The FIB also determines the best path to access content with the least amount of effort. Typically, a consumer generates an Interest packet as of CR1 to the producer’s location at CR3 to retrieve content. However, if the producer physically relocates from CR3 to CR4 router, the consumer’s new incoming Interest packets continue to follow the producer’s CR3 old location router. This can result in lost Interest packets at CR3 router due to producer inaccessibility and inadequate stored contents, as presented in Fig 3.  
  In NDN paradigm, when a producer relocates, the consumer constantly forwards the Interest packet in an attempt to locate the required content. However, this approach results in excessive loss of Interest packets, higher Interest retransmission rate, unnecessary bandwidth utilization, and extensive handoff latency. As a result, producer is not capable of fully supporting the mobility in NDN environment [5, 17]. Additionally, the current router-level protocol, Listen First Broadcast Later (LFBL), is not entirely capable of addressing the issues of losing Interest packets and constant Interest retransmission [13].  
  6 Classification of Producer Diverse Mobility Approaches Various solutions have been proposed by researchers to address the issues related to producer mobility in the NDN paradigm. These solutions have been categorized into specific approaches based on their characteristics and advantages. 6.1 Indirection-Based Mobility Approach (IMA) To enable support for producer mobility in the NDN paradigm, the Indirection-based Mobility Approach (IMA) introduces a Home Agent (HA) router in the network. The HA is responsible for maintaining information regarding the content prefix and its location. The content prefix identifies the producer, while the location indicates its physical position. During the handoff process, the HA redirects packets towards the producer’s new location [14]. As illustrated in Fig 4, When a producer moves from CR3 to CR4, it sends binding information to update its new location and name prefix to the HA router, which acknowledges the information. Consequently, when an incoming Interest packet reaches CR3, it is redirected to the HA, which in turn forwards it to the producer at CR4 using a triangular routing path. Triangular communication uses encapsulation and decapsulation in order to exchange Interest and data packet. However, during the handoff process Interest packets experiences loss and excessive Interest transmissions. Further, it uses the triangular routing after the handoff process which may result in high signalling and extra overhead for each packet.  
  Towards a Sustainable Digital Society  
  Fig. 7. Control data plane split-based mobility approach in NDN.  
  In CDPSMA, when a producer initiates a handoff process from CR3, it sends a deregistered query to the RH to remove its location information. At the same time, upcoming consumer Interest packets are cached at the RP. When the producer handoffs to CR4, it sends a register query to the RH to inform it of its new location information. The RH then forwards the producer’s new location information to the RP, which redirects the consumer Interest packets to the producer’s new location at CR4. However, this approach can result in high signalling to manage producer mobility. Additionally, frequent producer movements towards different CRs may cause redirection of consumer Interest packets towards an outdated route, leading to Interest packet loss and retransmission. An analysis of different mechanisms for supporting producer mobility, such as IMA, MMA, LISMA, and CDPSMA, shows that each approach has its strengths and weaknesses. IMA supports mobility through the use of a Home Agent (HA), but it may suffer from high signalling issues and packet encapsulation challenges in large networks. MMA avoids packet encapsulation but experience problems with Interest packet loss, high latency, and signalling overhead. LISMA addresses packet loss but may not be suitable for large networks and may not provide support in certain network scenarios. CDPSMA reduces the negative impact of mobility through monitoring but can also experience high overhead and signalling. Despite these approaches’ potential benefits, there is still a need for a feasible solution that addresses their limitations. This research proposes a new mechanism for controlling producer mobility that overcomes the challenges presented by existing approaches. Table 1 summarizes the analysis of the various producer mobility support mechanisms, including their benefits and drawbacks, and lists unresolved issues.  
  7 Proposed Producer Mobility Management Mechanism By critical analysis of previous approaches some notable factors have been identified such as packet loss, high signalling, extra bandwidth usage and excessive Interest retransmission. In order to overcome the impact of the identified factors, this research proposed a mechanism that will be able to control the associated issues due to producer handoff mobility. The proposed mechanism design consists of mobility packet and immobile  
  High overhead Long Handoff latency Intence handoff singnaling Consume extra bandwidth  
  anchor router. The mobility packet is responsible for updating the producer’s location information in the network, while the immobile anchor router manages the flow of Interest packets during the producer’s handoff mobility process, ensuring minimal packet loss. Moreover, the proposed design modifies the normal NDN forwarding plane. By modifying the normal NDN forwarding plane, the proposed mechanism effectively distinguishes between mobility packets and Interest data packets. This differentiation enables the network to prioritize mobility updates and maintain efficient routing during the handoff process, thus minimizing disruptions to ongoing data exchanges. By referring to Fig. 8, when producer moves from CR3 to CR4, the consumer Interest is unable to retrieve content from producer due to handoff process. During the handoff process, the consumer Interest redirected to CR2 which work as an immobile anchor. The immobile anchor buffers the redirected Interest packets. Meanwhile, when producer relocate and connects to CR4, it sends Mobility Notification (MN) packet to immobile anchor to broadcast its location in the network. The broadcast of MN packet updates the FIBs in the network router to inform about the producer new location. When the location updated, the immobile anchor redirects the cached  
  Towards a Sustainable Digital Society  
  Interest packets towards the producer location. The producer sends the data packets by following the Interest revers path towards the consumer. The new incoming Interest from consumer follows the optimal path due to update in the FIB. The consumer Interest moves from CR1 to CR2 and further moves to producer router CR4. In response, the producer forwards the data packets towards the consumer but in the reverse manner. As a result, the proposed mechanism effectively reduces Interest packet loss, signalling overhead, bandwidth usage, and excessive Interest retransmissions.  
  Fig. 8. Proposed producer mobility management mechanism.  
  The new incoming consumer Interest packets follow the optimal path, due to update in the FIB. The consumer Interest packets move from CR1 to CR2 and further moves to producer router CR4. In response, the producer forwards the data packets towards the consumer by following the Interest packets path in reverse manner. In this way, the proposed mechanism reduces the Interest packet loss, high signalling, extra bandwidth usage, and excessive Interest retransmission.  
  8 Message Flow Figure 9 illustrates the message exchange between a consumer and a producer under the proposed mechanism. It is assumed that the consumer and producer are already in communication with each other. To retrieve data packets, the consumer at CR1 sends an Interest packet (/pic.jpg) to the producer’s (IRL.my) router at CR3. The producer responds to the Interest packet with a data packet, which is divided into versions and several segments corresponding to the requested Interest packets (IRL.my/pic.jpg/v1/sn, where n = 1, 2, 3, 4……). The Interest path from the consumer location towards the producer is (Consumer/CR1/CR3/Producer), while the data path follows the Interest packet path in a reverse way (Producer/CR3/CR1/Consumer). During the communication, the producer decides to move and subsequently disconnects from CR3. While the producer is in the process of handoff, the consumer sends another Interest packet direct towards the producer’s router at CR3, following the path (Consumer/CR1/CR3). Since the producer is not available at CR3, the consumer’s Interest packet is redirected to the immobile anchor router, CR2, following the path (Consumer/CR1/CR3/CR2).  
  Fig. 9. The interest data stream in proposed mobility management mechanism.  
  The CR2 caches the redirected Interest packets. Meanwhile, the producer completes its handoff process by connecting to CR4 and sends the Mobility Notification (MN) packet to CR2. The MN packets updates the producer location towards the network and provides the new route towards producer location. Once the producer updates its location in the network, the cached Interest packets at CR2 are forwarded to producer router CR4 with path Consumer/CR1/CR3/CR2/CR4/Producer). In response, the producer sends the data packets to consumer by following the Interest reverse path (Producer/CR4/CR2/CR3/CR1/Consumer). Furthermore, after the producer’s location update, new incoming consumer Interest packets follow the updated optimal path (Consumer/CR1/CR2/CR4/Producer) and the data path (Producer/CR4/CR2/CR1/Consumer), ensuring efficient routing and minimizing latency. As a result, the associated concerns during the producer mobility are handled by the proposed mechanism. Additionally, the proposed mechanism provides an effective solution that can control excessive Interest packet loss, high signalling, Interest retransmission, and extra bandwidth consumption compared to existing approaches such as IMA, MMA, LISMA, and CDPSMA.  
  9 Conclusion The most interesting NDN producer mobility approaches have been addressed in this study. All of these approaches offer various ways to control producer mobility, that each have fundamental design and unique properties. However, following a rigorous examination of each approach, we draw attention to a number of problems, including losing of Interest packet, constant Interest retransmission, prolonged handover latency, and an inefficient routing path. To address these challenges, we propose a producer mobility management mechanism, which is expected to effectively resolve the identified issues  
  Abstract. Named Data Networking (NDN) is the most remarkable initiative of Information-Centric Network (ICN) to improve overall network performance. With its data-centric architecture and forwarding philosophy, NDN natively addressed consumer mobility. However, the producer mobility problem remains a challenging issue in NDN architecture. Among the critical issues of producer mobility are handover latency, Interest packet loss, and Interest retransmission. This paper classifies existing producer mobility solutions into rendezvous, anchorbased, and anchor-less approaches. Despite the efforts of many researchers poured into solving these issues, there is still room for improvement. This paper proposes a producer mobility management mechanism based on a proactive approach to managing producer mobility in NDN. The proposed mechanism proactively evaluates the handover time of the producer and sends the mobility notification packet to inform about the producer’s movement. In the meantime, a new Interest packet for the moving producer will be buffered on the router. Thus, the proposed mechanism aims to reduce the handover latency and packet loss. Keywords: Named Data Networking · Producer Mobility · Consumer Mobility  
  N. H. A. Zukri et al.  
  An emerging novel future Internet called the Information-Centric Network (ICN) is committed to the development of decentralized networks. With its new structure, ICN replaces IP addressing with the idea of addressing the content for network addressing. Respective research communities keep an eye on this subject matter. Thus, introducing some new architectures, namely Data-Oriented Network Architectural (DONA) [4], Content-Centric Networks (CCN) or Named Data Networks (NDN) [5], PublishSubscribe Internet Technology (PURSUIT) [6], and Network of Information (NetInf) [7]. NDN receives immense attention from the ICN architectural research group among these architectures. Future Internet architecture Named Data Networking (NDN) has caused a paradigm shift in network communication from point-to-point to name delivery. In addition, NDN depends on the stateful forwarding plane for datagram delivery. The adoption of the NDN forwarding plane indeed supports consumer mobility. If the consumer is relocated to a new attached content router (CR), no signaling from the network is required to handle consumer mobility. Instead, Interest retransmission is adequate for successful communication. However, producer mobility is a more complex problem that impacts overall communication. This is because the routers’ Forwarding Information Base (FIB) needs to be updated once the producer reconnects with the new attached content router. Failure to update FIB will affect the search for the desired Data. Ultimately, it results in high-Interest packet loss, high-Interest retransmission, increased bandwidth demand, and high handoff latency [8, 9]. Further, the aforementioned issues will restrain the work for building a sustainable and more inclusive digital future. Therefore, this paper introduces a producer mobility management mechanism design in NDN using a proactive approach. The proposed mechanism will handle the mobility of the producer by analyzing the producer’s mobility status and informing the router and consumer to take necessary action, such as delaying the Interest transmission. Managing the producer mobility before the handover process based on a proactive approach would minimize the handover latency and interest retransmission. Also, it aims to alleviate other critical issues, such as high signaling and Interest packet loss issues. The remainder of this paper is organized as follows. Section 2 presents the mobility in Named Data Networking. Section 3 discusses the producer mobility approaches. Section 4 shows the proposed design and the conclusion in Sect. 5.  
  2 Mobility in Named Data Networking One of the auspicious Information-centric networking (ICN) approaches to address the challenges of the future Internet is NDN [10]. The NDN relies on named data, namebased routing, and in-network caching to distribute content efficiently and improve network bandwidth. Additionally, the NDN facilitates consumer mobility easily and has incorporated security for every piece of Data. The essence of NDN comes from CCN [11, 12]. This can be shown in NDN architectural and protocol operations. Besides that, NDN has a modular and extensible codebase. As NDN architecture changes the Internet architecture model, the network communication changes to retrieve content. Any communication in NDN is based on two kinds of packets: Interest and Data. A Data packet is a named sequence of bytes. A name is a  
  N. H. A. Zukri et al.  
  As shown in Fig. 1, the consumer can resubmit any outstanding or expired Interests if they relocate while the network gets the desired data. By doing so, the intermediate router can update the path to the consumer’s current location. If the old and new paths cross, the consumer will retrieve the previously requested Data from a router’s cache without propagating further. Given this NDN nature, the consumer effortlessly gets the remaining content from these routers to fulfill its content needs. For the next request, the consumer can efficiently get the remaining Data packets only by re-issuing Interest again for similar content [13, 14], and it can be done within the minimum delay. In conclusion, the consumer mobility issue is inherently solved through cached contents at router space or resending the Interest packet to the intermediate router after the completion of consumer mobility [15]. 2.2 Producer Mobility In NDN, consumer issue Interest messages carrying the name of the requested information object. Although the name of the content is separated from the location, the hierarchical name comprises the location of the content such as the example ‘uum/edu/my/ahsgs/homepage.pic’. The NDN directly coupled the location with the content name, adding them to FIB in the routing protocol [15]. Consequently, whenever the consumer delivers an Interest packet, the connected router determines the path by looking up the FIB for a suitable producer (content source). Generally, FIB contains the entries of long prefixes address for the next-hop or content source. This allows it to determine the optimal path to access content and further the Interest. The producer satisfies the consumer’s content needs by sending the desired content from its location to the consumer. Whenever the producer shifts and connects with the other CR, the producer must update the FIB of all routers in the NDN network. Otherwise, Interest reaches the old CR to retrieve the content. Sometimes, it takes quite a long time to get a new CR. Due to the unavailability of the producer, no content is available for the consumer [16]. Furthermore, the network will suffer from high overhead, packet loss, and long handover latency [8]. Ultimately, the consumer lost communication until the producer informed its new name prefix in the network. Figure 2 illustrates the producer mobility in NDN. To control mobility in ad-hoc networks, NDN uses the Listen First Broadcast Later (LFBL) protocol. LFBL floods the Interest packet to the serving routers and broadcasts name prefixes across all the serving routers after the producer handoff process is completed [17, 18]. The decisions for prefix announcements are made depending on the availability of the wireless channel. If no other node has sent a matching data packet, the router forwards the packet. However, the LFBL protocol generates high signaling and has no mechanism to recover the dropped and incoming Interest packets sent at the previous attached CR. Furthermore, producer mobility is not natively supported due to the unavailability of content locators and failures in content access through the routing system. The second NDN solution to minimize the producer mobility effect is caching content at the router.  
  Enabling a Sustainable and Inclusive Digital Future  
  Fig. 3. Operation of a Rendezvous Approach [8].  
  3.2 Anchor-Based Approach Similar to the idea of MobileIP in TCP/IP, the anchor-based solution made use of an anchor node called Home Agent (HA). HA keeps tracking the position information of relocated mobile nodes. Figure 4 shows a consumer resubmitting the Interest to HA and finding a new location for the producer. Then, HA transmits the Interest directly to the producer. Various schemes [16, 22] introduce the notification method about the producer’s relocation to HA. Although these schemes are simple and reduce the handover latency, this approach is vulnerable because of heavily dependent on a single node (HA). One of the drawbacks is the transmission of all packets via the anchor node will lead to the bottleneck effect [23]. 3.3 Anchor-Less Approach The anchor-less approach allows a producer to update its new location information in the network without relying on third-party [24, 25], as shown in Fig. 5. Upon receiving the notification, routers update the FIB table with the latest location information. Since FIB is keeping up to date, the Interest can be submitted to the current location of the producer. Thus, no specific node like a rendezvous point or HA is required to forward the packet. [26] apply this strategy and update FIBs of associated routers to facilitate forwarding the Interest to the node. However, the disadvantage of this scheme lies in a scenario in which the producers are relocating regularly, which frequently updating the FIB of the connected routers may trigger traffic [23].  
  Enabling a Sustainable and Inclusive Digital Future  
  Fig. 4. Operation of the Anchor-based Approach.  
  In [27], the producer announces the leaving status of the network, including the previous attached CR. The producer’s frequent movement hampered the propagation of the concurrent FIB updates. This scheme provides a fast and lightweight handover but incurs high signaling due to extra packets sent for the notification from the leaving producer.  
  Fig. 5. Operation of the Anchor-less Approach.  
  High traffic, high signaling  
  4 Proposed Mechanism In this section, we present the mechanism that describes the main ideas in addressing the issues of long handover latency, high-Interest packet loss, and Interest retransmission. Based on these issues, Fig. 6 illustrates the proposed proactive producer mobility management mechanism. Despite the exchange of Interest and Data packet, this mechanism proposed three schemes for normal communication in NDN which are: 4.1 Determine the Producer’s Mobility Handover Time In order to reduce handover latency, the mechanism shall proactively determine the producer’s leaving time. This research overcomes the issue by calculating the minutes’ handover begins. Every few seconds, the scheme will trigger the sensor to detect the Received Signal Strength (RSS) of the producer with CR1. If RSS is lower than the threshold (th), the scheme will calculate the producer’s mobility handover time. It is important to determine the handover time so that the method to minimize the producer’s mobility effect would be implemented sooner. 4.2 Send Mobility Packet After determining the producer’s handover time, it is necessary to inform the consumer that the producer is leaving the router. The mobility packet acts as a notification signal to the consumer that contains information about the producer’s mobility. The consumer may delay the Interest transmission to the same producer upon receiving the packet. The Interest retransmission is delayed until the consumer receives further notice about the producer’s new location.  
  Enabling a Sustainable and Inclusive Digital Future  
  The proposed mechanism will be measured based on handoff latency, interest packet loss, data packet delivery, and throughput for the evaluation. The formula for each parameter will be studied. Results from the simulation will be accumulated and compared with the OPMSS [25] and MAP-Me [27].  
  5 Conclusion The NDN architecture is key to building a more inclusive and sustainable future Internet. Nevertheless, the NDN does not provide enough assistance for mobile producers, particularly if they lose their connections. Given the severe effects of mobility towards communication in the network, much research has been done, and these research works are categorized into various approaches. However, after thoroughly investigating previous research, we have identified several problems, such as high signaling, high packet loss, and long handover latency. Thus, this paper proposed a proactive producer mobility management mechanism to solve such problems. The contribution of this paper is twofold. First, we look at mobility in NDN and producer mobility approaches. The approaches are categorized into rendezvous, anchor-based, and anchor-less approaches. All papers are being analyzed and criticized, respectively. By comparing both approaches, we can identify the best fit for designing the proactive mobility management mechanism. Second, we design the proactive producer mobility management mechanism by incorporating three strategies: determining the producer’s mobility handover time, sending the mobility packet, and buffer Interest. The proposed mechanism is expected to provide the mobility solution to support producer mobility in NDN architecture by minimizing the Interest retransmission and handoff latency.  
  Enabling a Sustainable and Inclusive Digital Future  
  2.3 Eligibility and Exclusion Criteria Several eligibility and exclusion criteria are determined. The first criterion, Literature type, specifies that only research articles published in journals and proceedings will be eligible for inclusion. These articles are typically peer-reviewed and provide original research findings, making them more reliable and valuable for research purposes. The exclusion criteria under this criterion include systematic reviews, book series, books, chapters in books, and conference proceeding books. The second criterion, Language, specifies that only articles published in English will be considered. This criterion ensures that the selected articles can be easily understood and reduces the potential for languagerelated bias in the selection process. The final criterion is Timeline. Since the GitHub Copilot is a new tool, selecting articles is unlimited. All articles published in 2023 and the years before, have been selected. This criterion ensures that all articles related to the GitHub Copilot were chosen (see Table 1). Table 1. The inclusion and exclusion criteria. Criterion  
  Eligibility  
  Exclusion  
  Literature type Journal (research articles), proceeding Journal (systematic review), (research articles) proceeding (systematic review), book series, book, chapter in book, conference proceeding book Language  
  English  
  Timeline  
  Any time < 2023  
  None  
  2.4 Systematic Review Process The systematic review was conducted in April 2023 and comprised four distinct stages. The first stage involved the identification of relevant keywords for the search process. Since GitHub Copilot is a unique keyword, “GitHub Copilot” was used. Following a meticulous process, 12 duplicated articles were eliminated. The second stage was the screening process, which excluded two articles due to their status as conference proceedings, leaving 26 eligible articles for review. In the third stage, the eligibility criteria were applied to the full articles, excluding nine articles that focused on topics other than GitHub Copilot. Finally, the review’s last stage yielded 15 articles suitable for qualitative analysis, as presented in Fig. 1.  
  2.5 Data Abstraction and Analysis The remaining articles were assessed and analyzed. Efforts were concentrated on studies that met the predetermined inclusion criteria, which ensured that the analysis focused on relevant sources of evidence. To extract relevant data, the team initially read through the article abstracts, followed by a more in-depth analysis of the full articles to identify significant trends related to GitHub Copilot. Qualitative analysis was performed using content analysis techniques, which allowed the team to identify relevant themes and patterns in the data. This rigorous and systematic approach to data analysis ensured that the study’s findings were grounded in the available evidence and provided a robust synthesis of the research on GitHub Copilot.  
  3 Results Table 2 shows the recent trends of GitHub Copilot research. There are four main areas studied by researchers in 2022 and 2023: developer productivity, code quality, code security and education.  
  The Recent Trends of Research on GitHub Copilot  
  √ √  
  Nguyen and Nadi (2022) [11] Al Madi (2022) [2] Siddiq et al. (2022) [17] Pearce et al. (2022) [13] Finnie-Ansley et al. (2023) [6] Denny et al. (2023) [3] Wermelinger (2023) [23]  
  √ √ √ √ √ √ √  
  3.1 Developer Productivity The study [9] aimed to assess the performance of GitHub Copilot, an automatic program synthesis tool, and compare it with genetic programming approaches on common program synthesis benchmark problems. The results revealed that both approaches had similar performance on benchmark problems. However, genetic programming approaches still needed to be mature enough to support practical software development due to their reliance on expensive hand-labelled training cases, long execution times, and bloated and hard-to-understand generated code. The researchers suggested that future work on program synthesis with genetic programming should prioritize improving execution  
  insight into Copilot’s potential for programming education and to highlight the need for instructors to adapt their teaching practices accordingly. The findings indicate that while Copilot can generate correct and understandable code, it cannot replace the process of learning programming. Therefore, educators must incorporate Copilot into their pedagogical strategies judiciously.  
  4 Discussion GitHub Copilot is an AI-powered programming tool that has gained attention for its ability to generate code automatically. Several studies have explored Copilot’s performance, productivity, and usability in different contexts. One study compared Copilot with genetic programming approaches on common program synthesis benchmark problems, concluding that both approaches had similar performance but that genetic programming approaches still needed to be mature enough for practical software development. Another study investigated the impact of Copilot on user productivity and found that the rate of acceptance of suggestions was the primary driver of productivity. The third study compared Copilot with human pair programming and found that while Copilot increased productivity, the quality of generated code was inferior. The fourth study revealed that Copilot provided a useful starting point for programmers but needed help understanding, editing, and debugging generated code snippets. Several studies have recently assessed the code quality generated by GitHub Copilot, which assists programmers by generating code based on natural language descriptions of desired functionality. The studies have used various methods to evaluate the correctness and understandability of the generated code and have generally found that Copilot holds significant promise as a programming tool, generating valid code with high success rates. However, the studies also identify potential shortcomings, such as generating code that could be further simplified and relying on undefined helper methods. Further assessments and improvements are necessary to optimize Copilot’s performance in generating entirely accurate code that meets all requirements. Using machine learning-based code generation models, such as GitHub Copilot, raises ethical and security concerns. Several recent studies highlight the potential for such models to generate vulnerable code and the need for careful selection and scrutiny of training data to minimize risks. To address these concerns, researchers have introduced SecurityEval, a dataset for evaluating the security of code generation models, and CoProtector, a prototype aimed at safeguarding open-source code from misuse and breaches. While Copilot’s performance varies considerably based on the diversity of weaknesses, prompts, and domains, the studies emphasize the importance of vetting generated code for security vulnerabilities to prevent potential breaches. The studies explore using OpenAI’s Codex machine learning model in programming education through its implementation as the GitHub Copilot plugin. They investigate Copilot’s impact on the learning process, its ability to generate original code, and its performance on diverse programming problems. The studies show that Copilot has the potential to support students in completing programming assignments and exams and can promote equitable access to high-quality programming education. However, the studies also suggest that Copilot cannot replace the process of learning programming, and  
  Abstract. With infinite apps and online services, future Internet architecture will face new challenges and consequences, such as scalability, dependability, suitable mobility, and security. Internet use has changed spectacularly from one-way communication to content distribution, as much content is generated every minute. Blockchain and Named Data Networking (NDN) are two cutting-edge technologies on the verge of revolutionizing how we use the Internet. Blockchain is a decentralized ledger technology allowing users to store and share data securely. On the other hand, NDN is a new way of networking that focuses on content instead of location. Combining blockchain and NDN can create a safer, more efficient, and more decentralized internet. Blockchain can provide tamper-proof data records, and NDN can deliver content to users efficiently and securely. This paper emphasizes the importance of research in the field of blockchain over Named Data networks. It highlights the advantages of combining blockchain with NDN and discusses the difficulties and open research questions related to the use of blockchain over NDN. Also, the potential impact of blockchain over NDN on the future of the Internet as it can create a safer, more efficient, and more decentralized Internet. Keywords: Blockchain · Content Distribution · Content-Centric Network · Future Internet  
  The original Internet design is elegant and powerful due to its hourglass architecture. The narrow waist of the hourglass represents the core network layer, which implements the essential features necessary for global interconnectedness. This small size has been essential to the development of the Internet, as it has freed higher- and lower-layer technologies from unnecessary limitations. The NDN design also has a narrow waist, but it differs from the IP architecture in a fundamental way [3]. The use of data names in NDN allows for more efficient and scalable content delivery, as well as improved security and privacy. Data names are hierarchical, making finding and retrieving content easy. They are also self-describing, which means they contain information about the content, such as its type, size, and last modified date. Routers can use this information to make more informed decisions about how to route data packets [4]. Integrating blockchain technology with NDN has recently gained much attention in the research community. Scholars have highlighted the potential benefits of this integration, such as improved security, privacy, and scalability. NDN can also fulfill the needs of blockchain applications by providing a secure and efficient way to store and transfer data. This study focuses on integrating NDN and blockchain technology and discusses the potential benefits of this integration [5, 6]. The use of blockchain over NDN will be examined in this paper. The document layout is as follows: We’ll present the Methodology in Sect. 2 and the Background review in Sect. 3. A review of the latest studies of blockchain over NDN will be covered in Sect. 4. The discussions on the value of blockchain over NDN will be covered in Sect. 5. Open challenges will be presented in Sect. 6 and Sect. 7 will conclude the paper.  
  Fig. 2. Articles from different resources.  
  3 Background First Section Blockchain and named data networking (NDN) are two emerging technologies that have the potential to revolutionize the way we interact with the Internet. Blockchain is a distributed ledger technology that can securely record and share data, while NDN is a new networking paradigm focusing on content rather than location. In this section, we will provide a background on blockchain and NDN. We will then discuss the potential benefits of combining these two technologies [6]. 3.1 NDN Forwarding Plane NDN architectural design proposes two packet types: interest packets and data packets. NDN users can access the data by subscribing to an Interest packet, which requests a content object and returns it as a Data packet; both packets contain the name of the content object. The Forwarding Information Base (FIB), the Pending Interest Table (PIT), and the Content Store are the three significant Data structures that an NDN router must maintain (CS) [3]. The FIB of an NDN router is often similar to the FIB of an IP router, with the architectural difference that it holds name prefixes rather than IP address prefixes. By doing this, the name prefixes may be sent to various interfaces. Each PIT section keeps a record of the Interest’s name, arriving interface(s), and forwarding interface(s) that it has been passed to (like a history table), when a router receives an interest packet, it first checks the Content Store (CS) to see whether there is any matching Data. The CS provides short-term in-network storage (caching) of the incoming Data packet. The interface from which the Interest is coming receives the data immediately [7]. On the other hand, if the name matches, the interest will proceed to look up the PIT entries. Suppose the name already appears in the PIT. In that case, it may be a duplicate Interest that has to be discarded or a retransmitted Interest from the same customer sent via a different outgoing interface (or an Interest from an alternate consumer requesting the same Data). This causes the PIT to check the nonce of the Interest and update the existing PIT record with the number of incoming interfaces. This effectively creates a  
  Blockchain Over Named Data Networking Architecture: A Review  
  multicast tree for users requesting the same Data simultaneously. If the Interest name does not already exist in the PIT, it is added to the PIT and sent to the FIB, where the forwarding plane module will handle it. When a Data packet comes at this stage, the PIT is checked using the packet’s name. The router delivers the Data packet to the interface(s) from which the Interest arrived and deletes the PIT entrance if there is a match in the PIT entrance. Data packets then frequently follow the interests’ backward routes. The Data packet is discarded or cached in the CS if no match is detected. Every Interest has an associated lifespan that the consumer determines; if an Interest is unsatisfied before its lifetime expires, a PIT item is removed. Nevertheless, an NDN router may keep a Data packet in the CS due to the signature’s uniqueness and the dependability of the caching strategy. Even so, future interests can be satisfied using the data packets stored in the CS [8]. 3.2 Blockchain Framework Most modern blockchain systems use a common framework that was first established in Bitcoin and Ethereum and may be organized generally into four levels [9] as follow: Application Layer: By utilizing smart contracts, the decentralized applications of blockchain technology, such as supply chain management, identity management, and notarial services, have grown to include the application layer, which is used for cryptocurrency transfers. Data Layer: A blockchain framework’s data layer contains structures for consensus, data transfer, and ledger maintenance. Blocks are connected via hash references; however, block architectures may vary. Consensus Layer: The shared ledger is created via a process followed by the consensus layer in blockchain nodes, with copies on each node. Consistency requires agreement on how transactions should be executed in order. For quicker processing, transactions are organized into blocks, and the block sequence is chosen instead. The PoW consensus, used by many blockchain systems, is infamous for its lengthy transaction times and lack of transactional finality because of ledger forking. Researchers are looking into novel consensus methods to get over these restrictions and support a range of blockchain technology use cases. Transport Layer: The blockchain network’s transport layer defines how transactions are recorded to the ledger and how blockchain data is propagated. Public blockchains like Bitcoin and Ethereum use a P2P overlay to transfer data items to all nodes from a single source. The transport layer of the blockchain network controls how transactions are recorded and propagated.  
  4 Blockchain over Named Data Networks Most research on Named Data Networks and Blockchain technology has been done independently. Table 1 shows recent years that adopted Blockchain over NDN in various fields, including security and privacy, networks, the Internet of Things, mobility, and  
  2022 Security Presents hierarchical identity-based cryptography (HIBC) and blockchain-based security method for NDN  
  [16] 2023 Security Proposed efficient and secure auditing of data transmission behavior for NDN IIoT networks [17] 2023 Security Proposed a NACDA approach for data verification in NDN, which improved the considerable delays brought on by the extremely dynamic nature of vehicle networks [18] 2023 Security Proposed a decentralized data authentication mechanism based on blockchain technology [19] 2023 Trust  
  Build mechanics trust between vehicles using NDN to route data efficiently, and blockchain to record transactions securely  
  [20] 2022 Security proposed a system called BIoVN, to secure IoV over NDN [21] 2023 Routing Present a new data dissemination protocol called A-C is based on the NDN forwarding [6]  
  2022 Routing Proposed a deployment of named data networking (NDN) at the network layer of the blockchain to provide differentiated QoS assurance  
  [22] 2023 Routing Proposed a framework called AFFIRM for generating, validating, storing, and retrieving mobility data in Web3 applications [23] 2022 Trust  
  Present a trust management system is to allow well-behaved peers to gain a good reputation  
  Proposes a proof-of-trust-based data authentication system for blockchains in NDN  
  [26] 2022 Routing Proposed access control system based on NFT enables NDN routers to forward ciphertext data [27] 2022 Routing This paper proposes integrating blockchain and NDN to improve document content storage (continued)  
  Contribution  
  [28] 2023 Security Proposed a CCN-based secure content delivery scheme for V2G networks [29] 2022 Security Proposes a security architecture for NDN based on a consortium blockchain and bootstrapping procedures  
  The authors [16] In the Industrial Internet of Things (IIoT), the study provides a simple transmission behavior audit scheme for Named Data Networking (NDN). The blockchain-based system makes it possible to audit data transmission behavior in NDN networks safely and effectively. It consists of three basic parts: a lightweight auditor for gathering and submitting records to the blockchain, a blockchain-based audit system for managing records, and a data packet for carrying audit records. The findings show that NDN networks can effectively detect malicious activities and have high throughput and low latency. IN [17] proposed Naming-Based Access Control and Decentralized Authorization (NACDA) system addresses challenges in data verification in dynamic vehicular net works by enabling secure and flexible data sharing on the Named Data Network (NDN) using Identity-Based Encryption with Wildcard Key Derivation (WKD-IBE) and blockchain. A new mechanism has been proposed in [18] to provide a decentralized data authentication mechanism based on blockchain technology that is both efficient and straightforward. A new framework proposed in [19] uses VSNs to build trust between vehicles, NDN to route data efficiently, and blockchain to record transactions securely. The framework is designed to be P2P, meaning that vehicles can trade energy directly with each other without needing central authority. While in [20] the authors proposed another system called BIoVN, which is a combination of blockchain technology and named data networking (NDN) for the Internet of Vehicles (IoV). The purpose of this system is to improve the security of vehicular communications over NDN. The authors in [21] introduce the Named Data Networking (NDN) and Erasure Coding (EC)-based A-C data distribution protocol. The protocol uses a two-layer NDNbased publication-subscription mechanism to maximize bandwidth efficiency and speed up data dissemination. It focuses on the prompt distribution of blocks and transactions in blockchain systems, which is crucial for consensus, effectiveness, and security. The A-C protocol improves data transmission efficiency and security in blockchain systems, reduces data redundancy, and addresses shortcomings of flooding-based gossip protocols. A deployment of named data networking (NDN) at the network layer of the blockchain to provide differentiated QoS assurance is proposed in [6]. It discussed the use of window sliding and forwarding strategies to speed up packet processing and meet the delay requirements of delay-sensitive packets. Also, a blockchain framework called AFFIRM for generating, validating, storing, and retrieving mobility data in Web3 applications. This framework enables nearby devices to self-organize as a fog network  
  and collaboratively train machine learning algorithms locally to securely generate, validate, store, and retrieve mobility data via consensus leveraging Information Centric Networking as the underlying architecture [22]. Author in [24] proposes a novel encryption-based data access control scheme for Named Data Networking (NDN) using Role-Based Encryption (RBE). The scheme ensures efficient data access control over hierarchical content, making it suitable for large-scale content-centric applications like Netflix. The study [25] proposes a proof-oftrust-based data authentication system for blockchains. The technique collects votes from a group of nodes to distribute and store items in the cache memory. The suggested system provides a fresh data authentication option for the upcoming Internet environment while attempting to address difficulties with tainted cache memory. In [26] smart contracts are used to distribute AttributeNFT and AccessNFT, a proposed access control system based on Non-Fungible Token (NFT) that enables NDN routers to forward ciphertext data packets only to authorized users, assuring data security and secure distribution. To increase document content distribution, security, and network speed, research in [27] suggests fusing blockchain technology with Named Data Networks (NDN). Three key contributions are made in the paper’s proposal for a CCN-based Three key contributions are made in the paper’s proposal for a CCN-based secure content delivery scheme for V2G networks: in-network caching for quick content delivery; a contract theory-based incentive scheme to entice vehicle participation, and the proof of authority consensus algorithm for secure content delivery and network trust [28]. Authors in [29] Used a symmetric-key-based authenticated encryption technique and a one-way hash chain for source authentication, this article suggests a security architecture for NDN that is based on a consortium blockchain and bootstrapping procedures. In [30] proposed CPA detection and prevention mechanism includes a threshold-based content caching system, a blockchain system for privacy, and an extension of NDN to push-based content dissemination.  
  5 Discussion With the goal of replacing TCP/IP at the network layer, adopting blockchain technology over NDN offers special benefits and applications that will benefit both the blockchain community and established online services. [5]. By focusing on network-level connectivity and adopting “data-driven authenticity” to assure the security of the data’s source, blockchain over NDN prioritizes data over location and ensures real decentralization. Researchers are interested in how specific technologies are emerging. Data retrieval is efficient using NDN, and data security is ensured via blockchain. Some scholars believe using blockchain technology for the current IP would be unwise. Instead, using blockchain technology over NDN may lead to more effective performance [6, 11]. NDN, a hypothetical future Internet architecture, can support blockchain technology, offering a dependable way to maintain databases without central authority. Blockchain over NDN fixes IP network problems and provides a decentralized system, making connecting nodes and synchronizing data simpler. Trust models can be centralized or decentralized; a prior method involved a central credit authority to collect and disseminate reputation values, but this method still entailed communication costs [15].  
  4 Classification of Creative Industry: Cross Country Comparison A comparative analysis was conducted to find differences between the classification of the creative industry in a few countries. It was also referred to by the World Intellectual Property Organization (WIPO), United Nations Conference on Trade and Development (UNCTAD), European (EU) Commission, and United Nations Educational, Scientific and Cultural Organization (UNESCO). The countries include Malaysia, Thailand,  
  Author Index  
  Author Index  
  Author Index  
