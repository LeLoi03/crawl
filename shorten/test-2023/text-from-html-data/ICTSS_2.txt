 This book constitutes the refereed proceedings of the 17th International Conference on Software Architecture, ECSA 2023,  
 Author / Uploaded 
  Silvia Bonfanti 
  Angelo Gargantini 
  3.4 Limitations of the Study  
  4 Results  
  5 Final Remarks  
  References  
  Enhancing Synthetic Test Data Generation with Language Models Using a More Expressive Domain-Specific Language  
  2 Preliminaries  
  3 Motivating Example: A Parking System  
  4 Mutual Acceptance  
  5 Compositionalility for Uioco  
  6 The Parking System Revisited  
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  Test Case Generation  
  A Rapid Review on Fuzz Security Testing for Software Protocol Implementations Alessandro Marchetto(B) University of Trento, Trento, Italy [email protected]  Abstract. Nowadays, devices and systems are always connected for providing everyday services. Hence, there is a growing interest concerning the adoption of secure software implementations of communication protocols that allow heterogeneous systems to exchange information and data. In the last decade, several approaches and techniques for applying fuzz security testing to such implementations have been proposed. Fuzz security testing is a promising approach to discover software vulnerabilities. It aims at exercising the implementation under test by means of unexpected and potentially invalid inputs and data, aiming at triggering misbehaviors, exceptions, and system crashes. This paper presents a Rapid Review (RR) conducted to study fuzz security testing for software implementations of communication protocols. The following evidences emerged from our RR: (i) Industrial Control System and Internet of Thing protocols are among the most studied ones; (ii) black-box fuzz security testing is frequently investigated and, often, the proposed approaches require protocol or data speciﬁcations as input; (iii) most of the detected vulnerabilities are related to memory management and, less frequently, to input and data management and validation, and (iv) only few tools are publicly available. Keywords: Fuzzing Testing  
  1  
  A. Marchetto  
  under test by means of a large set of unexpected and potentially invalid inputs and data, aiming at triggering system’s failures (e.g., misbehaviors, exceptions, and crashes). A system failure could reasonably be due to the presence of an exploitable vulnerability in the target system [9]. Diﬀerent types of fuzz testing exist depending on the type of the target system, e.g., application fuzzing, ﬁle format fuzzing and protocol fuzzing. This paper presents a Rapid Review (RR) conducted to study the literature of the last decade concerning fuzz security testing for software implementations of communication protocols (software implementing communication protocols). A protocol enables the communication between entities by deﬁning rules, syntax, semantics, synchronization, and possible errors of the communication. In this paper, we aim at investigating: (i) the type of protocols tested with fuzz testing; (ii) the type of fuzz testing approaches and techniques investigated in the literature; (iii) the existing tools that can support protocol fuzz testing; and (iv) the types of software vulnerabilities typically discovered by fuzz testing. Rapid Reviews (RRs) [4] are a recent type of review approach proposed for conducting evidence-based studies [10]. These studies aim at identifying the current best evidence from the literature in a speciﬁc domain, thus improving the decision making process in that domain. Diﬀerently from a conventional Systematic Literature Review (SLR), a RR takes into account constraints related to practical situations and cases, such as the need of delivering evidence promptly, with low costs, the need of reporting evidence in a clear and simple way without formal synthesis. RRs omit and simplify some steps of conventional SLRs, thus in a sense, lowering the generalizability of results, while preserving a systematic and rigorous approach. RRs are not intended to replace SLRs: SLRs provide in-depth insights, while RRs are lightweight review studies to easily and quickly transfer knowledge to practitioners, through evidences. Indeed, diﬀerently from SLRs, RRs i start from a practical need and problem of practitioners [1], e.g., the beginning of a new engineering project, the need of increasing the practitioners conﬁdence on decisions or to let them quickly acquire new information, concepts and pointers concerning a speciﬁc problem or technology. ii focus on a single source of data and involve a single researcher [4], thus reducing costs, time (e.g., weeks rather than months required by conventional SLRs) and resources at the expenses of the number of paper analyzed. RRs are not expected to be exhaustive on the analysis of the literature studies, this is not their goal; iii apply a rigorous, even if simpliﬁed process, with respect to SLRs. For instance, RRs do not conduct a rigorous quality appraisal of the literature studies. However, it is worthy highlight that RRs are not informal reviews. iv aim at collecting evidences and, even they can represent a preliminary step towards the identiﬁcation of research challenges and open issues, however, this is not their goal; v report the results in a narrative and descriptive form by focusing on the evidence emerged, according to the goal of communicating to practitioners [4]. RRs do not analyze data and present results by using a formal synthesis.  
  A RR on Fuzz Security Testing for Software Protocol Implementations  
  A. Marchetto  
  kittyfuzzer7 and Mutiny8 are fuzz testing frameworks for non-TCP and TCP channels. Snipuzz [5] is a black-box fuzzing tool for IoT ﬁrmwares. The eﬀectiveness of fuzz testing is strongly dependent on the target system under test. This paper focuses on software implementations of communication protocols, one of the area in which fuzz testing is widely adopted. Secondary Studies. A few systematic literature reviews (SLRs) exist about fuzz security testing for software implementations of communication protocols. [8] presents the basic principles and the typical process applied to network protocol fuzzing, as well as the fundamental structure of a fuzzing system. The paper also focuses on the adoption of machine learning algorithms for fuzz testing. [11] surveys on protocol fuzz testing adopted for Industrial Control Systems. [12] surveys on network protocol fuzz testing for information systems and applications. [12] conducts an in-depth analysis on a few selected studies that mainly apply fuzz testing to network TCP/IP protocols. [15] reviews the techniques, e.g., symbolic execution, taint analysis, artiﬁcial intelligence, that can be combined with conventional fuzz testing approaches applied, in particular, to IoT protocols and ﬁrmwares. [13] contextualizes the adoption of fuzz testing with respect to others vulnerability discovery strategies (e.g., static analysis), without focusing to software implementations of communication protocols. Despite these SLRs are related to our RR, they did not ﬁt our research goal, i.e., not address our RQs. We do not focus on a speciﬁc domain (e.g., Industrial Control Systems) or technology (e.g., IoT), as well as done by these SLRs. Furthermore, these SLRs adopt a more research-oriented point of view with respect to our RR, they discuss about general principles and processes to conduct fuzz testing. Differently form these SLRs, we mainly focus on how the investigated approaches and tools work, targeting practitioners. To the best of our knowledge, this paper is the ﬁrst one that presents a RR on the ﬁeld of fuzz security testing for software protocol implementations and that reports the result by using an evidence brieﬁng method.  
  3  
  Analysis Criteria  
  According to the analysis strategy delineated by Cartaxo et al. [3] for a RR, we conducted a thematic analysis [1] on each selected literature paper, by searching on it notions and concepts according to some criteria identiﬁed for investigating our RQs. Table 1 summarizes such criteria in terms of aspects and examples of values. Four groups of aspects are considered: – Work: goal and description of the paper under analysis, e.g., presentation of a new fuzzing approach or a new technique, experimentation, reviewing fuzzing techniques; – Target system: aspects related to the target system under test, i.e., type of the protocol, architecture and application domain of the protocol implementation; – Investigated solution: in-depth aspects related to the investigated fuzz testing approach; – Available solution: aspects related to the presented or used tool that support the investigated solution; – Vulnerability and attack: aspects related to the type of vulnerabilities that can be discovered and the attacks that can exploit such vulnerabilities. A three-step procedure has been applied to deﬁne such criteria: (1) an initial set has been deﬁned; (2) the initial set has been applied to some papers, aiming at collecting feedback to reﬁne and ﬁnalize the criteria. Finally, (3) all papers  
  A RR on Fuzz Security Testing for Software Protocol Implementations  
  Limitations of the Study  
  The main limitation of the study aﬀects the generalization of the RR’s ﬁndings and it concerns the analysis of the literature (i) conducted by only one author, which might have introduced a bias, and (ii) the adoption of only one data source for searching the literature to be analyzed, that can have left some papers out of the review. Another threat to the validity of our RR concerns (iii) the set of criteria adopted in the in-depth analysis of each papers. We are conscious that diﬀerent criteria could be adopted and could lead to diﬀerent results and ﬁndings. These threats can limit the validity of our work but they are expected for a RR [3].  
  10  
  1  
  We applied a two-step approach to analyze the one-sentence descriptions, manually written in the analysis phase by the author for each considered paper, and that brieﬂy summarizes the data generation strategy of fuzzing approaches and techniques (Strategy description). (1) Group the one-sentence descriptions, based on the their textual commonality, and (2) analyze the inter-cluster descriptions, by looking for relevant terms and their connection. We excluded the papers reviewing the literature from this analysis. To this aim, we ﬁrst used Lexos10 . Lexos analyzes each sentence (i.e., document) by: (i) tokenizing it, (ii) removing stop-words, (iii) computing Term Frequency-Inverse Document Frequency (TF-ID), with the aim of reﬂecting the relevance of each term with respect to the whole term’s corpus (terms of all sentences), and ﬁnally, (iv) applying the K-Means clustering algorithm to group sentences according to their terms. Figure 1(a) shows the clusters of the one-sentence strategy descriptions. Figure 1(a) clearly shows that three groups of strategy descriptions emerged from the investigated papers. We then used Voyant11 to extract the Collocated Graphs built by considering all the one-sentence strategy descriptions within each group/cluster. A collocated graph represents terms of a text that occur in close proximity as a force-directed network graph; dark boxes in the graph are the most frequent terms, while edges are relations between contextual terms. In our case, such a graph highlights the relationship among relevant and common terms used to describe the input/data generation strategy of each approach presented in each paper. Figure 1(b) (c) (d) present the three Collocated Graphs built by considering all the one-sentence strategy descriptions within 10 11  
  17  
  Table 7. RR results: Vulnerability and attack Type of Vulnerability Buﬀer overﬂow (7), Heap buﬀer overﬂow (5), Null-pointer-de/referenced (5), Stack buﬀer overﬂow (5), Memory errors (4), Use-after-free (4), Improper Input Validation (3), Improper Memory Allocation (3), Response timeout - DoS (3), Divide by Zero (2), Integer overﬂow (2), Invalid sequence (2), Malformed packets (2), Out-of-bound read (2), Segmentation Fault (2), Assertion violation (1), Authentication vulnerabilities (1), Command injection (1), Cross-site scripting XSS (1), Display error happens (1), Ethereum speciﬁc vulnerabilities (e.g., message and function calls, timestamps/block number dependencies, exceptions) (1), Global buﬀer overﬂow (1), Improper data-access in messages (1), Improper variable assignment (1), Inﬁnite Loop (1), Memory leakage (1), Overheating (1), Power loss (1), String vulnerabilities (1), switch set (1), Uncontrolled resources (1), Unexpected device behavior (1), Vulnerability due to communication latency (1), Web-type vulnerabilities (1) Type of the attack Denial of Service - DoS (15), Remote code execution - RCE (4), Information leakage (2), Advanced Persistent Threats (1), Command injection (1), Control-ﬂow attacks (1), Data tampering (1), Eavesdropping (1), Elevation of privilege (1), Impersonation (1), Integrity attack (1), Man-in-the-middle -MITM (1), Memory corruption (1), Ping of Death (1), Security bypass (1), Web-based attacks (1)  
  5  
  Final Remarks  
  This section collects observations and evidences we derived from our RR, that is essential questions that can guide us when we have to start fuzzing, limitations of the literature, and paper’s conclusions. Start Fuzzing. We observe that when we have to start working with fuzz testing, we have to clearly identify the target protocol and the target protocol implementation that we have to test. We need to acquire some essential knowledge about the protocol (e.g., Do we know type and format of the protocol data and the protocol speciﬁcations?) and also about the speciﬁc protocol implementation (e.g., What is the technology adopted? Which type of system architecture is implemented?). By analyzing such an information, relevant aspects of the protocol have to be clearly identiﬁed (e.g., Is it a state or stateless protocol? Do the protocol adopt protection mechanisms such as authentication and message encryption?). Then, we have to identify the artifacts that we can use as input source for fuzzing (e.g., Do we have the protocol speciﬁcations? Is the data model available? Do we have some data samples? Do we have the implementation source code or a binary image of the protocol implementation? Can we capture some real traﬃc data? Do we have functional test cases to be used as seeds?). Another aspect to consider is the type of vulnerability we are mainly interested to discover (e.g., Which type of vulnerability and attack we are interested in limiting?). This is also related to the capability of monitoring the target  
  18  
  A. Marchetto  
  system and to analyze its response during fuzzing (e.g., What can be monitored in the target system?). Finally, a key aspect concerns the selection of the tool (e.g., Can I use an existing tool to fuzzing my target system?). We can select an existing tool or, in most of the cases, plan to dedicate eﬀort to extend an existing tool to customize it for our target system. Limitations of Fuzz Approaches and Tools. We observe the following limitations of the existing approaches that need to be considered by practitioners that have to work with fuzz testing. – It is often unclear and under-investigated how initial seeds are composed and generated. Even if it is a crucial enabler for fuzzing, in most of the cases, the seeds’ composition is not considered. – Only few tools are available and most of them are based on only two “parents” tools. Furthermore, since fuzzing depends on the technology of the target system, some development eﬀort to customize such tools is expected before adopting them. – Often, the smartness/intelligence degree of existing tools appears limited and they require manually-deﬁned data/protocol models and speciﬁcations. This implies human eﬀort and domain knowledge, thus limiting the adoption of fuzzing. – Modern tools recover data/protocol models and speciﬁcations from traﬃc samples, aiming at reducing the required human involvement. However, the adoption of these tools is limited by several factors, e.g., ad-hoc recovery capabilities are needed for dealing with speciﬁc protocols, which require speciﬁc knowledge and a non-trivial amount of traﬃc samples. Furthermore, the use of data encryption and protection mechanisms can make these tools inefﬁcient. – Tools have a limited capability of monitoring and analyzing both the target system and the responses received from such a system, when the system is exercised during fuzzing. In particular, often, in embedded and IoT systems, the monitoring is limited to system crashes and memory management. This strongly restricts the discovery of sets of vulnerabilities. – The complexity of fuzz testing increases when state-based protocols or protocols that use authentication and protection mechanisms have to be tested. Most of existing tools are based on single-data sessions, or pairs of fuzz test requests and response checks; however, this is not enough for such kind of situations. Conclusions. This paper reports the ﬁndings of our RR on fuzz security testing for software implementations of communication protocols. We can summarize the main evidences as follows: (i) Industrial Control System and IoT protocols are the most studied ones; (ii) black-box fuzz testing is often investigated and the proposed approaches require protocol or data speciﬁcations as input; (iii) the investigated gray-box fuzzing approaches, often, drive the test generation with  
  A RR on Fuzz Security Testing for Software Protocol Implementations  
  University of Oslo, Oslo, Norway 2 Testify AS, Oslo, Norway [email protected]   
  Abstract. Generating production-like test data that complies with privacy regulations, such as the General Data Protection Regulation (GDPR), is a signiﬁcant challenge in testing data-intensive software systems. In our previous research, we posed this challenge as a language modeling problem. We trained a language model to capture the statistical properties of production data, and showed that it can eﬀectively generate production-like test data. However, the richness of the generated data in our earlier work was limited by the information capacity of the domain-speciﬁc language that we used for representing the data and the training corpus. In this paper, we present an enhanced approach, by using a more expressive domain-speciﬁc language with a higher information capacity. We show that using the new domain speciﬁc language allowes better leveraging the deep-learning technology and generate even richer, production-like test data. Our experiment results show that with higher information capacity and constraints complexity, the new language performs better regarding generated data quality, with an aﬀordable increase on computational cost.  
  Keywords: synthetic data generation deep learning · language modelling  
  23  
  – we measure and quantify the impact of the choice of the DSL on the modeltraining performance and quality of the generated data, by applying our approach to a real-world case study. The remainder of the paper is organized as follows. We introduce our case study in Sect. 2 before we present the two DSLs and discuss their expressiveness and information capacity in Sect. 3. Section 4 presents our language model evaluation framework, and Sect. 5 presents our experiments, results and comparison between the two languages. We discuss related work in Sect. 6 and conclude the paper in Sect. 7.  
  2  
  Abstract Data Model  
  The software systems of the NPR and its data consumer organisations are eventbased systems. In these systems, each record of personal data, and any event that happens to a person (birth, marriage or relocation) that gets registered into NPR, are stored as XML documents. We model the NPR data as an abstract event-based model in Fig. 1. An event-based system can be seen as a collection of StatefulEntities, whose states can be altered by events. Each stateful entity (or entity for short) consists of a collection of StateDescriptors. The StateDecriptors are typed, and each type of StateDescriptor describes one group of information about the entity. At any time, an entity contains only one currently applicable instance of one type of StateDescriptor. An event consists of one or more StateDescriptors. An event  
  24  
  Domain Specific Language Design and Comparison  
  In event-based systems, in order to build a synthetic population that is dynamic and statistically representative, it is suﬃcient to generate statistically representative events. Statistically representative events propagate through the systems and maintain a statistically representative state of the population. 3.1  
  Domain Specifical Language - Steveflex  
  The Steveflex language design is also based on the abstract data model. We design a set of structural tokens to denote the elements in the abstract data model, as listed in Table 1. The event type, the current state of the entity and the event details are denoted with token T, S and E. A state descriptor consists of the state type and the state description. Angle brackets encloses the type  
  DSL for Test Data Generation  
  Evaluation Framework  
  Language models are evaluated based on their performance in downstream tasks. In the context of deep learning language models applied to generate productionlike data, we conduct experiments to address the following research questions: – Syntactic validity: to what extent is the generated data valid? – Statistical representativeness: to what extent is the generated data representative? – Semantic validity: to what extent does the generated data conform to the con- straints? While only syntactically valid data holds value for testing purposes, it is the combination of representative and semantically valid data that creates realistic testing scenarios. It is important to note that the evaluation of statistical representativeness and semantic validity is carried out exclusively on the subset of data that meets the criteria for syntactic validity. Syntactic Validity. The syntactic validity rate represents the percentage of generated data that comply with the syntax and grammar of the DSL. Representativeness. Representativeness is evaluated by measuring the similarity between the distributions (and joint distributions) of information ﬁelds in the generated and training data, but only on syntactically valid data. A person’s data record in the NPR has many ﬁelds, including name, gender, birth date, address, marital status, family relation, and many others. The events in NPR also have many ﬁelds, and event type is one of them, and many event speciﬁc information ﬁelds, such as spouse information for marriage event, and original country for immigration event. Similarity of distributions and joint distributions of these information ﬁelds indicates how well the generated person and events represents the real data in the NPR We measure distribution similarity with the symmetric and bounded JensenShannon divergence (JSD) metric [5], which is zero when two probability distributions are identical and grows to a maximum value of 1.0 as the distributions diverge.  
  30  
  Semantic Validity. The semantic validity rate represents the percentage of generated data that conforms to the application domain’s constraints. These constraints can be speciﬁed in any logic language, and we use Python in Example 3 to illustrate the format of the constraints: given a logical expression as the condition, the semantic validity equals the evaluation result of a logical expression. A constraint with such a deﬁnition speciﬁes a relationship between two or more data ﬁelds. Example 3 (Constraint definition). if condition : semantic_validity = expression  
  The following is an example of a constraint in the NPR domain: if EventType == " Marriage " : semantic_validity = ( person . age >= 18) and ( person . civilStatus not in [ " pertnership " , " married " ]) and ( person . c u r r e n t S p o u s e O r P a r t n e r I n f o = null ) and ( event . spouseAge >= 18)  
  This constraint speciﬁes that if an event is of type marriage, the person it happens to should be at least 18 years, has a civil status that is neither married nor partnership and has no registered spouse or partner in the national registry. Additionally, the event must state that the new spouse is also at least 18 years old. The validity rate for each constraint is the ratio of the number of generated strings that are valid for this constraint, to the total number of generated strings. Given a set of constraints, the aggregated or total validity rate is the ratio of the number of total data records for which all of the constraints are valid to the total number of data records. Due to the complexity of real-world domains and applications, exhaustively checking every constraint in a domain is impossible. High conformance to a representative subset of the constraints can indicate that the model has learned the business rules of the domain well. Therefore, a high semantic validity rate for a subset of the constraints implies that the majority of the other constraints will also hold for the majority of the generated data.  
  5  
  Cyclomatic complexity Valid rate  
  State ID number 2 11 State civil status 20 Event death Event Change in civil status 27  
  Event Change C i v i l S t a t u s " " " isEventValid_ChangeCivilStatus ( event , personState ) : s e m e n t i c _ v a l i d i t y = True i f e v e n t . h a s E n t i t y C i v i l S t a t u s == F a l s e return False match e v e n t . e n t i t y C i v i l S t a t u s . civilStatus : case [ " married " | " registeredPartnership " ] : i f event . e n t i t y C i v i l S t a t u s . h a s S p o u s e I n f o == F a l s e : return False c a s e [ " unknown " ] : pass c a s e _: i f event . e n t i t y C i v i l S t a t u s . h a s S p o u s e I n f o == True return False  
  match e v e n t . e n t i t y C i v i l S t a t u s . operationCode : case " RegisterNew " : i f personState . hasEntityCivilStatus == F a l s e : r e t u r n True i f personState . entityCivilStatus . h a s O n e A p p l i c a b l e == F a l s e : r e t u r n True match e v e n t . e n t i t y C i v i l S t a t u s . civilStatus : c a s e " unknown " : i f personState . h a s E n t i t y C i v i l S t a t u s == True return False case [ " married " | " registeredPartnership " ] : i f personState . entityCivilStatus . c i v i l S t a t u s in [ " married " , " r e g i s t e r e d P a r t n e r s h i p " ] : return False c a s e _: i f personState . entityCivilStatus . c i v i l S t a t u s in [ " married " , " r e g i s t e r e d P a r t n e r s h i p " ] r e t u r n True return False case " AlterCurrentState " : i f personState . hasEntityCivilStatus == F a l s e : return False i f personState . entityCivilStatus . h a s O n e A p p l i c a b l e == F a l s e : return False i f event . e n t i t y C i v i l S t a t u s . c i v i l S t a t u s == p e r s o n S t a t e . entityCivilStatus . civilStatus return False r e t u r n True case " CancelState " : i f personState . hasEntityCivilStatus == F a l s e : return False i f personState . entityCivilStatus . h a s O n e A p p l i c a b l e == F a l s e : return False i f event . e n t i t y C i v i l S t a t u s . c i v i l S t a t u s == p e r s o n S t a t e . entityCivilStatus . civilStatus r e t u r n True return False case " AlterHistoricalState " : i f personState . hasEntityCivilStatus == F a l s e : return False i f personState . entityCivilStatus . h a s N o n A p p l i c a b l e == F a l s e : return False i f event . e n t i t y C i v i l S t a t u s . c i v i l S t a t u s == p e r s o n S t a t e . entityCivilStatus . history . civilStatus return False return sementic_validity  
  B  
  check e v e n t v a l i d i t y """ Event Death " " " isEventValid_death ( event , p e r s o n S t a t e ): i f e v e n t . h a s E n t i t y D e a t h == F a l s e : return False match e v e n t . e n t i t y D e a t h . o p e r a t i o n C o d e : case " RegisterNew " : i f p e r s o n S t a t e . h a s E n t i t y D e a t h == True : i f personState . entityDeath . h a s O n e A p p l i c a b l e == True : return False case [ " AlterCurrentState " | " CancelState " ] : i f p e r s o n S t a t e . h a s E n t i t y D e a t h == False : return False i f personState . entityDeath . h a s O n e A p p l i c a b l e == F a l s e : return False case " AlterHistoricalState " : i f p e r s o n S t a t e . h a s E n t i t y D e a t h == False : return False i f personState . entityDeath . h a s O n e N o n A p p l i c a b l e == F a l s e : return False i f personState . h a s E n t i t y R e s i d e n c y A d d r e s s == True i f e v e n t . h a s E n t i t y R e s i d e n c y A d d r e s s == False return False i f event . entityResidencyAddress . h a s O n e A p p l i c a b l e == F a l s e return False i f p e r s o n S t a t e . h a s E n t i t y S t a t u s == F a l s e r e t u r n True i f person . e n t i t y S t a t u s . hasOneApplicable == F a l s e r e t u r n True i f e v e n t . h a s E n t i t y S t a t u s == True : i f event . e n t i t y S t a t u s . operationCode == " C a n c e l S t a t e " and e v e n t . e n t i t y S t a t u s . s t a t e u s == " Dead " : r e t u r n True i f p e r s o n S t a t e . h a s E n t i t y S t a t u s == F a l s e : return False i f not ( p e r s o n S t a t e . e n t i t y S t a t u s . h a s O n e A p p l i c a b l e and p e r s o n S t a t e . e n t i t y S t a t u s . s t a t u s == " Dead " ) : return False i f ( event . entityDeath . hasOneApplicable and event . e n t i t y D e a t h . hasDate ) : r e t u r n True return False  
  Event Change C i v i l S t a t u s " " " isEventValid_ChangeCivilStatus ( event , personState ) : s e m e n t i c _ v a l i d i t y = True i f e v e n t . h a s E n t i t y C i v i l S t a t u s == F a l s e return False match e v e n t . e n t i t y C i v i l S t a t u s . civilStatus : case [ " married " | " registeredPartnership " ] : i f event . e n t i t y C i v i l S t a t u s . h a s S p o u s e I n f o == F a l s e : return False c a s e [ " unknown " ] : pass c a s e _: i f event . e n t i t y C i v i l S t a t u s . h a s S p o u s e I n f o == True return False match e v e n t . e n t i t y C i v i l S t a t u s . operationCode : case " RegisterNew " : i f personState . hasEntityCivilStatus == F a l s e : r e t u r n True i f personState . entityCivilStatus . h a s O n e A p p l i c a b l e == F a l s e : r e t u r n True match e v e n t . e n t i t y C i v i l S t a t u s . civilStatus : c a s e " unknown " : i f personState . h a s E n t i t y C i v i l S t a t u s == True return False case [ " married " | " registeredPartnership " ] : i f personState . entityCivilStatus . c i v i l S t a t u s in [ " married " , " r e g i s t e r e d P a r t n e r s h i p " ] : return False c a s e _: i f personState . entityCivilStatus . c i v i l S t a t u s in [ " married " , " r e g i s t e r e d P a r t n e r s h i p " ] r e t u r n True return False case " AlterCurrentState " : i f personState . hasEntityCivilStatus == F a l s e : return False i f personState . entityCivilStatus . h a s O n e A p p l i c a b l e == F a l s e : return False i f event . e n t i t y C i v i l S t a t u s . c i v i l S t a t u s == p e r s o n S t a t e . entityCivilStatus . civilStatus return False r e t u r n True case " CancelState " : i f personState . hasEntityCivilStatus == F a l s e : return False i f personState . entityCivilStatus . h a s O n e A p p l i c a b l e == F a l s e : return False i f event . e n t i t y C i v i l S t a t u s . c i v i l S t a t u s == p e r s o n S t a t e . entityCivilStatus . civilStatus r e t u r n True return False case " AlterHistoricalState " : i f personState . hasEntityCivilStatus == F a l s e : return False i f personState . entityCivilStatus . h a s N o n A p p l i c a b l e == F a l s e : return False i f event . e n t i t y C i v i l S t a t u s . c i v i l S t a t u s == p e r s o n S t a t e . entityCivilStatus . history . civilStatus return False return sementic_validity  
  Therefore, industries like automotive introduce methods for testing systems at all stages of development utilizing physical and 3D simulation. Especially in the case of autonomous driving, such virtual veriﬁcation becomes inevitable because of the vast number of potential interactions between the autonomous car and its environment that we need to consider for quality assurance [7,14,17]. However, when relying on simulation, someone might be interested in answering whether testing based on simulation is suﬃcient for fault detection. Unfortunately, there is only little scientiﬁc work tackling this question. Sotiropoulos and colleagues [11] carried out experiments in the area of mobile robotics, where they used simulation to detect faults already discovered in reality. The paper indicates that a substantial part of all faults can be identiﬁed in simulation too. However, utilizing the simulation for testing also comes with challenges. El Mostadi and colleagues [9] discussed several technical challenges inﬂuencing simulation outcomes to virtual testing. In this paper, we contribute to the question regarding the diﬀerence between virtual and real veriﬁcation and validation. In particular, we consider the area of computer vision and focus on testing photometric stereo applications. Such applications use pictures from diﬀerent angles or lightning conditions to extract 3D models. In our case, we consider an implementation for quality assurance of riblet surfaces. Riblets, which mimic natural surfaces like shark skin, are microstructures aiming at reducing drag. Application areas are wind turbines, airplanes, and other devices where drag negatively inﬂuences performance. Therefore, any damages in riblets decrease drag reduction (see e.g. [8]) and may lead to replacing the surface. For more information regarding the system under test and its testing, we refer to previous publications [15,16], where we utilized an image modiﬁcation framework for generating tests. We used modiﬁcation operators that mimic faults potentially occurring in practice, like changing light conditions, missing pixels, or rotations. In particular, we focus on the rotation operator. We compare the outcome of the riblet inspection tool [2] on surfaces where we apply manual rotation and rotations of images utilizing the corresponding operator from an image modiﬁcation framework. This comparison partially answers whether testing based on an image modiﬁcation framework captures reality and is the main contribution of this paper. It is worth noting that besides the already introduced papers, there is little research on testing riblet inspection tools and photometric stereo systems. Research in riblet inspection focuses mainly on improving photometric stereo and other algorithms rather than on testing implementations and systems. For example, [5] presents a CNN-based photometric stereo method that learns the relationship between the input and surface normals of the scene. It improves conventional BRDF-based photometric stereo algorithms by taking the global light into account and not only the artiﬁcial light sources. Alternatively, [6] introduces a photometric stereo method that makes the photometric stereo more robust from image noise, and [10] develops a method for testing drag-reduction riblet-surfaces based on the Spaldig formula. Most of the papers dealing with testing computer vision focus on the underlying methods, e.g., neural networks. Applications of testing include checking  
  F. Wotawa et al.  
  the robustness of neural networks. [3] creates perturbations (modiﬁcations) of images that a neural network fails to detect or classify correctly. The main focus is road signs since they are the most vulnerable objects allowing easy manipulation with potentially catastrophic eﬀects. [4] suggest types of attacks on data that a machine learning algorithm uses to train and test itself. It also points out machine learning algorithms that cannot defend against these attacks. [12] shows that diﬀerent realistic image transformations, like contrast or the presence of fog, can be used to generate tests that increase neuron coverage. Their experiments show that we can use these images for retraining neural networks. Their paper uses the testing results as new input for the system to learn and improve. [13] is another paper considering image misclassiﬁcation of neural networks due to adversarial manipulation. In contrast to these papers, we do not focus on a particular computer vision method like neural networks for image classiﬁcation. Instead, we are interested in the impact of real versus simulated modiﬁcations on the outcome of computer vision algorithms like photometric stereo. We organize this paper as follows: We start introducing the underlying overall framework and setup of photometric stereo. Afterward, we discuss the carriedout experiments. The experimental part of this paper is central. We discuss the setup, the obtained results, and threats to validity. Finally, we summarize the paper.  
  2  
  Fig. 1. Principle of the photometric stereo application.  
  each lighting angle. In addition, the software implements the photometric stereo algorithms. For this, it requires a calibration procedure, which uses a sample of four images of the riblet and an image of ﬁve spheres. The calibration procedure shall be performed before each new experiment. Afterward, the implemented algorithm takes the image sample and constructs a 2.5D model. The output of this process using the four images is information about the surface normals in x, y, and z directions. These normals are vectors that show the surface orientation. In addition, the tool provides the Albedo map that gives information about the reﬂectivity or brightness of the surface. It is a 2D image that shows the variations in the surface’s reﬂectance properties across diﬀerent regions. The photometry output information is input for the inspection part, which is implemented in Matlab. The inspection output is a defect map showing the defect, scratch, and abrasion pixels detected. Apart from the defect map, the inspection tool also outputs a general info text ﬁle, which shows all the inspection results in a textual format. We use the provided textual information for evaluation. In our previous work [15], we tested the described riblet inspection tool using diﬀerent image modiﬁcation methods. These methods include: increasing and decreasing image brightness, changing the pixel color in random areas of the image, image rotation, color inversion, and simulating camera lens distortion. We used three diﬀerent samples of riblet surfaces to apply the modiﬁcations. The carried-out tests revealed a signiﬁcant impact on the system’s output under the test of the modiﬁcation method used. Therefore, we developed an automation framework for computer vision applications to generate and carry out the tests. We implemented this general framework for testing computer vision applications in Python, utilizing native and third-party libraries for data management, image modiﬁcation, and user interface. We presented the ﬁrst version of the framework in a previous paper (see [16]), and the latest version in [15]. The framework handles three main components (see Fig. 2). The ﬁrst is the system under test. It is used without any changes and connected to the rest of the framework via a user interface. The second component is the modiﬁcation component. The modiﬁcation component implements several modiﬁcation methods and is independent of the image format. The last component is the evaluation component. In this component, we call the modiﬁcation methods on the image  
  • In the present discussion’s static approach, the goal is to prove the program correct. The semantics of the check instruction is that it is correct if and only if the condition C alway has value True at the given program point. If the prover cannot establish that property, the proof fails. In a general FP-FT approach, the key property is that in the static view, if the proof fails, an SMT-based prover will generate a counterexample. In the Seeding Contradiction approach, C is False: the proof always fails and we get a counterexample exercising the corresponding branch—exactly what we need if, as part of a regression test suite, we want a test exercising the given branch. For the simple code seeded with a check False end, such a counterexample will, by construction, lead to execution of the ﬁrst branch (a > 0) of the conditional. If we have an eﬃcient mechanism to turn counterexamples into tests, as described in earlier work [13,14], we can get, out of this counterexample, a test of the original program which exercises the ﬁrst branch of the conditional. Such a generated test enjoys several interesting properties: • It can be produced even in the absence of a formal speciﬁcation (contract elements such as the postcondition above). • Unless the enclosing code (here the routine simple) is unreachable, the test can be produced whether the program is correct or incorrect. • If the program is correct, the test will pass and is useful as a regression test (which may fail in a later revision of the program that introduces a bug). • Generating it does not require any execution. • That generation process is fast in practice (Sect. 5). The next sections will show how to generalize the just outlined idea to produce such tests not only for one branch as here but for all branches of the program, as needed to obtain an exhaustive-coverage regression test suite. 2.2  
  Block Variables  
  The solution to this “Seeded Unreachability” issue is to make the check themselves conditional. In the seeded program, for every routine under processing, such as simple, we may number every basic block, from 1 to some N, and add to the routine an argument bn (for “block number”) with an associated precondition require bn ≥ 0 -- See below why 0 and not 1. bn ≤ N  
  To avoid changing the routine’s interface (as the addition of an argument implies), we will instead make bn a local variable and add an initial instruction that assigns to bn, non-deterministically, a value between 0 and N. Either way, we now use, as seeded instructions, no longer just check False end but if bn = i then check False end end  
  around too, but do not require it.) For the SC strategy we are interested in the trivial case for which C is False. Also for simplicity, we assume that all correctness properties are expressed in the form of check instructions; in particular, we replace any contract elements (preconditions, postconditions, loop invariants and variants, class invariants) by such instructions added at the appropriate places in the program text. With this convention, a block is correct if all its check instructions are, and a program is correct if all its blocks are. For a normally written program, this deﬁnition means that the program is correct in the usual sense; in particular, if it has any contracts, it satisﬁes them, for example by having every routine ensure its postcondition. The SC strategy, by adding check False end to individual blocks, makes these blocks—and hence the program as a whole—incorrect. A test suite is a collection of test cases for a program. A test suite achieves exhaustive coverage if for every reachable block in the program at least one of its test cases causes that block to be executed. (Note the importance of having a reachability-sound prover: if it could wrongly mark some reachable blocks as unreachable, it could wrongly report exhaustive coverage, which is not acceptable. On the other hand, if it is reachability-sound, it may pessimistically report less-than-exhaustive coverage for a test suite whose coverage is in fact exhaustive, a disappointing but not lethal result. This case does not occur in our examples thanks to the high quality of the prover.) A test-suite-generation method (such as Seeding Contradiction) is coveragecomplete if the generated test suite achieves exhaustive coverage for any correct program. In other words, for each reachable basic block of a correct program, at least one test in the suite will execute the block. Finally, consider a prover that can generate counterexamples for programs it cannot prove correct. The prover is counterexample-complete if it generates a counterexample for every block that it determines to be reachable and incorrect. With these conventions, the correctness of the Seeding Contradiction method is the property (proven next) that If the prover is reachability-sound, correctness-sound and counterexamplecomplete, SC is coverage-complete. 3.2  
  Proof of Correctness  
  Optimized gnome sort 7 8 –  
  • AutoTest does not guarantee that the test inputs satisfy the routine’s precondition, while SC and IntelliTest always generate precondition-satisfying test inputs. The reason is that SC and IntelliTest rely on the results of constraint solving, where the routine’s precondition is encoded as an assumption and will always be satisﬁed. • The SC approach is has a prerequisite: the program under test has to be proved correct (the proof of the original program has no failure), while AutoTest and IntelliTest have no such constraint. • As to the values of the generated test inputs, IntelliTest and AutoTest always apply small values that are easy to understand. SC initially produces test inputs that may contain large values; its “minimization” mechanism [14] corrects the problem.  
  6  
  Conclusions and Future Work  
  The approach presented here, Seeding Contradiction (SC), automatically generates test suites that achieve exhaustive branch coverage very fast. The presentation of the approach comes with a proof of correctness, deﬁned as the guarantee that the generated test suite achieves exhaustive coverage (full coverage of reachable branches). While technical limitations remain, the evaluation so far demonstrates the eﬀectiveness and eﬃciency of the SC approach through the comparison with two existing test generators IntelliTest and AutoTest, in terms of achieved coverage, generation time, and size of the test suite. Ongoing work includes handling larger examples, processing entire classes instead of single routines, providing a mechanism to generate tests covering branches that a given test suite fails to cover, and taking advantage of the SC strategy to identify dead code. Acknowledgement. We are particularly grateful, for their extensive and patient help, to Yi Wei (AutoTest) and Jocelyn Fiat (EiﬀelStudio and AutoProof). The paper beneﬁtted from perceptive comments by the anonymous referees on the original version.  
  Seeding Contradiction: Fast Generation of Full-Coverage Test Suites  
  Vestel Electronics, Manisa, Turkey [email protected]  Ozyegin University, İstanbul, Turkey [email protected]   
  Abstract. There are various kinds of software applications like mobile and Web applications. These applications have diﬀerent types of user interfaces and user interaction methods. Hence, test automation tools are either dedicated or conﬁgured for a particular kind of application. Test scenarios can be implemented in the form of scripts and test execution can be automated separately for each type of application. However, there are systems of systems that embody multiple types of applications deployed on various platforms. Test scenarios might cross-cut these applications to be controlled collectively in the test script. In this paper, we propose an approach for testing cross-platform systems of systems. We present an application of it on a real system that involves a mobile and a Web application that are supposed to work in coordination. Our approach integrates a set of existing tools to facilitate test automation. It provides testers with a uniﬁed interface for developing test scripts that involve both mobile and Web applications. We conduct an industrial case study and show that our tool can reduce the testing eﬀort signiﬁcantly. Keywords: Test automation · Systems of systems · Mobile applications · Web applications · Behaviour driven development  
  1  
  Fig. 1. A test scenario that interacts with multiple cross-platform applications.  
  There have been studies [6–8,10] to explore test automation strategies when dealing with multiple platforms. However, these studies focus on testing a single system that is deployed on various platforms. Their aim is to identify inconsistencies across platforms, such as cross-platform inconsistencies for mobile applications [10] and cross-browser inconsistencies for Web applications [8]. Additionally, in cases where diﬀerent user interfaces are adopted for a wide range of platforms, the research goal is to reduce duplicated testing eﬀorts for these platforms [6]. We tackle a diﬀerent problem in this paper. Our goal is to facilitate the automated testing of multiple, cross-platform systems that are supposed to work in coordination. We integrate a set of existing tools to provide testers with a uniﬁed interface for developing test scripts involving both mobile and Web applications. We facilitate test automation by following the Behavior-Driven Development (BDD) approach [13]. Test scripts can be developed as a composition of natural-language constructs that access multiple platforms concurrently. We present an application of our approach on a real system that involves a mobile and a Web application working in coordination. We conduct an industrial case study and show that our approach can reduce the testing eﬀort signiﬁcantly. The remainder of this paper is organized as follows. We summarize the related studies in Sect. 2. We explain the implementation of our approach in Sect. 3. We present a motivating example and a case study from the industry in Sect. 4. Finally, in Sect. 5, we conclude the paper.  
  2  
  platforms. In this work, we focus on the testing of systems of systems, where each involved system is deployed on a diﬀerent platform. The increasing number and variety of platforms increase test automation eﬀorts as well [6]. Frameworks like Apache Cordova, Xamarin, and React Native enable the development of a single application that can be deployed on multiple platforms. However, a separate test script has to be developed for testing the application on each platform. x-PATeSCO [14] automatically generates test scripts for multiple platforms. However, it focuses on testing a single application at a time, unlike our approach. Test scripts can be fragile due to changes in the GUI layout or code, as well as diﬀerences among the deployed platforms. Visual GUI Testing (VGT) [1] gained popularity due to its adaptability and resilience to these changes and diﬀerences. However, VGT approaches proposed so far focus on testing a single application rather than multiple types of applications on various platforms at the same time. Appium [9] supports test automation across many platforms covering mobile (iOS, Android, Tizen), browser (Chrome, Firefox, Safari), desktop (macOS, Windows), and TV (Roku, tvOS, Android TV, Samsung) applications. It also supports several programming languages (e.g., Java, Pyhton, Ruby) for developing test scripts. However, to use Appium, one needs to set up a set of drivers and plugins depending on the target platform, and programming experience is necessary to develop test scripts. Our goal is to provide support test automation for multiple platforms at the same time and at a hig level of abstraction by following BDD principles [13]. With our approach, test scripts can be written using natural-language constructs that access multiple platforms concurrently, without worrying about the underlying setup. In the following section, we discuss our approach and its implementation in detail.  
  3  
  Abstract. TQED is a universal test heuristic that assists testers in creatively designing eﬀective test cases. It involves deﬁning the test problem in terms of component elements, each of which is classiﬁed into one of the four so-called dimensions, which are: time (T), quantity (Q), event (E) and data (D). Then, test ideas are created by considering speciﬁc combinations of the components, aided by the interpretation of combinations of dimensions. In this article, we compare the TQED model with other well-known test heuristics and risk analysis techniques, and then present an empirical veriﬁcation of the eﬀectiveness of the TQED model. We compare the eﬀectiveness of tests written by 24 developers who were asked to implement code for the same problem, together with the unit tests. The developers were divided into two groups, one of which used TQED when designing unit tests and the other did not. Eﬀectiveness was measured in terms of code coverage, mutation coverage and failure rate of test cases. To increase the objectivity of the study, a cross-experiment was conducted in which each developer’s tests were run on the source code of all other developers. Our research showed that TQED can signiﬁcantly support testers in creating strong tests that are more likely to detect defects in code.  
  Keywords: test heuristics creativity  
  Formal black-box test design techniques (cf. [4,7,19,21]) tend to be very speciﬁc, as they allow us to, among other things, provide speciﬁc test cases by covering the relevant elements of the models they use. For example, Boundary Value Analysis gives us the exact test data values that test cases should cover. However, each of these techniques is a model of only one speciﬁc aspect of the system under test. Hence, their universality is limited. White-box techniques can be considered not very prescriptive and not very versatile. This is because each white-box technique can only be applied to a speciﬁc work product (e.g., source code). In addition, these techniques do not provide speciﬁc test data, but only deﬁne criteria for covering structural elements of the test object. Fault attacks are based on speciﬁc defects and failures and do not provide the speciﬁc test data needed to cause those failures or reveal those defects. Experience-based test techniques [4,30], standards, standards and norms (e.g., the quality model [17]) are universal, but do not allow the derivation of any speciﬁc test data – this task fully rests with the tester, who is assumed to be experienced and can best design the most appropriate test data. Testing heuristics, such as those proposed by Bach [3] or Edgren [13], are also quite general. However, compared to formal test design techniques, they are less speciﬁc. For example, the “test each function, one at a time” heuristics proposed by Bach can be applied to any type of functional testing, but provides no guidance on how to do so. The TQED approach, like random testing [5,24] or metamorphic testing [6,29] techniques, can be considered a universal yet speciﬁc approach. All of these techniques oﬀer some general test case generation mechanisms that can be applied to virtually any context of use. On the other hand, these procedures are speciﬁc enough to derive speciﬁc test data (e.g., a random number generator in the case of random testing) or give speciﬁc instructions on how to obtain such data (e.g., metamorphic relations in the case of metamorphic testing). In the case of the TQED approach, the generality of the test case generation mechanism is due to the generality of the concept of the so-called problem dimensions, while the ability to derive speciﬁc test data is related to the interpretation of the combination of elements belonging to each problem dimension. The above analysis shows that the TQED method may be an interesting and useful approach with a very nice trade-oﬀ between versatility and concreteness. In Sect. 3 we give a detailed description of the method, and in Sect. 4 we verify empirically the eﬀectiveness of the TQED.  
  3  
  Fig. 2. TQED model  
  The TQED model is shown in Fig. 2. It depicts all four primary dimensions and their compositions. The three dots in the upper left corner suggest that there may be more combinations, covering three, four, etc. of the basic dimensions. Each dimension and each combination of dimensions can be interpreted by the tester in terms of actual software characteristics, which should help the tester come up with creative test ideas. The primary dimensions can stand for various concepts related to the actual software – an example interpretation is shown in table 1. The procedure for using the TQED model is as follows. First, identify the basic features of the software that can be classiﬁed into one of the basic dimensions (D, E, Q, T). Then, various sensible combinations of these features are considered. Each corresponds to a combination of their dimensions (for example, D+D, E+T, D+D+E+Q, etc.), which is subject to interpretation in terms of test conditions or test ideas. These form the basis for the test cases. Example heuristics for interpreting certain combinations of dimensions are as follows:  
  Empirical Veriﬁcation of TQED - A New Test Design Heuristic Technique  
  A. Roman et al.  
  diﬀerent partitions from diﬀerent categories. Hence, its versatility is less than that of TQED. The TQED approach is also similar to test heuristics in terms of universality – it can be applied to any type of system, be it software or hardware, regardless of the type of test, software lifecycle, etc. However, compared to test heuristics, TQED is more speciﬁc. We showed earlier that the use of TQED involves some normative rules for combining software dimensions, but it is always left to the tester to interpret them. Finally, TQED shares some similarities with the Hazard and Operability (HAZOP) approach [10] – a form of risk management to identify, evaluate, and control hazards and risks in complex processes. The HAZOP approach ﬁrst identiﬁes operational processes. From these, parameters or safe operating limits of the process elements are deﬁned so that deviations can be identiﬁed and so-called “guide words” selected. Examples of common HAZOP guiding words are: No or Not, More, Less, High, Low. With the use of guide words, workplace hazards can be clearly deﬁned as deviations that fall outside acceptable parameters or safe operating limits. However, HAZOP was designed for use in the chemical, pharmaceutical, oil and gas, and nuclear industries, not in software engineering. In addition, the set of guide words is closed, limited and directed at describing the parameters of industrial processes. TQED is ﬂexible and open in this regard. An Example of Using the TQED Approach. In this chapter, we show a simple application of the TQED that results in a failure triggering. We will test the most important feature of the well-known web-based time management application, Google Calendar - adding an event to the calendar. When we want to add an event to the calendar, we see a window like the one in Fig. 3a.  
  Fig. 3. Adding an event to Google Calendar (in Polish)  
  The creative process assisted by the TQED is ﬁguratively shown in Fig. 4. From the interface itself, we know that before clicking the ’Save’ button, the application wants us to add both the event name and its time in the ﬁeld. From here we can derive the following dimensions: D1 (event name), D2 (time), E (click the ’Save’ button). The tester can interpret the combination D1+D2 as  
  Empirical Veriﬁcation of TQED - A New Test Design Heuristic Technique  
  Conclusions  
  In this paper, we described the TQED technique for supporting testers’ creativity during the test analysis and design phase. We compared it with other similar approaches, justifying its relatively high generality while being highly prescriptive when it comes to providing ideas for test data or test ideas. We conducted an experiment that showed that the use of TQED increases the strength of test cases, expressed in terms of mutation coverage and failure rate. The results of the study allow us to conclude that TQED can be successfully applied in practice as an eﬀective heuristics for test case design. Links to Data Files. The code containing the deﬁnitions of the interfaces the participants were to implement, as well as the solutions provided by the participants, can be found at github.com/Software-Engineering-Jagiellonian/TQEDexperiment  
  D. Istanbuly et al.  
  that they wish to achieve through the act of designing tests, even if those goals are not explicitly enumerated in requirements or other documentation [9,18]. Despite the prominence of testing as a development practice, we lack a clear understanding of how speciﬁc types of testing goals inﬂuence the practice of test design. For example, Enoiu et al. proposed a model of test case design as a problem solving process [9]. This model makes it clear that tests are designed to show the attainment of speciﬁc goals, but does not discuss what common types of goals are, or how diﬀerent goal types could inﬂuence this process. The purpose of this research is to explore the types of goals that testers pursue and the inﬂuence of these goal types on the process that developers follow to design tests that assess attainment of those goals. Understanding the relationship between test case design and diﬀerent types of testing goals could provide beneﬁts to both researchers and practitioners. For example, such understanding enables characterization of test design practices and the ability to oﬀer clear guidance to developers creating tests for speciﬁc types of goals—potentially leading to improved eﬀectiveness or eﬃciency of the testing process. In addition, characterization of design practices can beneﬁt automated test generation, potentially leading to the development of more human-like generation tools [8,11]. In particular, we are interested in the exploring the types of goals pursued, the relative importance of diﬀerent goal types, quantitative relations between goal types and test cases—tests-per-goal and goals-per-test—and personal, organizational, methodological, and technological factors that may inﬂuence the relationship between goal types and the test design process. To address these topics, we have conducted a series of interviews with software developers in various domains and of varying experience. Thematic analysis of the interviews was then used to develop a survey for wider distribution. Based on analyses of the interview and survey responses, we have identiﬁed nine goal types, including correctness, reliability, performance, quality, security, customer satisfaction, risk management, improving maintenance cost, and process improvement. Correctness was ranked most important, followed by reliability and security. Customer satisfaction and maintenance cost were seen as least important, but were still valued. We focused on correctness, reliability, and quality for analysis. Test design for correctness forms a “default” design process, which is followed in a modiﬁed form for other goals. For all three goal types, several tests are needed to assess goal attainment, and tests focus on 1–2 goals at a time. Testers often start the design process following pre-existing patterns (e.g., using past tests as templates). We also make observations regarding diﬀerences in testing practices and tools employed during test design for these three goal types. We further observe that test design can be inﬂuenced by process, organization, and team structure. This study provides a foundation for future research on test case design and testing goals, and enables deeper modeling of test design as a cognitive problemsolving process. To help enable future research, we also make our thematic interview coding and survey responses available1 . 1  
  Background  
  During software testing, input is applied to the SUT and the resulting output and other observations are captured [19]. These observations are compared to embedded expectations, called test oracles, to determine whether the SUT operated within expectations. Oracles often directly reﬂect the goals of test creation [2]. Testing can take place at multiple levels of granularity [19]. At the lowest level, unit tests examine small code elements in isolation with dependencies substituted for “mock” (static) results [21]. During integration testing, dependent units are combined. Then, during system testing, high-level functionality is invoked through interfaces. Tests can be written at all three levels as executable code, using frameworks such as JUnit, PyTest, or Postman [21]. Testing can also be performed manually. This is common during exploratory testing—where humans perform ad-hoc testing based on “tours” [24]—and acceptance testing—where customers oﬀer feedback [22]. Tests are often written after code has been developed. However, test-driven development advocates for test creation before code development [14].  
  3  
  Data Collection  
  Interviews: Interviews were conducted electronically between January– February 2022. At the start of an interview, we gave a brief overview of the research topic. Interview responses were recorded, with permission, for analysis and observer triangulation. The interviews were semi-structured, following the interview guide in Table 2. However, follow-up questions were asked if further discussion was needed. The interviews were transcribed using speech-to-text software, then manually corrected through consultation with the original audio. Survey: The survey consisted of mostly quantitative questions, with a small number of open-ended questions, and was designed according to the guidelines of Linåker et al. [15]. The questions are listed in Table 3. We focused on quantitative questions to decrease the time burden [16]. To complete the survey, the participants were required to answer all multiple-choice questions, but openended questions were optional. The survey was pre-tested with two participants, and the feedback was used to clarify wording and question order. The survey was conducted using Google Forms4 . Links for the survey were then distributed via email, as well as on social media platforms. 4  
  “Write tests, write minimal code to make tests pass, examine if there are features or corner cases that don’t have tests yet, go to step one.” - SP13  
  Respondents stressed the importance of simplicity: “I try to isolate requirements and design as simple a test case as possible ... I focus on making the test easy to understand, partly by making it small and independent ... I may write multiple test cases for one requirement because there may be multiple modes of failure.” - SP4  
  There was emphasis on considering perspectives, tools, and environments: “... cover all aspects of the test, such as—bare minimum—up to maximum range of values, diﬀerent user types if they exist, positive/negative aspects, etc.” - SP30 “1. try to understand the functionality speciﬁcation 2. try to understand the test environment needed 3. trying to understand what tools are needed 4. trying to understand acceptance criteria 5. create test steps” - SP19  
  Perform Addt. Research (Risk Mgmt.)  
  Design Additional Test Cases  
  Formulate Acceptance Criteria Consider Environment, Dependencies, Repeat Execution (Reliability, Performance) Consider Sensitivity to Code Change, Documentation of Code (Maintenance Cost) Consider Recent Code Changes (Risk Mgmt.)  
  Fig. 4. Typical design process for correctness, with modiﬁcations for other goals.  
  We asked interviewees to describe their typical test design process. Although speciﬁc goal types were not considered at that time, their comments largely echo the process outlined above for correctness and illustrated in Fig. 4. Testers collect information, brainstorm, then iteratively create test cases until all functional outcomes are covered—focusing on individually simple and understandable tests. Enoiu et al. proposed that testers follow a seven stage process—identiﬁcation and deﬁnition of testing goals, knowledge analysis, strategy planning, information and resource organization, progress monitoring, and evaluation [9]. Our observations suggest that this model is largely followed when testers pursue correctness goals. Some aspects of this process may be given more or less weight at times—or even skipped entirely—depending on the form of testing or due to personal experience and preferences. For example, during unit testing, there may not be active discussions with stakeholders. However, this basic process oﬀers a basic outline for discussions on test design. The responses written about other goal types suggest that the process in Fig. 4 is followed—in a modiﬁed form—when pursuing the other goals. For example, the core diﬀerences for reliability-demonstrating tests are that (a) reliability  
  D. Istanbuly et al.  
  must be measured over a period of time, and (b), the SUT should be tested in a realistic environment—which may contain unreliable dependencies. For both reliability and performance, it was also suggested that tests must execute operations multiple times: “... stress test both that failing dependencies are correctly handled and that failures don’t happen too often over a longer time of nominal operation.” - SP4  
  For quality goals, tests examine both functional correctness and attainment of non-functional properties: “... to improve the quality, there could be some overlap of functional and non functional requirements testing here.” - SP7  
  RQ2.3 (Relationship Factors): Testers often start test design following patterns. For quality goals, testers often deviate from these patterns. System Type: The type of system may inﬂuence the goal types pursued and their importance. In Fig. 6, we indicate the percentage of respondents who target various system types. We observe a potential relationship between reliability’s importance and the level of required trust in a SUT. Correctness  
  Reliability  
  Fig. 6. % of respondents designing tests for correctness, reliability, and quality goals for diﬀerent system types.  
  Embedded systems make up the largest proportion of reliability responses. Such systems have high safety demands, and testers may need to show evidence of correctness, reliability, and quality. No respondents indicated that they develop reliability tests for mobile applications. This may be due to the lack of criticality in such programs. This observation should be further explored in future work. RQ2.3 (Relationship Factors): Demonstrating reliability is a focus for embedded systems. Reliability may not be important for mobile apps. Practices and Tools: The testing practices employed—as well as the tools— may diﬀer between goal types. Figure 7 indicates the percentage of respondents who employed diﬀerent approaches, including levels of granularity (e.g., unit testing), focuses (e.g., functional versus non-functional testing), and other practices (e.g., mocking, test-driven development). Figure 8 does the same for diﬀerent types of tools. For both, the initial set of options were derived from interviews. However, some respondents suggested additional options. “Automated Testing  
  Fig. 8. % of respondents using diﬀerent tools when designing tests for correctness, reliability, and quality goals.  
  All three goal types are pursued at all major levels of granularity. However, reliability is relatively uncommon when using human-driven practices—i.e., acceptance, exploratory, and manual testing. Reliability is often demonstrated by executing tests repeatedly or over a period of time [19]. This typically requires automation. In addition, reliability is often attained before presenting the SUT to a client, reducing the importance of acceptance testing. Test design for reliability also typically does not seem to use mocking or test-driven development, perhaps because reliability is most meaningful for a near-ﬁnal product. In contrast, quality is often a focus of GUI and human-driven practices. Some typical quality types, such as usability, rely on replicating the typical user experience. This may lead to prominent use of human-driven practices. Correctness,  
  How Do Diﬀerent Types of Testing Goals Aﬀect Test Case Design?  
  8  
  Fig. 9. Organizational factors that may aﬀect test design.  
  Mobile GUI Testing: State of the Art and Practice  
  Both industry and academia have proposed many diﬀerent tools and techniques to perform GUI testing on mobile applications and identiﬁed several challenges for the practice. We adopt two orthogonal classiﬁcation schemes to categorize mobile GUI testing techniques. The process of GUI testing revolves around the identiﬁcation of elements on the GUI. The properties and means to identify the elements are called locators. Test cases also require oracles, i.e., properties that have to be veriﬁed with assertions to verify that the test executed correctly. GUI testing techniques can be classiﬁed according to how the elements on the GUI are identiﬁed (i.e., based on the type of locator that is used) [1]: – Coordinate-based GUI testing techniques identify widgets through exact onscreen candidates. Due to major volatility of such properties, these techniques have been largely abandoned in practice. – Layout-based GUI testing techniques identify widgets through properties that are declared in the Android layout descriptor of the current screen (e.g., ids, text content, content description, widget type). – Visual, or Image recognition-based GUI testing is based on computer vision techniques, utilizing screen captures as locators. State-of-the-art image-based approaches leverage techniques ranging from pixel-per-pixel comparison to more elaborate matching algorithms (e.g., SIFT, SURF or Akaze feature vectors [3]). GUI testing techniques can also be classiﬁed according to how the test sequences are generated. Adopting a taxonomy proposed by Linares-Vazquez et al. [19]: – Automation APIs or Scripted testing tools allow manual writing of test scripts, using platform-speciﬁc scripting languages; test scripts can then be executed using dedicated or universal test runners. – Capture & Replay (C&R) testing tools automatically generate test scripts from sequences of user interactions with the AUT (Application Under Test), thus ‘capturing’ (recording) real usage scenarios [10]. – Automated Test Input Generation techniques automate the deﬁnition of the test sequences; the generation can be based on heuristics (e.g., a random selection of locators and veriﬁcation of the occurrence of exceptions or bugs) or on the coverage of a GUI model, which can be itself automatically generated [5].  
  Multi-device, Robust, and Integrated Android GUI Testing  
  Despite the availability of tools for Android GUI testing, research has highlighted that the practice is often conducted manually by developers [7]. The reasons behind the limited adoption of automated tools are manifold and speciﬁc to the mobile domain: Fragmentation: One of the main issues for Android GUI testing is the intrinsic Fragmentation of the domain: developers must verify and validate the compatibility of the AUT with multiple target conﬁgurations where it may be deployed [16]. Such conﬁgurations include screen sizes, screen densities and aspect ratios (hardware fragmentation) or diﬀerent versions of the operating system where the AUT has to be installed (software fragmentation). The fragmentation issue particularly plagues the Android domain because of the multitude of devices running the operating system and the coexistence of many releases of the o.s. (Operating System) that receive parallel support. Therefore, the tester must run identical test sequences on multiple devices, either real or emulated. This repetition represents an obstacle to the adoption of continuous integration/development practices. Fragility and Flakiness: Test scripts (either manually written or automatically generated) for mobile applications need critical maintenance to cope with even small changes in the application GUI during its life cycle. This issue is typically referred to as Fragility and is considered among the main hindrances to a wide adoption of GUI testing tools [9]. Fragility can be caused by changes in the GUI layout properties that invalidate layout-based locators and oracles or by aesthetic changes of the widgets that invalidate visual locators and oracles. The Fragility in GUI tests requires the tester to analyze the outcome of each test execution carefully. This action is needed because functionally valid test sequences may lead to false positives due to the inability to ﬁnd the locators, and proper refactoring is therefore needed for fragile locators. Test cases are hampered also by a high level of flakiness, i.e., they may have a nondeterministic outcome over repeated runs. Flaky executions can be related to unpredictable execution times, network availability, concurrency in the test devices, and interaction with the execution environment [12]. Limited generalizability: The high complexity of the GUI and the many different ways of composing the individual screens (with orchestrations of Activities, Fragments and other components) pose relevant generalizability issues to GUI testing techniques. It is not trivial to identify universal models able to represent the GUI states of any application at the desired granularity. Several eﬀorts have been provided for mutation testing [11]. As well, to the best of our knowledge, no coverage models have been deﬁned yet in the academic literature [6]. Therefore, evaluating the quality of generated test sequences or comparing multiple testing tools’ results over multiple AUTs is still a complex task. Hybrid application testing: Finally, even though Native mobile applications still represent the vast majority in the Android marketplace, many diﬀerent frameworks are available to construct hybrid or progressive web applications that are rendered on a mobile browser to guarantee a similar – if not equivalent  
  R. Coppola et al.  
  real hardware devices; however, they mainly allow pure manual and exploratory test sequences. In general, the current state of the art and practice lacks ways to automatically and extensively execute image-recognition based test suites on multiple conﬁgurations. At the end of the test execution, we envision the generation of complete test reporting, including the test cases outcome, found defects, statistics and analyses of the execution issues that can lead to reﬁnement and enhancements of the test suites. We envision that each testing environment should track the quantity of fragile or flaky test cases. A possible solution to identify ﬂaky test cases is to execute each test case multiple times in order to ﬂag the test cases as passing (all executions are passing), ﬂaky (some executions are passing) or failing (all executions fail). Failing tests can furtherly be divided into true positives (i.e., test cases failing due to real defects in the AUT) and fragile tests (i.e., test cases failing due to unrelated changes in the AUT). Sets of change-based metrics have been deﬁned in the empirical software engineering literature to measure the number of fragile tests and their impact on the maintenance eﬀort for the test suite [9]. The objective of fragility tracking is to mark the test cases as fragile if they require too much intervention during the evolution of the AUT. Other modules of the framework can use this ﬂagging activity to aid test prioritization. Regarding the Execution phase of our framework, we identify a primary research gap about the coverage measurement when mobile test cases are executed since no coverage model for mobile applications has been widely accepted by research in the ﬁeld. Albeit several precise coverage models exist (e.g., multidevice coverage proposed by Vilkomir et al. to measure the reduction of the fragmentation issue [22]), a universally generalizable coverage model is still missing. 3.3  
  Test Maintenance and Repair  
  The literature highlights that test scripts maintained manually (either layoutbased or visual) can be tedious and costly when they become obsolete. Since mobile applications typically have a quick evolution, the cost of test case maintenance can be required frequently and become unsustainable for developers. In our framework, we envision a module in charge of automatically adapting the test cases to the changes in the APK, in the GUI pictorial appearance and the application code. Since two equivalent sets of locators and oracles are maintained for the test suite, both layout properties and visual locators have to be updated automatically. Several approaches have been proposed for the automated repair of test cases. Some tools are based on event-sequence models describing the behaviour of the application and abstracting the changes made to the GUI between diﬀerent releases of the same application. These tools, however, are mainly aimed at preserving the connection between diﬀerent screens of the AUT traversed during the execution of test sequences and still require the manual intervention of the testers for the preparation of the original models that guide the testing process. Alternative model-less approaches rely on the deﬁnition of similarity indexes for the widgets to be interacted during test cases  
  Multi-device, Robust, and Integrated Android GUI Testing  
  to identify locators that should be treated as the same one even in the presence of changes in their properties or visual appearance [23]. Even incorporating a sophisticated mechanism for test case repair, some test sequences may still need manual maintenance during the evolution of the application. The execution of test cases on multiple devices and conﬁgurations, especially when the GUI has to be rendered and veriﬁed through computer vision algorithms, can also become unsustainable if the test suite grows signiﬁcantly. Therefore, a module for the maintenance of test cases should include mechanisms to prioritize and select them to reduce the execution and maintenance time for the subset of test sequences to execute in continuous integration and development settings. Test prioritization should be guided from metrics resulting from the test case execution module, e.g., generated bugs and coverage reports. At the same time, the test prioritization and selection module should incorporate diverse information gathered from repository mining. Usage metrics gathered through mobile APIs on pilot users can help identify the activities and user interaction sequences on which test cases should focus. Suppose the AUT is available as an open-source repository (e.g., on GitHub). In that case, mechanisms can be developed to mine and interpret the issues left by contributors to the project to identify the most critical sections of the code. As well, if the AUT is released on a marketplace, techniques of marketplace analysis can be deployed to mine the user reviews and identify typical usage patterns leading to crashes. We identify important research gaps regarding the Maintenance phase of our framework. To the best of our knowledge, no automated mobile test suite repair tools have been validated with real-world test suites. Similarly, no prioritization model has been speciﬁcally described for mobile GUI test cases used for regression testing, and existing ones are mostly applied to non-functional properties (e.g., for security testing [21]).  
  4  
  Conclusions  
  In this paper, we envisioned future trends in the automated GUI testing landscape and provided action points for diﬀerent stakeholders in the ﬁeld. Our framework can serve as an instrument for researchers and developers of testing tools to assess which among envisioned modules are available in the literature and the market and which need further research and development. A tentative evaluation of the proposed framework would involve assessing its potential to address the challenges and goals it sets out to achieve. In particular, the framework aims to tackle issues related to hardware fragmentation, test fragility, and test maintenance costs. It is important to note that a full evaluation would require empirical research and industrial case studies, to fully validate the beneﬁts produced by each module of the framework in addressing the challenges it aims to tackle. It is still worth stressing that some of the modules in the framework, such as fragility prediction mechanisms and translation-based tools, are still in earlystage academic investigation or have not been fully implemented by available tools. The success of the proposed framework in reducing test maintenance costs would rely on the development and integration of these modules and their eﬀectiveness in practical scenarios.  
  Softeam, Paris, France  
  Abstract. Secure software systems are crucial in today’s digital world, where there is an ever-increasing amount of IT systems, leading to more risks of exposing sensitive data and service outages. One of the key aspects of secure software development is ensuring that security requirements are met through the various stages of software development. The process of testing security requirements is often complex and timeconsuming, notably because of the gap between the veriﬁcation process of security requirements and the testing process. To address this issue and simplify the testing of security requirements, this paper proposes to use the Requirements as Code approach (RQCODE). RQCODE combines security requirements with code in a way to support automated testing and continuous veriﬁcation of security requirements throughout the software development life cycle. This paper contributes to the ﬁeld of software security by providing a practical and eﬀective approach to bridge the gap between veriﬁcation of security requirements and testing, ultimately leading to more secure software systems. Additionally, it discusses the beneﬁts of this approach, such as its ability to improve the accuracy and consistency of testing, enabling the early detection of security issues, and reducing the time and eﬀort required for security testing. It also discusses the challenges and limitations of the approach. Keywords: Security Requirements · Security Testing · Seamless Object-Oriented Requirements · Requirements As Code  
  1 1.1  
  Importance of Security Testing  
  Testing is a critical aspect of software veriﬁcation and validation, and several testing techniques are available to verify and validate software requirements, including functional testing, performance testing, and security testing [16]. Security testing is a crucial part of ensuring that these security requirements are met. By checking whether these requirements are satisﬁed under various conditions, security testing aims to uncover vulnerabilities that could be exploited by attackers. Due to the openness of modern service-oriented systems, security testing has gained much interest in recent years and has become a vast ﬁeld of research [5]. Security testing involves the evaluation of security requirements that pertain to key security properties such as conﬁdentiality, integrity, availability, authentication, authorization, and nonrepudiation. The goal of security testing is to determine whether the speciﬁed or intended security properties are correctly implemented for a given set of assets of interest. This can be achieved through conformance testing, where the system’s conformance to the security properties is assessed based on well-deﬁned, expected inputs. Alternatively, known vulnerabilities can be addressed through destructive testing, which involves the use of malicious, non-expected input data that is likely to exploit the identiﬁed vulnerabilities [?]. According to Tiang-yang and et al. [21], there are two primary approaches in security testing: security functional testing and security vulnerability testing. Security functional testing aims to validate the correct implementation of speciﬁed security requirements, including security properties and mechanisms. On the other hand, security vulnerability testing focuses on identifying unintended vulnerabilities in a system. This type of testing involves simulating attacks and penetration testing to assess the system’s resilience against potential threats. However, security vulnerability testing requires speciﬁc expertise and can be challenging to automate. The security testing method the following techniques [21]: Model-based security testing, Code-based testing and static analysis, Penetration testing, and dynamic analysis, and Security regression testing. Model-based security testing relies on requirements and design models created during the analysis and design phase. This includes testing based on architectural and functional models, threat, fault, and risk models, as well as weakness and vulnerability models. Code-based testing and static analysis involve manual code reviews and static application security testing using source and byte code developed during the development phase. Penetration testing and dynamic analysis focus on running systems in test or production environments. This includes techniques such as penetration testing, vulnerability scanning, dynamic taint analysis, and fuzzing. Security regression testing is performed during the maintenance phase and includes approaches like test suite minimization, test case prioritization, and test case selection.  
  RQCODE: Security Requirements Formalization with Testing  
  Even though security testing techniques are crucial for identifying vulnerabilities and ensuring the security of software systems, have certain limitations that can impact their eﬀectiveness for customers. Some of the limitations include: Incomplete Coverage: Security testing techniques may not provide complete coverage of all possible security vulnerabilities. It is challenging to anticipate and test for every potential security weakness, and new threats can emerge over time. Therefore, there is always a possibility of undiscovered vulnerabilities remaining in the system. False Sense of Security: Customers may develop a false sense of security if they rely solely on security testing techniques without considering other security measures. Security testing alone cannot guarantee a completely secure system. It is important to implement other security measures such as secure coding practices, regular security updates, and user awareness training. Resource Intensiveness [13]: Comprehensive security testing requires signiﬁcant resources, including time, expertise, and tools. Small businesses or individual customers with limited resources may ﬁnd it challenging to implement and aﬀord rigorous security testing practices. Limited Human Expertise [13]: Security testing techniques rely on human expertise and judgment, which can introduce limitations. Testers may overlook certain vulnerabilities, misinterpret results, or lack the necessary skills to uncover complex security issues. Evolving Threat Landscape: Security threats are constantly evolving, and new vulnerabilities can emerge rapidly. Security testing techniques may struggle to keep pace with the evolving threat landscape, leading to potential gaps in security coverage. To overcome these limitations, it is important for customers to adopt an approach to cover security requirements. However, there are certain lacks in security requirements speciﬁcations. They are often very high-level or vague mainly specifying the need to comply with a speciﬁc cyber-security standard. There is deﬁnitely a need to make security clear and veriﬁable or at least as veriﬁable as security tests. This work introduces an approach to formalize the security requirements by means of associated security tests. We present our approach and provide examples from the cyber-security domain. Section 2 discusses the related work. Section 3 presents our approach and provides illustrative examples. Section 4 discusses diﬀerences between RQCODE in comparison with security testing in general. Section 5 gives the ﬁnal conclusions.  
  2  
  I. Nigmatullin et al.  
  requirements, validating security requirements, tracing security requirements, managing security requirements, and ensuring ongoing security. Through these steps, SQUARE ensures that security requirements are clearly deﬁned, traceable, and measurable. It helps to identify potential security risks and provides a framework for identifying security controls to mitigate those risks. It also ensures that security requirements are integrated into the software development process and that security considerations are taken into account throughout the development lifecycle. Some potential SQUARE’s limitations include [11]: – Time-Consuming Process: SQUARE can be a time-consuming process, which may not be suitable for projects with tight schedules. – Requires Skilled Practitioners: SQUARE requires a team that has a good understanding of security concepts, as they have to identify, categorize, and prioritize security requirements. – Lack of Automation: The SQUARE process is largely manual, which can lead to human errors. Also, the lack of automated tool support may make the process slower and more expensive. – Lack of Formalization: SQUARE does not provide a notation that enforces formalization and veriﬁability of security requirements. For the automated veriﬁability of requirements, one may consider BehaviourDriven Design (BDD) [19]. BDD promotes starting development by specifying a scenario for a feature to be developed. In BDD, a scenario in natural language is automatically translated into an acceptance test. This is achieved through the use of a structured, natural language format such as Gherkin, which is easy to read and understand even for non-technical stakeholders. Gherkin can be useful for several reasons in the context of security requirements. Firstly, Gherkin supports the creation of scenarios that clearly describe the expected behavior of a secure system. This aids in ensuring that security requirements are properly deﬁned and understood. Secondly, Gherkin scenarios can be used to create automated security tests that verify that the system is functioning as expected. This helps to identify security vulnerabilities early in the development process when they generally are easier and less expensive to ﬁx. For example, a security requirement for a banking application is that “only authorized users are able to access account information”. A Gherkin scenario for this requirement may look as follows: 1 2  
  Only authorized users can access account information Given I am logged in as an unauthorized user  
  The ReQuirements as CODE Approach (RQCODE) RQCODE Definition and Concepts  
  ReQuirements as CODE (RQCODE) is an approach to software development that involves writing requirements in a code-like format, rather than as traditional text-based documentation. This approach enables developers to use code to test requirements, automates the process of verifying requirements, and improves collaboration between developers and other stakeholders. RQCODE aims to bridge the gap between requirements and tests, enabling more eﬃcient and eﬀective testing and earlier identiﬁcation and resolution of issues [15]. This approach supports modern software development practices, such as continuous integration and automated testing, thereby improving the quality and accuracy of requirements [1]. RQCODE, as described in [9], introduces a new way of utilizing the Seamless Object-Oriented Requirements (SOOR) paradigm in Java programming. RQCODE involves representing requirements as classes that encompass multiple forms of representation, including a natural language description of the requirement, along with methods for testing and verifying the requirement, such as acceptance tests. This creates a direct traceability link between a requirement and its implementation, which can be veriﬁed at any time through the execution of the associated test. Furthermore, the object-oriented approach facilitates the reusability of requirements and tests via standard mechanisms, such as inheritance in Java. A requirement can be either an extension or a specialization of another requirement and can serve as a template for similar requirements by initializing a requirement class with diﬀerent parameters. Seamless Object-Oriented Requirements (SOOR) [14] focuses on the seamless integration of requirements and object-oriented modeling, with the goal  
  17 18 19 20 21 22 23  
  return CheckStatus.FAIL; } return CheckStatus.PASS; } public class InsufficientFunds implements RQCODE { private final int MIN_BALANCE = 0;  
  I. Nigmatullin et al.  
  private final ScriptRequirement requirement = new STIGScript(" →{ACCOUNT_BALANCE}",  
  26  
  Related Work Problem-Solving Models  
  Author Experience  
  ey Thematic Analysis  
  E. P. Enoiu et al.  
  reviewing test speciﬁcations, writing test scripts, executing test scripts, and reviewing results. Multiple tasks were performed by participants during test case execution, such as test environment setup, test case execution (including fulﬁlling preconditions), log-ﬁle gathering, archiving of execution and log ﬁles, documentation, and analysis of any found discrepancies. Automation of the test environment was also discussed, with one participant stating that they try to automate everything, including the setup of the test environment, running test cases, and observing the output. Some participants focused on debugging and defect identiﬁcation when software bugs appeared during test execution, while others emphasized the importance of regression testing. When executing test cases, participants often rely on documents and code to review speciﬁcations, report bugs, and document test results. However, one participant claimed that documentation is unnecessary once the tests are ready to be executed, except for instructions on how to report the results. Another participant mentioned that they only require knowledge of administering tests for automated test runs. The most popular tools used by testers and developers during test execution were Selenium and Pytest, as they provide frameworks for automating web application testing and scalable and straightforward tests, respectively. Some participants perform tests manually, while others use custom-made tools. Checking Test Results: Participants were asked to describe their routine for checking test results (Table 5). Their answers largely centered around comparing test speciﬁcations with the results they obtained. Testers undertake several activities while checking test results, e.g., comparing the requirements with the test results. One participant emphasized that, during this process, it is important to keep an eye on any events not speciﬁed in the test case. Some participants also discussed the results with the development team or other testers, examined test scripts, or provided feedback to the designers. In the event of a test failure, testers and developers iteratively modify either the tests or code until achieving the desired outcome. One participant shared: “We rerun the test multiple times to conﬁrm if it’s a ﬂuke. We then proceed to ﬁx the test, the code being tested, or even the testing environment. This may involve checking for errors in parameters when setting up dockers or regenerating test data.”  
  In terms of the knowledge utilized when checking test results, documents and code remain the most common sources. One participant emphasized the signiﬁcance of documentation testing: “When tests fail, we almost always refer to the test case documentation. Though sometimes insuﬃcient, we write at least one sentence about the test’s purpose. Since the test cases are usually small, this is usually adequate.”  
  “The tests created contain all the necessary information to check the results.”  
  Participants identiﬁed the three primary challenges encountered while checking results that they would like to see addressed in software testing research. One participant highlighted some challenges that arise when tests fail: “1: It can be diﬃcult to recognize that a failed test already has an open bug report. 2: Multiple failed tests may be caused by the same underlying error. 3: It can be challenging to diﬀerentiate between failing test cases due to actual software errors versus test environment issues.”  
  Another also mentioned lack of observability into the causes of SUT behavior: “The primary challenge is understanding whether the obtained result is correct by chance or if the application is performing as intended.”  
  Other challenges identiﬁed include the need for test selection (due to having too many tests to execute) and challenges that emerge from having to make this selection—e.g., the time between executions and lack of certainty in SUT correctness—visualization of test results over time, establishing traceability between documentation sources, and the diﬃculty of knowing who is responsible for dealing with test results (e.g., the test case creator, the feature developer, or the test environment developers). We also asked about the criteria participants utilized to determine whether testing activities had been completed. One of the criteria that participants used was ensuring that all the tests were executed successfully and met the desired coverage levels of code and functionality requirements. Another participant stated that all planned tests must be executed without any stopping errors. Other participants mentioned budget and deadline constraints. Another participant indicated that the stopping criterion is when all test steps in the test speciﬁcation have been executed and assigned a pass/fail grade. 4.2  
  The Extended Problem Solving Model  
  E. P. Enoiu et al.  
  Identify the Test Goal: This is the phase when a tester understands and deﬁnes the test objective as a problem that requires a solution. Getzels [6] identiﬁes three types of problems—those that are given, those that are discovered, and those that are created or generated. A given test goal is presented to the tester (e.g., a pre-deﬁned criteria-based test goal, such as applying particular input partitions). A discovered test goal, however, must be identiﬁed. Such a test goal exists but has not been clearly stated to the tester, which has to seek out the knowledge gap to discover what the test goal is. In contrast to given and discovered test goals, a created test goal must ﬁrst be recognized and formulated. In these cases, testers may use exploratory test methods and develop new test objectives based on their knowledge, skills, and interactions with the SUT. Define the Test Goal: This relates to how one can mentally deﬁne the test goals and what the linked tests must accomplish. The test goal deﬁnition phase of testing is when the scope and objectives of each test are established and deﬁned precisely. A test goal presents a collection of “givens”. When dealing with these constraints, a tester performs certain procedures to achieve the desired state (i.e., creating a test fulﬁlling a test goal). A test goal can be expressed in many ways, including graphically or audibly. For example, to achieve pairwise coverage, one must describe the objective as the task of generating all available pairs of parameter values that may be applied by at least one test case. Analyze Knowledge: This phase structures the tester’s knowledge concerning testing scenarios. Every tester addresses a particular scenario with a diﬀerent set of knowledge. For example, someone familiar with test design techniques will assess their past knowledge and use various methods and representations of the test goal to clearly state the needed strategies. To develop test cases, we might have to use broad abilities such as inference, case-based logic, and generalization to organize the data gathered throughout the various processes. On a broader level, higher cognitive abilities such as inspiration and allocating mental resources such as awareness and eﬀort must be used. Additionally, domain expertise, such as electrical, mathematics, computer science principles, programming concepts, and regulations, would be required. We discovered that testers’ primary activities involve acquiring a deep understanding of the SUT, obtaining the exact requirements that led to its development, and comprehending the generated test solution. They also develop tests that cover a distinct portion of the system or algorithms. Form Strategy: In this step, one needs to create a solution approach for generating the required test cases using certain operators. These operators are cognitive frameworks of the operations that a tester may conduct on the “givens.” For instance, some computations require the use of mental operators. The activities required to reach the target state are the set of actions required to construct test cases that satisfy a particular test goal. While the operators are often not listed in the test goal, we may infer them based on our past knowledge (e.g., mathematical operators, cognitive operators). Organize Information and Allocate Resources: After deﬁning the test goals, the next step for the tester is to manage their cognitive and physical  
  Understanding Problem-Solving in Software Testing  
  Who Is Afraid of Test Smells? Assessing Technical Debt from Developer Actions Zhongyan Chen(B) , Suzanne M. Embury , and Markel Vigo Department of Computer Science, The University of Manchester, Manchester M13 9PL, UK [email protected]  Abstract. Test smells are patterns in test code that may indicate poor code quality. Some recent studies have cast doubt on the accuracy and usefulness of the test smells proposed and studied by the research community. In this study, we aimed to determine whether developers view these test smells as sources of technical debt worth spending eﬀort to remove. We selected 12 substantial open-source software systems and mapped how 19 test smells from the literature were introduced and removed from the code base over time. Out of these 19 smells, our results show that: 1) four test smells were rarely detected in our selected projects; 2) three test smells are removed rapidly from the projects while another three are removed from code bases slowly; 3) the remaining nine test smells did not show a consistent pattern of quick or delayed removal. Our results suggest that the test smells currently being studied by researchers do not capture the true concerns of developers regarding test quality, with current testing tool sets, with only three of the 19 smells studied showing clear evidence of developer concern. Keywords: Test Smells Engineering  
  1  
  Sleepy Test  
  A test method that invokes the Thread.sleep() method, which may introduce additional delays to the test execution [16].  
  Unknown Test  
  HikariCP  
  2023/02/01  
  11  
  jib  
  2023/01/09  
  7  
  mybatis-3  
  2023/03/13  
  14  
  retroﬁt  
  2023/03/31  
  14  
  Mean Time to Removal  
  Technical debt is usually assessed using the presence of code smells, with other measures such as time/cost of removal also being used [11]. Since we aim to assess test smells from technical debt, not the other way around, we cannot use the smell-based approach. Instead, we choose time to removal (TTR) of the smell, i.e., the interval between the timestamps of the commits introducing and removing it, as our main measure, on the assumption that smells are removed due to the actions of developers, reﬂecting their views on what makes for valuable use of their time for their system. To aggregate the actions of many developers, we use a mean time to removal (MTTR) score, applied to individual code bases and across code bases. 3.3  
  Data Collection  
  Fig. 1. Data Collection Process (Rounded rectangle: state; Rhomboid: input; Rectangle: process; Wavy base rectangle: document)  
  where some change has been made to test code. To do this, we scan through the commit history and identify the commits that meet the conditions below: – Commits on the development mainline: The development mainline of a repository records the key versions of the code base that contain only completed and integrated feature implementations. We limited our analysis to commits on the development mainline and ignored work-in-progress commits on feature branches. To do this, we manually identify the mainline branch for each target project and conﬁgure the pipeline to seek out the ﬁrst parent commits on that branch. – Commits changing at least one file having “.java” and “Test” in its name: we use the approach of Peruma et al. [15] to identify test ﬁles, which is based on best practice recommended by JUnit3 . A commit is selected for analysis if it adds, deletes or modiﬁes a ﬁle meeting these criteria. The number of commits in each project meeting both conditions is shown in the rightmost column of Table 2. Test Smell Detection. In this step, we check out each target commit and run JNose to detect test smells for every detected smell instance, JNose records the path of the ﬁle containing the instance, the path of the corresponding production ﬁle (if applicable), the detected smell type, the test or production method names involved in the smell, and the line number(s) on which the smell instance is located. At the end of this step, we have a set of CSV ﬁles detailing the smells detected in every target commit. MTTR Computation. To compute the time to removal for each smell instance, we need to know the commit when it was introduced into the code base and the commit when it was removed. To do this, we must track a smell instance across a sequence of commits. This is complicated by the fact that other changes to the ﬁle may cause the smell to change its position, even if the smell itself is unaﬀected by the commit. Therefore, when checking whether a smell has 3  
  Fig. 3. Data Analysis Process (Rounded rectangle: state; Wavy base rectangles: documents; Rectangle: process; Rhombus: Decision)  
  – intro(A)x : the number of instances of test smell A introduced across all commits in project x. – avgIntro(A): the average of intro(A)x across n target projects. – M T T R(A)x : the mean time to removal across all instances of test smell A in project x. T T R(A)x – R(A)x : the ratio M Lif etimex for test smell A in project x. To allow MTTRs to be compared across projects, we normalize by dividing by the project’s lifetime. Since no tests may have been written in the early stages of a project, we use the interval between the ﬁrst commit in our target set for project x and the latest observed commit as the project lifetime. – M dnR(A): the median of R(A)x across all n projects. – σR(A) : the standard deviation of R(A)x across all n projects. Smells classed in the mid-range removal are neither removed rapidly from the code bases we studied nor left to languish for long periods of time. To distinguish them, we can look at the level of agreement on their positioning on our scale from the diﬀerent projects using the standard deviation of R(A) across all n projects. The ﬁrst step of our analysis separates out those test smells for which we found insuﬃcient examples to allow us to draw any ﬁrm conclusions. For each project x, we calculate Intro(A)x and avgIntro(A) for each test smell A. Based on central limit theorem [9], we set 30 as the threshold and classify all smells for which avgIntro(A) is below that as under-represented, and therefore providing insuﬃcient samples from which to make a signiﬁcant statistical inference. The next step is to use the M dnR(A) and σR(A) to classify each remaining test smell. We use the ﬁrst and third quartiles (denoted Q1 and Q3 respectively) as thresholds for this classiﬁcation. A test smell A will be classiﬁed as rapidly removed if M dnR(A) is below Q3 . If M dnR(A) is within the interquartile range, it will be a mid-range test smell. Finally, if M dnR(A) is in the range above Q3 , it will be classiﬁed as a slowly removed test smell.  
  Assessing Technical Debt from Developer Actions  
  Conclusions and Future Work  
  We have presented the results of a study into the introduction and removal of test smells by developers in established open-source software projects, with the aim of determining where there might be consensus on the smells that represent genuine technical debt. Our aim is to supplement earlier studies evaluating test smells and to understand which of the smells are suitable targets on which to build future research. The results, shown in Sect. 4, give our classiﬁcation for each test smell included in this study into four categories. We only found 3 test smells that tend to be rapidly removed from all the projects in our study, and that would appear to represent sources of signiﬁcant technical debt. This raises questions as to the suitability of these smells to be included in the “canon” of test smells or as subjects for further research on test code quality. Several future directions are possible. One is the improvement of the detection rules used to identify smell instances for the test smells so that the scale of occurrence of the smells in code can be more accurately assessed. In addition, it would be useful to be able to distinguish the intentional removal of smells by developers from the accidental removal as a result of other changes. It may also be useful to study smell instances in non-mainline commits, to see if code quality management processes are helping to remove smell instances before integration into the mainline. Information on developer intention may also be present in commit messages and code reviews, or in discussions on pull/merge requests and on issue trackers. Beyond this, the number of test smells that survived our analysis is small— far smaller and more limited in scope than the well-documented and accepted production code smells. It seems unlikely that these surviving smells are a complete reﬂection of all the quality issues that can arise when writing test code. Indeed, several important aspects of modern testing practice are not covered, particularly in the writing of ﬁxture code, with the use of test doubles [12], for example, as well as the role of test harness code. Yet more smells may be found in the pragmatic aspects of test run conﬁgurations and CI pipeline deﬁnitions. Perhaps suﬃcient time has now elapsed since the ﬁrst test smells were proposed to justify a revisiting of best practices in automated testing, to derive new smells that reﬂect the concerns of current developers aiming to manage the quality of their systems through testing.  
  2 3  
  Universidade Estadual Paulista - Unesp, Guaratinguet´ a, Brazil Instituto Nacional de Pesquisas Espaciais - INPE, S˜ ao Jos´e dos Campos, Brazil Universidade Tecnol´ ogica Federal do Paran´ a - UTFPR, Corn´elio Proc´ opio, Brazil 4 Instituto Federal de Educa¸ca ˜o, Ciˆencia e Tecnologia de S˜ ao Paulo - IFSP, Jacare´ı-Campos do Jord˜ ao, Brazil 5 Gran Sasso Science Institute - GSSI, L’Aquila, Italy [email protected]  Abstract. Software Testing is a costly activity since the size of the test case set tends to increase as the construction of the software evolves. Test Case Prioritization (TCP) can reduce the eﬀort and cost of software testing. TCP is an activity where a subset of the existing test cases is selected in order to maximize the possibility of ﬁnding defects. On the other hand, Markov chains representing a system, when solved, can present the occupation time of each of their states. The idea is to use such information and associate priority to those test cases that consist of states with the highest probabilities. This journal-ﬁrst paper provides an overview of a systematic survey of the state-of-the-art to identify and understand key initiatives for using Markov chains in TCP.  
  1  
  Contributions. In this paper, we reﬁne and optimise the black box checking approach in several ways and specialise it for the purpose of white box software module testing, including tool support. For B, we admit (nondeterministic) symbolic finite state machines (SFSM) with ﬁnite state space, input and output variables over arbitrary primitive data types (including inﬁnite types like Z and R) and transitions labelled by guard conditions over input variables and output expressions over output and input variables. We advocate a white-box approach which is quite realistic for software in safety-critical systems, where source code needs to be veriﬁed by independent veriﬁcation teams [28]. This allows us to determine upper state bounds n and identify the guard and assignment expressions used in the code by means of static analyses. These static analyses ensure that a passed black box checking suite corresponds to an actual proof that I satisﬁes property ϕ. Regarding methodological contributions, the application of black box checking to software with conceptually inﬁnite input and output types is enabled by an equivalence class partitioning method previously developed by the authors [14]. Otherwise black box checking would be infeasible, due to the large size of the alphabets involved, when using interface variables of type double, float, int directly. Furthermore, we reduce the number of situations where tentative models B = Mi need to be checked by means of a complete testing method. In particular, our strategy allows to check tentative models later, after many distinguishable states (say, ) of the IuT have already been discovered. This signiﬁcantly reduces the exponential term pn−+1 inﬂuencing the size of the complete test suite, where n ≥  is the upper bound of potential distinguishable states in I, and p is the number of input/output equivalence classes derived from guard conditions and output expressions extracted from the code, as described below. Instead of the “classical” L∗ -algorithm, we use a novel, highly eﬀective state machine learning algorithm proposed by Vaandrager et al. [25]. For generating complete test suites, a variant of the complete H-Method [12] is used, which needs signiﬁcantly fewer test cases than the W-Method in the average case [13]. We have modiﬁed the HMethod for online testing: this means that the test case generation is incremental and interleaved with the test execution, so that it is unnecessary to create a complete suite, when tests of I against the current version of B fail early. We apply the monitor concept proposed by Bauer et al. [3] for detecting safety violations on the ﬂy, during tests intended for model learning. This reduces the need to perform complete model checking runs of B × P against ϕ. To speed up the learning process and to avoid having to create complete suites for too many intermediate increments of B, we apply coverage-guided fuzz testing [5,17] for ﬁnding many distinguishable states of the implementation at an early stage. Again, this leads to small exponents n −  + 1 in the term pn−+1 dominating the number of test cases to perform for a complete language equivalence test. While these techniques for eﬀort reduction cannot improve the worst case complexity that was already calculated by Peled et al. [21,22], their combination signiﬁcantly improves black box checking performance in the average case.  
  F. Brüning et al.  
  We conﬁrm this by several experiments verifying control software from the automotive domain. These experiments also show that the property testing approach described in this paper is eﬀectively applicable for testing modules performing control tasks of realistic size and complexity. Therefore, the approach advocated here is an interesting alternative to proving code correctness by means of codebased model checkers or proof assistants. From the perspective of standards for software development in safety-critical systems [8,16,28], our approach even has a signiﬁcant advantage in comparison to “pure” code veriﬁcation, since tests are actually executed against the IuT. The standards emphasise that veriﬁcation may never be based on static analyses (model checking, formal proof) alone: it is always necessary to perform dynamic tests of the integrated HW/SW system as well. To the best of our knowledge, the approach presented here is the ﬁrst to use equivalence class abstractions for enabling complete property testing of source code with large interfaces, using black box checking in combination with fuzzing. Regarding the implementation of the approach, we present the open source library libsfsmtest for complete model-based or property-oriented module testing1 , whose latest version supports the module testing strategy described in this paper. For users only interested in the application of the library for practical testing, a cloud interface2 is provided, supporting both test suite generation and module test execution. Related Work. Meng et al. [18] conﬁrm that fuzz testing can be eﬀective for testing software against properties speciﬁed in LTL. However, their approach does not provide any completeness guarantees: the tool LTL-fuzzer created by the authors is to be used as an eﬀective bug ﬁnder. Pferscher et al. [23] also combine model learning and fuzzing, but with the objective to check whether an implementation conforms to a reference model, while our focus here is on property-oriented testing. The fuzzer is not guided by the code coverage achieved, as in our approach, but by the coverage of a reference model. Since the latter has not been validated with respect to completeness and consistency, the testing process can only reveal discrepancies between reference model and implementation, but not a correctness proof. The model learning aspect of black box checking has received much attention since Angluin’s seminal paper [1], and a comprehensive overview about improvements and alternative approaches to automata learning is given by Vaandrager et al. [25]. We could have made use of the LearnLib library [15] for the model learning part in our Algorithm 2 (see Sect. 3). However, we would not have used the W-Method or Wp-Method implemented there for equivalence testing and ﬁnding counter examples, since our own library libfsmtest provides methods like the H-Method that requires far less test eﬀort in the average case. Moreover, the new data structure and associated algorithms for learning that has been pro1 2  
  posed by Vaandrager et al. [25] is not yet available in LearnLib, and it seemed particularly attractive with respect to maintainability and performance to us. An alternative to LearnLib is AALpy by Aichernig et al. [19]. While its Python implementation seems less attractive to us, due to the better performance of C++, AALpy uses a strategy for disproving conformance between preliminary model versions and an implementation that is an interesting alternative to our current implementation: AALpy tries to avoid the generation of unnecessary complete conformance test suites by combining random testing with the W-Method, expecting to ﬁnd early discrepancies between a preliminary version of the model and the implementation by means of random testing. In our approach, we prefer to focus the application of random testing in an initial phase using coverage guided fuzzing with the objective to ﬁnd an initial candidate for machine B with as many states as possible. After that, we relay on conformance tests without randomisation, but create the cases of the H-Method incrementally, which also avoids the creation of a full conformance test suite as long as B and I do not conform. Waga [27] presents a black box checking approach that is complementary to ours in several ways. (1) The main objective is bug ﬁnding for cyber-physical systems, while we focus on complete property checks for software modules. (2) Waga applies signal temporal logic, while we apply LTL. (3) Waga does not use any means of abstractions comparable to the equivalence class abstractions we consider to be crucial for complete property checking. Summarising, Waga’s approach performs well for the purpose of bug ﬁnding on system level, while the method advocated here provides complete checks on module level. Overview. In Sect. 2, we summarise the foundations required for the combined testing and black box checking approach described in this paper. In Sect. 3, the methodological main result is presented. In Sect. 4, a short summary of the available tool support is given. In Sect. 5, the application of our approach with this tool platform is described, and performance data is presented. Section 6 contains a conclusion.  
  2 2.1  
  Theoretical Foundations Black Box Checking  
  The strategy for combined learning, model checking, and testing proposed by Peled et al. [22] is shown in Algorithm 1, with some adaptations for the notation used in this paper. The strategy uses two sub-functions for learning and testing: (1) As the ﬁrst sub-function, Angluin’s L∗ -algorithm [1] is invoked (lines 6, 25) for learning the internal structure of the black box B representing the true behaviour of implementation I. The L∗ -algorithm is called in Algorithm 1 with three parameters (I, Mi , π): I is the implementation, and the L∗ -Algorithm may execute additional tests against I, in order to produce a new model. Parameter Mi speciﬁes the latest assumption for the representation of B, and π is a word  
  F. Brüning et al.  
  representing a counterexample that is either accepted by Mi , but not by B, or vice versa. Based on this information, L∗ returns a more reﬁned model Mi+1 . (2) As the second sub-function, the W-Method [9,26] VC(I, Mi , , k) is used as a conformance test that is able to prove or disprove the language equivalence between Mi and I, under the hypothesis that I has no more than k distinguishable states. The algorithm is called with the implementation I to be used in the test, the currently learnt, minimised model Mi that may or may not be equivalent to I, the number  of distinguishable states in Mi , and the currently assumed upper bound k ≤ n of distinguishable states in I. Note that the worst case estimate for the number of test steps to be executed for such a conformance test is O(3 pn−+1 ) [9]. Initially, the L∗ -algorithm is set up with the empty machine (line 6). Then the implementation is tested until (a) either the learnt model B satisﬁes ϕ and has been shown to be language-equivalent to I by means of complete tests, under the hypothesis that I has at most n states (line 18), or (b) an approximation Mi of B has been learnt that violates ϕ on an inﬁnite word π1 π2ω , and this word is accepted by the implementation (line 22).  
  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  
  Equivalence Class Construction for SFSM  
  We summarise here previously obtained results [14] that are relevant for the present paper. A symbolic ﬁnite state machine (SFSM) M is a state machine operating on a ﬁnite set of control states and input variables and output variables from a symbol set V = I ∪ O. Variables are typed by some (possibly inﬁnite) set D. A variable valuation is a function σ ∈ DV , associating a value σ(v) with each variable symbol v ∈ V . Given a quantiﬁer-free ﬁrst order expression e with free variables in V , we say that σ is a model for e (written σ |= e), if and only if the formula e[v/σ(v) | v ∈ V ], that is created from e by exchanging every occurrence of a variable symbol v ∈ V by its valuation σ(v), evaluates to true. g/a  
  A transition relation s1 −−→ s2 connects certain control states s1 , s2 . The transition label g/a consists of a guard expression g, that is, a quantiﬁer-free ﬁrst order expression over variables from I, and update expressions a that are  
  3. Let P denote the set of all formulae φ that have been constructed according to Eq. (1) and that possess at least one valuation σ ∈ DV as model, so that σ |= φ. 4. For each φ ∈ P, deﬁne an input/output equivalence class io(φ) by io(φ) = {σ ∈ DV | σ |= φ}. 5. Let A = {io(φ) | φ ∈ P} denote the set of all input/output equivalence classes.  
  ﬁrst order expressions over at least one output variable and optional variables from I. The language of M is the set L(M ) ⊆ (DV )ω of all inﬁnite traces of valuations σ1 , σ2 , · · · ∈ DV , such that there exists a sequence of states s0 , s1 , . . . starting in the initial state and guard and output expressions (g1 /a1 )(g2 /a2 ) . . . , such that gi /ai ∀i > 0  si−1 −−−→ si σi |= gi ∧ ai . The property testing approach described in this paper applies to all software modules whose input/output behaviour can be described by means of an SFSM. The class of real-world applications that can be modelled by SFSM is quite large, examples are airbag control modules, anti-lock braking systems (see Sect. 5) or train control systems. An input/output equivalence class io is a set of valuations σ ∈ DV constructed according to the speciﬁcation in Table 1. The intuition behind this speciﬁcation is that the input/output equivalence classes partition the set DV of valuations: two members of the same class are models for exactly the same conjunction over all guard conditions, output expressions, and atomic propositions occurring in the LTL property ϕ to be veriﬁed, each conjunct occurring either in positive or negated form. No valuation can be in more than one class, since two classes diﬀer in the sign (positive/negated) of at least one conjunct. Two sequences π1 , π2 ∈ (DV )ω of valuations are equivalent if each pair of corresponding sequence elements (π1 (i), π2 (i)), i = 1, 2, . . . is contained in the same input/output equivalence class. The following properties of equivalent  
  Complete Property-Oriented Module Testing  
  traces π1 , π2 ∈ (DV )ω are crucial in the context of this paper [14, Theorem 2]3 : (1) π1 ∈ L(M ) if and only if π2 ∈ L(M ). (2) π1 and π2 , when contained in L(M ), cover the same sequences of states in M (there is only one uniquely determined state sequence if M is deterministic). (3) π1 |= ϕ if and only if π2 |= ϕ.4  
  3  
  F. Brüning et al. M1 := L# (I, H, A, T, P, −) -- start learning using input alphabet H, output alphabet A, and the observations T observed during fuzzing i := 1; while ( true ) begin X := Mi × P ; -- Product of machine learnt so far and BA checking ¬ϕ if L(X) = ∅ then -- Mi does not violate ϕ begin  := number of states of Mi ; (conforms, π) := H(I, Mi , T, P, , n); -- apply an online H-Method if ( conforms ) then return pass; -- Implementation conforms to Mi , and Mi fulfils ϕ end else begin -- current model Mi violates ϕ let π1 , π2 such that π1 π2ω ∈ L(X); -- this word violates ϕ if I passes test π1 π2n then return fail; else π := shortest preﬁx of π1 π2ω not accepted by I; end Mi+1 := L# (I, H, A, T, P, π); -- learn more elaborate model, -- based on counterexample π i := i + 1; end end  
  Phase 1: Setup. In the ﬁrst phase, we exploit white box knowledge on the IuT in order to abstract from its possibly inﬁnite input and output domains to ﬁnitely many equivalence classes. To this end, the algorithm uses the two input parameters ΣI and ΣO , denoting the guard conditions and output expressions occurring in the IuT, respectively. Together with the atomic propositions AP occurring in the LTL property ϕ to check, these are employed in computing input/output classes A (lines 10 and 11) using the techniques described in Sect. 2. These classes could then already serve as symbolic inputs. However, since multiple input/output classes may share the same input valuations, this could introduce superﬂuous inputs. Thus, line 12 of the algorithm attempts to minimise the number of inputs by only selecting suﬃciently many input valuations σ ∈ H to provide input representatives of all input/output classes. In the following, we use elements of H both as symbols and as concrete input valuations. The ﬁrst phase concludes by initialising a tree T representing a preﬁx-closed set of symbolic traces observed in the IuT (line 13), as well as a property monitor P constructed as proposed by Bauer et al. [3] (line 14) to accept ¬ϕ. This monitor detects violations of safety properties ϕ observed during the subsequent execution of Algorithm 2. Since violations of liveness properties can only be determined on inﬁnite traces, these are accepted, but do not lead to failure indications by the monitor.  
  Complete Property-Oriented Module Testing  
  Experiments  
  For evaluation of the property testing approach described in this paper, we re-implemented an anti-lock braking system (ABS) for cars with lane stability control, as designed and published by Bosch GmbH [11]. The full functionality described there has been reduced to ABS for the front-left wheel only, and we do not consider gravel road conditions. The ABS system implements two fundamental tasks: (1) locking a wheel should be avoided if the driver brakes too hard or brakes on slippery roads. The ABS controller prevents wheel locking by alternately holding, reducing and increasing the brake pressure for each wheel individually so that each wheel rotates recurringly while braking, in order to keep the car steerable. (2) The ABS controller implements a lane stability control to prevent the car from swerving on asymmetric road conditions during braking with straight steering angle. The ABS controller then adjusts the braking force in a car for all wheels, to facilitate the steering intervention by the driver, while still applying the maximal possible braking force. The ABS controller measures constantly the wheel velocity vU and calculates the brake slip λB for each wheel, relative to the vehicle target speed vR , which in this example is measured at the car powertrain. The equation to calculate the slip is [11] vU − v R λB = . vR The ABS controller evaluates the momentary acceleration α of each wheel to detect each wheel’s tendency to lock. If α falls below the threshold −a < 0, a possible wheel lock is detected and the input valve VI (in front of the brake ﬂuid inlet of the wheel brake cylinder) is closed, as well as the output valve VO (after the brake ﬂuid outlet of the wheel brake cylinder) to hold the current brake pressure. The additional brake pump P to artiﬁcially increase the brake pressure is set to mode OFF. Consequently, the negative wheel acceleration α is not reduced any further. Then, if the brake slip falls below the maximum slip threshold, the output valve is opened again. Thus, the brake pressure decreases again, and α and the slip increase. When α = −a, the valves are switched to hold pressure (both valves closed, pump oﬀ). Now, the acceleration increases and can exceed two thresholds +a, +A satisfying +a < +A. In the ﬁrst iteration, the brake pressure will be increased when α > +A, by setting VI := OPEN, VO := CLOSED, and P := ON. After a certain time, α decreases and reaches +A, so that the ABS controller switches again to hold pressure. The braking pressure is held until the +a threshold is reached. At this point, the second iteration begins (henceforth repeating) and the brake pressure is slowly increased until −a is reached. In the following cycles, α is kept between the two thresholds −a and +a by the ABS controller. If the ABS controller receives a signal from the yaw sensor that the car rotates around the z axle during braking, an asymmetric road condition is detected. If the car rotates to the direction of the current wheel, the driver is braking and the steering angle is in direction straight ahead, the controller then tries to facilitate the steering intervention by the driver by alternately reducing and increasing the  
  Conclusion  
  In this paper, a novel white box module testing strategy based on learning has been presented. This strategy is complete: given an LTL property ϕ and an implementation under test I, it decides whether I satisﬁes ϕ under the assumption that a representation of I as a symbolic ﬁnite state machine contains at most n states and employs only guard and assignment expressions contained in a set of expressions Σ. As n and Σ can be determined from static analysis of I, the strategy eﬀectively performs a proof whether I satisﬁes ϕ. The strategy improves previous checking strategies based on learning [22] in several aspects. First, it performs input/output abstraction and hence allows 13  
  Compositionality in Model-Based Testing Gijs van Cuyck1(B) , Lars van Arragon1 , and Jan Tretmans1,2 1  
  Institute iCIS, Radboud University, Nijmegen, The Netherlands {gijs.vancuyck,lars.vanarragon,jan.tretmans}@ru.nl 2 TNO-ESI, Eindhoven, The Netherlands  
  Abstract. Model-based testing (MBT) promises a scalable solution to testing large systems, if a model is available. Creating these models for large systems, however, has proven to be diﬃcult. Composing larger models from smaller ones could solve this, but our current MBT conformance relation uioco is not compositional, i.e. correctly tested components, when composed into a system, can still lead to a faulty system. To catch these integration problems, we introduce a new relation over component models called mutual acceptance. Mutually accepting components are guaranteed to communicate correctly, which makes MBT compositional. In addition to providing compositionality, mutual acceptance has beneﬁts when retesting systems with updated components, and when diagnosing systems consisting of components. Keywords: model-based testing · component-based testing compositional testing · labelled transition systems · uioco  
  1  
  respect to its component model, then it can be inferred that the composition of component implementations, i.e. the system under test, is correct with respect to the composition of component models, i.e. the system model. In this paper, we investigate compositionality for MBT with labelled transition systems as models, uioco as the conformance relation, and parallelism modelling component composition [4]. We deﬁne a relation over component speciﬁcation models, called mutual acceptance, which guarantees that components communicate neatly, and that uioco is preserved under composition. We generalise existing results on compositionality [4,8,11] by making less restrictive assumptions and using a composition operator that is associative so that also compositions of more than two components can be easily considered. Moreover, we use the more recent uioco conformance relation instead of ioco [19]. A more detailed comparison with related work can be found in Sect. 8. In addition to compositionality, mutual acceptance also beneﬁts testing evolving systems and software product lines. It enables more eﬀective testing when a component is replaced by an updated version, as will be elaborated in Sect. 7. Diagnosis is the converse of compositionality: if the whole system has a failure, then diagnosis tries to localise the failure in one of its components; Sect. 7 will also discuss the use of mutual acceptance in diagnosis. Overview. Section 2 contains preliminaries. Section 3 shows why the current approach to compositional model-based testing is not desirable by means of an example. Section 4 formalises what it means for two models to be compatible with each other for use in model-based testing, and deﬁnes the mutual acceptance relation . Then Sect. 5 goes on to prove that this leads to desirable properties, after which Sect. 6 revisits the example. Section 7 discusses how these properties also lead to a reduced testing eﬀort when substituting components, and how  can be used in diagnosis. Section 8 describes some of the large body of related work previously done in the area of compositional model-based testing. Finally, Sects. 9 and 10 discuss possible future work and summarise the main results of this paper, respectively. All proofs for lemmas and theorems can be found in the extended version of this paper [6].  
  2  
  Preliminaries  
  We give the formal deﬁnitions for the MBT theory that we consider. We base our work on the theory developed in [4,18]. The main formalism used is that of labelled transition systems (LTS) (Deﬁnition 1). An LTS has states and transitions between states that model events. An event can be an input, an output or τ ; τ represents an internal transition which is not observable from the outside and can therefore not be tested. Is , Us , etc., indicate inputs and outputs, respectively, coming from LT S s. The shorthand Ls means Is ∪ Us . The name of an LT S is sometimes used as shorthand for its starting state. LT S(I, U ) denotes the domain of labelled transition systems with inputs I and outputs U , or just LT S if I and U are known. For technical reasons we restrict this class to strongly converging and image-ﬁnite systems. Strong convergence means that inﬁnite  
  4  
  Mutual Acceptance  
  In order to reason about speciﬁed and unspeciﬁed behaviour an explicit notion of what it means for behaviour to be speciﬁed is required. For uioco, the allowance of outputs is always explicitly speciﬁed. They are either present in the model and therefore allowed, or absent and disallowed. After a speciﬁed input, the model again deﬁnes what is allowed. Inputs are always implicitly allowed, but if an input is not part of the model all behaviour after that input is allowed. This means that the behaviour after an absent input is unspeciﬁed: the model does not tell us what should or should not happen. Therefore, if all outputs given by one component, are inputs present in the model of the other component, there will be no unspeciﬁed behaviour. This requirement is formulated in Deﬁnitions 9  
  The  relation, and by extension Lemma 3, only consider Utraces and not arbitrary traces because only states reachable by Utraces are important for uioco. This does, however, create the extra requirement of checking that after projecting a trace to a speciﬁc component, it is still part of the Utraces for that component. Lemma 4 shows that the  relation ensures that Utraces are preserved when projecting from a composed system, in both directions. This is not trivial, as the special label δ is not normally preserved under composition. ∗  
  Lemma 4. Let s, e ∈ LT S be composable, σ ∈ Lδs||e . s  e =⇒ ( σ ∈ Utraces(s || e) ⇐⇒ σ  Lδs ∈ Utraces(s) ∧ σ  Lδe ∈ Utraces(e) ) We now present Theorem 1, which is the main statement of this paper. It states that for mutually accepting speciﬁcations, uioco is preserved under parallel composition. The other way around, if there is a problem that causes two  
  Compositionality in Model-Based Testing  
  )  
  composed implementations to be uioco-incorrect to their composed speciﬁcations, then this problem can also be found by testing with at least one of the components. The reverse of this implication, however, does not hold, even if both speciﬁcations are mutually accepting. The reason for this is that the mutual acceptance relation only guarantees that no invalid outputs are communicated. It does not enforce that something is actually communicated. Therefore, it is possible for one of the two implementations to produce quiescence when this is not allowed, which is then masked in the combined system by the outputs generated by the other component. This highlights a property of the uioco relation: presence of speciﬁc outputs cannot be enforced. One possible way to deal with this might be to extend the uioco theory with a more ﬁne grained concept of quiescence, allowing the detection of quiescence in speciﬁc components, instead of only over the whole system. This is further explored in [17]. Theorem 1. Let s, e ∈ LT S be composable, is , ie ∈ IOT S, then s  e ∧ is uioco s ∧ ie uioco e =⇒ is || ie uioco s || e Another thing to note is that when applying Theorem 1 in practice, this makes the implicit assumption that you can correctly compose components. In order to guarantee the correctness of the composed system, the composition of components is and ie must actually behave as is || ie . This means that any communicating channels must be connected as described in s and e, and that there must not be some hidden implicit environment part of the composition setup that further inﬂuences the behaviour of either of the components.  
  6  
  Component Substitution and Diagnosis  
  In addition to providing compositionality in development and testing, mutual acceptance has beneﬁts when retesting systems with updated components, and when diagnosing systems consisting of components. A common situation is that one component becomes deprecated and needs to be replaced. This traditionally has a high cost, because even if the new component is well tested, there is a chance using it will cause problems with the other components already in use. These issues mainly occur because replacing a component changes the environment for the other components. This means the other components, which are the environment of the replaced component, might be called with new inputs which have not yet been tested. A well known example where reuse of an old, well tested component in a new environment caused the whole system to fail is the crash of the Ariane-5 rocket [15,21]. Here, an important subsystem put implicit requirements on the environment which were not documented or checked to hold. Correctness was inferred from extensive testing, but after changing the environment this testing became invalid, and the component failed anyway. These problems can be reduced by using a speciﬁcation-based analysis like  in combination with model-based testing. Model-based testing can generate tests for every deﬁned sequence of inputs. If two speciﬁcations are mutually accepting, then they only communicate outputs which are deﬁned inputs for the intended communication partner. These two points together mean that all the model-based testing done up to the point of replacing a component is still useful, because it was testing for all possible inputs, and not just the ones that were in current use. This can give a much higher conﬁdence that a component switch will not cause any problems, because testing does not have to start from square one. If the speciﬁcation of the new component is not mutually accepting with all the rest of the system, then the counterexamples point to all the places where undeﬁned inputs are given. This information can be used to improve the speciﬁcations, and focus testing toward these possible problem areas. The correctness reasoning made possible by the  relation can also be used during diagnosis, by taking the converse of Theorem 1. If the whole system contains a problem, and one or more components are found to be uioco correct, then the problem must be located within one of the remaining components. Together with Lemmas 3 and 4 this can then be used to narrow down a trace showing uioco incorrectness of the whole system to a shorter trace showing uioco incorrectness of one speciﬁc component. This idea is expressed in Lemma 5. Since composable requires each label to be part of at most one output set, the last output of the counterexample uniquely identiﬁes the problem component. This does not work if the last output was δ, which could have been caused by a number of components. In this case we can still ﬁnd the faulty component by replaying the projected traces in all components until the faulty one is found. This can,  
  Related Work  
  The work in this paper is closely related to ideas already discussed in the context of interface automata [8–10]. Interface automata are a type of labelled transition system which can be used to model both the behaviour of a component and the constraints it puts on its environment. These constraints are encoded in the form of missing input transitions, which then signify that the component can only be used in an environment that does not give these inputs. This closely resembles the main idea behind the  relation. Apart from a slightly diﬀerent composability requirement which makes it associative, our deﬁnition for parallel composition coincides with the one from [8]. Our deﬁnition for  also seems to coincide with the absence of reachable (by Utraces) error states as deﬁned in [8]. The solution to reachable error states taken for interface automata is to apply a pruning algorithm. This will remove input transitions to further restrict the valid environments until all error states become unreachable. A downside of this approach is that it becomes easy to generate composed models that after pruning no longer give errors, but also no longer express the desired correct behaviour. This is noted in [8] as the observation that the environment that does not give any inputs at all, always avoids all avoidable error states. The interface automata approach consists of removing transitions from the composed speciﬁcation until problem areas are unreachable. We instead choose to add transitions to the component speciﬁcations until the problem areas no longer exist. Another contribution of our work is the inclusion of quiescence, and the direct link to the uioco implementation relation. This makes the theory easier to apply in practice in the context of existing MBT tools. An earlier attempt at formalising the correctness of a component with respect to its environment was developed in [11]. It deﬁnes the eco (environmental conformance) relation with similar semantics to the accepts relation. The relation eco, however, works on a speciﬁcation for the environment, and a black box implementation of the component. This means that eco conformance can only be checked by testing, and this needs to be redone completely whenever a component changes. Additionally, all labels of the component and its environment have to communicate, i.e., there is no external communication, which further restricts applicability. The eco approach also has a couple of advantages. Since eco is checked using testing, it can be done on the ﬂy. It also does not require how a component calls other components as part of its input speciﬁcations. Instead,  
  Compositionality in Model-Based Testing  
  this information is gathered while testing and compared against the speciﬁcations of the components being called. This makes a possible combination of our work with the eco theory and algorithms interesting. In this paper, we describe when a component is a valid environment for another component. Earlier work looking into the set of valid environments for a given component was done in the ﬁeld of contract-based design. A detailed overview of this ﬁeld can be found in [2]. A contract is deﬁned as a tuple of a set of valid environments and a set of valid implementations, where every combination of environment and implementation can be composed. The deﬁnition of what it means for an environment to be composable with an implementation is very similar to our deﬁnitions, and it also describes how a labelled transition system can be seen as a contract. The scope under consideration in [2], however, is limited to receptive environments with the same label set as the components. All components also have to be deterministic, and internal transitions or quiescence are not discussed. A more recent addition to contract theory extends the scope of [2] to hyper-contracts [13]. While this extends the scope of properties that can be expressed as contracts, the current instantiation of the meta-theory for labelled transition systems still has many of the restrictions imposed in [2]. In contrast to the bottom up approach of combining component contracts into a composed contract, a top down approach is also possible and sometimes desired. Decomposing a set of requirements into individual component contracts has been studied in [14]. Another way of describing compatible components is deﬁning a speciﬁcation for the most permissive communication partner. All concrete communication partners are then in some form of a reﬁnement relation with this “operating guideline”. This approach is outlined in [16] for acyclic ﬁnite labelled transition systems. It assumes all communications to be asynchronous, while we assume synchronous communication.  
  9  
  Future Work  
  Making speciﬁcations mutually accepting involves deﬁning extra behaviour. Some of this extra speciﬁcation is desirable, for instance the beep transition from state B in S4 . This transition represents interesting behaviour that was missed in the speciﬁcation phase. Most other added transitions, however, are just simple self-loops, which represent that the input has to be ignored. If receiving an input that was not speciﬁed is considered undeﬁned behaviour, this is required to ensure correct behaviour. Another possible interpretation would be that unspeciﬁed inputs are buﬀered, until the other component is ready to receive them. In such a setting, it would not be required that every input that can be given is speciﬁed immediately. It would then be enough that such inputs are speciﬁed always eventually, after some amount of internal actions of the receiving component. In general, it can be investigated how to (automatically) repair non-mutually accepting systems. In this paper, we have deﬁned mutual acceptance, but no ways for practically checking it have been given. Algorithms to eﬃciently check mutual acceptance  
  G. van Cuyck et al.  
  between speciﬁcations, or testing procedures to test mutual acceptance, analogous to eco, need to be developed. The theory introduced so far works on two components. Larger systems consist of many components. Mutual acceptance can still be inferred by repeatedly applying the parallel composition operator and Theorem 1. For example, when combining speciﬁcations s1, s2 and s3 into (s1 || s2) || s3, we must check that s1 || s2  s3. Doing this directly using s1 || s2 might be complicated due to the increasing number of parallel components. We postulate that multiway-mutual acceptance can be inferred from pairwise-mutual acceptance. In general, mutual acceptance of many components, with complicated communication structures, should be further investigated. Requiring  for all intermediate steps means that there cannot be any unexpected outputs. For real systems however, these outputs are only a problem if they appear in the ﬁnal composition of all the components. The fact that two components do not work well in all environments is not a problem if you plan to use them together with other components that will prevent this. Therefore, a diﬀerent deﬁnition of mutual acceptance for more than two components at a time might be investigated. To apply the theory in this paper to a practical use case, it will need to be extended with the concept of data. Real systems can seldom be modelled with a ﬁnite set of labels, but will instead send instances of data types to each other. This has been formalised in the theory of symbolic transition systems (ST S) [5,12], which is the underlying formalism of several MBT tools. The concepts in this paper could be extended to ST S which would bring them closer to being applied in practice.  
  10  
  Conclusion  
  Model-based testing is a promising technology for increasing the eﬃciency and eﬀectiveness of testing. The applicability of MBT, however, is limited by the availability of models. Larger system models are hard to create, but can be composed from multiple smaller component models. In this paper, we have deﬁned the mutual acceptance relation  between speciﬁcations, which guarantees that model-based testing is compositional, i.e. if two components have been tested for uioco-correctness with respect to their respective speciﬁcations, then the composition of these implementations is also uioco-correct with respect to the composition of their speciﬁcations, under the assumption that the parallel composition operator itself is faithfully implemented. This is an improvement over previous results which obtained the same conclusion with a stricter requirement, viz. that all speciﬁcations must be input-enabled [4]. In addition, we have shown that this result can also help when updating older components with newer ones, and when localising a faulty component during diagnosis of a large, componentbased system.  
  Compositionality in Model-Based Testing  
  monitoring intrusionDetected initializing alarm ready idle policeNotified sensorsLost assistance  
  Case Study III (CS III): Safe-Home System. This case study is a CyberPhysical System (CPS), namely Safe-Home, i.e., an open-source security software adapted from [8]. It consists of an intrusion detection system to control alarms and sensors that implement some home safety features. Figure 4 illustrates the behavior of the system modeled as a DTMC and the transition matrix is illustrated in (8). We can notice that the model is composed of nine states and seventeen transitions. First, there is a setup (composed of idle and ready states). Then, there are three main states (initializing, monitoring, and alarm) which are in charge of sensor initialization, detection, and alarm handling, respectively. If an intrusion is detected, some actions must be done, e.g., calling the police.  
  Fig. 4. DTMC model for the Safe-Home, adapted from [8]  
  L. Rebelo et al.  
  Table 6. List of the ﬁrst test cases sequences generated for CS III, revealing the highest probabilities using H2. Travelled states idle-ready-initializing-monitoring-assistance idle-ready-initializing-monitoring-intrusionDetected-alarm-policeNotified idle-ready-initializing-monitoring-monitoring-assistance idle-ready-initializing-monitoring-monitoring-intrusionDetected-alarm-policeNotified idle-ready-initializing-monitoring-monitoring-monitoring-assistance  
  N. Kushik et al.  
  separability relation to be able to distinguish each faulty implementation from the speciﬁcation with a given level of certainty, P . We now extend this work by taking away a number of assumptions. First of all, we allow all the machines to be non-initialized and thus, we consider a test suite represented by a single (checking) sequence. Secondly, as no reset can be applied during testing, we do not minimize the test suite cardinality, instead we shorten the overall length of the checking sequence, whenever possible. The latter is based on the introduction of P -probably separating sequences, a P -probably checking sequence and a proper use of related transfer sequences. The structure of the paper is as follows. Section 2 contains preliminaries. Noninitialized probabilistic machines are introduced in Sect. 3, while the checking sequence minimization strategy is presented in Sect. 4. Section 5 concludes the paper.  
  2  
  Preliminaries  
  An FSM is a 4-tuple S = S, I, O, hS  where S is a ﬁnite nonempty set of states, I and O are ﬁnite input and output alphabets, and hS ⊆ S×I ×O×S is a transition relation. The FSM S is non-deterministic if for some pair (s, i) ∈ S × I, there exist several pairs (o, s ) ∈ O × S such that (s, i, o, s ) ∈ hS ; otherwise, the FSM is deterministic. The FSM S is observable if for every two transitions (s, i, o, s1 ), (s, i, o, s2 ) ∈ hS it holds that s1 = s2 ; otherwise, the FSM is non-observable. The FSM S is complete if for every pair (s, i) ∈ S × I, there exists a transition (s, i, o, s ) ∈ hS ; otherwise, the FSM is partial (partially speciﬁed). We hereafter consider complete observable FSMs, if not stated otherwise. A non-deterministic FSM S = S, I, O, hS , pr is probabilistic, when for each non-deterministic transition (s, i, o, s ) ∈ hS , the function pr deﬁnes the probability for the output o to be produced at state s under input i, pr : S × I × O −→ [0, 1]. For a non-deterministic FSM, the function pr is deﬁned in such a way  that ∀s ∈ S ∀i ∈ I o∈O pr(s, i, o) = 1, and it is extended over input/output sequences from (IO)∗ . Given an input/output sequence α/β = (α /β  ).(i/o) and a state s0 , pr(s0 , α, β) = pr(s0 , α , β  ) ∗ pr(s, i, o), where s is the α /β  -successor of the state s0 of the speciﬁcation FSM S; if the trace α /β  is not deﬁned at state s0 then pr(s0 , α , β  ) = 0; pr(s, ε, ε) = 1. In this paper, similar to our previous work [4], for test minimization, we consider the following fault model S, ∼ =, F D, where S is complete possibly non-deterministic observable FSM, ∼ = is the non-separability relation, and all the implementations from F D are explicitly enumerated, F D = {I1 , I2 , . . . , Ik }.  
  S), if there exists a separating FSMs Ij and S are separable, (written Ij ∼ = sequence α ∈ I ∗ such that the sets of output reactions of Ij and S to α do not intersect, i.e., out(Ij , α) ∩ out(S, α) = ∅. We are interested in exhaustive test suites, such that each Ij ∈ F D that is separable with S can be detected by the test suite. Moreover, we are interested in a test suite containing a single sequence which is referred to as a checking sequence with respect to a corresponding fault model. Therefore such checking sequence α should be able to detect all nonconforming implementations, i.e., all the implementations of the fault domain which are separable with the speciﬁcation.  
  Conclusion  
  In this paper, we presented a probabilistic approach for minimizing the length of a checking sequence, probably keeping a given level of its exhaustiveness. It is a continuation of the paper [4], where only initialized machines (speciﬁcation and its implementations) were considered. An interesting direction would be to combine both approaches, when some of the test sequences could be regrouped together, i.e., building a P -probably checking sequence for a subset of implementations. There are many possibilities for future work, we state some of these future directions. We did not discuss any P -probably checking sequence derivation scenarios in detail, assuming that a starting sequence to be shortened is given. At the same time, we did not consider any other testing assumptions nor other conformance relations. Finally, experimental evaluations need to be performed to see how often the approach brings good practical results.  
  Abstract. In this paper, we report on applying combinatorial testing to large language models (LLMs) testing. Our aim is to pioneer the usage of combinatorial testing to be used in the realm of LLMs, e.g. for the generation of additional training or test data. We ﬁrst describe how to create an input parameter model for the input of an LLM. Based on a given original sentence, we derive new sentences by replacing words with synonyms according to a combinatorial test set, leading to a speciﬁed level of coverage over synonyms while attaining an eﬃcient diversiﬁcation. Assuming that the semantics of the original sentence are retained in the derived sentences, we construct a test oracle based on existing annotations. In an experimental evaluation, we apply generated pairwise sentence test sets from the BoolQ benchmark set [4] against two LLMs (T5 [12] and LLaMa [15]). Having automated our approach for test sentence generation, as well as their execution and analysis, our experimental evaluations demonstrate the applicability of pairwise combinatorial testing methods to LLMs. Keywords: large language models  
  1  
  Fig. 1. Overview of a generic CT process [6].  
  In this paper, we present how the CT process (see Fig. 1) can be instantiated in order to be applied for the testing of LLMs. Our approach takes as input a sentence, e.g. a question from an existing training data set, and allows us to diversify the input to a chosen degree of coverage by deriving additional test sentences according to a combinatorial test set. Thereby our focus is to maintain the semantics of the sentence, such that a potential existing correct or expected answer is also valid for all of the derived sentences. In particular, the contributions of our work are as follows: 1. An approach to construct an input parameter model (IPM) from a sentence which is given as input to an LLM;  
  Applying Pairwise CT to LLM Testing  
  2. A method to derive a combinatorial test set based on a given sentence and a covering array (CA); 3. An initial experimental evaluation, where we use pairwise combinatorial testing against two SUTs. This Paper is Structured as Follows. In Sect. 2, we review related work on the testing of LLMs. In Sect. 3, we give a general description of our approach for applying CT to the testing of LLMs, and illustrate it by means of a running example. In Sect. 4 we give an outline of our initial experimentation. Finally, in Sect. 5, we brieﬂy discuss potential threats to validity of this work before we summarize the paper and mention some ideas for future work in Sect. 6.  
  2  
  Related Work  
  The approach of mutating input sentences to a LLM while preserving the meaning or sentiment has recently garnered attention. The work by Gardner et al. [5] proposes the generation of contrast sets from standard test sets for supervised learning by manually perturbing the test instances, leading to a more accurate and comprehensive assessment of a model’s linguistic capabilities. Similarly, Khashabi et al. [9] propose a novel method for generating training datasets by applying human based natural perturbations to a small scale seed dataset. In contrast to these works, the approach presented in this paper uses an automated way for extending test or training data. Ruane et al. [13] put forward a framework for using divergent input examples, generated by altering a textual user utterance while still maintaining the original intent, for testing the quality of conversational agents. In a continuation, Guichard et al. [7] propose and evaluate an approach with regard to the utilization of paraphrases. They generate divergent input examples by processing the input data through lexical substitutions, i.e. replacing words with their synonyms, using the Oxford Thesaurus for the retrieval of synonyms. Bozic [2] proposes a metamorphic testing approach for chatbot testing making use of an ontology. The method presented in our work can be used for deriving tests for such a metamorphic testing approach.  
  3  
  University of Camerino, Camerino, Italy [email protected]  2 IASI CNR, Rome, Italy  
  Abstract. Blockchain technology is increasingly being adopted in various domains where the immutability of recorded information can foster trust among stakeholders. However, upgradeability mechanisms such as the proxy pattern permit modifying the terms encoded by a Smart Contract even after its deployment. Ensuring that such changes do not impact previous users is of paramount importance. This paper introduces CATANA, a replay testing approach for proxy-based Ethereum applications. Experiments conducted on real-world projects demonstrate the viability of using the public history of transactions to evaluate new versions of a deployed contract and perform more reliable upgrades. Keywords: Replay Testing · Smart Contract Pattern · Ethereum · Software Testing  
  1  
  M. Barboni et al.  
  system under test to generate replayable test scripts, and is often used to support regression testing activities [3,10,12,13]. In conventional systems, capturing execution traces can be intrusive and introduce performance overhead. Public blockchains oﬀer a signiﬁcant advantage by permanently and publicly recording all operations (i.e., transactions) executed by their users. Despite this, the public history of transaction was only leveraged for inspection [11], monitoring [5] and test case generation purposes [20]. In this work, we propose the ﬁrst replay testing approach for proxy-based upgradeable smart contracts. Speciﬁcally, we analyze possible complexities and opportunities connected to the Ethereum environment, and we implement the proposed solution in the CATANA (Contract Assessment through TrANsaction replAy) tool. The rest of this paper is structured as follows: in Sect. 2 we provide background about replay testing and smart contracts. Section 3 describes the proposed replay approach, while Sect. 4 presents the experimental evaluation. Related work is reported in Sect. 5, while in Sect. 6 we draw conclusions and identify areas for further research.  
  2  
  const t x S e t I m p l e m e n t a t i o n = proxy [ c o n f i g . upgradeFunc ] ( l o g i c V 2 . a d d r e s s , { from : c o n f i g . DelegatorAdmin } ) ; ... }) ;  
  5) Replaying the transaction on the upgraded logic contract At this point, we are ready to replay the transaction on the upgraded implementation of P . This step is very straightforward, as we repeat the same call to the proxy described in Sect. 3, and we check if its outcome is consistent with the oracle.  
  4  
  detects the performance degradation (detection), then deﬁnes mitigation actions (mitigation), and ﬁnally, restores the system to an acceptable performance state (recovery), [2]. Recovering from a disruptive event may require additional energy consumption, which, in turn, may increase the CAIS energy adverse eﬀects, such as CO2 footprint, [6,9]. The eﬃcient usage of energy with minimizing adverse eﬀects is called greenness, [6]. In this in-progress work, we are interested in the relation between two properties of a CAIS: resilience and greenness. In particular, we aim to support the decision-making process to trade-oﬀ between them. Speciﬁcally, we introduce the approach GResilience 1 to select automatically recovery action(s) that ﬁnds the best trade-oﬀ between greenness and resilience while restoring the CAIS services to an acceptable performance state. Our approach formulates the trade-oﬀ decision in two ways: i) as a one-agent decision through optimization, and ii) as a multi-agent decision through game theory. In the former case, the decision is taken by selecting actions(s) and optimizing measures of resilience and greenness. While in the latter, actions are selected through a multi-participant game in which the measures of resilience and greenness deﬁne the payoﬀ for the actions. Finally, we plan to apply our approach to a CAIS demonstrator available at our laboratory. The demonstrator is a robotic arm that is equipped with an AI component and performs activities for in-production systems. To this aim, we have devised an experimental protocol that we present in the next sections. In summary, we aim to understand the relationship between resilience and greenness and discuss the one-agent and the multi-agent methods in case of CAISs. To achieve this goal, this study poses the following research questions: – RQ1: Is the optimization model a valuable solution for automatizing the decision-making process that ﬁnds a trade-oﬀ between the greenness and resilience in CAISs? – RQ2: Is the game theory model a valuable solution for automatizing the decision-making process that ﬁnds a trade-oﬀ between the greenness and resilience in CAISs? – RQ3: What are the major diﬀerences between the optimization model and the game theory model solutions? The rest of this paper discusses the related work (Sect. 2), our approach and experiment protocol to the trade-oﬀ between resilience and greenness (GResilience) along with our demonstrator CAIS (Sect. 3), and ﬁnally our conclusion and future work (Sect. 4).  
  2  
  Approach - GResilience  
  GResilience is our empirical approach to support the decision-making process and trade-oﬀ between greenness and resilience after a disruptive event. The approach provides one or more agents with the measurements required to take the decision and select the recovery action that best balance between the two properties. The approach is applied to CAIS in which the AI component learns from human movements. GResilience monitors and controls the collaboration between the human and the AI component to support the decision-making process after disruptive events. The core common component of the GResilience approach is the measurement framework for resilience and greenness. The framework is then used by two techniques to trade-oﬀ between greenness and resilience: optimization and game theory. While the former is typically used for trading oﬀ between  
  GResilience: Trading oﬀ Between the Greenness and the Resilience of CAIS  
  non-functional properties of a system [7], the latter, to the best of our knowledge, is novel in such context. The use of one or the other depends on the type of problem the decision-maker needs to solve. The goal of each technique is to select a recovery action after the disruptive event to return to an acceptable performance state. Recovery actions are categorized into two classes: i) general actions that are derived from a system or environment policies, and ii) actions deﬁned by the decision maker. In Fig. 1 we describe the resilience process from two perspectives. Figure 1 (A) describes the performance behavior of the running system over time, while, Fig. 1 (B), illustrates the GResilience approach state diagram based on such resilience process. In Fig. 1 (A), the process starts at a steady state and faces a disruptive event at te that may transition to a disruptive state at td . During the disruptive state, the system starts the recovery process and selects a recovery action to move to an acceptable recovery state at tr . In Fig. 1 (B), the system starts at a steady state and remains in the same state if there is no performance degradation. When a performance degradation occurs, the system moves to a disruptive state, where it either recovers by default system actions or policies, or it moves to the trade-oﬀ state. The trade-oﬀ state invokes the GResilience model (optimization or game theory) to select the action that ﬁnds the best trade-oﬀ between greenness and resilience. As an example in Fig. 1 (B), we illustrate two actions: a1) that targets a learning state, and a2) that targets anoperating state of the AI component. From either state, the performance recovery enters in the measuring state looping between one state and the measurement state until the system reaches an acceptable performance (recovered state).  
  Recovered State Acceptable Performance  
  results to the expected payoﬀ described in Table 1. Thus, to ﬁnd the probability q (resp. p) with MSNE, we equal the expected payoﬀs of Pg (resp. Pr ) for a1 and a2 and solve the resulting equation for q (resp. p). Experiments Protocol. We plan a series of experiments in three stages, i) setup, ii) iterative execution and data collection, and iii) data analysis. During the setup stage, we need to understand the disruptive events that might occur, and what are the feasible actions to recover from one of these events. As illustrated by Fig. 1 (B), GResilience wraps the system to detect the performance degradation in the second stage, and for each trading oﬀ technique, we collect the number of iterations to recover, the performance at the start and the end of each iteration, the selected action per iteration, and the values of the resilience and greenness attributes per iteration. Finally, by analyzing the collected data, we can understand for which disruptive event a technique is a valuable solution to automatize the decision-making process, and what are the major diﬀerences between them. Demonstrator. Our CAIS demonstrator “CORAL”2 is a collaborative robot arm learning from demonstrations. Figure 2 shows the robotic arm, (where 1, 3, and 4 represent the arm and its controllers) that works with the human (6) to classify objects moving on the conveyor belt (2) based on their colors. In addition to object color learning, CORAL learns background subtraction, object detection, and human movement. CORAL has two vision sensors, one through a Kinect (5) that monitors human movement, and a second above a conveyor belt that moves objects to be classiﬁed. Losing the lights that support the vision sensors or having another human in the vision range may disrupt CORAL ability to classify the objects and drop or wrongly classify them. Thus, CORAL requires more time to learn the objects with the faded environmental light and this consumes additional energy. We will apply our approach to CORAL under diﬀerent disruptive events.  
  Fig. 2. Collaborative Robot Learning from Demonstrations (CORAL) 2  
  Conclusion and Future Work  
  This in-progress work proposes an approach to support the decision-making process to recover from a disruptive event that has caused performance degradation and control the energy adverse eﬀects at the same time. To this aim, we have deﬁned a set of measures for resilience and greenness and two techniques leveraging optimization and game theory respectively. The techniques automate the selection process of the recovery actions by measuring the trade-oﬀ between the greenness and the resilience capability of CAIS. The ﬁrst technique evaluates each action separately using an optimization model (WSM), whereas the second technique evaluates greenness and resilience payoﬀs by selecting an action through a game theory model leveraging “The Battle of Sexes”. To verify our approach, we designed experiments to test our techniques on our CAIS demonstrator. In our future work, we plan to run experiments on CORAL. This will help us understand the relationship between resilience and greenness for CAIS and eventually extend our approach to test CAIS for other non-functional properties. In addition, we plan to extend our techniques with reinforcement learning, to incorporate a rewarding mechanism in the optimization and game theory techniques. Moreover, we plan to reconsider further human attributes as for example human energy and ﬁnancial costs.  
  security policies, implement traﬃc steering, and provide QoS guarantees [4]. SDN and NFV whenever combined together, bring signiﬁcant beneﬁts to SFC orchestration, including ﬂexibility, simpliﬁed management, resource eﬃciency, scalability, optimization, and faster service innovation. Modeling SFC orchestration in SDN/NFV environments with DRL is a promising approach that could enable more eﬃcient and eﬀective network management, particularly in complex and dynamic environments where traditional approaches may not be suﬃcient. However, there are still many challenges that need to be addressed, such as designing appropriate reward functions and ensuring scalability and robustness. DRL is a type of Machine Learning (ML) that enables an agent to learn how to make decisions by interacting with an environment composed with a Physical Substrate Network (PSN) processing each incoming SFC request according to an incremental strategy [2]. The environment provides feedback in the form of rewards or penalties based on agent’s actions. If the agent successfully places and chains the VNFs to achieve a speciﬁc performance metric, it receives a reward, otherwise, it receives a penalty. The DRL algorithm then uses this feedback to update the agent’s policy, which determines the agent’s actions. The policy is updated to maximize the expected future rewards. In this paper, we investigate quality testing of SFC orchestration problem in SDN/NFV environments. The problem is deﬁned as a DRL problem where the agent is responsible for orchestrating VNFs placement and chaining to achieve a speciﬁc goal related to maximizing QoE while meeting QoS requirements. Testing in online DRL is essential to ensure that the agent performs well and to proﬁt from beneﬁts concerning: – Performance evaluation, where the environment is dynamic and unpredictable. This helps also to validate that the agent can learn and adapt to changing conditions and still perform well. – Robustness enhancement, through handling unexpected situations by exposing the agent to a variety of scenarios to identify any weaknesses in the agent’s learning process and reﬁne the algorithm accordingly. – Scalability evaluation, as the number of network devices (PSN nodes, PSN links) the complexity of the deployment environment increases as well. Online testing can help ensuring that the agent can handle this increased complexity without compromising performance. In this paper, we adopt DRL approach to formulate SFC orchestration problem maximizing QoE while meeting QoS requirements in SDN/NFV environment. Along the DRL process we focus on training, testing and evaluation phases and we investigate agent behavior and how it strives to achieve suitable tradeoﬀ between performing good learning quality (corresponding to a near-optimal policy) during testing while converging in a reasonable time. The rest of this paper is organized as follows: we address, in Sect. 2, the RL and DRL process including the diﬀerent phases of training, testing and evaluation. We detail, in this same section, why it is important to ﬁnd a suitable performance-convergence trade-oﬀ. In Sect. 3, we describe next DQN and its variant, Double DQN, used in this paper to implement the DRL approach. In Sect. 4, we investigate the  
  M. Escheikh et al.  
  DRL approach used for SFC orchestration in SDN/NFV environments and we particularly detail the reward design. We devote, Sect. 5, to numerical results and performance evaluation before concluding this paper in Sect. 6.  
  2  
  The goal of training is to improve the agent’s performance over time by ﬁnding an optimal or near-optimal policy that maximizes the expected cumulative reward. For this purpose, the training process evolves over time by iteratively updating the Q-values using Bellman equation [6], until converging to an optimal Q-function. This update rule is repeated for each state-action pair the agent encounters during training, until the Q-values converge to the optimal values. In DRL, a Neural Network (NN) is used to approximate Q-values or policy function. The NN is trained to change its attributes (such as weights and learning rate) in order to reduce the losses and obtain faster results. The weights of the NN are updated iteratively using back-propagation, which computes the gradient of the loss function with respect to the weights. This gradient is then used to update the weights in the direction that minimizes the loss function, such as the mean squared error between the predicted Q-values and the target Qvalues. This update rule is repeated for each batch of training samples, until the NN converges to an optimal policy or value function. The convergence of the training process depends on various factors, such as the complexity of the problem, the quality of the training data, and the choice of DRL algorithm and hyper-parameters. Testing and Evaluation in RL. Once the agent has learned a policy, it can be tested on a new set of data to evaluate its performance. Testing can be done by running the agent in the environment and measuring its cumulative reward or success rate over a certain number of episodes. The goal of testing is to assess how well the agent has learned to make optimal decisions and to identify areas where it may need further improvements. The testing stage involves evaluating the agent’s performance on a separate set of test runs, which are generated by the same environment but are not used for training. The goal of testing is to measure the generalization performance of the agent and to identify potential over-ﬁtting or under-ﬁtting issues. Evaluation is the process of assessing the quality of the agent’s learned policy. It evaluates the agent’s performance on a speciﬁc task or benchmark, which is often used to compare the performance of diﬀerent RL algorithms or agents. The goal of evaluation is to provide a standardized and objective measure of the agent’s performance and to identify potential strengths and weaknesses. One common evaluation method is to use performance metrics such as average reward, success rate, or convergence rate to compare the agent’s performance to that of other agents or benchmarks. The goal of evaluation is to ensure that the agent’s learned policy is eﬀective and reliable in making optimal decisions in the environment. In practice, the testing and evaluation stages are often combined into a single process and the evaluation phase can actually occur during the training phase. This process can be repeated multiple times using diﬀerent sets of test runs to obtain a more comprehensive assessment of the agent’s performance. During learning (or training), the optimal policy may be unreachable and in such context we attempt to ﬁnd a near-optimal solution. The evaluation phase is needed to verify whether learned policy to solve actual realworld problem is good enough before deploying RL algorithm. The evaluation  
  M. Escheikh et al.  
  phase assesses quality of the learned policy and how much reward the agent obtains if it follows that policy. So, a typical metric that can be used to assess the quality of the policy is to plot the cumulative reward as a function of the number of runs. DRL algorithm is considered dominates another if its plot is consistently above the other. Moreover, assessment may cover the generalization of the learned policy by evaluating it in diﬀerent (but similar) environments to the training environment. Whereas during training, the goal is to ﬁnd the optimal policy, the evaluation phase aims to assess the quality of the learned policy (or RL algorithm). Notice that evaluation may be performed in online scenario during training phase. So in online DRL, there is only a learning or training phase and no a separate testing or evaluation phase. Online Evaluation in DRL. In DRL, online evaluation of the learning phase refers to the process of evaluating the performance of the learning algorithm as it interacts with its environment in real-time. The goal of online evaluation is to monitor the learning process and identify any issues or areas for improvement as they arise. There are several approaches to online evaluation in RL, including: 1. Monitoring the reward signal: The reward signal is a critical component of RL, as it guides the learning algorithm towards optimal behavior. By monitoring the reward signal over time, we can get a sense of how well the learning algorithm is performing. 2. Tracking performance metrics: Such as the average episode length, the number of episodes required to converge, or the average reward per episode. These metrics can help us identify areas where the learning algorithm is struggling and make adjustments accordingly. 3. Visualizing the learning process: In order to understand the learning process in RL, we can use plots, graphs, or animations to visualize the learning algorithm’s behavior over time, making it easier to identify patterns and trends. 2.3  
  Convergence-Performance Trade-Oﬀ in Training RL Algorithms  
  (1)  
  This extension enables more scalability by handling higher dimensional state spaces and allows to solve more complex decision-making problems. The DQN tackles the instability caused by using function approximation by leveraging two innovative techniques: experience replay and target Q-networks. Experience replay allows to break the correlation between consecutive experience tuples and stabilizes the training process. This is achieved by storing a history of experiences in a buﬀer and by randomly sampling from the buﬀer during training [10]. On the other hand, the target Q-network is a copy of the main Q-network that is used to generate the training targets. DQN uses a frozen target network to generate the target Q-values, used to update the main network. By freezing the target network, DQN attempts to improve stability of the training targets by decoupling them from the parameters being updated and prevent over-ﬁtting [11]. Freezing concerns the target network parameters Q(s , a ; θ− ) for a ﬁxed number of iterations while updating the online network Q(s, a; θi ) by gradient descent to minimize the loss function. The speciﬁc gradient update is given as follows: (2) ∇θi Li (θi ) = Es,a,r,s [(yiDQN − Q(s, a; θi ))∇θi Q(s, a; θi )] Given the state s , reward r, discount factor γ, DQN computes the target Q-value yiDQN as follows: yiDQN = r + γmaxa Q(s , a ; θ− ) (3) where θ− represents the parameters (weights) of a ﬁxed and separate target network. Despite the advantages of DQN, it is known to be computationally expensive algorithm and may require a large amount of training data. It may also suﬀer  
  M. Escheikh et al.  
  from instability during training due to the network’s weights oscillations, which can negatively aﬀect the convergence of the algorithm to an optimal solution. The major challenge is to ﬁnd suitable performance-convergence compromise while computing NNs. 3.2  
  Double DQN  
  Hence, the last step deals with service function monitoring to ensure that SFC is functioning correctly and to detect any potential issues. On the other hand, automating SFC deployment in SDN/NFV environments involves using tools and techniques to automate the process of creating, conﬁguring, and deploying NFs to create SFC. Among these tools SDN network controllers are essential and are commonly used to achieve Controller-based automation of SFC deployment. The controller can automatically conﬁgure NFs and route traﬃc through SFC based on policy and service requirements. This approach can simplify the deployment process and reduce the risk of human error. Also SFC orchestration technique may be leveraged to automate the management of SFCs, such as scaling, upgrading, and monitoring. Lastly virtualization enables creating VNFs that can be deployed on Virtual Machines (VMs) or containers and greatly facilitates deployment automation of NFs, as virtual resources. By automating SFC deployment, network operators can reduce time and eﬀort required to deploy and manage NFs, and ensure eﬃcient delivery of NS. Also, optimizing SFC in SDN/NFV environments involves balancing various factors such as QoE, service requirements, available resources, network topology and QoS parameters. 4.2  
  SFC Orchestration Based on DRL Approach  
  Reward Design  
  In this paper we adopt similar problem formulation given in [1], and we brieﬂy describe in the rest of this subsection reward function used by DRL agent to take actions that maximize user’s satisfaction while meeting QoS requirements. Indeed, designing an eﬀective reward function is a challenging task, and the speciﬁc form of the function will depend on application or service being optimized. In this regard, the reward function is designed by balancing the trade-oﬀ between maximizing QoE while meeting QoS constraints, to ensure that RL agent learns to take actions leading to good user experience.  
  (6)  
  Relationships between QoE and measurable QoS parameters: Measuring QoE versus QoS is a complex task that requires careful consideration of the speciﬁc application or service being evaluated and the user’s individual preferences. Diﬀerent methods may be more appropriate depending on context, and the ultimate goal is to optimize the user’s experience while meeting the necessary QoS requirements. From a service provider or network operator perspective there is an imperative need to apprehend the relationships between QoE and measurable QoS parameters. This is mainly achieved in literature through two diﬀerent generic and quantitative formula laws. The ﬁrst one proposed by Fiedler et al. [7] and based on an exponential function called IQX hypothesis to express relationship between QoE and QoS degradation. The objective of such formula is to capture correlations between network-level traﬃc features and user perceived QoE. The second one, proposed by Weber-Fechner Law (WFL) [12] relies on a logarithmic relationships between relative quality changes in network QoS and user QoE. We use a mixture of the above models to deﬁne QoE gain used in designing the reward function. QoE (Eq.(7)) [7,12] speciﬁes reward obtained in response to successful SFC deployment. QoEsf c =  
  K   
  Conclusion  
  Testing and evaluation of the quality of learning in DRL is critical for ensuring the generalization of learned policies and to assess their robustness, In this paper, we proposed an investigation of testing and evaluation of learning phase of the Double DQN agent through testing its capacity to learn a near-optimal policy to achieve SFC orchestration maximizing QoE while meeting QoS requirements of PSN in SDN/NFV environments. By evaluating the performance of DRL agent, we attempt to identify areas where the agent under-performs in speciﬁc scenarios and how modifying the related parameters to achieve the best quality of training enabling suitable performance-convergence trade-oﬀ. Trough numerical investigations, we particularly explore the impact of modifying progressively quality of learning (quantiﬁed by a QoE score) and the scale of the network (PSN) on achieving a good compromise between performance and convergence. We show that increasing these parameters often leads to mitigated results and we provide detailed explanations of this behavior based on the stochastic nature of the DRL process. In future works, we intend to make further explorations about other parameters, hyper-parameters and DRL algorithms (DQN variants) that may impact DRL agent behavior and the achieved performance-convergence trade-oﬀ during testing in learning phase.  
  Author Index  
  Author Index  
