 This book constitutes the refereed proceedings of the 17th International Conference on Reachability Problems, RP 2023, h  
 This book constitutes the proceedings of the 15th International Symposium on NASA Formal Methods, NFM 2023, held in Hous  
 This book constitutes the refereed proceedings of the 29th International Symposium on Model Checking Software, SPIN 2023  
 Author / Uploaded 
  Manuel V. Hermenegildo 
  José F. Morales 
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  Manuel V. Hermenegildo José F. Morales (Eds.)  
  Static Analysis 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings  
  Lecture Notes in Computer Science  
  •  
  Static Analysis 30th International Symposium, SAS 2023 Cascais, Portugal, October 22–24, 2023 Proceedings  
  Abstract. In traditional program verification, the goal is to automatically prove whether a program meets a given property. However, in some cases one might need to prove that a (potentially infinite) set of programs meets a given property. For example, to establish that no program in a set of possible programs (i.e., a search space) is a valid solution to a synthesis problem specification, e.g., a property ϕ, one needs to verify that all programs in the search space are incorrect, e.g., satisfy the property ¬ϕ. The need to verify multiple programs at once also arises in other domains such as reasoning about partially specified code (e.g., in the presence of library functions) and self-modifying code. This paper discusses our recent work in designing systems for verifying properties of infinitely many programs at once.  
  1  
  L. D’Antoni  
  Clearly y := 10 ∈ L(Start); moreover, all programs in L(Start) are incorrect on at least one input. For example, on the input x = 15 every program in the grammar sets y to a value greater than 15. Consequently, sy first is unrealizable. While it is trivial for a human to establish that sy first is indeed unrealizable, it is deﬁnitely not trivial to automatically verify that all the inﬁnitely many programs accepted by the grammar Gfirst are incorrect on at least one input. Another setting where one may want to prove a property about multiple programs is when verifying partial programs—i.e., programs where some components are unknown [10]. Example 2 (Symbolically Executing Partial Programs). Consider the following program foo that outputs the diﬀerence between the number of elements of an array for which applying a function f results in a positive number and the number of elements for which the result of applying f is a negative number. 1 2 3 4 5 6 7 8  
  def foo (f , array ): count = 0 for i in range ( len ( array )): if f ( array [ i ]) > 0: count += 1 else : count -= 1 return count  
  Unrealizability Logic  
  The works we discussed in Sect. 3 provide automatic techniques to establish that a problem is unrealizable; however, these techniques are all closed-box, meaning that they conceal the reasoning behind why a synthesis problem is unrealizable. In particular, these techniques typically do not produce a proof artifact that can be independently checked. Our most recent work presents unrealizability logic [8], a Hoare-style proof system for reasoning about the unrealizability of synthesis problems (In this section, we include some excerpts from [8].). In addition to the main goal of reasoning about unrealizability, unrealizability logic is designed with the following goals in mind: – to be a general logic, capable of dealing with various synthesis problems; – to be amenable to machine reasoning, as to enable both automatic proof checking and to open future opportunities for automation; – to provide insight into why certain synthesis problems are unrealizable through the process of completing a proof tree. Via unrealizability logic, one is able to (i ) reason about unrealizability in a principled, explicit fashion, and (ii ) produce concrete proofs about unrealizability. To prove whether the problem in Example 1 is unrealizble, one would use unrealizability logic to derive the following triple, which states that if one starts in a state where x = 15, executing any program in the set L(Start) will result in a state where y is diﬀerent than 10: {|x = 15|} L(Start) {|y = 10|} Unrealizability logic shares much of the intuition behind Hoare logic and its extension toward recursive programs. However, these concepts appearing in Hoare logic alone are insuﬃcient to model unrealizability, which motivated us to develop the new ideas that form the basis of unrealizability logic. Hoare logic is based on triples that overapproximate the set of states that can be reached by a program s; i.e., the Hoare triple {P } s {Q} asserts that Q is an overapproximation of all states that may be reached by executing s, starting from a state in P . The intuition in Hoare logic is that one will often attempt to prove a triple like {P } s {¬X} for a set of bad states X, which ensures that execution of s cannot reach X. Unrealizability logic operates on the same overapproximation principle, but diﬀers in two main ways from standard Hoare logic. The diﬀerences are motivated by how synthesis problems are typically deﬁned, using two components: (i ) a search space S (i.e., a set of programs), and (ii ) a (possibly inﬁnite) set of related input-output pairs {(i1 , o1 ), (i2 , o2 ), · · ·}.  
  8  
  L. D’Antoni  
  To reason about sets of programs, in unrealizability logic, the central element (i.e., the program s) is changed to a set of programs S. The unrealizability-logic triple {|P |} S {|Q|} thus asserts that Q is an overapproximation of all states that are reachable by executing any possible combination of a pre-state p ∈ P and a program s ∈ S. The second diﬀerence concerns input-output pairs: in unrealizability logic, we wish to place the input states in the precondition, and overapproximate the set of states reachable from the input states (through a set of programs) as the postcondition. Unfortunately, the input-output pairs of a synthesis problem cannot be tracked using standard pre- and postconditions; nor can they be tracked using auxiliary variables, because of a complication arising from the fact that unrealizability logic must reason about a set of programs—i.e., we want our possible output states to be the results of executing the same program on the given input (for all possible programs) and prevent output states where diﬀerent inputs are processed by diﬀerent programs in the search space. To keep the input-output relations in check, the predicates of unrealizability logic talk instead about (potentially inﬁnite) vector-states, which are sets of states in which each individual state is associated with a unique index—e.g., variable x of the state with index i is referred to as xi . We defer the reader to the original unrealizability logic paper for these details [8]. The proof system for unrealizability logic has sound underpinnings, and provides a way to build proofs of unrealizability similar to the way Hoare logic [3] provides a way to build proofs that a given program cannot reach a set of bad states. Furtheremore the systems is relatively complete in the same sense as Hoare logic is.  
  4  
  Conclusions  
  This paper outlines recent advances in reasoning about inﬁnite sets of programs at once. We presented techniques for proving unrealizability of synthesis problems that draw inspiration from traditional program veriﬁcation. However, such techniques did not provide ways to produce proof artifact and to address this limitation, we discussed unrealizability logic, the ﬁrst proof system for overapproximating the execution of an inﬁnite set of programs. This logic is also the ﬁrst approach that allows one to prove unrealizability for synthesis problems that require inﬁnitely many inputs to be proved unrealizable. The name “unrealizability logic” is perhaps misleading as the logic allows one to reason about many properties beyond unrealizability. The fact that unrealizability logic is both sound and relatively complete means that this proof system can prove (given powerful enough assertion languages) any property of a given set of programs expressed as a grammar. For example, the problem given in Example 2 of identifying whether a symbolic execution path is infeasible can be phrased as proving whether an unrealizability triple holds.  
  Verifying Infinitely Many Programs at Once  
  11  
  give an overview of our experience in development and commercialization of two industry-strength sound analyzers, aiT WCET analyzer and Astr´ee. We will discuss the lessons learned, and present recommendations to improve dissemination and acceptance in industrial practice.  
  2  
  Sound Runtime Error Analysis  
  The purpose of the Astr´ee analyzer is to detect source-level runtime errors due to undeﬁned or unspeciﬁed behaviors of C programs. Examples are faulty pointer manipulations, numerical errors such as arithmetic overﬂows and division by zero, data races, and synchronization errors in concurrent software. Such errors can cause software crashes, invalidate separation mechanisms in mixed-criticality software, and are a frequent cause of errors in concurrent and multi-core applications. At the same time, these defects also constitute security vulnerabilities, and have been at the root of a multitude of cybersecurity attacks, in particular buﬀer overﬂows, dangling pointers, or race conditions [31].  
  Abstract Interpretation in Industry – Experience and Lessons Learned  
  Further Development  
  From a technical perspective, the ensuing development activities can be grouped into several categories: Usability. The original version of Astr´ee was a command-line tool, however, to facilitate commercial use, a graphical user interface was developed. The purpose is not merely to make the tool more intuitive to use, but – even more importantly – to help users understand the results. Astr´ee targets corner cases of the C semantics which requires a good understanding of the language, and it shows defects due to behavior unexpected by the programmer. To facilitate understanding the unexpected behavior, we have developed a large set of graphical and interactive exploration views. To give some examples, all parents in the call stack, relevant loop iterations or conditional statements that lead to the alarm can be accessed by mouse click, tool tips show the values of values, the call graph can be interactively explored, etc. [28]. In all of this, there is one crucial requirement: all views and graphs have to be eﬃciently computable and suitable for large-scale software consisting of millions of lines of code [20]. Further usability enhancements were the integration of a preprocessor into Astr´ee (the original version read preprocessed C code), automated preprocessor  
  18  
  Certification for Testing Model Safety  
  DNN certiﬁcation can be seen as an instance of program veriﬁcation (DNNs can be written as programs) making it undecidable. State-of-the-art certiﬁers are therefore incomplete in general. These certiﬁers can be formulated using the elegant framework of abstract interpretation [15]. While abstract interpretationbased certiﬁers can certify both local and global properties, for the remainder of this paper, we focus on the certiﬁcation of local properties as they are more common in real-world applications. Figure 1 shows the high-level idea behind DNN certiﬁcation with abstract interpretation. Here, the certiﬁer is parameterized by the choice of an abstract domain. The certiﬁer ﬁrst computes an abstract element α(φ) ⊇ φ that includes the input region φ. Next, the analyzer symbolically propagates α(φ) through the diﬀerent layers of the network. At each layer, the analyzer computes an abstract element (in blue) overapproximating the exact layer output (in white) corresponding to φ. The element is computed by applying an abstract transformer that approximates the eﬀect of the operations (e.g., ReLU, aﬃne) applied at the layer. Propagation through all the layers yields an abstract element g(α(φ)) ⊇ f (φ) at the output layer. Next, the certiﬁer checks  
  30  
  Uppsala University, Uppsala, Sweden [email protected]   
  Abstract. Modular static program analyses improve over global wholeprogram analyses in terms of scalability at a tradeoﬀ with analysis accuracy. This tradeoﬀ has to-date not been explored in the context of sound ﬂoating-point roundoﬀ error analyses; available analyses computing guaranteed absolute error bounds eﬀectively consider only monolithic straight-line code. This paper extends the roundoﬀ error analysis based on symbolic Taylor error expressions to non-recursive procedural ﬂoating-point programs. Our analysis achieves modularity and at the same time reasonable accuracy by automatically computing abstract procedure summaries that are a function of the input parameters. We show how to eﬀectively use ﬁrst-order Taylor approximations to compute precise procedure summaries, and how to integrate these to obtain endto-end roundoﬀ error bounds. Our evaluation shows that compared to an inlining of procedure calls, our modular analysis is signiﬁcantly faster, while nonetheless mostly computing relatively tight error bounds. Keywords: modular veriﬁcation error · Taylor approximation  
  1  
  R. Abbasi and E. Darulova  
  or in the context of procedural code, by abstracting procedure calls by summaries or speciﬁcations to obtain a modular analysis [8]. A modular analysis allows each procedure to be analyzed independently once, regardless of how often it is being called in an application, rather than being reanalyzed in possibly only slightly diﬀerent contexts at every call site. This saves analysis time and thus increases the scalability of the analysis at the expense of some loss of accuracy: the procedure summaries need to abstract over diﬀerent calling contexts. This paper presents a modular roundoﬀ error analysis for non-recursive procedural ﬂoating-point programs without conditional branches. Our approach extends the roundoﬀ error analysis ﬁrst introduced in the FPTaylor tool [27] that is based on symbolic Taylor expressions and global optimization and that has been shown to produce tight error bounds for straight-line arithmetic expressions. Our analysis ﬁrst computes, for each procedure separately, error speciﬁcations that provide an abstraction of the function’s behavior as a function of the input parameters. In a second step, our analysis instantiates the error speciﬁcations at the call sites to compute an overall roundoﬀ error bound for each procedure. The main challenge is to achieve a practically useful tradeoﬀ between analysis accuracy and performance. A naive, albeit simple, approach would simply compute the worst-case roundoﬀ error for each procedure as a constant, and would use this constant as the error at each call site. This approach is, however, particularly suboptimal for a roundoﬀ error analysis, because roundoﬀ errors depend on the magnitude of arguments. For reasonable analysis accuracy, it is thus crucial that the error speciﬁcations are parametric in the procedure’s input parameters. At the same time, the error speciﬁcations need to introduce some abstraction as we otherwise end up re-analyzing each procedure at each call site. We achieve this balance by computing error speciﬁcations that soundly overapproximate roundoﬀ errors using ﬁrst-order Taylor approximations separately for propagation of input and roundoﬀ errors. By keeping ﬁrst-order terms of both approximations unevaluated, we obtain parametric procedure summaries, and by eagerly evaluating higher-order terms we achieve abstraction that has a relatively small impact on accuracy. Available sound ﬂoating-point roundoﬀ error analyses have largely focused on abstractions for the (global) analysis of straight-line code and require function calls to be inlined manually [10,18,24,27] and are thus non-modular. The tool PRECiSA [26,28] analyzes function calls compositionally, however, does not apply abstraction when doing so. The analysis can thus be considered modular (in principle), but the computed symbolic function summaries can be very large and negatively aﬀect the eﬃciency of the analysis. Goubault et al. [19] present a modular roundoﬀ error analysis based on the zonotopic abstract domain that does apply abstraction at function calls. However, the implementation is not available and the roundoﬀ error analyses based on zonotopes have been shown to be less accurate than the alternative approach based on symbolic Taylor expressions.  
  Modular Floating-Point Error Analysis  
  (2)  
  where f (x) denotes an idealized (purely) numerical program, where x is a possibly multivariate input, and f˜(˜ x) represents the function corresponding to the ﬂoating-point implementation, which has the same syntax tree but with opera˜ , is a tions interpreted in ﬂoating-point arithmetic. Note that the input to f˜, x rounded version of the real-valued input since that may not be exactly representable in ﬁnite precision and may need to be rounded. We want to maximize the above equation for a set of meaningful inputs I that depends on a particular application. Bounding roundoﬀ errors for unbounded input ranges is not practically useful as the error bounds are then in general unbounded. In this paper, we consider programs that consist of several procedures and the goal is to compute an error bound for each of them. The procedure bodies consists of arithmetic expressions, mathematical library function calls, (immutable) variable declarations and (possibly) calls to procedures deﬁned within the program. To estimate the rounding error for such programs with existing roundoﬀ error analyses, the procedure calls need to be eﬀectively inlined—either manually by a user before an analysis tool is run, or automatically by the tool [26]. For larger programs, especially with more procedure calls, this can result in very large (symbolic) expressions and thus long analysis times. This approach is also fundamentally not suitable for integration into modular veriﬁcation frameworks, such as KeY [3] or Frama-C [23]. For our modular analysis, the procedure calls do not need to be inlined. Instead, for each procedure of the program, our analysis ﬁrst computes an error speciﬁcation that is a function of the input parameters and that abstracts some of the error computation. Our analysis instantiates these error speciﬁcations at the call sites to compute an overall roundoﬀ error bound (it also checks that the preconditions of the called procedures are respected).  
  Modular Floating-Point Error Analysis  
  Modular Roundoﬀ Error Analysis  
  In principle, one can apply FPTaylor’s approach (Equation FPTaylor Error) directly to programs with procedure calls by inlining them to obtain a single arithmetic expression. This approach, however, results in potentially many reevaluations of the same or very similar expressions. In this section, we extend FPTaylor’s approach to a modular analysis by considering procedure calls explicitly. At a high-level, our modular error computation is composed of two stages: 1. The abstraction stage computes an error speciﬁcation for each procedure of the input program (Sect. 3.1 and Sect. 3.2); 2. The instantiation stage instantiates the pre-computed error speciﬁcations for each procedure at their call-sites with their appropriate contexts. Note that each procedure is processed only once in each of these stages, regardless of how often it is called in other procedures. The main challenge is to compute the error speciﬁcations such that they, on one hand, abstract enough over the individual arithmetic operations to provide a beneﬁt for the analysis in terms of performance, and on the other hand do not lose too much accuracy during this abstraction to still provide meaningful results. A naive way to achieve modularity is to compute, for each procedure, a roundoﬀ error bound as a constant value, and use that in the analysis of the procedure calls. This simple approach is, however, not enough, since in order to analyze a calling procedure, we do not only need to know which new error it contributes, but we also need to bound its eﬀect on already existing errors, i.e. how it propagates them. The situation is even further complicated in the presence of nested procedure calls. Alternatively, one can attempt to pre-compute only the derivatives from Equation FPTaylor Error and leave all evaluation to the call sites. This approach then eﬀectively amounts to caching of the derivative computations, and does not aﬀect the analysis accuracy, but its performance beneﬁt will be modest as much of the computation eﬀort will still be repeated. Our approach rests on two observations from the above discussion. We ﬁrst split the error of a procedure into the propagation of input errors and roundoﬀ errors due to arithmetic operations, following [11]: x) − f˜(˜ x)| |f (x) − f˜(˜ x)| = |f (x) − f (˜ x) + f (˜ x) − f˜(˜ x)| ≤ |f (x) − f (˜ x)| + |f (˜       propagation error  
  round-oﬀ error  
  R. Abbasi and E. Darulova  
  Partial Evaluation. Besides abstraction that happens in the presence of nested function calls due to considering only ﬁrst-order Taylor expansions and no higher-order terms, we abstract further by evaluating those error terms in (Roundoﬀ Speciﬁcation) that tend to be small already at the abstraction phase. Speciﬁcally, we evaluate: – the ﬁrst-order derivatives w.r.t. absolute errors for subnormals, i.e. di s, – the remainder terms that do not contain any β terms themselves. For this evaluation, we use the input speciﬁcation of the procedure call. By doing so, we skip the re-instantiation of these small term at the call sites and overapproximate the (small) error of these terms. Since these terms are mostly of higher-order (especially the remainder terms) over-approximating them improves the analysis performance while having small impact on the analysis accuracy. 3.2  
  Propagation Error Abstraction  
  That is, we compute and add the propagation error of the called procedures by computing the derivatives of the calling procedure w.r.t the called procedures and multiplying such terms by their respective γ function, which is the propagation error of the called procedure. The remainder terms w.r.t called procedures are computed similarly. Correctness. Just as with the roundoﬀ speciﬁcations, if we were to replace the γgi s by their corresponding formulas in γf , we would reach the same propagation error speciﬁcation as if we had computed it with Eq. 7 for a program with all procedures inlined. Again, higher-order Taylor expansion terms may be needed for an equivalence. Running Example. To see this, lets consider our (Running Example). Suppose ux , uy , and uz are the initial errors for procedures g and f respectively. The propagation speciﬁcations for g and f are computed as follows: ∂g ∂2g 2 ux + 1/2( u ) = 2xux + u2x ∂x ∂x x ∂f ∂f γg (y) + γg (z) = γg (y) + γg (z) = 2xuy + 2yuz + u2y + u2z γf = ∂g(y) ∂g(z) γg =  
  Note that replacing the γg functions with their equivalent Taylor expansion ∂f ∂f in γf and applying the chain rule (e.g., ∂g(y) × ∂g(y) ∂y = ∂y ), would result in the Taylor expansion of the inlined version of f (˜ x). Partial Evaluation. While computing the propagation speciﬁcation γ, we evaluate the small error terms of the error speciﬁcation and add them as constant error terms to the error speciﬁcation. These small terms are the remainder terms that do not contain any γ terms themselves. Doing so, we skip the re-evaluation of these small terms at the call sites and therefore, speed-up the analysis. 3.3  
  R. Abbasi and E. Darulova  
  not ﬁnd it feasible to extend an existing implementation of the symbolic Taylor expression-based approach in FPTaylor [27] (or another tool) to support procedure calls. We thus opted to re-implement the straight-line code analysis inside the Daisy analysis framework [10] which supports function calls at least in the frontend. We implement our modular approach on top of it and call it Hugo in our evaluation. Our implementation does not include all of the performance or accuracy optimizations that FPTaylor includes. Speciﬁcally, it is not our goal to beat existing optimized tools in terms of result accuracy. Rather, our aim is to evaluate the feasibility of a modular roundoﬀ error analysis. We expect that most, if not all, of FPTaylor’s optimizations (e.g. detecting constants that can be exactly represented in binary and thus do not incur any roundoﬀ error) to be equally beneﬁcial to Hugo. Nevertheless, our evaluation suggests that our re-implementation is reasonable. Hugo takes as input a (single) input ﬁle that includes all of the procedures. Integrating Hugo into a larger veriﬁcation framework such as KeY [3] or FramaC [23] is out of scope of this paper. In Hugo, we use intervals with arbitrary-precision outer bounds (with outwards rounding) using the GNU MPFR library [15] to represent all computed values, ensuring a sound as well as an eﬃcient implementation. Hugo supports three diﬀerent procedures to bound the ﬁrst-order error terms in equations Roundoﬀ Speciﬁcation and Propagation Speciﬁcation: standard interval arithmetic, our own implementation of the branch-and-bound algorithm or Gelpia [4], the branch-and-bound solver that FPTaylor uses. However, we have had diﬃculties to obtain reliable (timely) results from Gelpia. Higher-order terms are evaluated using interval arithmetic.  
  5  
  framework [2]. We adapted the originally object-oriented ﬂoating-point Java programs to be purely procedural. We also added additional procedures and procedure calls to reﬂect a more realistic setting with more code reuse where a modular analysis would be expected to be beneﬁcial. Note that the standard ﬂoating-point benchmark set FPBench [9] is not suitable for our evaluation as it consists of only individual procedures. matrix. The matrix case study contains library procedures on 3 × 3 matrices, namely for computing the matrix’ determinant and for using this determinant to solve a system of three linear equations with three variables, using Cramer’s Rule. Finally, we deﬁne a procedure (solveEquationsVector) that solves three systems of equations and computes the average of the returned values, representative of application code that uses the results of the systems of equations. See Listing 1.2 in the Appendix for the (partial) matrix code.  
  The complex case study contains library procedures on complex numbers such as division, reciprocal and radius, as well as procedures that use complex numbers for computing properties of RL circuits. For example, the radius procedure uses Pythagoras’ theorem to compute the distance to the origin of a point represented by a complex number in the complex plane. The computeRadiusVector demonstrates how the radius library procedure may be called to compute the radius of a vector of complex numbers. The approxEnergy procedure approximates the energy consumption of an RL circuit in 5 time steps. Listing 1.1 shows partial code of our complex case study. The procedure _add is a helper procedure that implements an arithmetic expression (and not just a single variable) that is used as argument of a called procedure; see Sect. 3.4 for how our method modularly incorporates the roundoﬀ and propagation errors resulting from such an expression. For now this refactoring is done manually, but this process can be straight-forwardly automated. Table 1 gives an overview of the complexity of our case studies in terms of the number of procedures and procedure calls, as well as the number of arithmetic operations in both the inlined and the procedural (original) versions of complex.  
  (ii) to limit the number of basis states representing the quantum state of an entanglement group by a chosen constant. Note that the number of basis states allowed in the quantum state of one entanglement group corresponds to the number of amplitudes required to be stored; all other amplitudes are assumed to be zero. Reaching Maximum Complexity. The careful reader may ask how to proceed when the maximum number of allowed basis states is reached. We set the state of the entanglement group of which the limit is exceeded to , meaning that we no longer track any information about this group of qubits. By doing so, we can continue simulating the remaining entanglement groups until they may also end up in the . For this, we utilize a ﬂat lattice that consists of either an element representing a concrete quantum state of an entanglement group, , or ⊥ (not used) satisfying the partial order in Fig. 4. The following deﬁnitions establish the relation between the concrete quantum states and their abstract description.  
  Fig. 4. Lattice for the abstract description of quantum states.  
  Control Reduction  
  Classically Determined Qubits. The central task of quantum constant propagation is to remove superﬂuous controls from controlled gates. First, we identify and remove all classically determined qubits, i. e. those that are either in |0 or |1. If we ﬁnd a qubit always in |0, the controlled gate can be removed. If we ﬁnd qubits always in |1, those controls can be removed since they are always satisﬁed. Satisﬁable Combination. By ﬁltering out classically determined qubits as described above, a set of qubits may remain in some superposition. Even then, for the target operation to be applied, there must be a basis-state where each of the controls is satisﬁed, i. e., each is in |1. If no such combination exists, the gate can be removed entirely.  
  Quantum Constant Propagation  
  Theorem 4. QCP runs in O(m · k 2 · n). Proof. Lemma 1 and Lemma 2 show together, that processing one gate takes O(k 2 · n + n) = O(k 2 · n) time. With m the number of gates present in the input circuit, this gives us the claimed result.  
  In particular, this shows that the entire QCP runs in polynomial time which we consider important for an eﬃcient optimization. This is due to the restriction of the number of states in each entanglement group since this number could otherwise grow exponentially in the number of qubits, i. e., would be in O(2n ).  
  Quantum Constant Propagation  
  Evaluation  
  The QCP, we propose, only applies control reduction and gate cancellation because of unsatisﬁable controls. This may facilitate the elimination of duplicate gates or rotation folding afterward— optimizations which we leave for existing tools capable of this task. In more detail, with the evaluation presented here, we pursue three objectives: (i) Measure the eﬀectiveness of QCP in terms of its ability to facilitate widely used quantum circuit optimizers. (ii) Show that QCP extends existing optimizations that also use the idea of constant propagation, namely the Relaxed Peephole Optimization (RPO) [16]. (iii) Demonstrate the eﬃciency (polynomial running time) of QCP even when processing circuits of large scale. In the following, we describe the experiments performed to validate our objectives, and afterward, we show and interpret their results. The corresponding artifact [4] provides the means to reproduce the results reported here. 6.1  
  Experiments  
  Fig. 10. Those two plots show the reduction of gates and controls, respectively, when applying QCP with diﬀerent values for nmax (x-axis) after RPO and ﬁnally Qiskit.  
  such a pattern, this optimization can not be applied at this position anymore. However, for larger values for nmax , those plots show that QCP ﬁnds additional optimization potential and is therefore not subsumed by RPO. When looking at Fig. 10, one can see that RPO even beneﬁts QCP: In this setting, approximately 10 times more gates can be removed compared to only using QCP with Qiskit afterward. These remarkable results are mainly due to two circuit families, namely qpeexact and qpeinexact, where RPO removes some controlled gates with their technique in the ﬁrst place and facilitates that QCP can remove even more controlled gates. Analysis of QCP Alone. QCP only fails on six circuits, of which one is a timeout, and ﬁve produce an error because of an unsupported gate. QCP needs the most time on the grover and qwalk circuits; on all other circuits, it ﬁnishes processing after at most 3.6 s. In general, the running time of QCP is high if it must track high entanglement for many gates. Accordingly, Fig. 12 shows the running time of QCP on the circuits that belong to the family of Quantum Fourier Transform.  
  Quantum Constant Propagation  
  Y. Chen and Y. Stade  
  Fig. 12. This plot shows the running time of QCP for diﬀerent values of nmax against the number of qubits (x-axis). The outliers occur due to the structure in which the circuits are generated; more details can be found in the text.  
  of this paragraph on the circuit shown in Fig. 1, none of those could reduce the circuit to the equivalent empty circuit. Bitwise Simulation. As already mentioned in Sect. 3.2, the idea to use a hash table to store the quantum state goes back to a simulator developed by Da Rosa et al. [7]. They use a hash table in the same way as we described in Sect. 3.2 with the basis states as keys and their associated amplitudes as values. However, our approach improves upon theirs by keeping qubits separated as long as they are not entangled following the idea in [3] and, hence, be able to store some quantum states even more eﬃciently. In contrast, Da Rosa et al. use one single hash table for the entire quantum state. Since they want to simulate the circuit and not optimize it as we aim for, they do not change to  if the computed quantum state becomes too complex. Consequently, their simulation runs still in exponential time even though it is not exponential in the number of qubits but rather in the degree of entanglement [7]. Initial-State Optimizations. Circuit optimization tools developed by Liu et al. [16] and Jang et al. [13] both take advantage of the initial state. Liu et al. leverage the information on the single-qubit state which could be eﬃciently determined at compile time [16]. They implement state automata to keep track of the single-qubit information on each pure state for circuit simpliﬁcations. Singlequbit information is lost though when a multi-qubit gate is applied except for a few special cases since a pure state could then turn into a mixed state. To tackle  
  statements relevant for a given error that describe the essence of why the error occurred. In particular, we describe a fault localization technique based on socalled error invariants inferred via abstract interpretation. An error invariant for a given location in a program captures states that may produce the error, that is, there may be executions of the program continued from that location violating a given assertion. We observe that the same error invariants that hold for consecutive locations characterize statements in the program that are irrelevant for the error. A statement that is enclosed by the same error invariant does not change the nature of the error. Hence, error invariants can be used to ﬁnd only relevant statements and information about reachable states that helps to explain the cause of an error. They also identify the relevant variables whose values should be tracked when executing the program. The obtained relevant statements constitute the so-called statement-wise semantic slicing of the error program, which can be changed (repaired) to make the entire program correct. Abstract interpretation [7,29] is a general theory for approximating the semantics of programs. It has been successfully applied to deriving computable and approximated static analysis that infer dynamic properties of programs, due to its soundness guarantee (all conﬁrmative answers are indeed correct) and scalability (with a good trade-oﬀ between precision and cost). In this paper, we focus on applying abstract interpretation to automate fault localization via inferring error invariants. More speciﬁcally, we use a combination of backward and forward reﬁning analyses based on abstract interpretation to infer error invariants from an error program. Each next iteration of backward-forward analyses produces more reﬁned error invariants than the previous iteration. Finally, the error invariants found in the last iteration are used to compute a slice of the error program that contains only relevant statements for the error. The backward (over-approximating) numerical analysis is used for computing the necessary preconditions of violating the target assertion, thus reducing the input space that needs further exploration. Error invariants are constructed by going backwards step-by-step starting at the property violation, i.e. by propagating the negated assertion backwards. The negated assertion represents an error state space. When there is a precision loss caused by merging the branches of an if statement, we collect in a set of predicates the branching condition of that conditional. Subsequently, the forward (over-approximating) numerical analysis of a program with reduced abstract sub-input is employed to reﬁne error invariants in all locations, thus also reﬁning (reducing) the error state space. Based on the inferred error invariants, we can ﬁnd the relevant statements and relevant variables for the assertion violation. Initially, in the ﬁrst iteration, both analyses are performed on a base abstract domain (e.g., intervals, octagons, polyhedra). In the subsequent iterations, we use the set of predicates generated by the previous backward analysis to design a binary decision diagram (BDD) abstract domain functor, which can express disjunctive properties with respect to the given set of predicates. A decision node in a BDD abstract element stores a predicate, and each leaf node stores an abstract value from a base abstract domain under speciﬁc evaluation results of predicates. When the obtained set of predicates as  
  A. S. Dimovski  
  well as the (abstract) error state sub-space stay the same over two iterations, the iterative process stops and reports the inferred error invariants. Otherwise, the reﬁnement process continues by performing backward-forward analyses on a BDD abstract domain based on the reﬁned set of predicates as well as on a reduced error state sub-space. The BDD abstract domain and the reduced error state sub-space enable our analyses in the subsequent iterations to focus on smaller state sub-spaces, i.e. partitionings of the total state space, in each of which the program may involve fewer disjunctive or non-linear behaviours and thus the analyses may produce more precise results. We have implemented our abstract interpretation-based approach for fault localization of C programs in a prototype tool. It takes as input a C program with an assertion, and returns a set of statements whose replacement can eliminate the error. The tool uses the numerical abstract domains (e.g., intervals, octagons, polyhedra) from the APRON library [25], and the BDD abstract domains from the BDDAPRON library [24]. BDDAPRON uses any abstract domain from the APRON library for the leaf nodes. The tool also calls the Z3 SMT solver [30] to compute the error invariants from the information inferred via abstract interpretation-based analyses. We discuss a set of C programs from the literature, SV-COMP and TCAS suites that demonstrate the usefulness of our tool. In summary, this work makes the following contributions: (1) We deﬁne error invariants as abstract representations of the reason why the program may go wrong if it is continued from that location; (2) We propose an iterative abstract interpretation-based analyses to compute error invariants of a program. They are used to identify statements and program variables that are relevant for the fault in the program; (3) We implemented the approach in a prototype tool, which uses domains from APRON and BDDAPRON libraries as well as Z3 SMT solver; (4) We evaluate our approach for fault localization on a set of C benchmarks.  
  2  
  2  
  This condition guarantees that the assertion is valid for any non-deterministic choice [2, 3]. If we have used an over-approximating backward analysis, it would infer the necessary condition (z+3>y) that may lead to the assertion satisfaction for some nondeterministic choices of [2, 3] (e.g., the execution where the non-deterministic choice [2, 3] returns 3). A solution is simplest if it contains the fewest number of variables.  
  Generalized Program Sketching  
