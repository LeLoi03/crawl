  DOWNLOAD FILE   
 Author / Uploaded 
  Med Salim Bouhlel 
  Stefano Rovetta 
  Med Salim Bouhlel Stefano Rovetta Editors  
  Proceedings of the 8th International Conference on Sciences of Electronics, Technologies of Information and Telecommunications (SETIT’18), Vol.1  
  Editors  
  Proceedings of the 8th International Conference on Sciences of Electronics, Technologies of Information and Telecommunications (SETIT’18), Vol.1  
  Editors Med Salim Bouhlel SETIT Lab University of Sfax Sfax, Tunisia  
  Stefano Rovetta DIBRIS - University of Genoa Genoa, Genova, Italy  
  Information Processing  
  Meaning Negotiation Dalila Djoher Graba(B) , Nabil Keskes, and Djamel Amar Bensaber LabRi-SBA Lab., Ecole Sup´erieure en Informatique (ESI-SBA), Sidi Bel Abbes, Algeria {d.graba,n.keskes,d.amarbensaber}@esi-sba.dz Abstract. Nowadays, the web is becoming highly used technology, in our community. This technology allows us to work in collaboration and to share knowledge. The pragmatic web represents the most recent extension of the web (semantic web); which facilitates the exploitation and the interpretation of the data by the machine. This web is based on three important components, the context, the community, and the meaning negotiation. The Meaning negotiation is the most important component of the pragmatic web on which we will ﬁx our attention. It plays an important role in the exchanges and resolves conﬂicts in people cooperation activities. The knowledge (context) of each part in the community of users is heterogeneous; this will make the meaning negotiation complicated. This paper realizes a meaning negotiation scenario based on the ontologies merging into the geopolitical domain. This will reduce and simpliﬁes the process, and improves the semantics of data. Keywords: Meaning negotiation · Ontology Contextual ontology · Domain ontology  
  1  
  D. D. Graba et al.  
  However, there are diﬀerent ways to represent knowledge (logic, ontology, etc.), hence the problem of heterogeneity. So, in order to have a powerful meaning negotiation, it is necessary to present the knowledge in a clear and unambiguous way. The model in [1] is a basic example of meaning negotiation, but the realization of this process is very complex. This model was adopted by the authors [2] where they merge the contextual ontology with the domain ontology in a case study, to improve meaning negotiation and to simplify the process of [1]. For this, we cannot say that the hypothesis of [2] is valid for other domain ontologies. Our work is to realize a meaning negotiation scenario based on the model of [1], because of the advantages that we can draw from it (the improvement of the collaboration between the individuals in the context, the improvement of the semantics, etc.). The idea is to use the contribution in [2], in order to simplify and reduce the scenario of [1] in the geopolitical domain. The next four sections are organized as follows. Section 2 presents some deﬁnition of basic terms. Section 3 presents related work in the domain and synthesizes them with a comparative table. Finally, Sect. 4 brieﬂy introduces our work and gives an example. The last section summarizes the paper and outlining directions for future research.  
  2 2.1  
  Synthesis  
  The diﬀerent approaches of the meaning negotiation in the pragmatic web are classiﬁed and compared in Table 1. We compare the diﬀerent approaches by using ﬁve criteria (model of context, negotiation class, technical, Strengths, Weakness). The ontological representation is more widely used than the logic one that will allow us to use the ontological representation to deﬁne our own way. It also summarizes the diﬀerent technical and algorithm used, such as the self-organizing map that is based on the neuron network, similarity measurements, multi-agent system, etc. The class of negotiation argument basis is compatible with the logical contextual model. This paper is in the line of diﬀerent works in this ﬁeld, more particularly articulates on the work of [1,2] which represents the basic model of the meaning negotiation in order to minimize the problems mentioned previously in this document.  
  8  
  Conclusion  
  This paper studies and compares the approaches in the ﬁeld of meaning negotiation; it classiﬁes them according to their contextual representation. It collects the diﬀerent deﬁnitions of the basic concepts existing in the ﬁeld and proposes some new deﬁnitions. It also brieﬂy introduces our work and gives an example in geopolitics domain. Furthermore, in the future our contribution will be extended by the use of 30 domain ontologies. We will try to improve and validate the ontologies merging (domain with contextual ontologies) and generalize the model [2] for any domain. For this, a benchmark of 30 semantic ontologies will be used. To validate this proposition, we will use the static test KolmogorovSmirnov. The Internet of Things (IoT) is a new paradigm that provides multiple services between objects. These smart objects are interconnected in the internet network in a simple and transparent way [21]. It will be interesting to use our approach in the IoT domain.  
  17  
  claim service web application using springboot microservice architecture that was deployed in a Cloud environment. We notice that the overwhelming of claims management literature is based on data mining approaches, while only few works investigate practical perspectives of the decision-making process. It is also noteworthy that this trend is mostly due to the fact that most claims management studies were conducted in the ﬁeld of marketing, communication and/or legal sciences. Thus, the shortage of specialized tools in complaints management presents a tremendous limitation for its applicability. Today, professionals declare an obvious and urgent need for the automation of claims process starting from their collection until the decisions related to their responses, especially under the new General Regulations on Data Protection. Actually, there are some tools that support the entire or a speciﬁc part of the claims management process, including commercial, open source and free software, usually called Customer Relationship Management (CRM) tools. We mention Microsoft Dynamics CRM and the enterprise resource planning SAP among the most popular CRM software (e.g. [10]). Despite their performance and robustness, some professionals still express their interest in leading improvements to increase their flexibility and ease of use. Moreover, these commercial solutions require relatively high costs while monitoring of speciﬁc types of customer claims that are not necessarily aligned with the GDPR, namely the right of access, the right of opposition, and the right of portability. Let’s remind here that the inventory management, planning and resource management modules in the mentioned solutions could useless compared to the need of some companies. More precisely, some pro-software provide a complete solution of business management while customer complaint management is only one module that should be restructured to ensure its consistent with the GDPR context.  
  4 Modeling In order to comply with GDPR standards and to manage customer claims regarding this new regulation, we propose the following six phases to model the GDPR claim management process: 1. Dissemination phase: This phase essentially consists of broadcasting to customers’ searchable information regarding the GDPR content. Each customer should be informed on the privileges provided by this new law about its personal data protection. In terms of claim management, this step has a basic form regarding customer experience because it is simply informative. But in the GDPR context, this phase embodies an essential value within the new law, the different rights and the new topic. 2. Collection Phase: Within this phase, the received complaints are collected. The collection frequency and the collected claims amount would be justiﬁed by the Data Protection Ofﬁcer (DPO). 3. Treatment phase: During this phase, the DPO needs the proposed framework to input the necessary data for each submitted claims. 4. Transfer phase: This phase is dedicated to sending technical reports according to the appropriate customer rights, as well as litigation to working group managers to help them making the right decisions.  
  S. Bouslama et al.  
  connection. Thus, a tomcat server may be conveniently included and executed. It is worthy to mention that Springboot also supports the convenience of conﬁguration dependency. Moreover, the front-end development is made using typescript programming language and the object relational mapping software Hibernate. 5.4  
  System Architecture  
  25  
  This paper is organized around six sections. The ﬁrst section is devoted to the presentation of existing complex disfluencies processing approaches. In the second section, we illustrate the method that we propose for the Arabic complex disfluencies processing. An illustrative simulation example of our method is proposed in the third section. In the fourth section, we present the CDPM evaluation results. In the ﬁfth section we present a comparison between the initial version of the SARF system and that improved by the integration of our CDPM. The sixth section is reserved for the conclusion and for the perspectives of this work.  
  2 Related Works In this section, we propose to dwell on the methods of the complex disfluencies processing. To address them, we have chosen to classify them into three approaches that we present as follows: the symbolic approach, the numerical learning-based approach and the hybrid approach. 2.1  
  The failure cases of our module are mainly due to the errors of the semantic labeling of the conceptual segments of the utterance. Indeed, a labeling of erroneous conceptual segments can cause an error in the detection and the delimitation of disfluent segments. In this case, the CDPM will not be able to detect the disfluencies. Consider the following example: twns AlY SfAqs mn EAdY qTAr sryE qTAr sfr vmn mA hw What is the price of the trip by fast train normal train from Sfax to Tunis  
  In the utterance above, the segment “ “Train_Type_CS” while the segment “  
  This paper presents an approach based on cooperative answers and a proximity measure for improving fuzzy querying in order to deal with empty answer problem. We attempt to enhance workload-based approaches by the proposition of approach more efﬁcient in terms of performance for detecting the closest query to failing fuzzy query with decrease both response time and memory space. The main idea of this work is to focus only on query conditions include in “where” and “having” clauses rather than on the full query. Our approach is based on efﬁcient proximity measure allows us to identify the closest queries by measuring the Hausdorff distance between queries. Thus, we propose four gradual operators based on usual ones between predicates for building and handling fuzzy queries in traditional SQL language so that it can support such imprecise expressions. Finally, we assign the closest query’s answers to the one that failed as cooperative answers having the same importance degree, which is deﬁned by the greatest proximity value, obtained between the failing query and the closest one. Following this introduction, the next section illustrates our measuring proximity between fuzzy queries. Section 3 shows how proximity can be used for improving fuzzy querying in order to deal with the empty answer problem. In Sect. 4, we summarize the experimental results with different analyses. The ﬁnal section draws conclusions and suggests further research.  
  2 Proximity Measure Between Fuzzy Queries According to the complexity of the fuzzy query, we have distinguished two types of queries: atomic queries containing one fuzzy predicate and compounded queries which are composed of many fuzzy predicates related between them by conjunction or disjunction operators. In this topic and according to the related work, we take the following deﬁnitions: “The notation P is used to denote the fuzzy predicate based on fuzzy logic of an attribute A and which is represented by a membership function denoted by lP whose values are from the interval [0, 1]” [26]. “Let be lP a membership function of the fuzzy predicate P, we denote by lP(x), the degree of truth that x is an element of the attribute A” [27].  
  The algorithm presented above explains how to calculate the proximity measure between two conjunctive queries Q and Qʹ. This measure is based on the use of Hausdorff distance for computing the proximity between each pair of predicates compounded these queries (see loop bloc). The ﬁnal proximity is the minimum of proximity measures between predicates. In the case of disjunctive fuzzy queries, we apply the same process instead of calculating the smallest proximity value in proximity measures between fuzzy predicates; we select the greatest one.  
  3 Improving Fuzzy Querying In this paper, we extend our previous approach for dealing with empty answer problem in fuzzy queries by the proposition of gradual operators between predicates in order to express user’s need in traditional SQL. On the other hand, we associate to our approach a new method of fuzzy query preparation which attempts to detect fuzzy conditions in “where” and “having” clauses in order to reduce the memory space and simplify the measuring of proximity between queries. Firstly, it was necessary to remember our previous approach before presenting its extension. We denote by W(D) a workload of database D with Qʹ2W(D) is a previous query successfully executed by a system. Let |D(Q)| denotes the number of domains of the attributes speciﬁed in Q. We present our previous approach according to the following items [21]. 1. Partitioning the workload W(D) in three subsets according to the three cases previously cited. 2. We distinguish three cases: • If the ﬁrst subset does not empty, for element Qʹ, estimate the proximity Prox(Q, Qʹ) and ranking in descending order of queries, • Else if the second subset does not empty, then we apply ﬁrst point, • Else (the third subset does not empty), we apply ﬁrst point. 3. Choose Qapp the closest query to Q, and affect its answers a same degree of membership, which takes as its value, the proximity measure between Q and Qapp named Prox (Q, Qapp). In this paper, we propose four gradual operators used to simplify the expression and handle fuzzy queries. The main advantage of these operators is to facilitate the expression of fuzzy queries using traditional operators of SQL. In the following, we present our gradual operators: more or less equal, at most, at least, best. More or less equal operator is represented through three operators: greater (>), less ( predicate a or attribute < predicate + a or attribute = predicate. Thus, we know that the result of operator OR is the ‘union’ of all results of its operands. For example, the condition processor more or less equal speed is interpreted by processor > speed a or  
  City University of Science and Information Technology, Peshawar, KP, Pakistan [email protected]  , {m.ullah,rashid}@cusit.edu.pk 2 Bahria University, Islamabad, Pakistan [email protected]   
  Abstract. In the recent, the World Wide Web has become a platform for online news publications. Many sources started publishing digital versions of news articles online to vast users through a variety of devices, i.e. television channels, magazines, and newspapers. It is observed that the news articles available can be very huge and recommendation systems can help to recommend relevant news to the news readers by ﬁltering news articles based on some predeﬁned criteria or similarity measure, i.e. collaborative ﬁltering or content-based ﬁltering approach. The paper presents named entities based similarity measure for linking digital news stories published in various newspapers during the preservation process in a digital news stories archive to ensure future accessibility. The study compares the similarity of news articles based on human judgment with a similarity value computed automatically using the proposed technique. The results are generalized by deﬁning a threshold value based on multiple experimental results using diﬀerent datasets of diﬀerent size.  
  Keywords: News archiving Similarity measure  
  news stories published online vary from one newspaper to another, i.e. from one day to a month or even more. Though, a newspaper may be backed up and archived by the news publisher or national archives, it will be diﬃcult to access particular information published in various newspapers about the same new. The issues become more complicated if a story is to be tracked through an archive of many newspapers, which require diﬀerent access technologies. To facilitate the accessibility of news articles preserved from multiple sources, some mechanisms need to be adopted for linking the archived digital news articles. The Common Ratio Measure for Stories (CRMS) technique [12] is introduced to manipulate the terms appearing in the news articles to linking digital news stories during preservation process in the Digital News Stories Archive (DNSA) [10]. A news article contains many terms, i.e. noun (also known as named entity), verb, adverb, etc. The term Named Entity (NE) was introduced in 6th Message Understanding Conference (6th MUC) and mostly appeared concept in Natural Language Processing (NLP) applications [1,7]. NE plays important role in information management in many domains and Named Entity Recognition (NER) is the process to identify, extract and classify NE from textual contents. For example, NER is vital in diﬀerent areas like opinion mining, populating ontologies, semantic annotation, localizations, personalization, question & answering, classify news contents, help to design eﬃcient search algorithms, powering content recommendation, customer support, research articles customization, etc., and use for many other application domains [8,16,21]. Keeping in view the role and importance of NE in textual contents, it is decided to see, how vital are these NE appears in the news articles and how these NE can be utilized in linking digital news articles during preservation process in DNSA. In the article, a new NE based linking mechanism is introduced to link news stories in DNSA. The approach in empirically analyzed and the results are compared to get conclusive arguments.  
  2  
  Background  
  The news readers read about a happening or an issue from various sources in order to get a broader perspective and diverse viewpoints that help to better understand the world around, and some time to authenticate the information itself by comparing similar news from multiple news sources. This article is the continuation of Digital News Story Preservation (DNSP) framework studies [9,13]. In DNSP, an extraction tool Digital News Stories Extractor (DNSE) is developed for extracting news articles from multiple online news sources and to normalized news articles to a common format and created DNSA [10]. The DNSA has news articles from multiple sources, needs to create a mechanism that helps the reader to read a set of relevant news stories about an event or issue. The DNSA needs an eﬃcient mechanism to link the digital stories and recommend to the readers. The CRMS technique is introduced to facilitate the linking mechanism by manipulating terms appearing the news articles [12], but other terms like noun, verbs and adverbs may also play important role in similarity computation among news articles. As named entities play a very important  
  M. Khan et al.  
  role in information management and the identiﬁcation and extraction of NE from textual information is still a challenging task. Therefore, to ﬁnd out the utility of NE in linking news articles, is considered and manipulated in such a way that helps to compute the similarity between news articles. The NE is used for diﬀerent purposes in literature. For example, linking multiple versions of a news story into group using salience NE to reduce reader’s cognitive load [6], a rule-based NE recognizer in semantic retrieval architecture is used for Turkish news videos, priorly annotated with corresponding named entities in textual news transcripts [15], a news event tracking technique based NE is proposed for patterns like When, Where, What and Who with their relationship with news categories like news related to Economy, Politics, Entertainment and Sports, etc. [17], duplicate news detection using NE [22], semantic similarity of NE, which extracted from news body are combined with lexical similarity function introduced post-click recommendation “TULIP” for information retrieval systems [14], etc. There are some corpus management systems, i.e. Sketch Engine, Manatee, EXMARaLDA, etc., and some language speciﬁc corpus management system, e.g. Tatar corpus management system for Turkish language [18]. The NE may help to classify and indexing textual documents for many purposes in diﬀerent languages, e.g. Arabic [5] or more similar language Urdu, etc., and with other textual collection like newspaper collection exist around the world in many diﬀerent languages [11]. Here, in the articles we are proposing a similarity measure based on weighted named entities in the news articles for horizontal linkage of news during preservation and creation of DNSA.  
  3  
  O. El Mariouli and A. Abouabdellah  
  In the literature there are several researches that deal with the issue of selection of suppliers (SS), but most of this research does not include the three pillars of SD in the selection, they are focused on the environmental dimension and neglects the social dimension [1, 2], this research presented different methods and technique of SS [3–5]. In this paper we developed a new mathematical model that allows measuring the score of supplier’s sustainability to choose the best between them. This model is the result to merge the DEMATEL approach with fuzzy set theory.  
  ð9Þ  
  Final crips values:  
  The Initial Direct Relation Matrix is: 2  
  Software Effort Estimation Using an Optimal Trees Ensemble: An Empirical Comparative Study Abdelali Zakrani1(&), Ali Idri2, and Mustapha Hain1 1 ENSAM, Hassan II University, Casablanca, Morocco {abdelali.zakrani,mustapha.hain}@univh2c.ma 2 ENSIAS, Mohammed V University, Rabat, Morocco [email protected]   
  Abstract. Since information systems have become the heartbeat of many organizations, the investment in software is growing rapidly and consuming then a signiﬁcant portion of the company budget. In this context, both the software engineering practitioners and researchers are more interested than ever about accurately estimating the effort and the quality of software product under development. Accurate estimates are desirable but no technique has demonstrated to be successful at effectively and reliably estimating software development effort. In this paper, we propose the use of an optimal trees ensemble (OTE) to predict the software development effort. The ensemble employed is built by combining only the top ranked trees, one by one, from a set of random forests. Each included tree must decrease the unexplained variance of the ensemble for software development effort estimation (SDEE). The effectiveness of the OTE model is compared with other techniques such as regression trees, random forest, RBF neural networks, support vector regression and multiple linear regression in terms of the mean magnitude relative error (MMRE), MdMRE and Pred(l) obtained on ﬁve well known datasets namely: ISBSG R8, COCOMO, Tukutuku, Desharnais and Albrecht. According to the results obtained from the experiments, it is shown that the proposed ensemble of optimal trees outperformed almost all the other techniques. Also, OTE model outperformed statistically the other techniques at least in one dataset. Keywords: Software development effort estimation  Optimal trees ensemble  Random forest  Regression trees  Multiple linear regression  RBF neural networks  Support vector regression  Accuracy evaluation  
  1 Introduction In today’s software industry, companies are systematically and continuously seeking to strengthen their competitiveness in order to survive in a highly competitive environment. One of the main factors to achieve this goal is to allocate software project resources efﬁciently and schedule activities appropriately. In this respect, predicting software development effort is critical.  
  Numerous techniques have been considered for software effort estimation, including traditional techniques such use case point [1], and, recently, machine learning techniques such as MLP neural networks [2], radial basis function (RBF) neural networks [3], random forest (RF) [4], fuzzy analogy (FA) [5] and support vector regression (SVR) [6]. Machine learning methods employs data from historical projects to construct a regression model that is then used to estimate the effort of future software projects. However, no single method has been found to be entirely stable and reliable for all cases. Furthermore, the performance of any method depends generally on the characteristics of the dataset employed to construct the model (dataset size, outliers, categorical attributes and missing values). More recently, there is a trend to overcome the weakness of the single method by using bootstrap aggregating (bagging) methods [7]. These bagging paradigms try to construct multiple learners to improve the accuracy of models used in SDEE. Many investigation studies have been demonstrated the effectiveness of the ensemble techniques over the single technique. For instance, Elish in [8] used multiple additive regression trees (MART) to estimate software effort and compared their performance with that of linear regression, RBF and SVR models on NASA dataset. The MART model outperforms the others in terms of MMRE and Pred(0.25). Idri et al. [9] used two types of homogeneous ensembles based on single Classical Analogy or single Fuzzy Analogy. Their results obtained over seven datasets showed that classical and fuzzy analogy ensembles outperform single techniques in terms of the four performance measures. In this context, Zakrani et al. [10] adapted the method proposed by Khan et al. [11] for classiﬁcation and regression to software effort estimation. The underlying assumption in this method is that combining only the strong regression trees can leads to stronger ensemble and more stable model in SDEE. The results obtained by the ensemble of optimal trees showed a signiﬁcant improvement over classical tree model. The present work aims to examine further the performance of the previous model. The main contributions of the present paper are twofold: (1) to develop a new model for software effort estimation using an optimal trees ensemble; (2) to evaluate the effectiveness of the proposed model by comparing it with recent methods used and investigated in the literature for SDEE, namely, (i) regression trees (RT), (ii) random forest (RF), (iii) RBF neural networks, (iv) support vector regression, and (v) multiple linear regression (MLR). This paper is organized as follows. Section 2 presents the ﬁve other SDEE techniques evaluated and Sect. 3 gives an overview of the proposed tree ensemble. In Sect. 4, we present a brief description of the datasets, the accuracy measures, the validation method used in this empirical evaluation and also the adopted statistical tests. The experiments and results are discussed in Sect. 5. Finally, Sect. 6 concludes the paper.  
  2 SDEE Techniques This section introduces the SDEE techniques compared with the proposed technique in this study namely optimal trees ensemble (OTE).  
  Random forest (RF) is an ensemble learning method that aggregates a large number of decision trees, thus enabling it to reduce the variance obtained as opposed to that generated when using only one decision tree. The use of random forest in SDEE requires the determination of a set of parameters such as: the number of trees constituting the forest (ntree), the number of variables chosen randomly at the level of each node (mtry), the size of the sample ‘in bag’ (sampsize) and the maximum number of nodes of each tree (maxnodes). We used in the current study the best conﬁguration as that found in [4].  
  3 The Proposed Model: Optimal Trees Ensemble As the number of trees in random forest is often very large, there has been a signiﬁcant work done on the problem of minimizing this number not only to reduce computational cost, but also to improve the predictive performance [10]. Since the overall estimation error of a random forest is highly associated with the strength of individual trees and their diversity in the forest. In recent work [11], Khan et al. proposed a further reﬁnement of random forest by proposing a trees selection method on the basis of trees individual accuracy and diversity using the unexplained variance. The proposed ensemble is referred to as optimal trees ensemble (OTE). In this paper we investigate the use of OTE in SDEE. To this end, we partition the training data L = (X,Y) randomly into two non-overlapping subsets, LB = (XB,YB) and LV = (XV,YV). Next, we grow T regression trees on T bootstrap samples from the ﬁrst subset LB = (XB,YB). While doing so, select a random sample of p < d features from the entire set of d project attributes. This inculcates additional randomness in the trees. Owing to bootstrapping, there will be some observations left out of the samples which are called out-of-bag (OOB) observations. These latter take no part in the training of tree. They are used to estimate unexplained variances of each tree built on a bootstrap sample. Trees are then classiﬁed in ascending order with respect to their unexplained variances and the top ranked M trees are chosen. The selection and combination of trees are carried out as follows:  
  5 Results of the Experimental Studies Once the six models were trained using training sets, we compared the generalization capability of the proposed ensemble with the other ﬁve SDEE models using the testing sets. The evaluation was based on the MMRE, MdMRE and Pred(0.25) measures. The empirical results are shown in Figs. 1, 2, and 3. It can be seen from these three ﬁgures that the proposed ensemble of optimal trees performs better, in terms of MMRE, MdMRE and Pred(0.25), than the other ﬁve models in all datasets except RBFNN with Albrecht dataset. As shown in Fig. 1, the proposed model generates always a smaller MMRE compared to the other SDEE techniques. For the optimal trees’ ensemble, the lowest MMRE was obtained when using Albrecht whereas the highest MMRE was obtained when using ISBSG R8 dataset. These values of MMREs are not surprising since the Albrecht dataset is exhibiting the lowest non-normality while ISBSG has the highest non-normality according to Kurtosis coefﬁcient (see Table 1). From the chart in Fig. 2, we observed that all models made much lower values of MdMREs than MMREs especially for ISBSG, COCOMO and Tukutuku datasets. This is due to the fact that MMRE measure is extremely sensitive to individual predictions with excessively large MREs [19], which is, in turn, a result of the presence of outliers in these datasets (kurtosis > 20 as shown in Table 1). Looking at Fig. 3, it is apparent that the ensemble of optimal trees yields, in general, to the highest values of Pred(0.25). The performance achieved by the proposed model presents a notable increase of 13.5 on average over RT and RF based models. The best improvement was obtained when using Tukutuku dataset with +25% followed by Albrecht dataset with +14.25%.  
  Fig. 1. Comparison of MMRE values for the six SDEE models.  
  As it can be seen from Table 3: • For COCOMO dataset: the optimal trees ensemble statistically surpassed the other ﬁve SDEE methods. • For ISBSG R8 dataset: the OTE statistically outperformed RBFNN. Nevertheless, the difference of OTE performance compared with the RT, RF, SVR and MLR was not signiﬁcant. • For Tukutuku dataset: the OTE statistically outperformed RT and MLR. However, the difference of OTE performance compared RBFN, RF, SVR was not signiﬁcant. • For Desharnais dataset: the OTE signiﬁcantly outperformed SVR and RT. By the contrary, the difference of OTE performance compared with RBFN, RF and MLR was not signiﬁcant. • For Albrecht dataset: the p-value indicate that the difference OTE performance compared with the ﬁve SDEE methods is not signiﬁcant (p-value > 0.05).  
  6 Conclusion and Future Work In this paper, we have empirically investigated the use of a novel tree’s ensemble for software effort estimation. This ensemble is built by combining only the top ranked trees from each generated RF whose it inclusion decrease the unexplained variance of the ensemble for SDEE. Next, the proposed model was compared to the regression trees, random forest, RBF neural networks, MLR and SVR models using 30% holdout validation method over ﬁve datasets namely: COCOMO, ISBSG R8, Tukutuku, Desharnais and Albrecht. The accuracy measures employed were MMRE, MdMRE and Pred(0.25). The results indicated that the ensemble of optimal trees outperforms almost all the other techniques. Also, OTE model outperformed statistically the other techniques at least in one dataset. In the light of these empirical results, we can conclude that the ensemble of optimal trees is a promising technique for software development effort estimation. As future work, we are planning to replicate this study using new datasets and employing a leave-one-out validation method.  
  A. Derbel and Y. Boujelbene  
  3 Unsupervised Classiﬁcation: CAH Unsupervised classiﬁcation is a mathematical method of data analysis that facilitates grouping into multiple distributions. Individuals grouped within the same class (intraclass homogeneity) are subjected to a similar process, while heterogeneous classes have a dissimilar proﬁle (inter-class heterogeneity). Clusters can be considered as classes or groups of similar entities separated by other clusters with non-common features. In our case, we have classiﬁed the public transport operators in Tunisia, so that the operators must belong to one of the two classes generated by the classiﬁcation. We have a set of operators that we denote by X={x1, x2, …, xN} characterized by a set of descriptors (D). The objective of unsupervised classiﬁcation is to ﬁnd the groups K= {C1, C2, …, Ck} and verify which elementary operators (x) belong to each cluster. This means for determining a function noted by (Y) that associates each element of (X) with one or more elements of (C). There are several unsupervised classiﬁcation algorithms to give results on the problem of the data classiﬁcation. Subsequently, we present two categories of unsupervised classiﬁcations, ascending hierarchical classiﬁcation (CAH) and nonhierarchical classiﬁcation (K-means). We chose the (CAH) method for the following reasons. The (CAH) method offers a clear and simple approach to facilitate structuring of information and gives a high visibility in the area of multi-criteria analysis. The hierarchical classiﬁcation is based on three principles: The dendrogram: the (X) partitions made at each stage of the (CAH) algorithm can be visualized via a tree called a dendrogram. On one axis appears the individuals to be grouped and on the other axis are indicated the differences corresponding to the different levels of grouping, this is done graphically by means of branches and nodes. The dendrogram, or hierarchical tree, shows not only the links between classes, but also the height of the branches which indicates the level of proximity. Indeed, this technique is based on the measurement of a distance between clusters. And again, there is the choice, depending on the options selected and depending on the different methods of aggregation. The cut-off point of a dendrogram: it is the conﬁguration of the dendrogram, a predeﬁned number of clusters make it possible to trace a break at a certain level of aggregation. This method determines the number of classes retained for subsequent events. To select a partition of the population, simply cut the dendrogram is obtained at a certain height. An analysis of the shape of the dendrogram may give us an indication of the number of classes can be selected [2]. Estimation the number of clusters (k): from the CAH method, the number of classes is not necessarily known a priori. Different techniques exist, and one of the most common is based on information criteria such as BIC (Bayesian Information Criterion) or AIC (Aikake Information Criterion). Homogeneity (intra-class distance) and separation (inter-class distance) are the most common technique used for estimating the number of classes. The silhouette criterion is considered a relevant measure for assessing the quality of partitioning. We chose to use this criterion as a concrete measure aimed at ensuring better consideration of both the homogeneity and heterogeneity of classes. Let a(i) is the average of the dissimilarities (or distances) of the observation (i) with all the other observations within the same class. The more a(i) is small, the assignment of (i) is giving a better classiﬁcation. Let b(i) is the lowest  
  Automatic Classiﬁcation and Analysis of Multiple-Criteria  
  “Apprenticeship Works” data to perform the classiﬁcation. During the learning phase, the algorithm makes it possible to elaborate classiﬁcation rules on this data set that will be used for testing and prediction. Given a set of variables X = {x1, x2, …, xd}, we want to calculate the posterior probability of the event (Yj) among a set of possible Y = {c1, c2, …, cd}. In more common terminology, (X) represents the preachers and (Y) represents the variable to predict (the attribute that has K modalities). The Bayes rule is deﬁned as follows: PðY ¼ c=X ¼ xÞ /  
  PðX ¼ x=Y ¼ cÞPðY ¼ cÞ Likelihood  prior / PðX ¼ xÞ evidence  
  CEDRIC Laboratory, Conservatoire National des Arts et Métiers (CNAM), Paris, France [email protected]  , {metais,faycal.hamdi}@cnam.fr 2 MIRACL Laboratory, University of Sfax, Sfax, Tunisia [email protected]   
  Abstract. In the context of a prosthesis for Alzheimer’s patient, we want to show the family and entourage tree of the patient from their saved data structured based on the PersonLink ontology. The generated graph ought to be accessible and readable to this particular user. In our previous work, we proposed our ontology visualization tool called Memo Graph. It aims to offer an accessible visualization to Alzheimer’s patient. In this paper, it is extended to address the readability requirement. The second version is based on our approach called IKIEV. It extracts instance summarizations from a given ontology and generates a set of “summary instance graphs” from the most crucial data (middle-out browsing method). The extraction and visualization processes are undertaken incrementally. First, an “initial summary instance graph” is generated, then permitting iteratively the visualization of supplementary key-instances as required. This tool is integrated in the memory prosthesis to visualize data structured using PersonLink. We discuss the reassuring results of the usability evaluation of our IKIEV approach. Keywords: Ontology visualization  Readable visualization  Ontology summarization  Key-instances  Alzheimer’s patient  
  1 Introduction In the VIVA1 project, we are developing a memory prosthesis to help Alzheimer’s patient to palliate problems related to memory loss. It is called Captain Memo. Personal data of the patient are structured semantically using PersonLink [1] which is a multicultural and multilingual OWL 2 ontology for storing, modelling and reasoning about interpersonal relationships. Among the services offered by Captain Memo, one aims to “remember things about people” via the generation of the family and entourage tree. This graph is generated from their stored data. Hence, there is a need to integrate in Captain Memo an ontology visualization tool. 1  
  Alzheimer’s patient presents own characteristics that are different from non-expert users. Some of these characteristics are related to Alzheimer’s (e.g., attention and concentration deﬁcit) and other characteristics are linked to the aging process (e.g., sight loss). These characteristics impair this particular user to interact with graphs offered by standard ontology visualization tools targeting non-expert users e.g., Alzheimer’s patient has difﬁculty to read small nodes and loses concentration when reading dense and crowded graphs. Hence, there is a need to integrate in Captain Memo an ontology visualization tool that generates an instance graph which has the particularity to be accessible and readable to Alzheimer’s patient. Several ontology visualization tools have been proposed in the last two decades. However, to the best of our knowledge, there is no existing tool that is proposed to be used by Alzheimer’s patient. In [2], we proposed a tool, called Memo Graph, which aims to offer accessible ontology visualizations to Alzheimer patient. It is proposed mainly to be integrated in Captain Memo. In the present paper, we propose an extension of our early work that addresses the readability requirement. The aim is to alleviate the generated graphs. The new version of Memo Graph is based on our IKIEV approach (acronym for Incremental KeyInstances Extraction and Visualization). It extracts and visualizes, in an incremental way, instance summarizations of a given ontology to offer concise and readable overviews and support a middle-out navigation method, starting from the most important instances. The remainder of the present paper is structured as follows. In Sect. 2, we focus on related work, focusing on ontology visualization and summarization. Section 3 presents our ﬁrst version of Memo Graph. Section 4 presents our extension of Memo Graph that is based on our IKIEV approach. Section 5 details the evaluation results. In Sect. 6, we present the conclusions and we propose some future research directions.  
  2 Related Work The present work is closely related to the two following research areas: (1) ontology visualization and (2) ontology summarization. 2.1  
  F. Ghorbel et al.  
  only for expert users. Besides, almost all tools use technical jargon. For instance, WebVOWL and ProtégéVOWL, targeting users less familiar with ontologies, use some Semantic Web words. SOVA3, GrOWL [7], WebVOWL and ProtégéVOWL aim to offer understandable visualizations by deﬁning notations using different symbols, colors, and node shapes for each ontology key-element. However, the notations proposed by SOVA and GrOWL contain many abbreviations and symbols from the Description Logic. As a consequence, the generated visualizations are not suitable for Alzheimer’s patients. WebVOWL and ProtégéVOWL visualize only the schema of the ontology. Most tools overlook the importance of the readability requirement. According to [4], the current generated visualizations are hard to read for casual users. This problem becomes worse with Alzheimer’s patient. For instance, SOVA, GrOWL, IsaViz4 and RDF Gravity5 require the loading of the entire resulting graph in the limited space provided by the computer screen which generates an important number of nodes and a large number of crossing edges. Without applying any ﬁlter technique, the generated graphs appear crowded, which have a bad impact on its readability. According to [4], all RDF visualizations are hard to read due to their large size. KC-Viz [8] aims to offer a readable visualization of the TBox of the ontology to expert users. It is based on an approach that summarizes the schema of the ontology. Only few tools aim for a comprehensive ontology visualization. For instance, OWLViz6, OntoTrack [9] and KC-Viz visualize merely the class hierarchy of the ontology. OntoViz Tab [10], TGViz [11] and OntoRama [12] show only inheritance relationships between the graph nodes. Besides, most tools do not offer a clear visual distinction between the different ontology key-elements. This issue has a bad impact on the understandability of generated visualizations. For example, there is no visual distinction between datatype and object properties visualized by RDF Gravity. TGViz and NavigOWL [13] use a plain node-link diagram where all links and nodes appear the same except for their color. Most ontology visualization tools are implemented as Protégé plug-in. Thus, they cannot be integrated in Captain Memo. 2.2  
  Ontology Summarization  
  F. Ghorbel et al.  
  4 Extending Memo Graph: Our IKIEV Approach In this paper, we extend Memo Graph. Its second version is based on our IKIEV approach. It tends to avoid problems related to dense and non-legible instance graph by limiting the number of visible nodes and preserving the most important ones. It allows an incremental extraction and visualization of instance summaries of the ontology – incremental being the operative word. Initially, it generates an “initial summary instance graph” of N0 key-instances with the associated properties, then allowing iteratively the visualization of supplementary key-instances as required (key-instances are visualized as nodes and properties are visualized as labeled edges). For each iteration i, it extracts and visualizes Ni = Ni-1 + Ai key-instances; where Ai represents the number of additional key-instances compared to the previous iteration. N0 and Ai are set by the user. Figure 1 summarizes our IKIEV approach.  
  Ontology  
  All entities of KBi are instances of the same class (Person). Thus, the Class Centrality measure has no influence on determining key-instances. The overall mean of the precision associated to “IKIEV scenario @ 10 weeks” is better than the overall mean of the precision associated to “IKIEV scenario @ 2 weeks”. This difference is explained by the fact that the Hits Centrality measure is improved from one navigation session to another.  
  6 Conclusion This paper introduced an extension of Memo Graph to offer readable instance visualizations. It is based on our IKIEV approach. It allows an incremental extraction and visualization of instance summaries of the ontology. To determinate the relevance of a given instance, we are based on the relevance of its associated class and properties as well as the history of its user hits. The proposed tool is integrated in the prototype of Captain Memo to generate the family/entourage tree of the Alzheimer’s patient from their personal data structured using the PersonLink ontology. We evaluated the usability of our IKIEV approach in determining key-instances. The results are promising.  
  An Incremental Extraction and Visualization  
  D. Mukhamedshin et al.  
  1 Introduction Developing corpus management search engine architecture and database implies analyzing all the necessary functionality and testing various storage systems. Correct, fast enough and optimal in terms of resource consumption search engine performance depends entirely on a proper system and database architecture. One of the main goals of the development of the corpus management system “Tugan Tel” is to expand search capabilities for the Tatar National Corpus database. The corpus manager of the Tatar language is actively used in humanitarian and educational applications, as well as in applications belonging to computational linguistics sphere, which are used for researches of the Tatar language. Ready-made solutions are often used for developing national corpuses, which has its pros and cons. In particular, such solutions are usually proprietary.  
  2 Storage Systems for the Corpus Management System 2.1  
  ENERGARID Laboratory, SimulIA Team, Tahri Mohamed Bechar University, Bechar, Algeria [email protected]  2 LabRI-SBA Laboratory, ESI, Sidi Bel Abbes, Algeria [email protected]   
  Abstract. Imprecision in decision systems can negatively affect the data warehouse (DW) quality during a bad interpretation case. In order to evaluate the imprecision expression in decisional requirements and differently to our previous paper, we present an ontological solution using our GLMR ontology model for fuzzy connector evaluation in a query-based requirement. For simpliﬁcation reasons, we present in this paper only “and if possible” fuzzy connector case. We will propose a new solution combining two recent existent solutions. Although the fuzzy connector assessment was already treated but, according to our knowledge, never in the decision need context and proposing an ontological solution. The preliminary tests of our ontological solution are encouraging. Keywords: Requirement expression  Data warehouse  Design  Imprecision  Fuzzy ontology  Fuzzy connector  Quality  Evaluation  
  1 Introduction Few works, as [2] can notice, deal with the problems related to the DW design quality. In a previous literature review [3] on the decision-making requirements vagueness, we found that none of the works takes into account this vagueness problem. We have already presented, in later work [4], the vagueness study of the query-based requirements where the fuzziness is presented either in the predicate operator/value. In this work, we focus the study on the fuzzy connector. This assumes that the requirement includes, in this case, at least two predicates linked by a fuzzy connector, called a fuzzy bipolar query. The rest of this article is organized as follows: The second section presents the background. The third section presents the related works to the research topic. Our proposed approach is presented in the section four. The section ﬁve presents the preliminary tests. The section six concludes the paper and cites some perspectives.  
  Application client MAS console Browser or other client application Browser or other client application Browser or other client application  
  5 Conclusion In this paper, we proposed an approach allows running Web Services (REST and SOAP) pairs with Agent Jason BDI in a Java SE environment without using a modern Web-App server or application server, or developing different middleware. Because the development of a middleware requires a lot of time. And deploying an agent inside a server is a tedious task [2]. So in our strategy, we reused the existing Java frameworks called Non-Blocking Input Output or NIO APIs such as Grizzly and Netty to run the Web Services and Jason BDI Agent pairs in a Java SE environment. In this case, Grizzly takes the role of a server, and Netty provides the communication between the developed Web Service and the BDI agent Jason. The proposed strategy describes the operation of agents and Web Services called Jason-RS or Jason-WS to build an intelligent SOA application as a Multi-Agent System based only on Java SE. SOA require a service to handle multifaceted interfaces so Jason-RS and Jason-WS play an important role not only the both permit to design an orchestration of services but also we can serve it in complex workflow of goal oriented activities in term of Enterprise Architecture. In particular, different Agents collaborated together in the Jason  
  National Engineers School of Tunis, Communications Systems Department, University of Tunis El Manar, Tunis, Tunisia [email protected]  Sciences Faculty of Monastir, Electronics and Microelectronics Laboratory, Tunis, Tunisia [email protected]   
  Abstract. Quantum cryptography is a process for developing a perfectly secret encryption key that can be used with any classical encryption system. This paper presents a study of the EPR state protocol, the ﬁrst continuous variable quantum key distribution protocol. We propose an algorithm for this protocol and subsequently its implementation on FPGA (Field-Programmable Gate Array). For the implementation, we used Xilinx’s ISE System Edition tool as Software and Xilinx’s Artix7 Nexys4 DDR board as hardware. Keywords: Communication protocol  QKD  Security  Secret key  FPGA platform  EPR paradox  Bell’s inequality  Quantum cryptography  
  State of the art: This work was developed as part of the doctoral research work, in collaboration between the doctoral school of engineering sciences and techniques (EDSTI) within the National School of Engineers of Tunis (ENIT) and the Electronic and Microelectronics Research Unit within the Monastir Faculty of Science (FSM). The purpose of this work was to propose a prototype of a continuous variable quantum key distribution over an FPGA network. We chose the EPR protocol to rely on the properties of the variables involved in establishing a perfectly secret key.  
  J. Ikram and M. Mohsen  
  2 EPR Protocol Description It is a protocol whose states are correlated or entangled. In this protocol, the term EPR pairs is used to denote a pair of states emitted at a time t. The EPR pairs may be pairs of particles separated at great distances. This protocol also uses Bell’s inequality veriﬁcation for spy detection. The EPR protocol can be described as follows: • At each instant t, an EPR pair is created. The ﬁrst photon of this pair is transmitted to Alice while the second is transmitted to Bob. On their part, Alice and Bob, each randomly and with equal probabilities select their operator from Ai and Bi with i 2 f1; 3g. Depending on the chosen measurement operator, Alice and Bob proceed to measure their received photons respectively. They reserve their measurement results as well as their choice of measurement operators (see Table 1). Table 1. Measurement bases of Alice and Bob Alice A1 ¼ Z A2 ¼ X  
  Bob B1 ¼ Z B2 ¼ Xpþﬃﬃ2Z  
  ð1Þ  
  If the inequality is satisﬁed, no intrusion is detected and the communication is safe. Else, the system indicates the presence of the spy. Recall that the quantum noncloning theorem makes remarkable every movement of the spy. • We take back the “Raw Key” in this step and always via the public channel. In this step, which is common between various protocols, both interlocutors estimate the error rate QBER (Quantum Bit Error Rate) on their “Raw Key” sequences. They then correct the transmission errors to ensure that the generated key is secret. Several error correction algorithms are used [12]. To amplify the conﬁdentiality of the key, the two interlocutors can apply the parity check by adding a parity bit to their keys [13]. The EPR protocol uses an authenticated public channel, so the spy can’t pretend to be one of the two legitimate actors. Authentication is obviously possible by an appropriate algorithm. Therefore, the spy can’t perform an impersonation attack (Eve listening to the quantum and classical channels and pretending to be Bob). In this paper, we did not deal with the behavior of the spy, because his modeling is subject to two major difﬁculties. On the one hand, the particles transmitted to Alice and Bob by the EPR source are propagated on two different quantum channels, so the spy must make a global attack, rejected hypothesis. On the other hand, the spy must be equipped with the same technology (Hardware) as the two interlocutors and behave in the same way, an experimentally impossible requirement.  
  Fig. 1. Plot of secure key rates versus bit error rates of the EPR protocol  
  As we said before, in this paper we are working with continuous variables, but we need to discretise them once we achieve the reconciliation step. We used slice reconciliation in our protocol for error correction. In each step of our simulation, we record the sifted key generation rate (bit per second) and evaluate the QBER of the sifted key. The error revealed can be quantiﬁed by the ratio between the number of wrong bits to the total number of bits in some subset of the key. As shown in the curve, the bit error rate appears to be slightly decreasing. It starts from a value of 28% until reaching about 8%. Sifted keys with errors can’t be considered secure until proceeding to error correction and privacy ampliﬁcation in order to extract the ﬁnal secure keys. If the QBER value is too high, no secure keys can be extracted. However, the generation of the ﬁnal secure key  
  The values shown in this table reflect the sequential logic of our system. Indeed, each step is highly dependent on the previous one, from which comes the need to use an FSM (Finite State Machine) to control this sequence of events and the transition from one state to another. The use of Look Up Tables (LUTs) and Flip Flops is due to the use of logical operators (XoR, Comparator, etc.) during the implementation of the system.  
  Abstract. With the big amount of data that become available on the internet, and with the appearance of the information overload problem, it is becoming essential to use recommender systems (RS). RSs help users to extract relevant information that interests them, also to increase their quality decisions. These systems have proven their eﬀectiveness in several domains, such as: e-commerce, e-learning, etc. Furthermore, they play a very important role in the ﬁeld of medicine, in which the discovery of a tiny knowledge can save thousands of lives. In this paper, we will present a state of the art on RS approaches, their applications in general, and in the medical ﬁeld. Keywords: Recommender systems · Health recommender systems Quality decision · Machine learning · Artiﬁcial intelligence  
  1  
  made miracles, and they continue to progress and achieve results that have been impossible before. In this article, we will focus in particular on health RS. This paper will be presented as follows: In Sect. 2 we present the recommendation algorithms, in Sect. 3 we present the applications of these algorithms in medicine ﬁeld. We conclude and propose some perspectives in Sect. 4.  
  2  
  Conclusion  
  RSs are powerful tools to address the information overload problem. These systems are supposed to help users in diﬀerent domains. In this paper, we have presented an overview of the existing RSs. Some RSs are based on content, collaborative and demographic approaches, and their hybridization. Others take into account supplementary information: social, knowledge, context, and semanticbased approaches. Second, we have concentrated our study on the application of these approaches in medicine. The medical ﬁeld is known for his challenges and diﬃculties. These diﬃculties are due to the sensitivity of the domain itself, and to the complexity of the medical data. Despite these barriers, RSs are applied in diﬀerent medicine cases. In our future work, we will test and compare some recommendation approaches, for predicting diseases and their causes (disease origin) to avoid them.  
  Higher Institute of Computer Science and Telecom (ISITCom), University of Sousse, Sousse, Tunisia [email protected]  2 Modeling of Automated Reasoning Systems (MARS) Research Lab LR17ES05, Higher Institute of Computer Science and Telecom (ISITCom), University of Sousse, Sousse, Tunisia [email protected]   
  Abstract. An efﬁcient intrusion detection system (IDS) requires more than just a good machine learning (ML) classiﬁer. However, current IDSs offer a limited perspective in handling alerts’ databases. These databases must be local and structured in order to be referenced by these IDSs, offering an obsolete approach to solve advanced attacks and intrusions on distributed systems. With the emergence of big data, cyber-attacks have become a concerning issue worldwide. In situations where data security is paramount, swiftness becomes an obligation in processing and analytic operations. In that aspect, cloud-computing services can deal efﬁciently with big data issues. They offer storage and distributed analysis as the tools to be featured in our paper. To handle a large scale of alert data, we propose a new distributed IDS model that solves data storage problems, combines multiple heterogeneous sources of alert data, and makes data treatment much faster than local IDSs. For this purpose, this paper presents an approach of IDS using Databricks as a Cloud environment and Spark as a big data analysis tool. Keywords: Intrusion detection  Spark Machine learning  Naïve Bayes  
   Cloud  Databricks  DBFS   
  R. Ben Fekih and F. Jemili  
  They aimed to merge unstructured and different datasets and improve intrusion detection rate. As we talked about big data challenges and intrusion detection [4–7] are limited due to their local architecture. Among these limits, we consider, limited storage capacity, problems of processing speed and access to stored data (e.g. data stored on local disks). Consequently, Cloud becomes an inevitable alternative of unlocking the potential of Big Data. Cloud computing influence on data management and processing [1], not only it relates infrastructure and computation to the network, but it also supply management software and hardware resources with reduced costs. Furthermore, it results in a big emergence of programming frameworks such as Hadoop, Spark, and Hive for complex and large datasets. Using these tools, numerous studies have been performed in cloud environments. Esteves et al. [8] used Cloud computing for a distributed K-means clustering. The authors chose a large dataset to simulate big data challenges. Tests were executed using Mahout and Hadoop to solve data intensive problems, while running on Amazon EC2 during computation tasks. In our work, we chose Databricks as a cloud environment to process and manage data. More details about Databricks are given in Sect. 3.  
  3 About Databricks Databricks [9] was founded by the team that created Apache Spark™, the most active open source project in today’s large data ecosystem. Databricks is a Cloud based data platform and designed to ease the creation and deployment of advanced analysis solutions. Also, it provides the Databricks community edition as a free and open source version. The most important functionality is that Databricks provides a uniﬁed ecosystem with orchestrated Apache Spark [10] for implementation, development and scaling. In addition, It provides access to data easily and swiftly with Ingestion of nontraditional data storage based on cloud computing. Databricks integrates with Amazon S3 for storage. S3 buckets can be mounted into the Databricks File System (DBFS) and read the data into a Spark application as if it were on the local disk [11]. 3.1  
  • Spark acts its own flow scheduler (Due to in-memory computation). • Spark provides MLlib as an Apache Spark machine-learning library. Its goal is to make ML practical, scalable and easy. 3.2  
  Distributed Processing with Fast Data Access  
  Thanks to parallel-distributed processes, Spark simplify Big Data implementation and analytics. MapReduce is a great solution for one-pass computations, but less efﬁcient for use cases that require multi-pass computations and algorithms (Slow due to replication and disk storage), however, Spark presents several advantages compared to other technologies like Hadoop and Storm, for example spark enhances MapReduce with inmemory data storage, making the treatment less costly and much faster [14]. 3.3  
  ML Pipeline  
  NSL-KDD  
  NSL-KDD contains four attack classes: DOS, Probe, U2R, and R2L: • Denial of service attack (DOS): the goal of this attack is to render a service unavailable, and subsequently prevent legitimate users of a service from using it. These may include flooding of a network to prevent its operation, Disruption of connections between two machines, preventing access to a particular service. • Probing attack: This is the kind of attack in which the attacker scans a machine or network device to determine weaknesses or vulnerabilities that can be exploited later to compromise the system. This technique is commonly used in data mining. • User to Root Attack (U2R): Is an exploit class in which the attacker get access to a system and exploit a certain vulnerability to obtain Root access. • Remote to Local Attack (R2L): this kind of attacks occur when an attacker who has the ability to send packets to a machine on a network but does not have an account on that machine so attackers try to exploit a certain vulnerability to obtain local access as a user of that machine. 4.2  
  DARPA Dataset  
  Abstract. In the last years, the concept of Massive Open Online Course (MOOC) is widely regarded as new, innovative and creative model for free online learning at large-scale participation from the most prestigious universities around the world. On the other hand, the intelligent tutoring systems (ITS) have been developed to support one of the most successful educational forms “individual teaching”. Recent researches demonstrate that emotions can influence human behavior and learning, as a result, a new generation of ITS is born, that is Affective Tutoring System (ATS). However, there is no study showing the importance of using ATS in MOOCs. Therefore, this paper presents a novel approach for developing an affective tutoring system for the MOOCs, which is called ATS-MOOCs. Such system can easily help students to improve their learning performance by recognizing their affective states and then adapting the MOOC content accordingly. A prototype was developed and a case study was presented to demonstrate the feasibility of the proposed approach. Keywords: Massive open online course  MOOC  Affective tutoring system  Emotion detection  Emotional awareness  Intelligent tutoring system  Facial expression  
  M. Soltani et al.  
  – Emotional recognition, which includes the detection and analysis of the learner’s speciﬁc characteristics such as facial expression, voice, and gestures; and thereafter the application of a classiﬁcation tool to identify the emotion. – An emotional or affective response module, which is viewed as a part of the pedagogical module [6], that provides reasoning about the current situation and the learners’ emotional state [7]. On the other hand, the technology of MOOC is introduced as a new, innovative and creative model for free online learning at largescale participation from the most prestigious universities around the world. However, there is no study showing the importance of using ATS in MOOCs. Therefore, this paper presents a novel approach for developing an affective tutoring system for the MOOCs, which is called ATS-MOOCs. Such system can easily help students to improve their learning performance by recognizing their affective state and then adapting the MOOC content accordingly. The reminder of this paper is organized as follows. In Sect. 2, we present some of the related works. In Sect. 3, we show the main components of the proposed approach. In Sect. 4, we present a case study showing both the implementation and the use of the proposed approach. Finally, Sect. 5 provides our conclusions and possible future work.  
  2 Related Work 2.1  
  Abstract. Nowadays, measurement becomes a primordial technique in any software project. By measurement, we mean, the process of assigning a value to an attribute. However, measurement must take into account the speciﬁcities of novel software paradigms. Hence, we propose in this paper some metrics to measure the rationality of agents. Despite the importance of the rationality as one of reasoning characteristic; there is no measure that targeted this characteristic. The proposed metrics are applied on Jadex platform which is one of well-known agent platforms. In addition, a tool is developed to measure automatically the proposed metrics. The developed tool is based mainly on aspectoriented programming. Keywords: Rationality  Measurement  Multi-agent systems Aspect-oriented programming  AspectJ  
   Jadex   
  software like the complexity [8], the quality [9] and the architecture [10] as well as speciﬁc characteristics of multi-agent systems like the autonomy [11], the social ability [12] and the pro-activity [13]. We think that proposing new metrics for multi-agent systems is a very promising area because the current application ﬁelds of this software paradigm. The rationality is an important agent’s characteristic [14]. However, there is no study addressed its measurement. In our opinion the lack of speciﬁc metrics for this characteristic is due to considering it by the research community as an optional agent’s characteristic compared to the fundamental characteristics (like autonomy, social ability and pro-activity). Almost works proposed in this ﬁeld targeted only the fundamental characteristics [11–13]. However, we think that this ﬁeld is reached a maturity level allowing the study of more optional ones. The aim of this paper is to measure the rational of agents developed using Jadex platform [15]. Moreover, we developed a prototype tool allows calculating these metrics automatically. This paper is structured as follows: Sect. 2 is devoted to present related works. Then, we present in Sect. 3 an overview about Jadex platform followed by presenting the proposed metrics and the developed tool. Finally, we present conclusion and some future works.  
  2 Related Work Proposing new metrics for multi-agent systems is in full evolution [7]. In fact, we can distinguish mainly two kinds of metrics proposed in this ﬁeld. Firstly, some works proposed new metrics for common characteristics of software with taking into account the speciﬁcities of multi-agent systems. The quality [9], the complexity [8] and the architecture [10] are examples of these characteristics. Secondly, several works targeted measure of speciﬁc attributes of multi-agent systems. On this context, works proposed by Alonso et al. [11–13] represent the most prominent. In a serial of works, the authors proposed metrics for the most important characteristics of multi-agent systems like the autonomy, the social ability and the pro-activity. In each work, the targeted characteristic is devised into its main attributes and these attributes are measured using a set of metrics. Despite works presented in this ﬁeld, we can remark that several characteristics of multi-agent systems are omitted. In fact, we think that the research community targeted in ﬁrst time the fundamental characteristics of such systems. We believe also that this software paradigm has reach a maturity level allowing addressing some characteristics considered as optional. In this work we addressed one of these characteristics called the rationality.  
  Fig. 3. The abstract architecture of the developed tool.  
  In this paper, we applied the proposed metrics to Mars World application [26]. This later is composed of three agents with a mission of researching resources that exists in their environment. The found resources will be transferred to the agent’s homebase. Thanks to the perception capabilities of the sentry agent, this later is the responsible to ﬁnd resources. If a sentry agent found that a resource can be exploited, it calls the second agent (production agent) to produce ore using the found resources. Finally, the carry agent transfers the produced ore to the homebase. Figure 4 shows the interface of this application.  
  Fig. 4. The interface of Mars World application.  
  T. Marir et al.  
  Hence, we proposed in this paper some metrics that allow measuring the rationality of agent. Naturally, proposing these metrics is passed by analyzing this concept. Moreover, we presented in this paper a tool developed as part of this project that allows calculating the proposed metrics. This tool is developed using aspect-oriented programming to assess the dynamic metric, as it analysis the code of the application (XML and Java ﬁles) to assess the static metrics. We propose as a future work to enhance this work to assess other attributes like the flexibility and the intelligence. In addition, we think that the proposed metrics will be more beneﬁcial if they are generic. So, we propose to generalize the proposed metrics to be suitable with other multi-agent platforms.  
  We present in this paper a hybrid scheme based on differential evolution and local search to solve optimization problems in the continuous domain. This hybridization aims to beneﬁt from the advantages of both methods. Differential evolution, because of its exploratory power, could offer good starting points for intense local search processes, accelerating convergence towards the best solutions in the neighbourhood of these points. An alternation between differential evolution and local search would thus give a better chance to converge towards better solutions. Careful analysis of the behaviour of the differential evolution made it possible to understand the role of the different parameters and the effect of their variation on the efﬁciency and the quality of the optimization process. We have opted for a strategy of mutation and crossover with variable parameters to get variable exploration and exploitation aptitudes throughout the generations of the search process. To get out of blocking situations in local optima, we have integrated three types of population regeneration. The ﬁrst one concerns individuals; we have set a maximal age after which the individual should be replaced by a new generated one. The second is a systematic controlled disturbance of the whole population when its diversity is judged too restricted. The last is a total regeneration of the population if no improvement of the better solution has been observed for a very long period. To test the efﬁciency of the proposed approach, we have considered the optimization of the set of functions included in the reference Black-Box Optimization Benchmarking (BBOB) of the COCO platform [3, 4]. The functions have different properties and belong to different classes. They are good representatives of the different real-world problems with their different difﬁculty levels. The rest of the article is organized as follows. Section 2 reviews the basic notions of differential evolution and its variants. The proposed approach is presented in Sect. 3. In Sect. 4, are exposed the experimental results, analysed and compared to those offered by the state-of-the-art algorithms. The main conclusions and perspectives are presented in Sect. 5.  
  2 Differential Evolution Differential evolution is a stochastic search metaheuristic inspired by the genetic algorithm, it incorporates a geometric technique. From the original algorithm proposed by Storn and Price [1], several research works have been carried out to propose more robust and more efﬁcient variants. 2.1  
  H. Talbi and A. Draa  
  5 Conclusion In this paper, we have presented an optimization scheme combining an improved version of the differential evolution algorithm and a local method using trust regions. The key choices that led to very good results are: (1) the use of a mutation strategy that accelerates the exploitation of the neighbourhood of the best current solutions, while trying to discover new areas of the search space; (2) the controlled random adaptation of mutation and crossover parameter values that gives the algorithms more improvement opportunities; (3) the three mechanisms of population regeneration that maintain a good diversity that prevents from blocking into local optima; and (4) taking advantage of the local search technique effectiveness to quickly exploit the neighbourhood of the best solutions found by the differential evolution phase. Among the limitations of the proposed approach, one can cite the difﬁculty of adjusting the few static parameters to treat with the same efﬁciency problems of different sizes or belonging to different classes. It would be interesting to implement mechanisms to dynamically adjust these parameters, especially the threshold beyond which the algorithm decides to regenerate the population.  
  Abstract. This paper describes a new optimization technique to perform an embedded implementation of convolutional neural networks (CNN). In this case, only the inference of convolutional neural networks is discussed. As known that both pooling layer and strided convolution can be used to summarize the data. So, the proposed technique aims to replace only max pooling layers by a strided convolution layers using the same ﬁlter size and stride of the old pooling layers in order to reduce the model size and improve the accuracy of a CNN. Also, pooling layer is parameter less. However, convolution layer has weights and biases to optimize. Then, the CNN can learn how to summarize the data. By replacing max pooling layers with strided convolution layers enhance the CNN accuracy and reduce the model size. This technique is proposed in order to build a CNN accelerator for real time application and embedded implementation. The proposed optimizations are applied on some state-of-the-art CNN models and the obtained results are compared with the original ones. The proposed optimization is demonstrated for reducing the memory occupation of the model and achieving accuracy enhancement. The proposed technique enables possibility of the implementation of the convolutional neural network models in embedded systems. Keywords: Deep learning  Convolutional neural networks Strided convolution  Memory efﬁciency  
    
  The main goal of this work is to replace only max pooling layers with strided convolution layers in existing CNN model for memory efﬁciency of an embedded implementation and not generating new CNN model. This work, introduce convolution neural networks and why they are the most used for computer vision tasks. Then, the proposed approach is applied in some state-of-the-art models to approve its efﬁciency and it can be applied in all CNN models to reduce the memory occupation size and enhance the accuracy. The proposed approach is a feature key for the implementation of deep learning models in embedded systems that are equipped with a limited on-chip memory and a limited computation resource. In the second section of this paper, a brief introduction about the fundamental of convolutional neural networks (CNN) is provided. Related works on convolution methodology in CNNs are presented in Section three. The proposed optimization is explained in detail in the fourth section. in the ﬁfth section, the proposed approach is experimented on some of state-of-the-art models to approve its efﬁciency. Finally, this paper is concluded and future works are mentioned.  
  R. Ayachi et al.  
  increase the network depth, shrink the kernels size and the pooling stride. Becherer et al. [14] proposed the parameters ﬁne tuning technique to enhance the performance of CNNs. The proposed technique proves that ﬁne tuning the parameters is better than using random parameters. This paper introduces a new optimization to be applied on existing CNN models to reduce the model size and enhance the accuracy.  
  4 Efﬁciency of Strided Convolution Instead of Max Pooling As mentioned above that pooling layers are used to reduce dimensionality. The proposed optimization aims to replace only max pooling layers with convolutional layers using the same pooling stride and kernel size. Thus, strided convolutional provides the same dimensionality reduction of the max pooling. In this section, the proposed optimization technique is detailed. Assuming that a pooling function p applied on a feature map f with 3 dimensions (w, h, n) where w is the width, h is the height and n is the number of channels. The pooling function (p-norm) with pooling size k and stride s applied on feature map f is p(f). it is a 3-dimension array presented by Eq. 3. pi;j;u ð f Þð  
  R. Ayachi et al.  
  6 Conclusion CNN is the most used deep learning model in computer vision application. it needs a lot of optimization spatially for a low power embedded implementation. The proposed approach proves that the model size can be reduced to facilitate embedded implementation. In most cases real time applications are. So, a lot of other optimization can be applied to make convolutional neural networks ﬁt in embedded platforms with limited memory. One of the important optimization is provided in this paper. The proposed optimization lead to signiﬁcant reduction of the model size and the top-5 error. Thus, CNNs are enhanced with more optimizations by applying techniques enabled by this optimization like the data reuse technique and the fast convolution algorithms.  
  H. Doghmane et al.  
  In this paper, a novel representation for 2D ear recognition is proposed based on Improved Multi-Bag-of-Features Histograms (IMBFH). They are built using the following steps: (i) extract local texture image, (ii) quantized the descriptor images, using unsupervised clustering algorithm, and (iii) spatial pyramid histogram (SPH) decomposition. Then, the IMBFH features are projected using the Kernel Fisher Discriminant Analysis (KFDA) [15] in the KFDA subspace. This projection provides Discriminant IMBFH (D-IMBFH) feature vectors characterized by more relevant information and smaller dimensions. The proposed method requires the following steps: First, to reduce the effect of varying lighting and noise, a pretreatment step is applied to the raw images. Second, the multi bag-of-feature dictionary is learned from the training image responses of BSIF ﬁlter, using K-means algorithm. After that, the labeled images are constructed for training and testing sets. Third, the spatial pyramid histogram of horizontal decomposition is applied to labeled images. Next, the histograms obtained are normalized. Then, the global representation of the ear image is obtained by concatenating all the local feature descriptors. Afterwards, the discriminant representation of ear image is constructed, using kernel Fisher discriminant analysis (KFDA). This projection allows providing the Discriminant Improved Mulit-Bag-of-Feature Histograms (D-IMBFH) feature vectors, which are ensured with small dimensions. This makes it possible to minimize the computation cost during the classiﬁcation step. Finally, the obtained D-IMBFH feature representation is used for the classiﬁcation stage. The rest of the paper is organized as follows: Sect. 2 describes the main of the proposed feature extraction approach. Then, Sect. 3 reports and discusses the results of ear identiﬁcation experiments. Finally, Sect. 4 presents certain conclusions and future works.  
  2 The Proposed Method In this part of the present work, we describe our ear representation system. This paper proposes a novel ear representation method which explores not only the local texture property, but also the microstructure information among different image patches. Thus, it increases the descriptive power of the ear representation and further improves the ear recognition rate. Therefore, this paper presents a new attempt to combine the BSIF descriptor, Multi Bag of Features (MBF) model and Spatial Pyramid Histogram (SPH) method. The implementation scheme of the proposed method is based on ﬁve stages, as shown in Fig. 1: (i) preprocessing step, (ii) feature extraction using BSIF ﬁlter, (iii) Constructing the visual words, (iv) Quantizing descriptors, using visual words, (v) histogram pooling using spatial pyramid histogram and normalization. Then the global histogram representation is fed into a classiﬁer for ear recognition. As shown in the Fig. 1, our approach consists mainly of three phases: • Learning phase • Testing phase • Matching phase  
  Ear Recognition Based on Improved Features Representations  
  First Level of the Polar Graph Grid  
  We start by introducing the wheel graph, or polar graph ﬁrst degree as the ﬁrst level in this paper, which is composed of n+1 vertices noted vx¼0;y¼1 the center of polar graph and vx¼1;y¼y þ 1 the others with m = 2n edges. Lemma1.1: Let Pn be the polar graph (see Fig. 1) with DðPn Þ ¼ 2 . The number of vertices, edges and faces of Pn is N = n+1, m = 2n and f = n+1 respectively then:  
  Fig. 1. The polar graph Pn  
  Control and Energy Management Laboratory (CEM-Lab), University of Gabes, Gabes, Tunisia [email protected]  Control and Energy Management Laboratory (CEM-Lab), University of Sfax, Sfax, Tunisia [email protected]   
  Abstract. In this work, we propose to use recurrent deep learning method to model a complex system. We have chosen Deep Elman neural network with different structures and sigmoidal activation functions. The emphasis of the paper is to compare modeling results on a greenhouse and to demonstrate the abilities of Deep Elman neural network in a modeling step. For this, we used training and validation datasets. Simulation results proved the ability and the efﬁciency of Deep Elman neural network with two hidden layers. Keywords: Greenhouse  Elman neural network Recurrent neural network  
   Modeling   
  L. Belhaj Salah and F. Fourati  
  In this paper, we show the performance of using Deep Elman RNN to model complex system. To summarize, the plan is detailed as following: In part 2, we describe the Elman neural network. In part 3, we present the neural Elman deep learning with back propagation algorithm. In part 4, we describe the considered greenhouse. In part 5, we present simulation results of the modeling step. Finally in part 6, a conclusion and prospects are given.  
  2 Elman Neural Network The Elman network has been found a lot of success in several domains such as ﬁnancial prediction and identiﬁcation of dynamic systems. The Elman network is a type of recurrent network. The difference between this network and the feed forward neural network is deﬁned by the presence of context layer in Elman network. The main role of this layer is to memorize the previous hidden unit activations. The algorithm that can be used during the formation of the network is the back propagation algorithm, in addition the activation functions can be linear or non-linear for the hidden units [15–17]. Figure 1 and Table 1 describe an Elman neural network architecture.  
  Y. Hacha¨ıchi et al.  
  and add operations but the scaling factor is not. Even in the rare cases when the scaling factor is substituted, we have a signiﬁcant loss of precision, since many approximations are performed. In this paper, we propose scale-free DCT architectures based on a new choice of rotation angles and an accurate Taylor expansion. The proposed architectures provide both low power and high precision. Our contributions in this work are: – A new architecture based on an enhanced choice of rotation angles. – An improved architecture based on a generic precision Cordic based Loeﬄer DCT architecture. – A multiplierless DCT architecture based on an eﬃcient Taylor expansion. – A high quality architecture which provides the closest results to the Loeﬄer based DCT, the reference in terms of precision and accuracy. In the rest of the paper, we will ﬁrst give a state of the art of multiplierless DCT architectures. We will next give a background of the Cordic and the DCT, respectively in Sects. 3 and 3.1. The Generic precision DCT architectures are presented in Sect. 3.3. Section 4 details the proposed architectures. In Sect. 5, we analyze and discuss the experimental results before concluding our work in Sect. 6.  
  2  
  In [12], authors presented a CORDIC algorithm which completely eliminates the scaling factor. They based their work on an appropriate selection of the order of approximation of the sine and cosine Taylor series. In order to implemente the scale factor using shift and add operators, the authors proposed to use the third order expansion and approximate 3!=6 to 23 = 8. Moreover, they assumed that the elementary angle is equal to 2−i . This can be considered as a supplementary approximation since the elementary angle is actually equal to arctg(2−i ). These approximations lead to a signiﬁcant loss of accuracy. In [13], a scaling free cordic algorithm is proposed. This algorithm is based on both the sine and cosine Taylor series and the leading one bit detection algorithm. In this case, authors chosed also to use the third order expansion, but they approximate 3!=6 to 22 = 4. The leading one bit has been used to reduce the number of required iterations. In our case, we don’t need such algorithm since the Cordic angles are ﬁxed and their parameters are extracted from [6]. In [7], authors presented a fast DCT using multiplier-less method. This method consists on representing the constant coeﬃcients of the multiplications using an unsigned 12 bits precision, leading to shift/add based multiplications. The paper [8] also adopted the same technique but applied it on a 16 point DCT. Contrary to these works, we propose in this paper a DCT architecture which completely eliminates the scaling factor whilst also meeting the accuracy requirement by reﬁning our approximations.  
  3  
  Conclusion  
  In this paper we introduced diﬀerent architectures with high quality based on a new choice of the rotation angles and the Taylor expansion of the compensation stage. From the empirical results, we observe a signiﬁcant enhancement of the PSNR value (reaching 7.09 dB for P3TE5) in comparison with the Cordic based Loeﬄer architecture and a substantial decrease in the power consumption (attaining 14.6% for P1TE3). The results obtained make our architecture adequate to high precision applications. As a perspective, we can combine our results to recent improvements in the DCT architecture. We conjecture that this will give more eﬃcient architectures, Quality/Consumption tradeoﬀ.  
  University of Gabes, National Engineering School of Gabes, Gabes, Tunisia [email protected]  2 Pattern Analysis and Computer Vision (PAVIS), Italian Institute of Technology, Genoa, Italy 3 INRIA Sophia Antipolis Mediterranee, Biot, France 4 University of Sfax, National Engineering School of Sfax Laboratory of Electronics and Information Technology (LETI), Sfax, Tunisia  
  Abstract. The topic of Person Re-Identiﬁcation (Re-ID) is currently attracting much interest from researchers due to the various possible applications such as behavior recognition, person tracking and safety purposes at public places. General approach is to extract discriminative color and texture features from images and calculate their distances as a measure of similarity. Most of the work consider whole body to extract descriptors. However, human body maybe occluded or seen from different views that prevent correct matching between persons. We propose in this paper to use a reliable pose estimation algorithm to extract meaningful body parts. Then, we extract descriptors from each part separately using LOcal Maximal Occurrence (LOMO) algorithm and Cross-view Quadratic Discriminant Analysis (XQDA) metric learning algorithm to compute the similarity. A comparison between state-of-the-art Re-ID methods in most commonly used benchmark Re-ID datasets will be also presented in this work. Keywords: Person Re-Identiﬁcation (Re-ID) LOMO features  XQDA algorithm  
   Pose-driven body parts   
  #Camera 2 2 8 2 2 2 10 (5pairs) 10 (5pairs) 4 2 (continued)  
  5 Conclusion We proposed in this paper, to use a reliable pose estimation algorithm to extract meaningful body parts and then extract LOMO descriptors from each part separately and then compute the distances between those descriptors using XQDA metric learning algorithm as a measure of similarity. Preliminary experiments show some potentials of using pose estimation for Re-ID, but not as accurate as global signature. One shortcoming of our work may be that we relied on LOMO descriptor that is essentially designed for the whole image. Suitable descriptor such as deep features should be designed for body parts. In case of proper descriptor, part-based Re-Identiﬁcation is promising to cope with the problem of pose and viewpoint variations. This work can also be extended to detect mid-level features or attributes (such as gender, long hair, jeans, t-shirt etc.) that are more reliable than low-level descriptors (such as gradients and histogram).  
  M. Gafsi et al.  
  In cryptography, these functions are used for several services such data integrity and authentication, password protection, pseudo-random number generation, digital signature and more others. Especially, the SHA-2 family is very used due to its high performance. Technically, they process the input data by a block for multiple rounds to ﬁnally generate a data digest. Table 1 shows the functional characteristics of the SHA-2 family and their description can be found in [14]. These functions enable the integrity of data such that a tiny change in the input data, with one bit, will cause a greatly signiﬁcant change in the output. As a sequence, each data has its own data digest. 3.4  
  CTR Encryption Mode  
  O. Marrakchi Charﬁ et al.  
  1 Introduction The important progress of the microelectronics industry over the last twenty years has produced the fast development of digital sensors. Actually, these sensors are present in many ﬁelds of application (video surveillance and security, medical, general public) and their capacity in terms of resolution is constantly increased. Medical imaging has also beneﬁted from this development and CCD or CMOS detectors providing high deﬁnition image replace ﬁlm acquisition system in radiography. For that, many patients no longer need to go through invasive and often dangerous procedures to diagnose a wide variety of pathologies. With the widespread use of digital acquisition in medical imaging, the quality of digital medical images becomes an important issue [2]. In medical ﬁeld radiology, images must contain no artifacts and have a good quality to enable doctors and radiologists to perform the best possible diagnosis. Hence, radioprotection strategy imposes the reduction of dose radiation in radiography exploration exam. For this reason and the other considerations, denoising methods is still a valid challenge and methods for denoising and will be helpful in medicine. In literature, Wavelet transform is commonly used for image compression [7], image restoration [8, 9], image segmentation [10] and in recent years, it is essentially used for image denoising [1]. Some other methods are also used to denoise radiology images, such us, the conventional ﬁlters; the soft threshold method and the WCMS algorithm [1]. In this paper, we have shown that these last cited methods are so weak in performance for reducing the kind of noise enclosed in X-ray image acquired using flat detectors. Therefore, we propose, in this paper, a new algorithm named DWTTH to denoise X-ray images. The DWTTH method combines the DWT transform of the image and the Thresholding of the wavelet coefﬁcients of DWT sub-bands images. This article is structured in six sections. Section 2 presents the X- ray data base acquisition. Section 3 resumes the existing and recent denoising algorithms and methods. In Sect. 4, is developed the proposed DWTTH method. In Sect. 5, experimental results and discussions are presented. Finally, Sect. 6 gives the conclusion and the perspectives of this work.  
  The obtained results are compared with those obtained using WCMS method and conventional ﬁlters. Therefore, the developed DWTTH method gives for this instance the best results of SNR and CNR to reduce noise and preserve contrast in the Low_RX image (Tables 4 and 5). Hence, we can conﬁrm that noise energy coefﬁcients are localized in the low frequency domain. Hard thresholding can eliminate noise-related energy coefﬁcients, but it also affects the regularity of some ROI textures (Fig. 5(a)). Also, hard thresholding may be used to limit the interval of values of the energy coefﬁcients to be eliminated, but a selection criterion of the energy coefﬁcients must be developed in order to select some of these coefﬁcients which must be restored to preserve the regularity of the ROI. However, instead of restoring desirable energy coefﬁcients, we have chosen, in this case, to attribute for each pixel of one ROI the mean value (smoothing) of the corresponding denoised ROI with DWTTH method (Fig. 5(b)). Also, we can see that the gray scales of the reconstructed denoised image looks like those of the reference image (Fig. 5(c)), essentially from ROI3 to ROI7 which are denoised with DWTTH method. Finally, the obtained results are promising but DWTTH method must be applied and tested on real images for denoising. Hence, the DWTTH algorithm must be improved in order to automate the denoising process. For this reason the execution time of the algorithm is not considered in our study. Table 3. Average values of ROIs of Low_RX image and those one after applying the different denoising methods. ROI ROI1 ROI2 ROI3 ROI4 ROI5 ROI6 ROI7  
  6 Conclusion and Perspectives In this paper, we have present a novel method based on DWT and called DWTTH method for denoising a low X-ray dose image of the Pro-Digi phantom acquired with flat detector. The set of X-ray images is composed of a standard X-ray dose (S_RX) and a low X-ray dose (Low_RX) images. The last one is a high noisy image than the other (S_RX). Tests were done on seven contrasted ROIs located on the images set. Several denoising methods were tested. The conventional ﬁlters and WCMS algorithm are not powerful to denoise the image. Only DWTTH denoising algorithm is able to well localize noise in the DWT - approximation sub-bands images at seven decomposition levels. The efﬁcient denoising results with edge and contrast preservation are due to the optimal selection of thresholds if hard thresholding method is used on all levels of the DWT approximation sub-bands. To select thresholds, ﬁrstly, the convergence of the average pixels values criterion of denoised ROIs is satisﬁed and secondly, the SNR and  
  A Novel DWTTH Approach for Denoising X-Ray Images  
  the CNR ratios of each denoised ROI of the Low_RX image must converge to the SNR and the CNR ratios values of their respectively ROI on the S_RX image. DWTTH denoising algorithm presents the advantages of reducing patient’s X-ray dose, localizing noisy pixels in low frequencies domain and preserving edges and contrast of the image. However, some artifacts may be generated due to the thresholding of no noisy values of some pixels, which belong to the interval. So, selective criteria must be used to pick up only the noisy coefﬁcients from the interval in order to improve aberrant results. An alternative solution is done. Smoothing denoised ROI in order to have a result image look like to the reference image. Acknowledgment. Authors acknowledge are addressed to the Charles Nicolle Hospital radiology staff for the data base acquisitions.  
  Ground Systems  
  Traditional ground systems, also called terrestrial systems, are based on human supervision. Fire detection and monitoring is performed by supervising regions locally or by analyzing data provided from local sensors such as flame, smoke and heat detectors, and gas sensors. In order to increase systems efﬁciency and detect the exact location of ﬁres, ambient sensors were also integrated. These sensors are used during the day and night to detect ﬁre and smoke and identify their characteristics. The main sensors are employed in terrestrial systems are vision or infrared (IR) camera, IR spectrometers and Light detection and ranging systems (LIDAR) [3–5]. 2.2  
  Satellite-Based Systems  
  Comparison of Forest Fire Detection Systems  
  Ground systems are situated in look out spot which is able to detect ﬁre in real time. However, the flexibility of these systems can’t hide their drawbacks that are mainly caused by human error estimations, inaccuracy in visual estimation, lower ﬁre localizing accuracy and the difﬁculties in predicting the spread of ﬁre and smoke [16]. Satellite systems, when compared to ground and UAV systems, present several advantages, mainly large areas monitoring, and higher data acquisition frequency [3]. However, these systems are not qualiﬁed to early wildﬁres detection due to their low temporal resolution. In fact, it takes two days to acquire images of the earth. Besides, spatial resolution [1] and images quality can be affected by weather conditions [5]. It can be easily concluded that UAVs have a major signiﬁcance for ﬁre detection and monitoring thanks to their low cost and reliable data transmission. Furthermore, compared to ground and satellite systems, UAVs are used for early ﬁre detection due to their real time monitoring system and higher data acquisition frequency as well as the ﬁre localization accuracy.  
  4 Conclusion In this paper, a widespread literature survey on ﬁre monitoring and detection systems has been presented. The main objective of these systems is the detection and estimation of ﬁre evolution in real-time. A comparative analysis of ground, satellite and UAV systems in terms of reliability, flexibility and efﬁciency revealed that UAVs have a major signiﬁcance for ﬁre detection and monitoring thanks to their low cost and reliable data transmission and the most important real-time processing. We have also presented an up-to-date review of vision-based ﬁre detection techniques while focusing on the  
  Superpixel Based Segmentation of Historical Document Images Using a Multiscale Texture Analysis Emna Soyed(B) , Ramzi Chaieb, and Karim Kalti LATIS - Laboratory of Advanced Technology and Intelligent Systems, ENISo, Sousse University, Sousse, Tunisia [email protected]  , [email protected]  , [email protected]   
  Abstract. In this paper, a superpixel based segmentation of Historical Document Images (HDIs) using multiscale texture analysis is proposed. A Simple Linear Iterative Clustering (SLIC) superpixel technique and Kmeans classiﬁer are applied in order to separate the input image into background and foreground superpixels. The foreground superpixels are characterized by the standard deviation and the mean of the Gabor features. These features are extracted in a multiscale fashion to adapt to the variability of the textures that may be present in HDIs. Text/graphic separation is then performed by applying a classiﬁcation of the foreground superpixels for each texture analysis scale followed by a merging step of the obtained classiﬁcation results. Since the classiﬁcation results depend on the used classiﬁer, a comparative study is performed for supervised (Support Vector Machine (SVM), K-Nearest Neighbors (KNN)) and unsupervised (Kmeans, Fuzzy C-Means (FCM)) techniques. Experiments show the eﬀectiveness of our proposed method especially when compared with similar work in the literature. Keywords: Segmentation of Historical Document Images · Multiscale texture analysis · SLIC superpixel · Gabor features Merging classiﬁcation results  
  1  
  Related Work  
  For instance, Jian used the superpixel technique to perform the image background segmentation [4]. The particular aggregation of information provided by the superpixel has been proved by Li et al. to be useful for image segmentation [5]. Cohen et al. used spatial and color features extracted from superpixels to separate drawings regions from background of ancient documents [6]. Garz et al. used a multistage algorithm based on interest points to detect the layout entities in ancient manuscripts [7]. They proposed a new stroke width computing method by using intensity and Simple Linear Iterative Clustering (SLIC) superpixels region growing to segment text from low quality images. The limitation of this method is the fact that it cannot be used in text images with various strokes and it was proved to not be good for complex background text segmentation. Mehri et al. pointed out a background separation technique for ancient documents using spatial and color features extracted from superpixels [8]. Moreover, in order to extract the text and graphic regions, they presented an algorithm based on the use of the SLIC superpixel approach and Gabor descriptors to segment historical document images [9]. Mehri et al. used diﬀerent sliding window sizes for the texture analysis. The descriptors extracted from these diﬀerent windows were embedded in a single vector which may aﬀect the performance of the classiﬁcation step. In this paper, we propose a contribution to overcome these ineﬃciencies by developing a HDIs segmentation method based on using multiscale texture analysis. The proposed method is based on the SLIC superpixel technique and Gabor features. Accordingly, four diﬀerent scales analysis are used and four classiﬁers are applied to extract textual information from the graphical ones. In the literature, many methods for text/graphic classiﬁcation were elaborated [8,9]. Unfortunately, all these works did not make any comparative study between them. Our objective is to apply a comparison between four diﬀerent classiﬁcation methods in order to ensure the eﬀectiveness of the selective classiﬁer. First, an interactive feature learning step is introduced to train supervised classiﬁers. Then, a merging method is developed to deal with the variety of the multiscale analysis windows and to make easier the choice of the best classiﬁer. The remainder of this paper is organized as follows. Section 3 presents an overview of our proposed method. The adopted superpixel technique is described and the investigated Gabor features are detailed. In order to outline the pertinence of the experimental protocol, Sect. 4 describes the ground truth, the segmentation results and their evaluation. Finally, Sect. 5 gives a conclusion in the end of this paper.  
  Superpixel Based Segmentation of HDIs Using a Multiscale Texture Analysis  
  Merging Results  
  In many ﬁelds such as multiscale sliding windows, we are often confronted with multiple and conﬂicts sources of information. In this paper, we propose a merging method based on using the majority vote in order to resolve the conﬂicts of windows resolution. Four classiﬁcation results are fed as input. Then, for each superpixel, we assign it to the majority voted class. If we have an equal case, we take the majority vote of neighboring superpixels already classiﬁed. Otherwise, we assign it to the same class of the nearest neighbor. A detailed schematic block representing the merging results of each Gabor ﬁlter application window for one classiﬁer is illustrated in Fig. 3. For each superpixel, the majority vote can be formed as follows: V (s) = argmax(g(ci , s))  
  (1)  
  Conclusion  
  In this paper, we proposed an interactive method based on using SLIC superpixels and Gabor features to extract textual and graphical regions in HDIs. In this study, each superpixel is characterized by the average and the standard deviation of the response of Gabor ﬁlters application. The features are computed for diﬀerent sliding window sizes. Then, the extracted features are classiﬁed using supervised (KNN, SVM) and unsupervised (Kmeans, FCM) clustering techniques. In order to deal with the variety of windows, the results of the diﬀerent classiﬁers are merged using the majority vote. Finally, the evaluation shows that our approach brings satisfactory results and outperforms the one proposed in [9] thanks to the merging step. Our further work will be the use of the presented algorithm on an another type of database for example administrative documents. We will also focus on evaluating and merging other texture extraction methods.  
  Faculty of Technology, Lebanese University, Saida, Lebanon [email protected]  , [email protected]  2 Faculty of Technology, Lebanese University, Aabey, Lebanon [email protected]   
  Abstract. In this research, we present a new way of thinking using a Convolutional Neural Network (CNN) for palm-vein biometric authentication. In contrary to ﬁngerprint and face, palm vein patterns are internal features which make them very hard to replicate. The objective of this research is to examine the possibility of a contactless authentication of individuals by imply a series of palm veins photographs taken by a camera in the near infrared. Biometric systems based on palm veins are considered very promising for high security environments. In mean time, deep learning techniques have assisted in image classiﬁcation and tasks retrieval. The use of palm vein recognition through deep learning based methods and Convolutional Neural Network architectures (i.e., Inception V3 and SmallerVggNet) applications. Keywords: Palm-vein  Biometric authentication Convolutional neural network  
  Palm Vein Review  
  One study showed that the palm print can be attained using common web camera. In addition splitting the hands from frames was succeeded by ROI excerpt of palm print and traits were drawn out implementing Gabor kernel. Later, ICA traits were extracted and categorized implementing NN and distance-based classiﬁer [15]. In other studies, researchers have used vein pattern conﬁrmation through palm vein images which were primarily improved and traits were drawn out with neural networks, feed forward and SVM algorithms covered with high effectiveness and accuracy [16]. In other studies, obtaining creation for dorsa palm vein authentication implementing correlation method [17]. Some studies reported, implementing directional encoding and Back Propagation Neural Network functioned with palm vein identiﬁcation in the study Region of Interest and gamma correction practiced [18]. In other paper, palm vein were improved implementing histogram and drawing out trait was performed using Laplacian ﬁlter on convolved images [19].  
  Fig. 1. General CNN architecture.  
  Nowadays, routine ConvNets that demonstrate efﬁciency for image classiﬁcation. Previous studies proposed VGGNet [23] and GoogLeNet [24] showing the great different between their maps. In this paper, we show two ImageNet dataset pre-trained CNN designs for texture-based vein traits extraction. The ﬁrst one is the  
  Palm Vein Biometric Authentication  
  3 Method The proposed methods are tested and evaluated using a capture of dataset. The images were collected at Lebanese University-Faculty of technology by implementing a Near Infrared camera. The overall diagram of the method shows the different stages of the identiﬁcation of different individuals using the palm veins image (see Fig. 2).  
  Fig. 2. General block diagram of our study.  
  These steps consist ﬁrst of acquiring the palm veins image, then perform image processing algorithms to extract the veins map for the identiﬁcation and ﬁnally the convolutional neural network perform an automatic identiﬁcation. The aim of this research paper is to study deep learning ways on palm-vein recognition. The researchers’ asses’ two CNN designs, called Inception V3 and SmallerVggNet in showing and noticing palm vein forms.  
  4 Experimental Work and Results Palm vein images are taken under near infrared lighting system. The experiment aims to remove noise and enhancement image. These procedures are performed on multispectral palm vein image and useful to extract the vein pattern for further processing. In what follows, the image acquisition system, the palm vein images database, the preprocessing stage, CNNs architecture and the results are presented. 4.1  
  S. Chantaf et al.  
  Fig. 3. Image acquisition system: (a) Palm Veins Imaging System; (b) Infrared LEDs + Filter; (c) Raspberry Pi 3; (d) Raspberry Pi 3 NoIR Camera V2.  
  4.2  
  Preprocessing Stage  
  Preprocessing include algorithms that consist of region of ROI excerpts, image enhancement, image normalization and image segmentation. The segmentation process primarily converts the initial in an expressive depiction. This section describes the operations and transformations that were applied to the digital images in order to improve and process the captured image. Initially ﬁltering process is applied on the captured vein pattern image to remove the noise. There are many ﬁlters but in this paper, two ﬁlters are used: averaging ﬁlter and median ﬁlter. After that, the contrast is enhanced by histogram equalization. The framework of the proposed method comprises the following processes (see Fig. 5):  
  Fig. 5. Proposed system workflow for palm-vein pattern  
  M. Aﬁf et al.  
  Generally deep learning model demands a high capacity of memory, and energy consumption. For this, in this paper, we choose to work with DenseNet [20] model because it presents a compensated architecture which can be implemented in embedded systems.  
  3 Proposed Architecture for Indoor Object Recognition Deep Convolutional Neural Networks (DCNN) [15] are the widely used neural networks in computer vision applications and image and video processing tasks. Convolutional Neural Networks (CNNs) are multi-level forward propagation networks. The entrances of each Input layer of a CNN are called feature maps. For the ﬁrst layer (input layer), the input feature maps are images. The outputs on each layer are the characteristics extracted from all the locations of the input features map of that layer. Basically, a CNN is simply a stack of multiple layers of Convolution, non-linearity, Pooling and Fully-connected (FC). Figure 1 presents the architecture of a Convolution Neural Network.  
  An Efﬁcient Approach to Face and Smile Detection Alhussain Akoum, Rabih Makkouk(&), and Raﬁc Hage Chehade Faculty of Technology, Department of CCNE, Lebanese University, Beirut, Lebanon [email protected]  , [email protected]  , [email protected]   
  Abstract. The mental state of a person is judged by detecting smiles. The smile detection starts with face recognition. My algorithm in our paper detects by deﬁnition the face from the image we entered, and then it detects the mouth and ﬁnally the smile. Next, we make sure that the face we detected is a smiley face or not in a photo, detects the person’s mouth, and decides if they are pleased or not. Given a set of photos of a person entry in our system, we can compare their images using algorithms to detect face and other to detect automatic the corners and the features of the month then we determine which picture has the best smile. Keywords: Face detection Haar classiﬁer  
   Smile detection  Corner detection   
  Smile Detection  
  To determine a smiley or not smiley face, we can use multitechniques. The ﬁrst technique is tried simply to count all points of the borders detected because a person tended to produce more borders than someone without a smile, mainly due to the presence of the number of tooth in a smile [10, 11]. However, they appreciated that this method was inexact when the face gave a smile to the lips close together or was wordless but not laughing. The next technique was to design the points of border detection, since a minimum threshold is grasped, and calculate the best-ﬁt line to the resulting point cloud [12, 13]. This technique combined with our ﬁrst technique has proven to be an effective combination to detect the concavity of the region of the subject’s mouth and the density of border points in this area, letting us to regulate if the mouth was formed smile.  
  3 Production This initiate’s face detection can be considered as one of the object detection target. In object-target detection, the mission is to catch the locations and the sizes of all objects in a photo that belong to a certain target. Our algorithms face detection center on the detection of the front of person faces (Fig. 2).  
  5 Conclusion By detecting the characteristics and detection corners, we could detect if the person was smiling or not in the photo with Face Detection. This automatic identiﬁcation of Smiles has many potential applications, including: enhanced camera functionality for advanced functionality. In the future, we should update our mouth design such that we can implement larger head turning and face size scaling. In addition to that, we should update our model of mouth so that we can implement a greater turning of the head and scaling of the size of the face.  
  An Efﬁcient Approach to Face and Smile Detection  
  D. Mazzaccaro et al.  
  The use of the simulator since the ﬁrst years of the degree course in medicine and surgery aims to gradually bring the student closer to the most innovative practices of vascular surgery. Thanks to the simulator, the surgeon can train and improve his practical skills, optimizing learning time and reducing the risks associated with multiple maneuvers. Through our daily practice associated with the simulation of virtual reality, a scale of risk has been established in the execution of the CAS procedures of which all of us, from the less expert to the most advanced, should take it into account. Some may argue that difﬁcult performances could need more than three hours of training. For example, Van Herzeele et al. [15] described the beneﬁt achieved on virtual reality metric performances of experienced operators after a two-day training course. However, none of the studies reported in the literature has shown yet the possible transfer of competence gained during VR to “in vivo” procedures. This could be an excellent point of discussion for future prospective randomized studies, if the ultimate goal would be to use VR as a “stand alone” training tool prior to performing CAS procedures.  
  5 Conclusion “Easy” virtual CAS procedures of non-experienced operators signiﬁcantly improved after training. On the other side, “difﬁcult” virtual CAS procedures did not improve signiﬁcantly after training.  
  Bio-Inspired EOG Generation from Video Camera: Application to Driver’s Awareness Monitoring Yamina Yahia Lahssene1(&), Mokhtar Keche1,2, and Abdelaziz Ouamri1,2 1  
  University of Sciences and Technology of Oran Mohamed Boudiaf (USTO-MB), Oran, Algeria [email protected]  2 Signals and Images Laboratory, Electronique, University of Sciences and Technology of Oran Mohamed Boudiaf (USTO-MB), Oran, Algeria  
  Abstract. Electrooculogram (EOG), while being a source of useful information. Many researches and applications such as Human computer interaction (HCI) and physiological state monitoring rely match more on it then other resources like video surveillance. In our work we’re interested in awareness level classiﬁcation for driver drowsiness detection, in the purpose of improving road security. However, EOG acquisition requires placing electrodes around eye’s subject all the time which is not comfortable and convenient for many applications. Therefore, we propose a generation of a Bio-inspired EOG signal (pseudo EOG) using video camera. As for EOG signal, the bio-inspired one contains useful information of eye state and movements in time. Critical features were extracted or calculated from the bio-inspired EOG signal and used as inputs of a fuzzy logic classiﬁcation of the awareness level in time. Keywords: EOG  Bio-inspired EOG  Physiological state classiﬁcation Fuzzy logic  Driver awareness monitoring  
    
  Bio-Inspired EOG Generation from Video Camera  
  Fig. 3. Horizontal intensity average: (a) Opened eye, (b) Closed eye.  
  Bio-Inspired EOG Generation from Video Camera  
  Fig. 4. Sample of our generated pseudo EOG.  
  Many parameters were extracted from the pseudo EOG signal and its derivative which, in fact, presents the speed of the signal variation in time. These parameters may be instantaneous or averaged over time. Obviously, the later ones are more pertinent for the estimation of the driver’s state (awake or drowsy). Indeed, the best results were obtained by using parameters calculated every second by averaging over a sliding window of 20 s. The parameters selected for the classiﬁcation are deﬁned below: • • • •  
  Bio-Inspired EOG Generation from Video Camera  
  A. Classiﬁcation Using Fuzzy Logic  
  It is clear from Fig. 6a that the decision based on each blinking independently is not reliable, since unexpected slow blinking may occur while the subject is awake and also, fast blinking can be present during drowsiness phase, due to external effects. On the  
  1  
  Bio-Inspired EOG Generation from Video Camera  
  4 Conclusion and Future Work In this work we, ﬁrst, proposed a method of a pseudo EOG generation from video signal of the driver’s face, which is recorded by a medium speed camera (not expensive). Using a camera instead of electrodes is certainly more comfortable for the driver and offers the possibility of extrapolating the techniques applied with the EOG signal to the pseudo EOG signal. The idea of generating a signal similar to EOG signal, from a video, has already been discussed by Antoine Picot [13, 14] [14, 15]. However,  
  Bio-Inspired EOG Generation from Video Camera  
  CEDRIC Laboratory, Conservatoire National des Arts et Métiers (CNAM), Paris, France [email protected]  , {metais,faycal.hamdi}@cnam.fr 2 MIRACL Laboratory, University of Sfax, Sfax, Tunisia [email protected]   
  Abstract. Numerous studies have conﬁrmed that Alzheimer’s patients may beneﬁt from memory rehabilitation processes. In this context, we propose a non pharmacological training, named Autobiographical Training. It is an adaptive and accessible “Question/Answer” training. It has a purpose to stimulate the patient’s memory. It is proposed in the context of a memory prosthesis called Captain Memo. (1) Autobiographical Training do not use general facts or false examples, but it automatically propose for each user their speciﬁc questions related to his life e.g., events that he lived. (2) It adjusts automatically the level of difﬁculty of the generated question depending on the progression of Alzheimer’s disease. (3) It supports multilingualism and multiculturalism. (4) It offers accessible user interfaces. We evaluated the accessibility and the usability of Autobiographical Training. 18 participants entered the study. The results conﬁrmed that it is accessible and the frequent use of this training helps patients in reminding some information, in which they received the training. Keywords: Alzheimer’s disease  Memory training  Autobiographical question  Adaptive user interface  Accessibility Multilingualism  Multiculturalism  
    
  Alzheimer’s disease, include pharmacological and non pharmacological interventions [5, 9]. Non pharmacological interventions have a signiﬁcant role in delaying Alzheimer’s disease progression [1, 3, 5, 6, 8] reducing functional impairments [3, 5, 6] and helping in retaining some data longer [3, 9]. So, it helps in improving the quality of life of Alzheimer’s patients as well as their caregivers [3, 5, 6, 8, 9]. These training may be completely computerized [4, 6]. In this paper, we propose a non pharmacological memory training, called Autobiographical Training, for Alzheimer’s patient. It is a “Question and Answer” training that attempt to preserve the patient’s reminding abilities. The highlight of our work is that it keeps into account the patient’s proﬁle: (1) It generates for each user his own questions based on his private life. (2) It adjusts automatically the level of difﬁculty of these questions according to the progression of the disease. (3) It supports multilingualism and multiculturalism. (4) It offers accessible user interfaces and interactions. The remainder of the paper is structured as follows. In the Sect. 2, we present Autobiographical Training. Section 3 presents its evaluation. In Sect. 4, we details related work. In Sect. 5, we conclude and we give perspectives.  
  2 Overview of Autobiographical Training Autobiographical Training is an adaptive and accessible “Question and Answer” training. It takes aim to refresh the patient’s memory. It is proposed as a service of the Captain Memo memory prosthesis [10]. 2.1  
  Compared to related work, those generate only static questions based on general facts, our adaptive intervention generates for each user his own questions related to his private life. It sets the level of difﬁculty of these questions based on the progression of the disease. It supports multilingualism and multiculturalism.  
  5 Conclusion The present paper presents Autobiographical Training. It is an adaptive and accessible “Question and Answer” exercise training. It stimulates the Alzheimer’s patient’s reminding abilities. The training is repeatedly performed until the patient demonstrates the ability to recall information in everyday life. It is developed in the context of Captain Memo, which is proposed to support Alzheimer’s patients to palliate mnesic problems. Compared to related work, Autobiographical Training presents 4 main advantages. (1) It is not based on general content, but it intelligently proposes for each user their speciﬁc questions based on their life e.g., events that he lived. (2) It adjusts automatically the level of difﬁculty of these questions based on the progression of the disease. (3) It supports multilingualism and multiculturalism. (4) It offers accessible user interfaces. The evaluation phase conﬁrmed that Autobiographical Training is  
  A Memory Training for Alzheimer’s Patients  
  Abstract. The visualization of high dimensional data has an important role to play as an artifact supporting exploratory data analysis. There is growing evidence of the effectiveness of information visualization as it provides help in understanding data, increases the level of cognitive functioning and performs pattern recognition. This paper deals with the usefulness of Self-Organizing Map (SOM) neural network in the area of the banking sector. We want to show how SOM can be useful to convert huge amounts of ﬁnancial data into valuable information used to speed up the decision-making process and facilitate data analysis for deeper understanding Keywords: Information visualization Performance analysis  Banking  
   Self-Organizing map   
  both in size and complexity, the need to reduce their dimensionality while preserving information typology can be drawn as a great way to reduce screen crowdedness and the level of noise in the display [5]. Up to now, many studies have made signiﬁcant contributions to developing dimensionality reduction techniques. The SOM, which is a machine learning tool, was originally developed for data exploration through unsupervised learning. The main advantage of using the SOM lies in its strong ability to preserve the topology of data using neighborhood function. This preservation of the topological properties of data allows best visualization and identiﬁcation of data clusters. Our paper aims to contribute to the previous literature comparing the performance among banks by providing a complete and simple model. Until now, most of the comparative studies have used the Data Envelopment Analysis (DEA) model [6]. Even though it has many advantages such as ranking a set of entities called Decision Making Units (DMUs), identifying and estimating sources of technical inefﬁciency, etc. [7], some limitations should be noted. The classic DEA model assumes that both inputs and outputs variables must be non-negative and preferably strictly positive. This assumption does not always hold, particularly in the context the actual business world. DEA is also sensitive to the number of inputs and outputs. Indeed, if the number increases, the ability to discriminate between the DMUs decreases [8]. Our objective is twofold. Firstly, is to show the effectiveness of SOM for overcoming the DEA limitations and emphasize its great potential to achieve proper classiﬁcation and visual analysis of large and complex ﬁnancial data in the banking sector. Secondly, to compare the operational performance of Islamic and conventional banks during the ﬁnancial crisis. To the best of our knowledge, it is the ﬁrst study using SOM to differentiate between conventional and Islamic banks. Our paper is divided into ﬁve sections. Section 2 provides an overview of SOM and discusses its potential beneﬁts regarding of visualization and exploratory analysis for the banking sector. Section 3 tends to focus on our methodology and presents more technical details about SOM. Empirical results are given and discussed in Sect. 4. Conclusion and suggestion for future research are drawn in the last section.  
  2 Self-Organizing Map for Bank Classiﬁcation and Analysis: Review of Literature Several research studies have shown the effectiveness of SOM for achieving good classiﬁcation, generating visual clustering and facilitating visual analysis of large and complex data. In fact, Kohonen [9] was inspired by the biological visual systems of Hubel and Wiesel [10] to model his artiﬁcial neural networks based on unsupervised learning algorithms. SOM can perform two tasks simultaneously, vector quantization [11] and vector projection [12]. No target output is provided and the network evolves until convergence. Based on Gladyshev’s theorem, the convergence of SOM algorithm is proved [13]. There is no limit to the maximum amount of input data. The input matrix contains variables with positive, negative and zero values. SOM has ﬁve main favorable characteristics for the banking sector: handling of outliers, suitability for unbalanced panel data, resilience on problems of multicollinearity, identiﬁcation of nonlinear dependencies among variables, and the lack of a required assumption of  
  A Pattern Methodology to Specify Usable Design and Security in Websites Taheni Filali(&) and Med Salim Bouhlel Research Lab SETIT, University of Sfax, Sfax, Tunisia [email protected]  , [email protected]   
  Abstract. Over the past decade, with the progress in technology, there has emerged various needs for users to get more interactive with Web applications. Commercial web services have spread the use of rich interfaces to provide users with a meaningful interaction with these applications. Nevertheless, the dynamic nature of the context of interaction imposes practitioners to extract various requirements such as user needs and choose the appropriate actions to perform them. However, in real practice, practitioners hardly combine between exploring the space problem to extract users’ goals and creating a high quality user interface. To overcome this challenge, we propose a set of measurements criteria basically integrated into speciﬁc comprehensive indicators. Our goal is to evaluate the main aspects of quality requirements (Design, security). Thus, practitioners will have a flexible support based on a high quality model to improve the extracted services and the usability of commercial Web-based application in a dynamic context. Keywords: Context engine Assessment platform  
   Web interfaces  Quality model   
  1 Introduction Various quality issues have recently affected e-commercial services. One of these main issues is the difﬁculty of using a Web-application and to interact with it. This may contrarily influence the association’s position. In this manner, it is critical for any association to effortlessly lead an assessment of the nature of their e-business administrations. Accordingly, they can enhance their contributions after some time and their benchmark against rivals [1]. The programmed assessment of the nature of commercially oriented Web interfaces has been a rising ﬁeld of research over the most recent couple of years. Several approaches have been proposed under various names. Huge numbers of them are not consumer-based measures of quality, for example, measures incorporate devoured time per visit [2], deals exchanges [3], Web trafﬁc and server logs [4]. Thus, they are not designed to assess the site quality but the site efﬁciency. Others approaches listed multiple quality features of Web applications without investigating the relationships between them [5]. They do not demonstrate the structure of the quality dimensions.  
  T. Filali and M. S. Bouhlel  
  Nevertheless, several approaches are limited to evaluate the performance metrics for general Web-applications without treating the speciﬁc case of e-commercial services [6]. Based on these ﬁndings, our work introduces another technique to evaluate usability, security, and e-commerce prerequisites of sites. Past research works propose a rundow of essential prerequisites for human-computer interaction, security, and web based business in a free way, however they don’t incorporate these three perspectives into a solitary assessment strategy [7]. We introduce a review of these fundamental prerequisites, which are in this way fused into an arrangement of measurements. The proposed measurement model relies upon the Goal Question Metric system and enhanced with a plan of scientiﬁc equations [8]. We show the suitability of the estimations by using an illustrative case as a proof-of-thought together with a preliminary ease of use consider. The remaining of the paper is as per the following. Section 2 gives a short survey of the past work. Section 3 talks about and dissects the proposed methodology. Section 4 characterizes the dimensions of the proposed methodology and its markers. Section 5 ﬁnishes up the paper and recommends some future works.  
  WebTango [16] receives an unpredictable system: Usability rules are not executed as a computation for each standard. Or maybe, quantiﬁable techniques are used to ﬁgure the resemblance between the site that is surveyed and a course of action of “known-great” locales whose accessibility and usability has been assessed by authorities using manual examination. Kwaresmi [17] is a scholastic model with an accentuation on rapidly indicating extra tests for the validator utilizing an uncommon rule deﬁnition language. MAGENTA is a case for a device which does distinguish issues, as well as right a few mistakes if the client wishes. This can be worthwhile for web designers who don’t have the essential learning to recognize the right ﬁx for an issue. A comparable, XML based rule depiction language is utilized by EvalIris [18]. The ATRC Web Accessibility Checker is the upgraded electronic version of the APrompt GUI application. Beside extent of WCAG 1.0/2.0 and related tenets, it features yield in a machine-fathomable arrangement (W3C EARL, Evaluation And Report Language). It is exemplary in its modularized and systematic approach to manage WCAG endorsement. Additionally, it can get some data about a speciﬁc assumed issue (“Does the stay contain content that identif[ies] the association objective?”) rather than yielding a notice for every event of the issue. Thusly, it reduces the amount of false positives. Our model WQM separates itself from different instruments around there in light of the fact that it bases its website architecture and security provide details regarding web designing models of the site pages. Our model “WQM” is the only tool which takes advantage of the information in Web engineering models when performing Usability, Design and Security validation. Table 1 represents this comparison. Table 1. Correlation of tools for automated accessibility and usability validation of Web interfaces. Analysis Web quality type standard WebXACT Heuristics acc., privacy, content WebTango Statistics Reference sites Kwaresmi Heuristics Accessibility MAGENTA Heuristics Accessibility ATRC Heuristics Accessibility ArgoUWE Heuristics no acc./usab. tool WQM Heuristics Design, security  
  Run-time extensibility No  
  T. Filali and M. S. Bouhlel  
  3 Assessment Model for Secure and Usable CommerciallyOriented Websites The work done by the distinctive scientists is a fascinating beginning stage. They propose rules, criteria and measurements within a technique speciﬁc to every one of them. In light of this state of art examination, it would now eagerness to make a social occasion of the criteria proposed in the unmistakable examinations. The goal of this examination is to build up a theoretical, expansive, and quantiﬁable framework for evaluating the nature of business web interfaces to give straight forward criteria to connect with updates of website composition and its utilization [20]. What’s more, we except to build up a structure that is set up for reliable applications over an expansive degree of business web interfaces. Our method overlaid industry and academic research to see quality components with a speciﬁc extreme aim to meet the goals of this examination. The goal is the enhancement of methodology for other quality sections, particularly, appropriateness and the progress of a purpose behind a web application-speciﬁc quality tree, seeing the speciﬁc quantiﬁable points of view and proposing a test model to assess them. Thusly, after a trial of gathering, regrouping and expanding the criteria raised by the researchers, we propose 2-estimational criteria which are clear and joins every single past measurement and components, having in mind the ultimate objective being for it to be utilized as a general criteria to assess commercial web interfaces. The measurements of the proposed model are the quality of design and the quality of security. To ask about how our proposed norm were utilized as a bit of past examinations, we rearranged each fragment of each estimation of the past work to be under one of the two new measurements. The proposed approach endeavors to join information and experience from different sources, an extent of reference disciplines and observational practices. The goal is to see quantiﬁable features and indicators that beginning from now contains a convincing site. The proposed model can be utilized to break down between the quality of commercial web interfaces, to perceive a route for development of a site page, and to give a standard to originators and designers while making new goals. After we assessed every appraisal norm, we added its pointers to the ﬁtting spot of the proposed 2-estimations criteria, other than fusing two or three markers in which we see them essential from our own particular experience. Our criteria consolidates each and every principle indicator of the previous investigations of assessing the quality of commercial web interfaces Fig. 1 assembles the dynamic chain of importance of the proposed model. Reflecting to Lord Kelvin’s, as referred by Bellovin [21]: “If you cannot measure it, you cannot improve it, When you have the ability to assess what you are discussing, and present it in numbers, you know something about it; nevertheless when you can’t check it, when you can’t present it in numbers, your knowledge is of unsatisfactory kind; it may be the beginning of data, yet you have scarcely in your thoughts advanced to the state of science, whatever the issue may be”. It is critical to have an apparatus that gives quantitative dimensions to design, security, and e-commerce necessities.  
  A Pattern Methodology to Specify Usable Design and Security in Websites  
  The results found validate that our quality model can specify which quality dimensions need to improvement and which are satisfactory, so our model of quality is practical and it can give us quite promising results, it looks into the latest assessment strategies that were utilized as a part of assessing the quality of various web interfaces, and proposes a complete model for evaluating the quality of any commercial web interfaces. These measurements with their indicators, subsequent to being given sure weights, could be operationalized and changed over into a survey. This survey could be connected to all commercial web interfaces. Results from the examination of the survey will help in assessing these measurements and their indicators and make the required update on them and build up a compelling base of guidelines.  
  5 Conclusion and Perspectives The use of recent information and communication technologies delivered new generation in business, trade, and economics. The Web application made another business condition far unique in relation with anything that has proceeded it. This enhanced the need of measurement criteria to assess the different aspects related to the quality of Web applications. Nevertheless, quality issues have influenced every business area in recent years, since a society or a company with a website that is hard to interact with, gives a poor image on the Internet and enfeebles a company’s position. In this manner, it is essential for a company to evaluate the quality of its e-commercial services. Thus, it will be able to improve its services and benchmarks against competitors. In this paper, we focus on studying the evaluation measurement. We proposed a model based on the most relevant dimensions. We found that incorporating these dimensions into  
  University of Bourgogne Franche Comté, Dijon, France [email protected]  , [email protected]  , [email protected]  2 Lebanese University, Beirut, Lebanon 3 Faculty of Technology-SAIDA, Lebanese University, Beirut, Lebanon [email protected]  , [email protected]   
  Abstract. The purpose of this paper is to present an agents-based methodology that allows for the creation and optimization of schedule while taking into account a wide range of constraints or preferences. When some smart households beneﬁt from a common energy source, if the available power is limited, the problem to be solved for improving energy efﬁciency is how to program the power-on time of the peripherals according to the power limits and taking into account the preferences of the users. The proposed operating system was developed as multi-agent systems (MAS) on the JADE platform. The implementation is discussed by describing in detail each agent and the control algorithm. In addition, complementary metrics are proposed, to evaluate the performance of the planning method. Finally, to illustrate the proposed method, some simulation results are presented. Keywords: Multi-agents scheduler  Planner  Energetic efﬁciency  Smart grid  
  Schedule Creation Algorithm  
  We now explain the planning mechanism for a new schedule. The methodology is distinguished by the existence of a single “decision maker” (Manager-Agent), who has control over the current planning. Manager-Agent negotiates the calendar with OutletAgents, using information of Tslot-Agents about the current status of the calendar. The negotiations are spread over double iterations. Note that for a given day, different devices may require multiple time slots. For clarity in the description of the planning mechanism, we call “activity” the status of power-on one device during one time slot. During a round, each Outlet-Agent (with activities not yet planned) sends ManagerAgent a message requesting an activity that it has selected. Then the decision process start, The Manager-Agent examines all the requests received from Outlet-Agents during this round, it orders them according to their priority, it accepts those who respect the set of constraints speciﬁed at this level of decision and place the “reservations” in the current calendar, it refuses the others and informs the corresponding Outlet agents. Note that at this level, the set of constraints can be more or less flexible according to the chosen policy (for example we can admit to exceed the contractual power and refuse to exceed the maximum power limit). At this level, the manager’s decision depends on three situations: (1) if the requested activity causes the limit power to be exceeded during this time interval, the request is denied; (2) The priority of an requested activity is less than another and there is no location for both at once in terms of power, the activity with least priority will be refused; (3) Of course, multiple Outlet-Agents can request the same location. In this case, Manager-Agent selects the subset of queries containing activities with best values calculated by the algorithm of evaluation, provided that the limit power is not exceeded. The iterations for fulﬁlled locations end when the received requests list is emptied or when unplanned activities cannot be fulﬁlled. At the end of a series, Manager-Agent informs Outlet-Agents of his decision. It should be noted that in one round, the requests evaluated number equals the number of unplanned devices taken (it’s a small number and that explains the speed of the planning algorithm). Note that this approach ensures that all outlets have the same opportunities to operate, since each agent can, in one turn, reserve a place for one of its activities. After each regular round of activities placement, Manager-Agent consults the Outlet Agents about the activities that were rejected during this round. Each OutletAgent returns a proposal for the activity in which the requested time interval was eventually changed. Manager-Agent receives messages from Outlet-Agents that contain requested activity. To implement these activities in the schedule before moving on to the next round, the main process of negotiations is paused and the placement algorithm for rejected activities starts. Once the previously rejected proposals are added, Manager-Agent returns to the main road.  
  i¼1  
  In the Customer Satisfaction Indicator expression, the numerator represents the sum of the weights of all activities to which the planner software has assigned locations in slot time in accordance with the maximum preference indicated in the preference matrix. While the denominator represents the sum of the weights of all requested activities, in other words, the weight of all activities in the best potential program. The result obtained represents in a way the compliance rate of the calendar obtained compared to the ideal calendar. In addition, we considered and analyzed other indicators of performance measure: (1) The ratio sreal=virtual ¼ real cost=virtual cost is also an indicator that informs us about the rate of energy provided based on user preference. (2) The performance rate scontract=real ¼ cost respect contract=real cost is an indicator informing us of the performance in terms of compliance with the contract. (3) The ratio snotplan=expect ¼ energy not planned=energy expected is an indicator that informs us of the failure rate of the planning algorithm due to the technical limitation of available power. (4) The ratio sexpect=available ¼ energy expected=energy available is an indicator of the rate of energy requested in relation to the maximum energy available.  
  5 Conclusion In this article, it is proposed a methodology based on Multi-Agents System to plan the energy consumption in a smart area. The structure of the programming software tool for this application is presented and we have detailed its mechanism and its decision logic. We have presented the simulation results obtained on a typical example. Energy planner satisfactorily solves the problems of creation and optimization of energy consumption planning. Obviously, other simulations are necessary to validate the developed system. Here, it is interesting to note that although our scope in this paper is to plan electrical power consumption, we note that this same agent system has been successfully applied to the problem of predictive and dynamic planning of surgeries in operating theater in hospital setting. Finally, simulation with real data, near real-time planning and comparison of results with other time-management tools are the next research works that we will report in subsequent publications.  
  Information Technology and Communications Laboratory, FIL, International University of Rabat, Rabat, Morocco {sarah.elhamdi,mustapha.oudani}@uir.ac.ma 2 Laboratory Engineering Science, MOSIL TEAM, ENSA, Ibn Tofail University, Kenitra, Morocco [email protected]   
  Abstract. Nowadays, consumers in any community in the world, desire a progressive improvement of life quality, while thinking about sustainable approaches, and the industry has been advancing, evolving to keep up with these requirements. The consumption pattern faces a continuous and nearly unpredictable change; companies all over the word must adapt their business practices and cope with the customer needs the explicit and implicit ones. Rapid technologies upgrade, evolution and change are the actual main challenges of the industrial enterprises, data and data analytics has become a core asset related to the performance of the organizations. Industry 4.0 envisions a digital transformation in the enterprise, entwining the cyber-physical world and real world of manufacturing to deliver networked production with enhanced process transparency. The question is how about the developing countries on the African continent and in strategic geographical position such as Morocco will deal with this fourth revolution. The purpose of the current paper is to question the readiness of Morocco and highlight the challenges it faces to integrate the Industry 4.0. Keywords: Industry 4.0  
   Readiness  Challenge  
  Fig. 2. Comparison between leading countries and Morocco  
  This means that the digitalization process may be harder for the Moroccan very small companies as it represent a signiﬁcant investment. Second challenge: Skills sets Nevertheless companies have realized the need to understand change to better deploy it. It is no longer a simple computer update, but a profound change in the culture and organization of the company [23]. It is a scheme where digital is no longer a tool but a global mindset, a cross-cutting culture applied to all departments and businesses of the company, which implies implicitly the need to understand the human factor [24], in order to get rid of the legacy of the old system and to build speciﬁc skill sets required, such as robotic programming and Big Data Analytics. Third challenge: Digital transformation process Using the words of Professor Bocquet, holder of a doctorate in AI: “Industry 4.0, is fundamentally characterized by smart automation, adoption and integration of new technologies to the core of the company” [25], meaning radical changes are to occur to the company’s information systems, business processes, employees mindset, management procedures and means once the migration is initiated, that is the reason why the shift upsets the company manufacturing industry. The process of the digital transformation impacts the business at three levels: In the ﬁrst place, it is the Business Model that is impacted; the digital transformation revolutionizes traditional models. In a second step, it is the business processes that evolve. The offer is seen more and more digitized. Finally, the crucial point in the digital transformation, the customer journey changes. With the penetration of technology, the customer is now more connected, more mobile, more demanding. The government needs to manage this migration to the fourth industrial revolution well [26], offering assistance in the form of strategic support, facilitation of the process or subsidies, especially since the companies involved are very small, small and medium-sized enterprises (Fig. 2).  
  Requirements’ Framework  
  The government need to change its custom activity and tax structures to account for an environment in which physical goods of all kinds will rapidly decrease in value compared with the intangible’s ones may it be goods or services. The government has to legally consider a digital fabrication plant as a full scale industrial plant. The government ought to be able to answer if this type of manufacturing will create jobs to soothe the social actors questioning and fear of the change to come [27]. It must also relieve Entrepreneurs, directors and owners of companies that will move towards the integration of the technologies from cyber security challenges that will probably arise as well like IP Theft [28]. As we stand on the edge of an entirely new way of life studded with technological innovation, the challenges and relevant issues may come in any unfamiliar form. The government will have to design workshops to better understand the key elements of the fourth revolution, and the core technologies that represents the pillar of the shift, those workshop should not be intended to only industrial staff but also to common user and consumer to allow the philosophy of the industry 4.0 to spread regionally and nationally and create awareness of this move opportunities among youth and provide training for innovation managements [29].  
  5 Conclusion and Perspectives The technological development is rising with high speed in developed countries, even the developing countries felt the need to rush into the fourth revolution to reduce the gap between both worlds. It is important to note that the technological progress has no meaning whatsoever if it does not ameliorate the working conditions of humans, that’s why it is really primordial to ensure the engineers and researchers’s inventions are without harm to man, without fail, there is a need to be able to predict and to control the interactions between humans and machines [30], but it is not only the engineers’ responsibility even the users, employees must acquire skills to be better masters of the tool that is technology. This is a modest paper addressing the readiness of my country to the fourth industrial revolution, this work may be considered a new element to elevate the current discussion, to identify the actual Moroccan economical context, challenges and requirement in order to implement a policy to have the transition to a smart industry. In order to enrich this debate and deepen the framework of the proposed requirements, a “written and /or face-to-face” survey is conducted among the leaders of small and medium-sized enterprises in order to establish their need for material, technological, human, ﬁnancial resources and the needed state support to successfully transition to I4.0. The industrial revolution is already on going in the developed countries and it is happening around the globe [31]; the revolution of the industry requires from each company no matter the size and each individual not matter the status to rethink what to expect or desire from smart Internet-connected devices.  
  Morocco’s Readiness to Industry 4.0  
  Abstract. Online banking administrators usually try to use sufﬁciently strong security solutions to encourage individuals to use online banks. They provide virtual keyboards for users to log in with as a main security solution to facilitate their users’ trust in their security solutions. The virtual keyboards can be prone to screenshot capture by various means, such as Trojans, malware, and shoulder surﬁng attacks. Different proposals for virtual keyboards have been presented by many authors with different means of input. In this paper, we provide a cloaking virtual keyboard (CVK) as a model for a virtual keyboard with cloaked input. The proposed model goes beyond using a virtual keyboard to prevent a screenshot attack on the online banking application system. It provides a means to inform the administrators of the new attack and takes the attacker to a false account. Keywords: Online banking  Capturing screenshots Shoulder surﬁng  Cloaking virtual keyboard  
    
  H. Mohsin and H. Bahjat  
  overcome by [2, 5]. Recently, [6] presented a hardware Trojan to disable the software Trojan countermeasure. The term cloaking is used in many information security application techniques, such as [7–9], to describe a covered resource [7]. In this paper, we present a proposal for a cloaking virtual keyboard (CVK) as a new software tool to protect user passwords and accounts from being attacked by capturing screenshots in online banking. We do this by adopting cloaking with eye-gaze tracking.  
  2 Motivation and Contribution Key loggers can use either hardware or software connected to the computer to crack user login passwords in three modes of attacks (one-time, multiple-time, and over-time screenshots) with many different means of input. Nonetheless, only a few researchers have attempted to ﬁnd a solution beyond using a virtual keyboard. Since eye gaze can be a good input device instead of using a keyboard or mouse, it represents a cloaking input model. It uses the front camera of a computer or mobile device to track the position over the screen (virtual keyboard) to input the user name and password. In this case, we cloak the used input device, which is different from those used by previous researchers. We adopt the virtual keyboard with the cloaking input model to protect the user password and account in online banking systems from being attacked by a screen capture, even if the front camera video is captured. The proposed CVK is different from research in [7, 8] because we try to cloak the input device. The proposed CVK is protected from the possibility of gathering the user password via shoulder surﬁng attacks and malware attacks, which means it is better than [1]. Trojans take a snapshot of the screen in three types of modes, which is overcome by [2–5] and by our proposed CVK. In addition, [6] presented a hardware Trojan to disable the software Trojan countermeasure. We also provide another solution for this problem.  
  3 Proposed Modulo Background and Approach 3.1  
  Proposed Modulo Background  
  Cloaking. Cloaking is what is known as optimization technique of search-engine in which the content (information presented to the user) is different from that presented to the crawler’s part of search-engine for better indexing. It hides the true nature of an object by delivering obviously different content to users. These objects can be text [8], images [7], or websites [9]. Eye-gaze Detection System (EGDS). Recently, many authors have engaged EGDS in many hands-free applications [10–13]. The main parts of EGDS, as shown in Fig. 1, include image acquisition, in which the system input is an image from either a web camera or a pre-captured video. Next is the standardization block, in which the accused image initializes eye gaze for the pointer on the screen into sub-blocks, using detection  
  Anti-screenshot Keyboard for Web-Based Application  
  The arrangement of the alphabet on a virtual keyboard is the same as in a normal QWERTY keyboard [1]. Using our proposed CVK, each authentic user has his or her own arrangement of the alphabet on a virtual keyboard that is different from the QWERTY keyboard.  
  6 Conclusions and Future Work For this study, we have analyzed the security of the CVK modulo and addressed a number of attacks to be handled with our proposed CVK in terms of using a camera mouse and virtual keyboard as the means to cloak input characters and keystrokes. The CVK adds tools that do not affect the current system technology. Using CVK, we facilitate users to make more transactions. One important limitation is the need for users to have system training before using the system in the future. We will try to design our proposed CVK for users with many different disabilities. Acknowledgements. We are thankful to the anonymous reviewers for their great efforts to improve this work quality with their valuable suggestions and comments.  
  Fall Prevention Exergame Using Occupational Therapy Based on Kinect Amina Ben Haj Khaled(&), Ali Khalfallah(&), and Med Salim Bouhlel(&) Sciences and Technologies of Image and Telecommunications, Higher Institute of Biotechnology, University of Sfax, Sfax, Tunisia [email protected]  , [email protected]  , [email protected]   
  Abstract. Fall is a major health problem, especially among elderly people living alone at home. This age group is characterized by a loss of physical and motor skills, balance and posture disorder, and a reduction in daily activities. These factors are the main cause of falling. However, it is important to design technologies that prevent falls and can help older people to practice exercise to improve balance, posture and strength. Due to the low use of conventional physical therapy, fall prevention interventions through the game demonstrated their effectiveness. Unfortunately, the most existing exergames, used for fall prevention, were not designed speciﬁcally for the elderly. For this reason, we are interested on fall prevention. We used the advantages of serious games and Kinect to do an occupational therapy for older community at home and make them more active. The proposed exercises were well studied with an ergo-therapist and an orthopedist. They specially designed for the elderly and are speciﬁcally aimed at improving balance and muscle strength. Elderly can easily practice these exercises easily and safely at home. This occupational therapy using serious games is able to ameliorate activities of daily living. It does not only decrease the risk of fall but it has many positive effects in psychology, health and physiology. Keywords: Older adults  Serious game  Fall prevention User interface  Occupational therapy  Kinect  Depth map Skeleton detection  Skeleton tracking  
     
  A. Ben Haj Khaled et al.  
  to supervise the elderly to prevent age related disabilities such as decreased performance, walking disorders, abnormalities of posture, and loss of conﬁdence that can lead to falls [3]. Research has found that risk of fall can be extrinsic and intrinsic. Extrinsic fall factors are related to the environmental features. Intrinsic factors are individual and they depend on the person. Intrinsic factors are more severe than extrinsic. They can be associated to impairments in posture control during walking. It is known as flexed posture [4]. These impairments affect mobility and increase risk of falling and cause 10% to 25% of all falls [5]. Falls can have more or less several consequences which can be divided in two categories physical (fracture, injuries) and psychological. The psychological effects are very signiﬁcant as inactivity with 14.9% and functional dependence with 13.7% [6]. The loss of autonomy increases the risk of falling again. For this reason, it is important to monitor elderly. Traditionally, the wealthy older people move into a retirement home which number is much reduced in Tunisia (11 retirement homes) [7]. Other elderly prefers to live at home accompanied by a nurse or to use monitoring systems. All these methods do not respect patient privacy. They can effectively detect a fall but they cannot prevent it and its dangerous effects specially loss of autonomy. The use of serious games and smart sensors can prevent falls by improving physical capacity represents different advantages. On the psychological front, the elderly is maintained in its natural and autonomous environment in security. He is more conﬁdent and he can do his daily activity in security by improving his physical and motor abilities. On the economic front, fall prevention reduces the costs of therapy and the assistance compared to specialized hospital staff. In this paper, at ﬁrst, we present some existing solution for monitoring. Then, we propose our technique to improve the activity of elderly and prevent the risk from fall which is the occupational therapy. And ﬁnally, we focus on the result. The general objective of this study is to maintain and improve the activity of elderly at home and giving them the feeling of safety and comfort.  
  The second category is an external (not wearable). These sensors are placed inside the house. It includes environmental and ground sensors. The environmental sensors are a simple and accurate way to analyze the actimetry of elderly. The detection is based on a situation that differs from the normal as a failure to open a door, pass in a room or the lack of movement may be due to unconsciousness after a fall [11]. This monitoring technique is based on the detection of abnormal activities. So, it can detect unnatural domestic animal’s action as a fall. The ground sensors are installed to control the floor using special flooring called “smart floor”. The floor grid is used to analyze the bearing points of the person’s foot and to detect its movement. In research, the use of smart floor began with Rimminen and AL [12]. This floor sensor can’t make the difference between a liquid poured on the floor and a drop. The second category requires a complex setup and is still in their infancy. In the third way to detect falls, monitoring needs only a simple camera. The images are treated by computer vision algorithms. These algorithms are studying the movements of the subject, according to several criteria to detect the fall. Images 2D are extracted to calculate the velocity of subject’s movement. The velocity is inversely proportional to the distance of the person from the camera. The more the person is near to the camera, the more the velocity is. In this case, it is difﬁcult to differentiate between falls and a person sitting down abruptly. Rougier et al. solved this problem by using calibrated 2D camera to deal the real-world coordinates and they collected the 3D velocity vector [13]. Despite this solution, 2D cameras still have a major disadvantage. They can’t detect images, if an obstacle is placed in front of the sensor’s ﬁeld of view. All these classical methods of monitoring study the physical point of view of fall and they are not looking on its most important cause. The absence of standardization is the main reason of the difﬁculty of interpretation and evaluation of a fall. After a long study of various fall’s research, we can deduce that the laziness and the lack of activities are more important risk factors than chronic and extrinsic causes. The feeling of isolation leads to a reduction in physical activity and then muscle strength. To avoid fall, the elderly reduced its output and displacement. This attitude causes a lack of contact with the outside world, a weakening of social life and a strong dependence on family members or home health aides. The use of the existing rehabilitation technique in order to recover lost motor abilities is painful boring and it requires moving into a rehabilitation center which is very difﬁcult of elderly [14]. The emergence of innovative motion capture technology has revolutionized home monitoring. Kinect is a motion sensor. It allows person detection, tracking and recognition. It has transformed the use of video games in a revolutionary way. The interaction with the game does not need joystick; it simply needs gesture recognition. It motivates players to execute physical activities while playing. This activity has inspired researchers to model therapeutic serious game. In the literature, a considerable number of projects demonstrate an encouraging impact among elderly. This increased their enthusiasm and devotion to rehabilitation [15].  
  Kinect  
  To determine the most appropriate dispositive, we studied various motion sensors. Low-cost range sensors are very interesting. They are imposed in industry and several other domains. For this reason, we choose the Kinect sensor, this choice was made for its different advantages. Initially, Kinect is designed to control video games while allowing human machine interaction without markers or joystick. This is a simple and easy natural interaction where the body is simply used to interact with machine. Kinect allows the acquisition of RGB video, depth map and sound through its libraries supplied with the software kit. It includes an infra-red sensor, two cameras RGB and depth also called 3D camera which allows 3D motion capture and a microphone. The Fig. 1 explains the different compositions of Kinect.  
  a. Kinect sensor  
  This simple and powerful input device enables gesture recognition. Motion capture is based on processing the depth map. The ﬁeld of use of the Kinect is expanded and affects many other areas that play such as education, sport and medicine. The depth map permits to measure the distance between an object and the camera. The production of this card “depth map” is based on the principle of structured light. The projection of a light pattern allows having the information about its deformation when it encountered an obstacle in the scene. The infrared projector is shifted to the infrared camera. This enables it to detect the deformation of the pattern by comparing it to a reference pattern. The light pattern is a projected at a known distance from the Kinect sensor. Kinect uses structured light with the only difference that it is not light band but points of light. Indeed, a speckled pattern of dots that are projected on a scene using an infrared projector, and detected by an infrared camera. Each IR point has a unique peripheral zone so it allows identifying each point when projected on a scene. IR speckles projected by the Kinect have three sizes that are optimized for use in different depth ranges. This phenomenon is shown in Fig. 2.  
  Fig. 2. Light grid points created by the projector.  
  Methodology  
  The habitual methodology for the physical rehabilitation system using virtual reality is based on Cloning gestures of avatar without any assessment or correction. The contribution of our serious game is that not only it detects patient movement but also compares them with the movements of the avatar coach to check in late the validity of the gesture. Thus, the assessment of performance is flexibly and automatically, helping the patient to know if it is really progressing. The proposed exercise. One of the main causes of the fall is anomalies of posture and gait which is due as the decline of the capacity in the musculoskeletal system. Many studies examine the impacts of exercise to increase motilities, improve balance, and reduce risk of falls for elderly [19]. For this, we proposed an exercise to improve the patient’s posture. Good posture is an easy and important way to keep a healthy body (see Fig. 3). If a patient has a problem with balance, a right posture can help him to prevent expected fall. The principle is to do exercises that strengthen the upper back and shoulder muscles to maintain good posture. It does not need to have an athlete physique, the most important is to create a “muscle memory” to keep power naturally and unconsciously correct posture without fatigue.  
  Fig. 3. The proposed exercise is good for posture.  
  mainly of 2 windows (see Fig. 12). The ﬁrst window is dedicated to the patient while the second is reserved for Avatar. Each window allows displaying two scenes. The user window displays rendering RGB camera and skeleton of the same patient. The second window enables to display the RGB and avatar movement. Each window also displays the angle made by the movement of arm. Including the two angles values of the arm interface to validate or reject gesture made by patient.  
  Fig. 12. The proposed interface of exergame using occupational therapy.  
  The effectiveness of our serious games can be translated through several facets. Trying to solve the difﬁculty of the games, the elderly is obliged to make several attempts to succeed. It results in very positive effect distinguished as psychological facets. The increase of motivation, self-efﬁcacy, and enjoyment are highlighted. Depending on the competence and scoring, the game increases the sensory motor skills, posture, balance and gait. These are positive physical facets. Physiological facets can be explained as improving performance of the cardiovascular, cardio-respiratory or immune system and also re-establishing the neural plasticity. As a result, occupational therapy using serious games doesn’t simply mean serious purpose and motivation. Rather, it offers options for a new kind of prevention and rehabilitation with special emphasis on physical, psychological effects.  
  5 Discussion and Conclusion Falls is very dangerous phenomena. It is devastating and it increases the risk of mortality certainly for elderly. In this paper, we propose serious game speciﬁc to realize occupational therapy at home adapted to the limited abilities for elderly. This  
  Fig. 1. Our user-centered design approach  
  For the creation of solutions, the C and D phases of the process, there are many possibilities. The most common is to use low-ﬁdelity prototypes. These are produced as in our case by us designers from the ideas generated collectively. They are used to present to users solutions to evaluate, validate or refute concepts or interactions, and to choose or propose new ideas. For the realization of these prototypes the designer has the choice between several methods. These are often based on the use of visual content. Rettig [14] and Snyder [15] show, for example, the use of paper prototyping, in which manipulative and discussion interfaces are prepared in the form of drawings or collages. The “Wizard of Oz” experiment [16] proposes to simulate the interactive functioning of the ﬁnal prototype. This methodology is often based on such a visual paper mock-up. Serrano [17] proposes with “Open Wizard” a software solution of magician of Oz for multimodal systems. It allows to simulate input modalities but does not allow the simulation of the output modalities. An alternative to the Wizard of Oz is to code low-ﬁdelity prototypes. According to Sefelin [18], the results achieved with these prototypes are equivalent to those obtained with paper models. In addition, the interviews conducted at the end of the tests comparing paper models and software prototypes reveal that 22 of the 24 subjects say they prefer working with software prototypes. New technologies like Adobe Flash or MS Silverlight make it easy to create low-ﬁdelity prototypes.  
  An Assistance Tool to Design Interoperable Components  
  We coded a low-ﬁdelity prototype which addressed two very interesting points: 1. Adequacy between proposed functionalities and user needs 2. Adequacy between the interface and the users Designers working in the neOCampus project mostly use COTS (commercial offthe-shelf) simulation software [3] to build and test their simulation models. Integrating these models to form a single meta-model is a major issue especially when distributed simulation technologies are not anchored in these software [6]. We have seen that because of the lack of communication and collaboration between these different designers, their models were built completely in a disconnected way and do not beneﬁt from the exchange of information that can simplify and accelerate their work. Beyond these practices, another problem identiﬁed concerns the difﬁculty of being able to generate the co-simulation components (essentially components implementing the FMI) from the different simulators used. Indeed, the majority of designers are experts in their ﬁeld but 1/are not necessarily computer experts and 2/if they are, not often expert in co-simulation nor in FMI. This time of taking over the FMI technology (time and practice) has led us to propose a mechanism to support the generation of FMU components based on user practices. It’s an interface that is intended to be easy to use for novice users, adapted and allows to accompany the process of generation of components ready to connect to our platform (cf. Fig. 2).  
  Fig. 2. Overview of the FMI component generation process  
  Y. Motie et al.  
  and simulation step is done separately by each designer, where the model has to be set up and tested against its speciﬁcations ﬁrst. Standalone executables may be launched, traced, and debugged using additional tools like an IDE (Integrated Development Environment). The next step in the FMU generation process is to determine the interface of the simulation model, which is later exposed through the FMI. This consists of the deﬁnition of input and output quantities, or states, as well as internal timing (accuracy and precision required by the simulation model) and external timing (simulation step size for data exchange considerations. These informations are gathered with the information about the FMUs architecture in a modelDescription.xml ﬁle, which is connected to the software code using functions usually provided by the FMU SDK (software development kit) [11]. The parts of an FMU are put together after being compiled. A zip ﬁle is shaped and the “.dll ﬁle” is put in the binaries folder for the interrelated platform. The modelDescription.xml ﬁle is set in the root folder. It is then a deliberate choice to put the model source ﬁles in the source folder. We ﬁrst analyzed the information about an FMU stored in the modelDescription. xml ﬁle. For example, the latter contains elements like ModelVariables deﬁning all the variables of the FMU that are visible/accessible via the FMU functions. This inventory made it possible to extract the minimum necessary and sufﬁcient variables for a simulation in order to generate as easily as possible the FMU component allowing cosimulation between systems. Then, we built a task model to understand the steps to be performed, in order to build an FMU component, and proposed several models of which (cf. Fig. 3).  
  Fig. 3. Low-ﬁdelity models implementing the scenario  
  Author Index  
