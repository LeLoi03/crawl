Image Analysis and Processing – ICIAP 2023 | springerprofessional.de  Skip to main content    Menü   Fachgebiete Chevron down icon     Chevron up icon        Automobil + Motoren    Bauwesen + Immobilien    Business IT + Informatik    Elektrotechnik + Elektronik    Energie + Nachhaltigkeit    Finance + Banking    Management + Führung    Marketing + Vertrieb    Maschinenbau + Werkstoffe    Versicherung + Risiko      
 DE     
 nach oben    
 2023 | Buch  
 Kapitel lesen  Erstes Kapitel lesen     
 Image Analysis and Processing – ICIAP 2023  
 22nd International Conference, ICIAP 2023, Udine, Italy, September 11–15, 2023, Proceedings, Part I  
 herausgegeben von: Gian Luca Foresti, Andrea Fusiello, Edwin Hancock   
 Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi   
 Shallow Camera Pipeline for Night Photography Enhancement  
 Target-Driven One-Shot Unsupervised Domain Adaptation  
  In this paper, we introduce a novel framework for the challenging problem of One-Shot Unsupervised Domain Adaptation (OS-UDA), which aims to adapt to a target domain with only a single unlabeled target sample. Unlike existing approaches that rely on large labeled source and unlabeled target data, our Target-Driven One-Shot UDA (TOS-UDA) approach employs a learnable augmentation strategy guided by the target sample’s style to align the source distribution with the target distribution. Our method consists of three modules: an augmentation module, a style alignment module, and a classifier. Unlike existing methods, our augmentation module allows for strong transformations of the source samples, and the style of the single target sample available is exploited to guide the augmentation by ensuring perceptual similarity. Furthermore, our approach integrates augmentation with style alignment, eliminating the need for separate pre-training on additional datasets. Our method outperforms or performs comparably to existing OS-UDA methods on the Digits and DomainNet benchmarks.  
 Julio Ivan Davila Carrazco, Suvarna Kishorkumar Kadam, Pietro Morerio, Alessio Del Bue, Vittorio Murino   
 Combining Identity Features and Artifact Analysis for Differential Morphing Attack Detection  
  Due to the importance of the Morphing Attack, the development of new and accurate Morphing Attack Detection (MAD) systems is urgently needed by private and public institutions. In this context, D-MAD methods, i.e. detectors fed with a trusted live image and a probe tend to show better performance with respect to S-MAD approaches, that are based on a single input image. However, D-MAD methods usually leverage the identity of the two input face images only, and then present two main drawbacks: they lose performance when the two subjects look alike, and they do not consider potential artifacts left by the morphing procedure (which are instead typically exploited by S-MAD approaches). Therefore, in this paper, we investigate the combined use of D-MAD and S-MAD to improve detection performance through the fusion of the features produced by these two MAD approaches.  
 Nicolò Di Domenico, Guido Borghi, Annalisa Franco, Davide Maltoni   
 SynthCap: Augmenting Transformers with Synthetic Data for Image Captioning  
  Image captioning is a challenging task that combines Computer Vision and Natural Language Processing to generate descriptive and accurate textual descriptions for input images. Research efforts in this field mainly focus on developing novel architectural components to extend image captioning models and using large-scale image-text datasets crawled from the web to boost final performance. In this work, we explore an alternative to web-crawled data and augment the training dataset with synthetic images generated by a latent diffusion model. In particular, we propose a simple yet effective synthetic data augmentation framework that is capable of significantly improving the quality of captions generated by a standard Transformer-based model, leading to competitive results on the COCO dataset.  
 Davide Caffagni, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara   
 FERMOUTH: Facial Emotion Recognition from the MOUTH Region  
  People use various nonverbal communicative channels to convey emotions, among which facial expressions are considered the most important ones. Consequently, automatic Facial Expression Recognition (FER) is a crucial task for enhancing computers’ perceptive abilities, particularly in human-computer interaction. Although state-of-the-art FER systems can identify emotions from the entire face, situations may arise where occlusions prevent the entire face from being visible. During the COVID-19 pandemic, many FER systems have been developed for recognizing emotions from the eye region due to the obligation to wear a mask. However, in many situations, the eyes may be covered, for instance, by sunglasses or virtual reality devices. In this paper, we faced the problem of developing a FER system that solely considers the mouth region and classifies emotions using only the lower part of the face. We tested the effectiveness of this FER system in recognizing emotions from the lower part of the face and compared the results to a FER system trained on the same datasets using the same approach on the entire face. As expected, emotions primarily associated with the mouth region (e.g., happiness, surprise) were recognized with minimal loss compared to the entire face. Nevertheless, even though most negative emotions were not accurately detected using only the mouth region, in cases where the face is partially covered, this area may still provide some information about the displayed emotion.  
 Berardina De Carolis, Nicola Macchiarulo, Giuseppe Palestra, Alberto Pio De Matteis, Andrea Lippolis   
 Towards Facial Expression Robustness in Multi-scale Wild Environments  
  Facial expressions are dynamic processes that evolve over temporal segments, including onset, apex, offset, and neutral. However, previous works on automatic facial expression analysis have mainly focused on the recognition of discrete emotions, neglecting the continuous nature of these processes. Additionally, facial images captured from videos in the wild often have varying resolutions due to fixed-lens cameras. To address these problems, our objective is to develop a robust facial expression recognition classifier that provides good performance in such challenging environments. We evaluated several state-of-the-art models on labeled and unlabeled collections and analyzed their performance at different scales. To improve performance, we filtered the probabilities provided by each classifier and demonstrated that this improves decision-making consistency by more than 10%, leading to accuracy improvement. Finally, we combined the models’ backbones into a temporal-sequence classifier, leveraging this consistency-performance trade-off and achieving an additional improvement of 9.6%.  
 David Freire-Obregón, Daniel Hernández-Sosa, Oliverio J. Santana, Javier Lorenzo-Navarro, Modesto Castrillón-Santana   
 Depth Camera Face Recognition by Normalized Fractal Encodings  
 Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati   
 Improved Bilinear Pooling for Real-Time Pose Event Camera Relocalisation  
  Traditional methods for estimating camera pose have been replaced by more advanced camera relocalization methods that utilize both CNNs and LSTMs in the field of simultaneous localization and mapping. However, the reliance on LSTM layers in these methods can lead to overfitting and slow convergence. In this paper, a novel approach for estimating the six degree of freedom (6DOF) pose of an event camera using deep learning is presented. Our method begins by preprocessing the events captured by the event camera to generate a set of images. These images are then passed through two CNNs to extract relevant features. These features are multiplied using an outer product and aggregated across different regions of the image after adding L2 normalization to normalize the combining vector. The final step of the model is a regression layer that predicts the position and orientation of the event camera. The effectiveness of this approach has been tested on various datasets, and the results demonstrate its superiority compared to existing state-of-the-art methods.  
 Ahmed Tabia, Fabien Bonardi, Samia Bouchafa   
 A Large-scale Analysis of Athletes’ Cumulative Race Time in Running Events  
  Action recognition models and cumulative race time (CRT) are practical tools in sports analytics, providing insights into athlete performance, training, and strategy. Measuring CRT allows for identifying areas for improvement, such as specific sections of a racecourse or the effectiveness of different strategies. Human action recognition (HAR) algorithms can help to optimize performance, with machine learning and artificial intelligence providing real-time feedback to athletes. This paper presents a comparative study of HAR algorithms for CRT regression, examining two important factors: the frame rate and the regressor selection. Our results indicate that our proposal exhibits outstanding performance for short input footage, achieving a mean absolute error of 11 min when estimating CRT for runners that have been on the course for durations ranging from 8 to 20 h.  
 David Freire-Obregón, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana   
 Active Class Selection for Dataset Acquisition in Sign Language Recognition  
  Dataset collection for Sign Language Recognition (SLR) represents a challenging and crucial step in the development of modern automatic SLR systems. Typical acquisition protocols do not follow specific strategies, simply trying to gather equally represented classes. In this paper we provide some empirical evidences that alternative, more clever, strategies can be really beneficial, leading to a better performance of classification systems. In particular, we investigate the exploitation of ideas and tools of Active Class Selection (ACS), a peculiar Active Learning (AL) context specifically devoted to scenarios in which new data is labelled at the same time it is generated. In particular, differently from standard AL where a strategy asks for a specific label from an available set of unlabelled data, ACS strategies define from which class it is more convenient to acquire a new sample. In this paper, we show the beneficial effect of these methods in the SLR scenario, where these concepts have never been investigated. We studied both standard and novel ACS approaches, with experiments based on a challenging dataset recently collected for an ECCV challenge. We also preliminary investigate other possible exploitations of ACS ideas, for example to select which would be, for the classification system, the most beneficial signer.  
 Manuele Bicego, Manuel Vázquez-Enríquez, José L. Alba-Castro   
 MC-GTA: A Synthetic Benchmark for Multi-Camera Vehicle Tracking  
 Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation  
  This paper presents a novel approach for generating 3D talking heads from raw audio inputs. Our method grounds on the idea that speech related movements can be comprehensively and efficiently described by the motion of a few control points located on the movable parts of the face, i.e., landmarks. The underlying musculoskeletal structure then allows us to learn how their motion influences the geometrical deformations of the whole face. The proposed method employs two distinct models to this aim: the first one learns to generate the motion of a sparse set of landmarks from the given audio. The second model expands such landmarks motion to a dense motion field, which is utilized to animate a given 3D mesh in neutral state. Additionally, we introduce a novel loss function, named Cosine Loss, which minimizes the angle between the generated motion vectors and the ground truth ones. Using landmarks in 3D talking head generation offers various advantages such as consistency, reliability, and obviating the need for manual-annotation. Our approach is designed to be identity-agnostic, enabling high-quality facial animations for any users without additional data or training. Code and models are available at: S2L+S2D .  
 Federico Nocentini, Claudio Ferrari, Stefano Berretti   
 Benchmarking of Blind Video Deblurring Methods on Long Exposure and Resource Poor Settings  
  This paper presents a benchmark evaluation of blind video deblurring methods in specific challenging settings. The employed videos are affected by severe deblurring artifacts and acquisition conditions (e.g., low resolution, high exposure, camera motion, complex scene motion, etc.). An in depth state of the art investigation has been carried out. Then, a specific set of methods based on mathematical optimization with image priors has been involved in our benchmark evaluation. The selected methods have been evaluated quantitatively and qualitatively.  
 Maria Ausilia Napoli Spatafora, Massimo O. Spata, Luca Guarnera, Alessandro Ortis, Sebastiano Battiato   
 LieToMe: An LSTM-Based Method for Deception Detection by Hand Movements  
  The ability to detect lies is a crucial skill in essential situations like police interrogations and court trials. At present, several devices, such as polygraphs and magnetic resonance, can ease the deception detection task. However, the effectiveness of these tools can be compromised by intentional behavioral changes due to the subject awareness of such appliances, suggesting that alternative ways must be explored to detect lies without using physical devices. In this context, this paper presents an approach focused on the extraction of meaningful features from hand gestures. The latter provide cues on the person’s behavior and are used to address the deception detection task in RGB videos of trials. Specifically, the proposed system extracts hands skeletons from an RGB video sequence and generates novel handcrafted features from the extrapolated keypoints to reflect the subject behavior through hand movements. Then, a long short-term memory (LSTM) neural network is used to classify these features and estimate whether the person is lying or not. Extensive experiments were performed to assess the quality of the derived features on a public collection of famous real-life trials. On this dataset, the proposed system sets new state-of-the-art performance on the unimodal hand-gesture deception detection task, demonstrating the effectiveness of the proposed approach and its handcrafted features.  
 Danilo Avola, Luigi Cinque, Maria De Marsico, Angelo Di Mambro, Alessio Fagioli, Gian Luca Foresti, Romeo Lanzino, Francesco Scarcello   
 Real-Time GAN-Based Model for Underwater Image Enhancement  
  Enhancing image quality is crucial for achieving an accurate and reliable image analysis in vision-based automated tasks. Underwater imaging encounters several challenges that can negatively impact image quality, including limited visibility, color distortion, contrast sensitivity issues, and blurriness. Among these, depending on how the water filters out the different light colors at different depths, the color distortion results in a loss of color information and a blue or green tint to the overall image, making it difficult to identify different underwater organisms or structures accurately. Improved underwater image quality can be crucial in marine biology, oceanography, and oceanic exploration. Therefore, this paper proposes a novel Generative Adversarial Network (GAN) architecture for underwater image enhancement, restoring good perceptual quality to obtain a more precise and detailed image. The effectiveness of the proposed method is evaluated on the EUVP dataset, which comprises underwater image samples of various visibility conditions, achieving remarkable results. Moreover, the trained network is run on the RPi4B as an embedded system to measure the time required to enhance the images with limited computational resources, simulating a practical underwater investigation setting. The outcome demonstrates the presented method applicability in real-world underwater exploration scenarios.  
 Danilo Avola, Irene Cannistraci, Marco Cascio, Luigi Cinque, Anxhelo Diko, Damiano Distante, Gian Luca Foresti, Alessio Mecca, Ivan Scagnetto   
 HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains  
  We present HERO, an artificial assistant designed to communicate with users with both natural language and images to aid them carrying out procedures in industrial contexts. Our system is composed of five modules: 1) the input module retrieves user utterances and collects raw data, such as text and images, 2) the Natural Language Processing module processes text from user utterances, 3) the object detector module extracts entities by analyzing images captured by the user, 4) the Question Answering module generates responses to users’ specific questions on procedures, and 5) the output module selects the final response to give to the user. We deployed and evaluated the system in an industrial laboratory furnished with different tools and equipment for carrying out repair and test operations on electrical boards. In this setting, the HERO system allows the user to retrieve information on tools, equipment, procedures, and safety rules. Experiments on domain-specific labeled data, as well as a user study suggest that the design of our system is robust and that its use can be beneficial for users over classic methods for retrieving information and guide workers, such as printed manuals.  
 Claudia Bonanno, Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella   
 A Computer Vision-Based Water Level Monitoring System for Touchless and Sustainable Water Dispensing  
  In recent years, the need for contactless and sustainable systems has become increasingly relevant. The traditional water dispensers, which require contact with the dispenser and often involve single-use plastic cups or bottles, are not only unhygienic but also contribute to environmental pollution. This paper presents a touchless water dispenser system that uses artificial intelligence (AI) to control the dispensing of water or any liquid beverage. The system is designed to fill a container under the nozzle, dispense water when the container is aligned with the flow, and stop dispensing when the container is full, all without requiring any physical contact. This approach ensures compliance with hygiene regulations and promotes environmental sustainability by eliminating the need for plastic bottles or cups, making it a “plastic-free” and “zero waste” system. The prototype is based on a computer vision approach that employs an RGB camera and a Raspberry Pi board, which allows for real-time image processing and machine learning operations. The system uses image processing techniques to detect the presence of a container under the nozzle and then utilizes AI algorithms to control the flow of liquid. The system is trained using machine learning models and optimized to ensure accuracy and efficiency. We discuss the development and implementation of the touchless water dispenser system, including the hardware and software components used, the algorithms employed, and the testing and evaluation of the system. The results of our experiments show that the touchless water dispenser system is highly accurate and efficient, and it offers a safe and sustainable alternative to traditional water dispensers. The system has the potential to be used in a variety of settings, including public spaces, hospitals, schools, and offices, where hygiene and sustainability are of utmost importance.  
 Andrea Felicetti, Marina Paolanti, Rocco Pietrini, Adriano Mancini, Primo Zingaretti, Emanuele Frontoni   
 Hand Gesture Recognition Exploiting Handcrafted Features and LSTM  
  Hand gesture recognition finds application in several heterogeneous fields, such as Human-Computer Interaction, serious games, sign language interpretation, and more. Modern recognition approaches use Deep Learning methods due to their ability in extracting features without human intervention. The drawback of this approach is the need for huge datasets which, depending on the task, are not always available. In some cases, handcrafted features increase the capability of a model in achieving the proposed task, and usually require fewer data with respect to Deep Learning approaches. In this paper, we propose a method that synergistically makes use of handcrafted features and Deep Learning for performing hand gesture recognition. Concerning the features, they are engineered from hand joints, while for Deep Learning, a simple LSTM together with a multilayer perceptron is used. The tests were performed on the DHG dataset, comparing the proposed method with both state-of-the-art methods that use handcrafted features and methods that use learned features. Our approach overcomes the state-of-the-art handcrafted features methods in both 14 and 28 gestures recognition tests, while we overcome the state-of-the-art learned features methods for the 14 gesture recognition test, proving that it is possible to use a simpler model with well engineered features.  
 Danilo Avola, Luigi Cinque, Emad Emam, Federico Fontana, Gian Luca Foresti, Marco Raoul Marini, Daniele Pannone   
 An Optimized Pipeline for Image-Based Localization in Museums from Egocentric Images  
  With the increasing interest in augmented and virtual reality, visual localization is acquiring a key role in many downstream applications requiring a real-time estimate of the user location only from visual streams. In this paper, we propose an optimized hierarchical localization pipeline by specifically tackling cultural heritage sites with specific applications in museums. Specifically, we propose to enhance the Structure from Motion (SfM) pipeline for constructing the sparse 3D point cloud by a-priori filtering blurred and near-duplicated images. We also study an improved inference pipeline that merges similarity-based localization with geometric pose estimation to effectively mitigate the effect of strong outliers. We show that the proposed optimized pipeline obtains the lowest localization error on the challenging Bellomo dataset [11]. Our proposed approach keeps both build and inference times bounded, in turn enabling the deployment of this pipeline in real-world scenarios.  
 Nicola Messina, Fabrizio Falchi, Antonino Furnari, Claudio Gennaro, Giovanni Maria Farinella   
 Annotating the Inferior Alveolar Canal: The Ultimate Tool  
  The Inferior Alveolar Nerve (IAN) is of main interest in the maxillofacial field, as an accurate localization of such nerve reduces the risks of injury during surgical procedures. Although recent literature has focused on developing novel deep learning techniques to produce accurate segmentation masks of the canal containing the IAN, there are still strong limitations due to the scarce amount of publicly available 3D maxillofacial datasets. In this paper, we present an improved version of a previously released tool, iacat (Inferior Alveolar Canal Annotation Tool), today used by medical experts to produce 3D ground truth annotation. In addition, we release a new dataset, ToothFairy, which is part of the homonymous MICCAI2023 challenge hosted by the Grand-Challenge platform, as an extension of the previously released Maxillo dataset, which was the only publicly available. With ToothFairy, the number of annotations has been increased as well as the quality of existing data.  
 Luca Lumetti, Vittorio Pipoli, Federico Bolelli, Costantino Grana   
 Enhancing PFI Prediction with GDS-MIL: A Graph-Based Dual Stream MIL Approach  
  Whole-Slide Images (WSI) are emerging as a promising resource for studying biological tissues, demonstrating a great potential in aiding cancer diagnosis and improving patient treatment. However, the manual pixel-level annotation of WSIs is extremely time-consuming and practically unfeasible in real-world scenarios. Multi-Instance Learning (MIL) have gained attention as a weakly supervised approach able to address lack of annotation tasks. MIL models aggregate patches (e.g., cropping of a WSI) into bag-level representations (e.g., WSI label), but neglect spatial information of the WSIs, crucial for histological analysis. In the High-Grade Serous Ovarian Cancer (HGSOC) context, spatial information is essential to predict a prognosis indicator (the Platinum-Free Interval, PFI) from WSIs. Such a prediction would bring highly valuable insights both for patient treatment and prognosis of chemotherapy resistance. Indeed, NeoAdjuvant ChemoTherapy (NACT) induces changes in tumor tissue morphology and composition, making the prediction of PFI from WSIs extremely challenging. In this paper, we propose GDS-MIL, a method that integrates a state-of-the-art MIL model with a Graph ATtention layer (GAT in short) to inject a local context into each instance before MIL aggregation. Our approach achieves a significant improvement in accuracy on the “Ome18” PFI dataset. In summary, this paper presents a novel solution for enhancing PFI prediction in HGSOC, with the potential of significantly improving treatment decisions and patient outcomes.  
 Gianpaolo Bontempo, Nicola Bartolini, Marta Lovino, Federico Bolelli, Anni Virtanen, Elisa Ficarra   
 Metadaten   
 Titel  Image Analysis and Processing – ICIAP 2023    
 herausgegeben von  Gian Luca Foresti  
  Andrea Fusiello  
  Edwin Hancock  
 Copyright-Jahr  2023    
 Verlag  Springer Nature Switzerland     
