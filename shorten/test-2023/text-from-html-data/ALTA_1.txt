  Github 
 Australasian Language Technology Association Workshop (2023)  
 Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association | 24 papers 
   Catching Misdiagnosed Limb Fractures in the Emergency Department Using Cross-institution Transfer Learning    
  Filip Rusak  | Bevan Koopman  | Nathan J. Brown  | Kevin Chu  | Jinghui Liu  | Anthony Nguyen    
 We investigated the development of a Machine Learning (ML)-based classifier to identify abnormalities in radiology reports from Emergency Departments (EDs) that can help automate the radiology report reconciliation process. Often, radiology reports become available to the ED only after the patient has been treated and discharged, following ED clinician interpretation of the X-ray. However, occasionally ED clinicians misdiagnose or fail to detect subtle abnormalities on X-rays, so they conduct a manual radiology report reconciliation process as a safety net. Previous studies addressed this problem of automated reconciliation using ML-based classification solutions that require data samples from the target institution that is heavily based on feature engineering, implying lower transferability between hospitals. In this paper, we investigated the benefits of using pre-trained BERT models for abnormality classification in a cross-institutional setting where data for fine-tuning was unavailable from the target institution. We also examined how the inclusion of synthetically generated radiology reports from ChatGPT affected the performance of the BERT models. Our findings suggest that BERT-like models outperform previously proposed ML-based methods in cross-institutional scenarios, and that adding ChatGPT-generated labelled radiology reports can improve the classifier’s performance by reducing the number of misdiagnosed discharged patients.   
 pdf  bib  abs   
   Chat Disentanglement: Data for New Domains and Methods for More Accurate Annotation    
  Sai R. Gouravajhala  | Andrew M. Vernier  | Yiming Shi  | Zihan Li  | Mark S. Ackerman  | Jonathan K. Kummerfeld    
 Conversation disentanglement is the task of taking a log of intertwined conversations from a shared channel and breaking the log into individual conversations. The standard datasets for disentanglement are in a single domain and were annotated by linguistics experts with careful training for the task. In this paper, we introduce the first multi-domain dataset and a study of annotation by people without linguistics expertise or extensive training. We experiment with several variations in interfaces, conducting user studies with domain experts and crowd workers. We also test a hypothesis from prior work that link-based annotation is more accurate, finding that it actually has comparable accuracy to set-based annotation. Our new dataset will support the development of more useful systems for this task, and our experimental findings suggest that users are capable of improving the usefulness of these systems by accurately annotating their own data.   
 pdf  bib  abs   
   Enhancing Bacterial Infection Prediction in Critically Ill Patients by Integrating Clinical Text    
  Jinghui Liu  | Anthony Nguyen    
 Bacterial infection (BI) is an important clinical condition and is related to many diseases that are difficult to treat. Early prediction of BI can lead to better treatment and appropriate use of antimicrobial medications. In this paper, we study a variety of NLP models to predict BI for critically ill patients and compare them with a strong baseline based on clinical measurements. We find that choosing the proper text-based model to combine with measurements can lead to substantial improvements. Our results show the value of clinical text in predicting and managing BI. We also find that the NLP model developed using patients with BI can be transferred to the more general patient cohort for patient risk prediction.   
 pdf  bib  abs   
   Predicting Empathic Accuracy from User-Designer Interviews    
  Steven Nguyen  | Daniel Beck  | Katja Holtta-Otto    
 Measuring empathy as a natural language processing task has often been limited to a subjective measure of how well individuals respond to each other in emotive situations. Cognitive empathy, or an individual’s ability to accurately assess another individual’s thoughts, remains a more novel task. In this paper, we explore natural language processing techniques to measure cognitive empathy using paired sentence data from design interviews. Our findings show that an unsupervised approach based on similarity of vectors from a Large Language Model is surprisingly promising, while adding supervision does not necessarily improve the performance. An analysis of the results highlights potential reasons for this behaviour and gives directions for future work in this space.   
 pdf  bib  abs   
   CRF  -based recognition of invasive fungal infection concepts in CHIFIR  clinical reports    
  Yang Meng  | Vlada Rozova  | Karin Verspoor    
 Named entity recognition (NER) in clinical documentation is often hindered by the use of highly specialised terminology, variation in language used to express medical findings and general scarcity of high-quality data available for training. This short paper compares a Conditional Random Fields model to the previously established dictionary-based approach and evaluates its ability to extract information from a small corpus of annotated pathology reports. The results suggest that including token descriptors as well as contextual features significantly improves precision on several concept categories while maintaining the same level of recall.   
 pdf  bib  abs   
 pdf  bib  abs   
   Overview of the 2023 ALTA  Shared Task: Discriminate between Human-Written and Machine-Generated Text    
  Diego Molla  | Haolan Zhan  | Xuanli He  | Qiongkai Xu    
   Automatic Detection of Machine-Generated Text Using Pre-Trained Language Models    
  Yunhao Fang    
 In this paper, I provide a detailed description of my approach to tackling the ALTA 2023 shared task whose objective is to build an automatic detection system to distinguish between humanauthored text and text generated from Large Language Models. By leveraging several pretrained language models through model finetuning as well as the multi-model ensemble, the system managed to achieve second place on the test set leaderboard in the competition.   
 pdf  bib  abs   
   Feature-Level Ensemble Learning for Robust Synthetic Text Detection with D  e BERT  a V  3 and XLM  - R  o BERT  a    
  Saman Sarker Joy  | Tanusree Das Aishi    
 As large language models, or LLMs, continue to advance in recent years, they require the development of a potent system to detect whether a text was created by a human or an LLM in order to prevent the unethical use of LLMs. To address this challenge, ALTA Shared Task 2023 introduced a task to build an automatic detection system that can discriminate between human-authored and synthetic text generated by LLMs. In this paper, we present our participation in this task where we proposed a feature-level ensemble of two transformer models namely DeBERTaV3 and XLM-RoBERTa to come up with a robust system. The given dataset consisted of textual data with two labels where the task was binary classification. Experimental results show that our proposed method achieved competitive performance among the participants. We believe this solution would make an impact and provide a feasible solution for detection of synthetic text detection.   
 pdf  bib  abs   
