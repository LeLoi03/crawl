Image Analysis and Processing - ICIAP 2023 Workshops | springerprofessional.de  Skip to main content    Menu   Disciplines Chevron down icon     Chevron up icon        Automotive    Business IT + Informatics    Construction + Real Estate    Electrical Engineering + Electronics    Energy + Sustainability    Insurance + Risk    Finance + Banking    Management + Leadership    Marketing + Sales    Mechanical Engineering + Materials      
 Events       
 Read chapter  Read first chapter     
 Image Analysis and Processing - ICIAP 2023 Workshops  
 Udine, Italy, September 11–15, 2023, Proceedings, Part I  
 Editors: Gian Luca Foresti, Andrea Fusiello, Edwin Hancock   
 On-Device Learning with Binary Neural Networks  
  Existing Continual Learning (CL) solutions only partially address the constraints on power, memory and computation of the deep learning models when deployed on low-power embedded CPUs. In this paper, we propose a CL solution that embraces the recent advancements in CL field and the efficiency of the Binary Neural Networks (BNN), that use 1-bit for weights and activations to efficiently execute deep learning models. We propose a hybrid quantization of CWR* (an effective CL approach) that considers differently forward and backward pass in order to retain more precision during gradient update step and at the same time minimizing the latency overhead. The choice of a binary network as backbone is essential to meet the constraints of low power devices and, to the best of authors’ knowledge, this is the first attempt to prove on-device learning with BNN. The experimental validation carried out confirms the validity and the suitability of the proposed method.  
 Lorenzo Vorabbi, Davide Maltoni, Stefano Santi   
 Towards One-Shot PCB Component Detection with YOLO  
  Consumer electronic devices such as smartphones, TV sets, etc. are designed around printed circuit boards (PCBs) with a large number of surface mounted components. The pick and place machine soldering these components on the PCB may pick the wrong component, may solder the component in the wrong position or fail to solder it at all. Therefore, Automated Optical Inspection (AOI) is essential to detect the above defects even prior to electric tests by comparing populated PCBs with the schematics. In this context, we leverage YOLO, a deep convolutional architecture designed for one-shot object detection, for AOI of PCBs. This architecture enables real-time processing of large images and can be trained end-to-end. In this work we also exploit a modified architecture of YOLOv5 designed to detect small components of which boards are often highly populated. Moreover, we proposed a strategy to transfer weights from the original pre-trained model to this improved one. We report here our experimental setup and some performance measures.  
 Gabriele Spadaro, Gaspare Vetrano, Barbara Penna, Antonio Serena, Attilio Fiandrotti   
 Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data  
  Automated organ at risk (OAR) segmentation is crucial for radiation therapy planning in CT scans, but the generated contours by automated models can be inaccurate, potentially leading to treatment planning issues. The reasons for these inaccuracies could be varied, such as unclear organ boundaries or inaccurate ground truth due to annotation errors. To improve the model’s performance, it is necessary to identify these failure cases during the training process and to correct them with some potential post-processing techniques. However, this process can be time-consuming, as traditionally it requires manual inspection of the predicted output. This paper proposes a method to automatically identify failure cases by setting a threshold for the combination of Dice and Hausdorff distances. This approach reduces the time-consuming task of visually inspecting predicted outputs, allowing for faster identification of failure case candidates. The method was evaluated on 20 cases of six different organs in CT images from clinical expert curated datasets. By setting the thresholds for the Dice and Hausdorff distances, the study was able to differentiate between various states of failure cases and evaluate over 12 cases visually. This thresholding approach could be extended to other organs, leading to faster identification of failure cases and thereby improving the quality of radiation therapy planning.  
 Amin Honarmandi Shandiz, Attila Rádics, Rajesh Tamada, Makk Árpád, Karolina Glowacka, Lehel Ferenczi, Sandeep Dutta, Michael Fanariotis   
 Digitizer: A Synthetic Dataset for Well-Log Analysis  
  Raster well-log images are digital representations of paper copies that retain the original analog data gathered during subsurface drilling. Geologists heavily rely on these images to interpret well-log curves and gain insights into the geological formations beneath the surface. However, manually extracting and analyzing data from these images is time-consuming and demanding. To tackle these challenges, researchers increasingly turn to computer vision and machine learning techniques to assist in the analysis process. Nonetheless, developing such approaches, mainly those dependent on machine learning requires a sufficient number of accurately labelled samples for model training and fine-tuning. Unfortunately, this is not a straightforward task, as existing datasets are derived from scanned hand-compiled paper copies, resulting in digital images that suffer from noise and errors. Furthermore, these samples only represent images and not the digital signals of the measured natural phenomena. To overcome these obstacles, we present a new synthetic dataset that includes both images and digital signals of well-logs. This dataset aims to facilitate more effective and accurate analysis techniques, addressing the limitations of current methods. By utilizing this dataset, researchers and practitioners can develop solutions that mitigate the shortcomings of existing methods, ultimately leading to more reliable and precise results in interpreting well-log curves and understanding subsurface geological formations.  
 M. Quamer Nasim, Narendra Patwardhan, Javed Ali, Tannistha Maiti, Stefano Marrone, Tarry Singh, Carlo Sansone   
 Abstracts Embeddings Evaluation: A Case Study of Artificial Intelligence and Medical Imaging for the COVID-19 Infection  
  During the COVID-19 pandemic, a huge amount of literature was produced covering different aspects of infection. The use of artificial intelligence (AI) in medical imaging has been shown to improve screening, diagnosis, treatment, and medication for the COVID-19 virus. Applying natural language processing (NLP) solutions to COVID-19 literature has contributed to infer significant COVID-19-related topics and correlated diseases. In this paper, we aim at evaluating biomedical transformer-based NLP techniques in COVID-19 research to understand if they are able to classify problems related to COVID-19. Particularly, once collected COVID-19 publications encompassing the terms AI and medical imaging, fifteen BERT-based models have been compared with respect to modality prediction and task prediction.  
 Giovanni Zurlo, Elisabetta Ronchieri   
 Towards a Better Understanding of Human Emotions: Challenges of Dataset Labeling  
  A major challenge in automatic human emotion recognition is that of categorizing the very broad and complex spectrum of human emotions. In this regard, a critical bottleneck is represented by the difficulty in obtaining annotated data to build such models. Indeed, all the publicly available datasets collected to this aim are either annotated with (i) the six prototypical emotions, or (ii) continuous valence/arousal (VA) values. On the one hand, the six basic emotions represent a coarse approximation of the vast spectrum of human emotions, and are of limited utility to understand a person’s emotional state. Oppositely, performing dimensional emotion recognition using VA can cover the full range of human emotions, yet it lacks a clear interpretation. Moreover, data annotation with VA is challenging as it requires expert annotators, and there is no guarantee that annotations are consistent with the six prototypical emotions. In this paper, we present an investigation aiming to bridge the gap between the two modalities. We propose to leverage VA values to obtain a fine-grained taxonomy of emotions, interpreting emotional states as probability distributions over the VA space. This has the potential for enabling automatic annotation of existing datasets with this new taxonomy, avoiding the need for expensive data collection and labeling. However, our preliminary results disclose two major problems: first, continuous VA values and the six standard emotion labels are often inconsistent, raising concerns about the validity of existing datasets; second, datasets claimed to be balanced in terms of emotion labels become instead severely unbalanced if provided with a fine-grained emotion annotation. We conclude that efforts are needed in terms of data collection to further push forward the research in this field.  
 Hajer Guerdelli, Claudio Ferrari, Joao Baptista Cardia Neto, Stefano Berretti, Walid Barhoumi, Alberto Del Bimbo   
 Frontmatter  
 ONFIRE Contest 2023: Real-Time Fire Detection on the Edge  
  ONFIRE Contest 2023 is a competition, organized within ICIAP 2023 conference, among methods based on deep learning, aimed at the recognition of fire from videos in real-time on edge devices. This topic is inspiring various research groups for the underlying security reasons and for the growing necessity to realize a system that allows to safeguard the territory from the enormous damage that fires can cause. The participants are required to design fire detection methods, starting from a training set that consists of videos in which fire (flames and/or smoke) is present (positive samples), and others (negative samples) that do not contain a fire. The videos have been collected from existing datasets by selecting as positive videos only those that really frame a fire and not flames and smoke in controlled conditions, and as negative videos the ones that contain moving objects that can be confused with flames or smoke. Since the videos are collected in different conditions, the dataset is very heterogeneous in terms of image resolution, illumination, pixel size of flame or smoke, background activity, scenario (urban or wildfire). The submitted methods are evaluated over a private test set, whose videos are different from the ones available in the training set; this choice allows to test the approaches in realistic conditions, namely in unknown operative scenarios. The proposed experimental protocol allows to measure not only the accuracy but also the computational resources required by the methods, so that the top-rank approaches will be both effective and suited for real-time processing on the edge.  
 Diego Gragnaniello, Antonio Greco, Carlo Sansone, Bruno Vento   
 Generalized Deepfake Detection Algorithm Based on Inconsistency Between Inner and Outer Faces  
  Deepfake refers to using artificial intelligence (AI) and machine learning techniques to create compelling and realistic media content, such as videos, images, or recordings, that appear real but are fake. The most common form of deepfake involves using deep neural networks to replace or superimpose faces in existing videos or images on top of other people’s faces. While this technology can be used for various benign purposes, such as filmmaking or online education, it can also be used maliciously to spread misinformation by creating fake videos or images. Based on the classic deepfake generation process, this paper explores the Inconsistency between inner and outer faces in fake content to find synthetic defects and proposes a general deepfake detection algorithm. Experimental results show that our proposed method has certain advantages, especially regarding cross-method detection performance.  
 Jie Gao, Sara Concas, Giulia Orrù, Xiaoyi Feng, Gian Luca Marcialis, Fabio Roli   
 Real-Time Multiclass Face Spoofing Recognition Through Spatiotemporal Convolutional 3D Features  
  Face recognition is used in numerous authentication applications, unfortunately they are susceptible to spoofing attacks such as paper and screen attacks. In this paper, we propose a method that is able to recognise if a face detected in a video is not real and the type of attack performed on the fake video. We propose to learn the temporal features exploiting a 3D Convolution Network that is more suitable for temporal information. The 3D ConvNet, other than summarizing temporal information, allows us to build a real-time method since it is so much more efficient to analyse clips instead of analyzing single frames. The learned features are classified using a binary classifier to distinguish if the person in the clip video is real (i.e. live) or not, multi class classifier recognises if the person is real or the type of attack (screen, paper, ect.). We performed our test on 5 public datasets: Replay Attack, Replay Mobile, MSU-MSFD, Rose-Youtu, RECOD-MPAD.  
 Salvatore Giurato, Alessandro Ortis, Sebastiano Battiato   
 Enhancing Air Quality Forecasting Through Deep Learning and Continuous Wavelet Transform  
  Air quality forecasting plays a crucial role in environmental management and public health. In this paper, we propose a novel approach that combines deep learning techniques with the Continuous Wavelet Transform (CWT) for air quality forecasting based on sensor data. The proposed methodology is agnostic to the target pollutant and can be applied to estimate any available pollutant without loss of generality. The pipeline consists of two main steps: the generation of stacked samples from raw sensor signals using CWT, and the prediction through a custom deep neural network based on the ResNet18 architecture.We compare our approach with traditional one-dimensional signal processing models. The results show that our 2D pipeline, employing the Morlet mother wavelet, outperforms the baselines significantly. The localized time-frequency representations obtained through CWT highlight hidden dynamics and relationships within the parameter behavior and external factors, leading to more accurate predictions. Overall, our approach demonstrates the potential to advance air quality forecasting and environmental management for healthier living environments worldwide.  
 Pietro Manganelli Conforti, Andrea Fanti, Pietro Nardelli, Paolo Russo   
 Automatic Alignment of Multi-scale Aerial and Underwater Photogrammetric Point Clouds: A Case Study in the Maldivian Coral Reef  
  The research question that the paper investigates is whether the usage of state of the art algorithms for point clouds registration solves the problem of multi-scale vision-based point clouds registration in mixed aerial and underwater environments. This paper reports very preliminary results on the data we have been able to procure, in the context of a coral reef restoration project nearby Magoodhoo Island (Maldives). The results obtained by exploiting state of the art algorithms are promising, considering that those data presents hard samples, in particular for their multi-scale nature (noise in captured 3D points increases with depth). However, further investigation on larger data-sets is needed to confirm the overall applicability of the current algorithms to this problem.  
 Federica Di Lauro, Luca Fallati, Simone Fontana, Alessandra Savini, Domenico G. Sorrenti   
 Generative Data Augmentation of Human Biomechanics  
  Wearable sensors are miniature and affordable devices used for monitoring human motion in daily life. Data-driven models applied to wearable sensor data can enhance the accuracy of movement analysis outside of controlled settings. However, obtaining a large and representative database for training these models is challenging due to the specialised motion laboratories and expensive equipment required. To address this limitation, this study proposes a data augmentation approach using generative deep learning to enhance biomechanical datasets. A novel conditional generative adversarial network (GAN) was developed to synthesise biomechanical data during gait. The GAN takes into account the subject’s anthropometric measures to generate data that represents specific body types as well as information about the gait cycle for reconstruction back into the time domain. The proposed model was evaluated for generating biomechanical data of unseen subjects and fine-tuning the model with small percentages (1%, 2% and 5%) of the test dataset. Researchers and practitioners can overcome the limitations of obtaining large training datasets from human participants by synthesising realistic and diverse synthetic data. This paper outlines the methodology and experimental setup for developing and evaluating the GAN and discusses its potential impact on the field of biomechanics and human motion analysis.  
 Halldór Kárason, Pierluigi Ritrovato, Nicola Maffulli, Francesco Tortorella   
 Avatar Reaction to Multimodal Human Behavior  
  In this paper, we propose a virtual agent application. We develop a virtual agent that reacts to gestures and a virtual environment in which it can interact with the user. We capture motion with a Kinect V2 camera, predict the end of the motion and then classify it. The application also features a facial expression recognition module. In addition, to all these modules, we include also OpenAI conversation module. The application can also be used with a virtual reality headset.  
 Baptiste Chopin, Mohamed Daoudi, Angela Bartolo   
 Metadata   
 Title  Image Analysis and Processing - ICIAP 2023 Workshops    
 Editors  Gian Luca Foresti  
  Andrea Fusiello  
