 This book contains revised selected papers from the 25th International Conference on Selected Areas in Cryptography, SAC  
 Author / Uploaded 
  Claude Carlet 
  Kalikinkar Mandal 
  6 Conclusion  
  References  
  Author Index   
 Citation preview   
  Claude Carlet Kalikinkar Mandal Vincent Rijmen (Eds.)  
  Selected Areas in Cryptography – SAC 2023 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers  
  Lecture Notes in Computer Science Founding Editors Gerhard Goos Juris Hartmanis  
  Claude Carlet · Kalikinkar Mandal · Vincent Rijmen Editors  
  Selected Areas in Cryptography – SAC 2023 30th International Conference Fredericton, Canada, August 14–18, 2023 Revised Selected Papers  
  Editors Claude Carlet Université Paris 8 Saint-Denis Cedex, France  
  Preface  
  who nicely enhanced the conference. We also thank all colleagues who submitted a paper and all speakers. December 2023  
  Claude Carlet Kalikinkar Mandal Vincent Rijmen  
  Our Contributions  
  This paper focuses on practical key-recovery attacks against reduced-round Trivium. In order to attack a higher number of rounds, we propose the following methods. Simplify Superpolies by Variable Substitutions. At the cost of adding one extra variable, the variable substitution can greatly simplify the superpoly, so that some unbalanced superpolies can be transformed into balanced ones. Thanks to this technique, more simple and balanced superpolies are utilized, from which more information about secret variables can be extracted. More Balanced Polynomials by Canceling the Quadratic Terms. Balanced superpolies are important for practical attacks on round-reduced Trivium. However, for Trivium with a higher number of rounds, it is hard to ﬁnd cubes  
  Relationship Between a Cube and Its Subcubes  
  In our practical cube attacks on Trivium, we focus on balanced superpolies rather than superpolies of low degrees. For example, consider two superpolies 𝐴 and 𝐵. 𝐴 : 𝑝1 𝑝2 ⊕ 𝑝2 𝑝3 = 𝑐1 , 𝐵 : 𝑝 1 ⊕ 𝑝 3 𝑝 10 𝑝 4 𝑝 6 = 𝑐 2 . The degrees of 𝐴 and 𝐵 are 2 and 4, respectively. We prefer 𝐵 to 𝐴 because 𝐵 is balanced so that we can use it to deduce 𝑝 1 by enumerating 𝑝 3 , 𝑝 10 , 𝑝 4 , 𝑝 6 . Inspiration. In the search process, we obtain a set of cubes whose superpolies are estimated to have low degrees based on the division property, but the superpolies of these cubes are almost all unbalanced. While for another set of cubes, we ﬁnd that a lot of their superpolies are balanced, although their superpolies are estimated to have high degrees. This inspires us to investigate how to locate the balanced superpolies more accurately. With experiments, we have the following observation. Observation 1. For a given 𝑥-dimensional cube 𝐼, if its superpoly is balanced, then the superpolies of its (𝑥 − 1)-dimensional subcubes have a greater probability of being balanced. There are some data in [15, Appendix D] to support this observation. Based on this observation, we propose a method for constructing a better mother cube that may contain more subcubes with balanced superpolies and a heuristic search method to reduce the search space.  
  14  
  H. Lei et al.  
  A Modified Algorithm to Construct a Mother Cube for Balanced Superpolies. We modify the algorithm for constructing the mother cube in [29, Section 3.1] to obtain a better mother cube that may have more subcubes with balanced superpolies. First, at the stage of determining the starting cube, Ye et al. predicted a preference bit 𝑠𝜆(𝑟) for 𝑟-round Trivium. 𝑠𝜆(𝑟) can be written as 𝑠𝜆(𝑟) = 𝑠𝑖(𝑟−𝜆) · 𝑠𝑖(𝑟−𝜆) ⊕ 𝑠𝑖(𝑟−𝜆) ⊕ 𝑠𝑖(𝑟−𝜆) ⊕ 𝑠𝑖(𝑟−𝜆) , 𝜆 𝜆 𝜆 𝜆 𝜆 1  
  2  
  Notations  
  The notations used in this paper are summarized in Table 2. Table 2. Notations r  
  (2)  
  If Tk and Tpre are simply treated as 2r and 2nc , respectively, i.e., only the naive exhaustive search is performed, then Ttotal = (k − 1) · 2nk +nc −r + 2nk . In other words, the total time complexity is directly related to the probability of the diﬀerential characteristic, i.e., 2−nc −nk . In many cases, the attackers can optimize Tk by using some advanced techniques to satisfy partial conditions implied in the diﬀerential characteristic, i.e., Tk can be smaller than 2r . For example, the target diﬀerence algorithm proposed in [5] is one of such techniques. However, to optimize Tpre , one has to solve a problem similar to the −round preimage ﬁnding problem. In most cases, this is not optimized due to the increasing diﬃculty and it is simply treated as Tpre = 2nc . 3.1  
  The Literature and Our New Strategy  
  (3)  
  Speciﬁcally, we need on average 2nk −r diﬀerent valid solutions of (M1 , . . . , Mk−1 ). In this sense, we need about 2nk +nc −2r diﬀerent valid solutions of (M1 , . . . , Mk−2 ) because for each valid (M1 , . . . , Mk−2 ), we expect to have 2r−nc valid solutions of Mk−1 . Based on Eq. 3, if nc < r holds, we have 2nk +nc −2r < 2nk −r . Compared with Eq. 2, this case has indicated the possibility to optimize the attack if Tk-1 can be signiﬁcantly optimized and Tpre1 is relatively small, i.e., we  know Tpre1 ≤ 2nc . On the Purpose to Convert Conditions. As stated above, we have to optimize Tpre1 . This is related to the original purpose to introduce conditions on the 0 . Speciﬁcally, we expect that after adding these conditions, capacity part of Sk−1 we can eﬃciently enumerate the solutions of Mk−1 to satisfy the nc conditions on the capacity part of Sk0 . In other words, without these conditions, we still can only perform the naive exhaustive search over Mk−1 and no improvement can be obtained, i.e., the time complexity is (k − 2) · 2nk +nc −2r + 2nk +nc −2r · 2r + 2nk −r · Tk = (k − 2) · 2nk +nc −2r + 2nk +nc −r + 2nk −r · Tk . The Big Picture of Our New Attacks. In our attacks, we do not make more eﬀorts 0 to convert the nc conditions on Sk−1 into conditions on the previous input states due to the increasing diﬃculty. Hence, in our setting, we will make   
  Tpre1 = 2nc .  
  M.-J. O. Saarinen and M. Rossi  
  Currently, NIST and the cryptographic community are engaged in a widereaching transition eﬀort to use Post-Quantum Cryptography (PQC) algorithms such as Kyber [2] (a lattice-based key establishment scheme) and Dilithium [4] (a lattice-based signature scheme) to replace older quantum-vulnerable RSA and Elliptic Curve based cryptography [1,26]. In many prominent use cases, this transition requires physical side-channel security from PQC implementations: Authentication tokens, Mobile/IoT device platform security (secure boot, ﬁrmware update, attestation), smart cards, and other secure elements. Masking. Masking is a general technique to attain side-channel security by splitting sensitive variables into d randomized shares, where t = d − 1 is the masking order. Each share individually appears uniformly random, and all d shares are required to determine their sum, which is the actual masked quantity. We write x to denote a masked representation of x. The relationship may be either an exclusive-or operation (“Boolean masking”) or modular (“Arithmetic masking”): Boolean masking: x = x0 ⊕ x1 ⊕ . . . ⊕ xt Arithmetic masking: x = x0 + x1 + . . . + xd−1 (mod q).  
  (1) (2)  
  PQC algorithm side-channel countermeasures are primarily based on masking. For example, see [6,14] for details about masking Kyber, and [3,24] for Dilithium. High-order computation on the shares is relatively complex in the case of these two algorithms, requiring both Boolean and Arithmetic masking. Complexity of Attack and Defence. In addition to practicality, one main advantage of masking over more ad-hoc approaches is that it allows one to prove sidechannel security properties of implementations. In pioneering work, Chari et al. [7] showed that in the presence of Gaussian noise, the number of side-channel observations required to determine x from individual bits grows exponentially with the number of shares d. The understanding of this exponential relationship has since been made more precise both theoretically and in practice [13,18,23]. In [15], Ishai et al. introduced the probing model: the notion of t-probing security states that the joint distribution of any set of at most t internal intermediate values should be independent of any of the secrets. Thus, a circuit is t-probing secure iﬀ it is secure against observations of t = d−1 wires. Reductions from the probing model to the noisy leakage model [12,29] exist and allow to link t-probing security with realistic leakage models. In addition, [15] showed that any circuit can be transformed into a t-probing secure circuit of size O(nt2 ). It has since been demonstrated that quasilinear O(t log t) masking complexity can be achieved for some primitives, including the Lattice-based signature scheme Raccoon [27,28]. Structure of this Paper and Our Contributions. The mask compression technique is introduced in Sect. 2, which also discusses how it can be applied in practice. Further security discussion is given in Sect. 3, including requirements for composability (strong non-interference). Section 4 gives a practical example of a very  
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
  Mask Compression  
  Mask compression in a group G (Eqs. 1 or 2) requires a symmetric cryptography primitive SampleG (z) that maps short binary keys z to elements in G. The function is used to manipulate sensitive variables, but thanks to the way it is used, SampleG (z) itself does not need to be masked. Its input and output variables are generally ephemeral (single-use) individual shares. Definition 1. (Informal). The function x ← SampleG (z) uses the input seed z ∈ {0, 1}κ to sample a pseudorandom element x ∈ G deterministically. We assume that SampleG is cryptographically secure under a suitable deﬁnition. For a technical discussion of pseudorandomness, see [19, Section 3] (the deﬁnitions oﬀered for binary strings can be easily extended to other uniform distributions.) Intuitively, we assume that the task of distinguishing x from a uniformly random element in set G is computationally hard. Typically key size κ is selected to match the overall security level of the system. In this case, distinguishing x should not be substantially easier than an exhaustive search for z. Practical Instantiation. We can implement SampleG (z) with an extendable output function (XOF) such as SHAKE [25]1 The function can also be instantiated with a stream cipher or a block cipher (in counter mode). If a mapping from XOF output to non-binary uniform distributions is required, one may use rejection sampling since each XOF(z) deﬁnes an arbitrarily long bit sequence. Examples of sampled |G|  2κ include large-degree polynomials that are ring elements Zq [x]/(xn + 1) in Kyber [2] and Dilithium [4]. Note that implementations Kyber and Dilithium already have subroutines that generate uniform polynomial coeﬃcients in Z/qZ from XOF output via rejection sampling. In common lattice algorithms, an eﬃcient (unmasked) method for this task is required to create polynomials for A generator matrix on the ﬂy. This is the reason why a PQC hardware implementation (such as the one discussed in Sect. 4) will often have an eﬃcient instance of SampleG (z) available. Definition 2 (Compressed Mask Set). A compressed mask set consists of a t tuple xz = (x0 , z1 , · · · , zt ) satisfying x ≡ x0 + i=1 SampleG (zi ) with x0 ∈ G and zi ∈ {0, 1}κ for i ∈ [1, t]. Theorem 1. It is computationally infeasible to determine information about x from any subset of t = d − 1 elements in compressed masking d-tuple xz . 1  
  Algorithm 1: xz = MaskCompress(x) (Proved t-NI in Th. 2) Input: Masking x = (x0 , x1 , · · · , xt ).   Output: Compressed masking xz with xz0 + ti=1 SampleG (zi ) = ti=0 xi . z 1: x0 = x0 2: for i = 1, 2, · · · , t do  Random Bit Generator, κ bits. 3: zi ← Random(κ) 4: xz0 ← xz0 − SampleG (zi ) 5: xz0 ← xz0 + xi 6: return xz = (xz0 , z1 , z2 , · · · , zt )  
  Conversions. We obtain a trivial mapping from compressed encoding xz to the general masked encoding x by setting x0 = xz0 and xi = SampleG (zi ) for i ∈ [1, d]. Security follows from the observation that this conversion is “linear” in the sense that there is no interaction between shares. Mapping from regular to compressed format requires interaction between the shares since SampleG is not invertible. Algorithm 1 MaskCompress presents one way of performing this conversion. We note its resemblance to the RefreshMasks algorithm of Rivain and Prouﬀ ([31, Algorithm 4]); its NI security follows similarly (see Sect. 3 for more details). While it is secure if used appropriately, combining it with other algorithms may expose leakage, as demonstrated in [9]. Depending on requirements, it can be combined with additional refresh steps to build an SNI [5] algorithm (also see Sect. 3 for more details).  
  Algorithm 2: xi = LoadShare(xz , i)  
  We observe that t-strong non-interference implies t-non interference. Any non-interferent algorithm can achieve strong non-interference with an extra mask refreshing of its output [5]. Considering MaskCompress (Algorithm 1), we propose the following Theorem and prove it below. Theorem 2. Algorithm 1 is d-Non Interferent under the Pseudorandom Function hypothesis on the SampleG function (Deﬁnition 1). Hence, it is also t-probing secure. Let us ﬁrst assume that there exists an index i∗ ∈ {1, ..., d} such that both the seed zi∗ and the input xi∗ are left unobserved by the probing attacker. With a hybrid argument, under the pseudorandomness hypothesis on the SampleG (zi∗ ) function, SampleG (zi∗ ) may be replaced by a uniform random value in G, denoted y ∗ . Hence, all the intermediate values that intervene in the i∗ -th iteration can be simulated with uniform random. Therefore, the distribution of the observations can be simulated with at most t shares of the input (xi for i = i∗ ) under the computational assumption. Now assume that it is not possible to ﬁnd such an index i∗ ∈ {1, ..., d}. In that case, all the t observations are made on a combination of xi for i ∈ {1, t} and zi for i ∈ {1, t}. Let us note that in that case, the input x0 is always left unobserved. The distribution of xz0 over all iterations is then statistically indistinguishable from uniform random in G. Hence, the distribution of the observations can be simulated with at most t shares of the input (xi for i ∈ {1, t}). Algorithm 4: xz = SNIMaskCompress(x) (Proved t-SNI in Th. 4) Input: Masking x = (x0 , x1 , · · · , xt ).   Output: Compressed masking xz with xz0 + ti=1 SampleG (zi ) = ti=0 xi . 1: xz0 = x0 2: for i = 1, 2, · · · , t do 3: zi ← Random(κ) 4: xz0 ← xz0 − SampleG (zi )  Compared to Alg. 6, xi is directly accessed 5: xz0 ← xz0 + xi 6: for j = 1, 2, · · · , t do 7: for i = 1, 2, · · · , t do 8: xi ← SampleG (zi ) 9: zi ← Random(κ) 10: xz0 ← xz0 − SampleG (zi ) 11: xz0 ← xz0 + xi 12: return xz = (xz0 , z1 , z2 , · · · , zt )  
  Let us now consider the LoadShare algorithm. As noted above, the full version of Algorithm 2, presented in Algorithm 3, is very similar to the Non-Interferent RefreshMasks algorithm introduced in [31]. Hence, we introduce the following Theorem.  
  M.-J. O. Saarinen and M. Rossi  
  Theorem 3. Algorithm 3 is d-Non Interferent and thus t-probing secure under the pseudorandomness hypothesis on the SampleG function (Deﬁnition 1). Since there are t+1 iterations and at most t observations, there exists an index i∗ ∈ {0, ..., t} designating an iteration that is left unobserved by the probing attacker. Hence both the input seed zi∗ and the value xz0 (of the i∗ -th iteration) are left unobserved. In that case, all the subsequent updates of xz0 can be replaced with uniform random under the same pseudorandomness hypothesis of SampleG . Finally, all the attacker’s observations may be simulated with (x0 , (zi )i=i∗ ) if i∗ = 0 and all the (zi ) otherwise. There are no more than t shares of the input, which concludes the proof.  
  Algorithm 5: xi = SNILoadShare(xz , i)  
   Input: Compressed masking xz satisfying x = xz0 + ti=1 SampleG (zi ) Input: Index i for the share to be accessed. Output: If read in order, i = 0, 1, · · · t, the returned {xi } is a fresh masking x. 1: for i = 0 · · · , t do 2: xi ← SNILoadShare(xz , i) 3: return (x0 , x1 , · · · , xt )  
  Theorem 4. Algorithm 4, 6, and 7 are d-Strongly Non-Interferent under the Pseudorandomness hypothesis of SampleG function (Deﬁnition Algorithm 1). They may be safely composed in complex designs. In Algorithm 4 and 6, since there are t + 1 = d iterations (with one outside of the loop with index j for Algorithm 4) and t observations, at least one iteration is left unobserved. All the observations (including the observations on the output) performed after the unobserved iteration can be simulated with uniform random (under the same pseudorandomness hypothesis). All the observations performed before the unobserved iteration can be simulated with at most t shares of the input (inherited from the NI property of Algorithm 1). For Algorithm 7, one can switch the loops for i and j and apply the same reasoning.  
  2  
  The discussion applies to the version of Raccoon published at IEEE S&P 2023 [28]. There are diﬀerences to the Raccoon version submitted to the NIST PQC Call [27].  
  Mask Compression: High-Order Masking on Memory-Constrained Devices  
  Radboud University, Nijmegen, The Netherlands [email protected]  2 Leiden University, Leiden, The Netherlands  
  Abstract. The lookup table-based masking countermeasure is prevalent in real-world applications due to its potent resistance against side-channel attacks and low computational cost. The ASCADv2 dataset, for instance, ranks among the most secure publicly available datasets today due to two layers of countermeasures: lookup table-based aﬃne masking and shufﬂing. Current attack approaches rely on strong assumptions. In addition to requiring access to the source code, an adversary would also need prior knowledge of random shares. This paper forgoes reliance on such knowledge and proposes two attack approaches based on the vulnerabilities of the lookup table-based aﬃne masking implementation. As a result, the ﬁrst attack can retrieve all secret keys’ reliance in less than a minute without knowing mask shares. Although the second attack is not entirely successful in recovering all keys, we believe more traces would help make such an attack fully functional. Keywords: Side-channel analysis Correlation  
  1  
  (8)  
  = β. The masked state Ci only relies on β, and the multiplicative mask α is disabled in this scenario, links to the zero-value 2nd -order leakage mentioned in [17]. To exploit this attack path, an adversary would try all possible keys to calculate Xi and select the traces that satisfy Xi = 0. Then, the chosen traces are correlated with β. The traces set with the highest correlation would indicate the correct key. There are two ways to perform such an attack. The ﬁrst attack path requires the knowledge of β, indicating that an adversary should, for instance, access the output of a PRNG that provides the mask value. In this case, one could conduct the attack in the proﬁling SCA setting similar to other researches [8,22,23], namely learning β on the cloned and fully controlled device and predict β on a victim device, ﬁnally performing correlation analysis using the predicted values and leakage measurements. Since this attack path relies on the knowledge of the mask shares, it is less interesting considering the scope of this paper that aims at breaking ASCADv2 with no assumption on prior knowledge about the mask shares. The second attack path is similar to a side-channel collision attack in which an adversary compares two trace segments. Instead of correlating with the β value, an adversary could correlate with the leakage segments that process β.  
  Fig. 3. The selected time intervals including the additive mask (β).  
  The attack method is presented in Algorithm 2. Since the goal is to correlate the β leakages with the trace segments of SubBytes, the pairwise correlation corr is performed column-wise. Note that each column in trace segments Ti represents a leakage feature at a speciﬁc time location; the pairwise correlation ensures the dissimilarity of traces segments, due to diﬀerent operation steps and data handling methods, less inﬂuence the correlation results. After averaging the output correlation matrix with E, the k guess that leads to the highest correlation value would be the most likely candidate k ∗ . Algorithm 2. Correlation attack on ASCADv2 Input: trace segments Ti and Tβ , plaintext bytes di Output: most-likely key k∗ 1: for k in K do 2: indices = arg where(Sbox(di ⊕ k) == 0) 3: corrk = E(corr(Tiindices , Tβindices )) 4: end for 5: k∗ = arg max corr  
  Conclusions and Future Work  
  In this paper, we evaluate two vulnerabilities in lookup table-based aﬃne masking implementation, then leverage them to perform eﬃcient second-order attacks on the ASCADv2 dataset. Speciﬁcally, we notice that some mask shares remain constant during an AES encryption, which leads to an easy cancellation of masks with a side-channel collision attack. Another vulnerability relies on implementing the Galois ﬁeld multiplication, which always outputs zero when one input is zero. In this case, an adversary could choose speciﬁc traces that generate zero input. In this case, the aﬃne masking scheme is signiﬁcantly weakened, as only additive mask shares remain as the output. Multiple aspects would be interesting to consider in future research. First, the proposed attacks rely on the single masked SBox used during encryption. It will be interesting to investigate the applicability of the proposed attack when two or more masked SBoxm are used in a cryptographic operation. Next, the proposed attacks are grounded on the squared Euclidean distance and Pearson correlation coeﬃcient for similarity assessment. It would be interesting to explore deep learning in initiating attacks under more noisy circumstances, such as those involving desynchronization. Further, it would be compelling to study and augment the attack performance hinging on our second identiﬁed vulnerability: the zero output of the ﬁnite ﬁeld multiplication. Finally, an optimal objective would be to devise innovative techniques to overcome the complexity inherent in ﬁnite ﬁeld multiplication, enabling direct attacks on this dataset’s intermediate data.  
  could be computed empirically from estimates of the probability distributions of the leakage (i.e. trace points) under all possible secrets. We assume that any probed wire value can be labelled as ‘good’ or ‘bad’. The values labelled ‘good’ jointly reveal nothing about the secret. The ‘bad’ values may reveal secret information, but the leakage can be bounded in terms of λ and ε. The parameter λ is determined by physical aspects such as the leakage model and noise level. The parameter ε is instead determined by the mathematical properties of the masking. Speciﬁcally, it will be shown later how these parameters can be determined using linear cryptanalysis. Below is the deﬁnition of the bound on a ﬁrst-order noisy probing adversary given a bound on ε and λ. Theorem 1 ([5]). Let A be a noisy threshold-probing adversary for a circuit C. Take λ ≥ 1, and ε ≤ 1 as non-negative real numbers. Assume that for every query made by A on the oracle Ob with result z, there exists a partitioning (depending only on the probe positions) of the probed wire values into two random variables x (‘good’) and y (‘bad’) such that 1. The noisy leakage function f such that z = f (x, y) is λ-noisy. 2. The conditional probability distribution py|x satisfies Ex  py|x 22 ≤ ε. 3. Any t-threshold-probing adversary for the same circuit C and making the same oracle queries as A, but which only receives the ‘good’ wire values ( i.e. corresponding to x) for each query, has advantage zero. The advantage of A can be upper bounded as  Advnoisy (A) ≤ 2q ε/λ, where q is the number of queries to the oracle Ob . The security bound obtained in Theorem 1 depends on the parameter ε. This value will be determined by performing linear cryptanalysis of the masked cipher. Essentially, this follows regular linear cryptanalysis, except that masking schemes naturally incorporate linear relations (namely that the sum of the shares form the secret). As a result, the basic deﬁnitions of linear cryptanalysis need to be adapted to work over a quotient space where, in short, the last share is removed to avoid the previously mentioned linear relation. Viewing linear cryptanalysis over this quotient space is justiﬁed by the non-completeness property of threshold implementations, namely that a probe does not view all shares of a secret at once, and as a result, we only investigate relations over non-complete sets of shares. Correlation of Maskings. For any linear masking scheme, there exists a vector space V ⊂ F2 of valid maskings of zero. More speciﬁcally, an F2 -linear secret sharing scheme is an algorithm that maps a secret x ∈ Fn2 to a random element of a corresponding coset of the vector space V. Let ρ : Fn2 → F2 be a map that sends secrets to their corresponding coset representative. For convenience, we denote Va = a + V. We use the following deﬁnition of correlation matrices of a masking.  
  x∈V  
  for a function F  : Va → Vb with F  (x) = F (x + a) + b. The link between ε from Theorem 1 and linear cryptanalysis is completed by the theorem below. It shows that the coordinates of pz are entries of the correlation matrix of the state-transformation between the speciﬁed probe locations. In Theorem 2, the restriction of x ∈ Va to an index set I = {i1 , . . . , im } is denoted |I| by xI = (xi1 , . . . , xim ) ∈ F2 . This deﬁnition depends on the speciﬁc choice of the representative a, but the result of the theorem does not. Theorem 2 ([7], §5.2). Let F : Va → Vb be a function with V ⊂ F2 and I, J ⊂ {1, . . . , }. For x uniform random on Va and y = F (x), let z = (xI , yJ ). The Fourier transformation of the probability mass function of z then satisfies , v ∈ F2 /V⊥ are such that u I = u, u []\I = 0, vJ = v | pz (u, v)| = |Cv˜F, u˜ |, where u and v[]\J = 0. The above theorem relates the linear approximations of F to pz (u, v) and hence provides a method to upper bound ε based on linear cryptanalysis. Applying the Bound with Non-uniform Inputs. The analysis by Beyne et al. originally applied to threshold implementations working on a uniform input or consisting of uniform functions. In this work, we extend this framework by analysing threshold implementations with a non-uniform input, namely an input which is shared via a non-uniform function. More speciﬁcally, we use a limited number of random bits to mask the input of the threshold implementation and analyse the impact on its ﬁrst-order security. While previous works focus on reducing the online randomness of a masking, we instead propose to use the cryptanalysis technique to reduce the randomness requirement at the start of the masking. We model this limited-random input by considering a non-uniform input encoder Enc (shown in Fig. 2) which takes in the circuit’s input k (e.g. plaintext and key) and random bits r (modelled as shares of zero and as ‘bad’ values), and provides a shared input for the masked circuit. In particular, where this shared input is larger in size than the randomness that was given as input. Due to Enc being a non-uniform function, the correlation matrix’s entries  ∈ F2 /V⊥ are non-zero. As a result, when | pyJ (v)| = |CvH,0 | = 0 for CvEnc ,0 for v H = F ◦ Enc (some non-uniform function which maps the limited randomness r to the probed values), vJ = v and v[]\J = 0 where a single probe is placed on pyJ (v)| yJ , it is possible that this probe reveals a secret. The speciﬁc value ε = | determines the advantage of the ﬁrst-order probing adversary via Theorem 1. In other words, due to the threshold implementation using a non-uniform input,  
  Threshold Implementations with Non-uniform Inputs  
  Fig. 2. Depiction of the non-uniform encoder Enc masking the input (e.g., plaintext and key) k using a few uniform random bits r.  
  a ﬁrst-order probing attack is possible. However, we show the probability of success is limited in function of ε. In the rest of the work, we will look at trails over F (which is a uniform function) where we can pick non-zero input linear masks with certain conditions depending on how the input of the cipher was masked. We thus analyse the trails of the masked cipher ending in a single probe position with conditions on the input mask related to how it is masked. Cautionary Note. In this work, we use the piling-up principle [17,25] to obtain estimates of ε using a trail-based approach (which is often used in the ﬁeld of cryptanalysis). However, since Enc is a non-uniform (read non-balanced) function, a zero input mask will correlate to several output masks. As a result, and due to the ﬁrst S-box layer, many input masks of F from Fig. 2 are related to an output mask. As a result, the actual correlations may diﬀer from the trailbased estimates. More speciﬁcally, we make the assumption that the correlation of the probed values is determined by a trail with an outstanding value. In case a trail with high correlation is found, we can assume that there is an attack. However, we emphasise that the absence of such trails does not trivially imply that no attack is possible. For that reason, we use this piling-up principle as a heuristic to ﬁnd and verify promising non-uniform inputs for the threshold implementations. We then base ourselves on a practical veriﬁcation to analyse the promising candidates via tools like PROLEAD [19] for a noiseless veriﬁcation and via FPGA experiments for more realistic and noisy veriﬁcation.  
  3  
  Conclusion  
  In this paper, we have shown that using a non-uniform masked input for a threshold implementation can remain ﬁrst-order secure in face of a practical evaluation. We have also shown that the non-uniform masking itself should be chosen carefully and diﬀerently for each symmetric primitive following the prin-  
  J. H. Cheon et al.  
  vector having a hamming weight of h in a secret-independent running time. A detailed algorithm is given in the full version of this paper [21]. 3.3  
  Discrete Gaussian Noise  
  University of Birmingham, Birmingham, UK [email protected]  2 University of Bristol, Bristol, UK  
  Abstract. An oblivious pseudorandom function, or OPRF, is an important primitive that is used to build many advanced cryptographic protocols. Despite its relevance, very few post-quantum solutions exist. In this work, we propose a novel OPRF protocol that is post-quantum, veriﬁable, round-optimal, and moderately compact. Our protocol is based on a previous SIDH-based construction by Boneh, Kogan, and Woo, which was later shown to be insecure due to an attack on its one-more unpredictability. We ﬁrst propose an eﬃcient countermeasure against this attack by redeﬁning the PRF function to use irrational isogenies. This prevents a malicious user from independently evaluating the PRF. The SIDH-based construction by Boneh, Kogan, and Woo is also vulnerable to the recent attacks on SIDH. We thus demonstrate how to eﬃciently incorporate the countermeasures against such attacks to obtain a secure OPRF protocol. To achieve this, we also propose the ﬁrst proof of isogeny knowledge that is compatible with masked torsion points, which may be of independent interest. Lastly, we design a novel non-interactive proof of knowledge of parallel isogenies, which reduces the number of communication rounds of the OPRF to the theoretically-optimal two. Putting everything together, we obtain the most compact postquantum veriﬁable OPRF protocol. Keywords: Oblivious Pseudorandom Functions  
  1  
  A. Basso  
  It is possible to build an OPRF using generic multi-party computation techniques, but such solutions can be ineﬃcient, and they require more rounds of communication than what an ad-hoc construction can achieve. Indeed, highlyeﬃcient and round-optimal (i.e., with two rounds) constructions exist based on Diﬃe-Hellman [15] or RSA blind signatures [9]. All such constructions are vulnerable to quantum attacks, and very few quantum-resistant OPRFs are reported in the literature. The ﬁrst quantum-secure veriﬁable OPRF was proposed by Albrecht, Davidson, Deo and Smart [1]. The protocol is based on the learningwith-errors problem and the short-integer-solution problem in one dimension, and it only requires two rounds of communication. However, the construction can be characterized as a feasibility result, as a single OPRF execution requires communicating hundreds of gigabytes of data. The only other post-quantum solutions were proposed by Boneh, Kogan, and Woo [5]. The authors proposed two moderately-compact OPRFs based on isogenies, one relying on SIDH and one on CSIDH. The OPRF based on SIDH is veriﬁable, but requires an even higher number of communication rounds, since the veriﬁability proof is highly interactive. A later work by Basso, Kutas, Merz, Petit and Sanso [4] cryptanalyzed the SIDH-based OPRF by demonstrating two attacks against the one-more unpredictability of the protocol, i.e. it showed that a malicious user can recover suﬃcient information to independently evaluate the PRF on any input. The ﬁrst attack is polynomial-time, but it can be easily prevented with a simple countermeasure; the second attack is subexponential but still practical, and the authors argue that there is no simple countermeasure against it. More recently, a series of works [7,19,22] developed an eﬃcient attack on SIDH that also extends to the SIDH-based OPRF. Contributions. In this work, we propose an OPRF protocol that is postquantum secure, veriﬁable, round-optimal, and moderately compact (≈9 MB per execution), with a security proof in the UC framework [6] in the randomoracle model. To do so, we follow the same high-level approach as the SIDH-based OPRF by Boneh, Kogan, Woo [5], but with the following changes: – We propose an eﬃcient countermeasure against the one-more unpredictability attack by Basso, Kutas, Merz, Petit, and Sanso [4]. We modify the PRF definition, and in particular we use irrational isogenies to map the user’s input to an elliptic curve. In this way, the information that allowed an attacker to independently evaluate the PRF is no longer deﬁned over a ﬁeld of small extension. A malicious user may still attempt to carry out the attack from [4], but this would now require the attacker to work with points with exponentially many coordinates over the base ﬁeld, which makes the attack infeasible. Besides preventing the attack, this change results in a smaller prime and a more compact protocol. – We discuss how to integrate M-SIDH, a recently-proposed countermeasure [12] against the SIDH attacks that relies on masked torsion, into the OPRF protocol. This requires using longer isogenies and a larger prime, but a series of optimizations allow us to maintain a reasonable communication  
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
  Preliminaries  
  The Existing OPRF Construction. Boneh, Kogan, and Woo [5] introduced a veriﬁable OPRF protocol based on SIDH (for further preliminaries on SIDH, we refer to the full version of this paper [2]), which uses a prime p of the form p = NM NB NK N1 N2 f −1, where the values Ni are coprime smooth integers and f is a small cofactor. Initially, the server commits to its key k by publishing the curve ˜ with kernel P˜ + EC obtained as the codomain of the NK -isogeny starting from E ˜ ˜ ˜ ˜ [k]Q, where the values E, P , Q are protocol parameters. The commitment also includes a zero-knowledge proof πC of the correctness of the computation. Then, to evaluate the PRF on input m ∈ M, where M deﬁnes the input space, the user computes an isogeny φm of degree NM by hashing the input with a function H and computing φm : E0 → Em := E0 /P +[H(m)]Q, where the curve E0 and the points P, Q are also protocol parameters. Then, the user blinds the curve Em by computing a second isogeny φb : Em → Emb of degree NB . The user sends the curve Emb and the torsion images RK = φb ◦ φm (PK ), SK = φb ◦ φm (QK ) to the server, where the points PK , QK are also protocol parameters of order NK . The user also provides a non-interactive zero-knowledge proof that torsion information was honestly computed. The server validates the proof, computes the isogeny φk : Emb → Embk := Emb /RK + [k]SK  based on its secret key k, and sends the curve Emrk , the image φk (Emb [NB ]), and a non-interactive zero-knowledge proof of correctness to the user. Then, the server and the user engage in an interactive protocol where the server proves that the isogeny φk has used the same key k as the committed value. If the user is convinced, they use the provided torsion information to undo the blinding isogeny, i.e. to compute the translation of the dual of the blinding isogeny, to obtain the curve Emk .  The output of the OPRF is then the hash H m, j(Emk ), (EC , πC ) . The main exchange, without the commitments and the proofs, is represented in Fig. 1.  
  write φ : Ei → E  := Ei /B. Output i given E  and faux (φ(Pi ), φ(Qi )), where the latter is some auxiliary torsion information. The hardness of the problem clearly depends on the function faux ; if the torsion images were directly revealed, Problem 2 would be easy due to the SIDH attacks. We thus delay describing the function faux until Sect. 5, where we discuss the SIDH countermeasures and choose faux to reveal the values φ(Pi ), φ(Qi ), both scaled by the same unknown value. In the section, we also state the variant of the Decisional Isogeny problem that Problem 2 reduces to. One-More Unpredictability. A key property of an OPRF is that the user learns the output of the PRF only on its input of choice. That means that a malicious user should not learn the output on more inputs than the number of OPRF executions. The BKMPS attack [4] on the OPRF by Boneh, Kogan, and Woo [5] targets the one-more unpredictability, since it shows that a malicious user can extract enough information to independently evaluate the OPRF on any input of their choice. We propose an eﬃcient countermeasure against the one-more unpredictability attack in the next section; we thus delay until then a formalization of the isogeny-related assumption (see Problem 5) we need to guarantee the one-more unpredictability of the OPRF protocol. Commitment Binding. At the beginning of the OPRF protocol, the server commits to a secret key k, so that during each OPRF execution it can prove that the same key was used. To guarantee veriﬁability, we want a commitment scheme with an associated proof of input reuse. We propose to commit to a ˜ with a basis P˜ , Q ˜ of E[N ˜ K ] and revealing key k by ﬁxing a special curve E ˜ ˜ ˜ j(E/P + [k]Q). The proof of input reuse, which in the context of isogenies becomes a proof of parallel isogenies, is presented in Sect. 5.1. To guarantee that the commitment is binding, we want that the following problem to be hard. Problem 3 (Collision finding problem). Let E0 be a supersingular elliptic curve of unknown endomorphism ring. Find two distinct isogenies φ0 : E0 → E and φ1 : E0 → E  such that j(E0 ) = j(E1 ). Problem 3 has been studied in the context of the CGL hash function [8], and it has been shown to be heuristically equivalent to the following problem, which underpins every isogeny-based protocol [10]. Problem 4 (Endomorphism Ring problem). Let E be a supersingular elliptic curve. Find its endomorphism ring End(E).  
  4  
  Fig. 3. Summary of the proposed countermeasure (this does not depict the blinding/unblinding phase). Isogenies in red are known or can be computed by the attacker, isogenies in black are unknown to the attacker, and the dotted isogeny represents the missing isogeny that the attacker needs to compute to succeed in the attack. (Color ﬁgure online)  
  to evaluate the ﬁrst isogeny φm0 to obtain the curve Em0 k for any message m. However, the attacker has no way of computing the remaining isogeny φm1 . To do so, the attacker would need to map the canonical basis on E1 to Em0 k , which does not seem to be possible without knowing the server secret key. Alternatively, the attacker could map the points P, Q and Pk , Qk under the isogenies φ0 and φk . At least one of the image points on each curve has full order, and the point of full order on Em0 k is the image of the point of full order on E1 . This suggest such an approach could be used to ﬁnd a basis, but the second point on each curve is always a scalar multiple of the ﬁrst point.1 Hence, guessing the remaining point has exponential complexity e . Lastly, the attacker cannot use a similar strategy as the one-more unpredictability attack to recover a basis on Em0 k because the curve Em0 k depends on the message m. It thus changes at every interaction, and it is hard for an attacker to ﬁnd two messages that have the same ﬁrst curve E1 and Em0 k since we assume that the hash function H0 is collision-resistant. Note that we require H0 and H1 to be collision-resistant, but we conjecture that only H0 needs to be. Overall, the knowledge of Em0 k does not help the attacker learn any information on the curve Emk , which successfully prevents the one-more unpredictability attack. Optimizations. We can extend this approach to obtain a more compact protocol. Rather than limiting ourselves to two isogenies, we can extend this to an arbitrary number I. We obtain the optimal case when I is maximal, i.e. when deg φm = I . Let Hi be I distinct random oracles for every i ∈ {1, . . . , I}. Then, given an input m and a starting curve E0 , the isogeny φm and the curve Em by computing an isogeny φi determined by Hi (m), generating a canonical basis on the codomain curve, and repeating the process I times (see Algorithm 1). We refer to this hashing as HI (x), and in the rest of the paper, we write (φm , Em ) = HI (x) to refer to the function in Algorithm 1; we also write [P0 , P1 , . . . , PI−1 ]E,N to denote a list points of order N where the point P0 belongs to E, and the point 1  
  If ker φ = P + αQ, it follows that φ(P ) = −αφ(Q).  
  A. Basso  
  – D0 = {(E2 , P2 , Q2 , E3 , φ )}, where E2 is the codomain of an s-isogeny ψ : E0 → E2 , the points P2 , Q2 satisfy P2 = [α]ψ(P0 ), Q2 = [α]ψ(Q0 ) for some α ∈ Z∗n , and φ : E2 → E3 is a d-isogeny with kernel ker φ = ψ(ker φ). – D1 = {(E2 , P2 , Q2 , E3 , φ )}, where E2 is a random supersingular curve with the same cardinality as E0 , P2 and Q2 are two random points of order n such that e(P2 , Q2 ) = e(P0 , Q0 )s , and the isogeny φ is a d-isogeny between E2 and E3 . Note that [5] argues that a similar proof can only reveal one torsion point (either Pi or Qi ) at a time to prevent a distinguishing attack on the simulator. The attack they present relies on computing the Weil pairing between two points of coprime order, and thus their pairing is always one. The attack thus does not apply, and the simulated transcript remains indistinguishable under Weil pairing checks because the sampled points P2 , Q2 are guaranteed to have the same pairing as the honestly-generated points. By revealing both points Pi and Qi we obtain a signiﬁcantly more eﬃcient proof, since it has 1/3 soundness rather than 1/6. We discuss potential optimizations and the concrete cost of such a proof in the full version of this paper [2].  
  6  
  Verifiability  
  Oblivious PRFs can satisfy a stronger security property called verifiability. Informally, this guarantees that the server behaves honestly and always uses the same long-term static key. This is needed to guarantee the privacy of the user in those instances where the user may later reveal the output of the OPRF. A malicious server may behave “honestly” while also using diﬀerent secret keys on diﬀerent interactions. After learning the OPRF output of the user, the server can then test which secret key was used to produce that speciﬁc output and thus link the user to a speciﬁc user-server interaction. The OPRF protocol by Boneh, Kogan, and Woo achieves veriﬁability by relying on many SIDH exchanges: not only is this broken by the attacks on SIDH [7,19,22], this requires ﬁve rounds of interaction. We avoid such issues by introducing a novel public-coin proof protocol of parallel isogeny. Since the proof does not rely on private randomness, we obtain a proof of knowledge that can be made non-interactive via the Fiat-Shamir transform [11] or the Unruh transform [23]. In the OPRF setting, we will rely on the latter to achieve the online-extractability without rewinding needed to get a proof in the UC model. Our main approach relies on executing two proofs of isogeny knowledge in parallel with correlated randomness. Since part of the randomness used is shared, we can obtain a proof of parallelness without needing additional computations. Firstly, we formalize the notion of parallelness. We say that two d-isogenies φ : ˜0 → E ˜1 are parallel with respect to the bases T0 , V0 ∈ E0 [d] E0 → E1 and φ˜ : E and T˜0 , V˜0 ∈ E0 [d] if there exists coeﬃcients a, b ∈ Zd such that ker φ = [a]T0 + [b]V0  and ker φ˜ = [a]T˜0 + [b]V˜0 . This suggests that the parallelness relation  
  A Post-Quantum Round-Optimal Oblivious PRF from Isogenies  
   ⎧ ⎫  ker φ = [k0 ]T0 + [k1 ]V0 , ⎪ ⎪ ⎪ ⎪ ⎨((E0 , T0 , V0 , P0 , Q0 , E1 , P1 , Q1 ,  ⎬  ˜0 + [k1 ]V˜0 , ker φ = [k ] T 0  ˜0 , T˜0 , V˜0 , P˜0 , Q ˜0, E ˜1 , P˜1 , Q ˜ 1 ), = E  (E0 , P0 , Q0 , E1 , P1 , Q1 ), (φ, α) ∈ Riso , ⎪ ⎪  ⎪ ⎪ ⎩ (k0 , k1 , α, α )) (E ˜0 , P˜0 , Q ˜0, E ˜1 , P˜1 , Q ˜ 1 ), (φ , α ) ∈ Riso⎭  
  ˜0 with a d-basis T˜0 , V˜0 be ﬁxed protocol parameters. Now, let the curve E Using the same notation as before, assume that server has committed to its key (k0 , k1 ) by publishing the codomain of the d-isogeny φ˜ that has kernel [k0 ]T˜0 + [k1 ]V˜0 . The server may also reveal some torsion information in its commitment, but as we will discuss later, this is not strictly needed. During the OPRF execution, the server receives a curve E0 with a d-basis T0 , V0 on it, and it computes φ : E0 → E1 := E0 /[k0 ]T0 + [k1 ]V0 . The server then wants to prove that it knows the isogenies φ and φ˜ and that they are parallel. If the server simply ran two instances of the PoIK Riso in parallel, there would be no way to convince the prover that the isogenies are indeed parallel. If the proofs share the same challenges, i.e. the veriﬁer sends the same challenges to both proofs, the server would respond with both φ and φ˜ when chall = 0. However, the isogenies φ and φ˜ are parallel with respect to the bases ψ(T0 ), ψ(V0 ) ˜ V˜0 ) (where ψ is the vertical isogeny used in the proof of knowledge), ˜ T˜0 ), ψ( and ψ( which are not revealed in the proof. If we were to reveal them, the proof would not be zero-knowledge, because when chall = 0, the veriﬁer could recompute the secret isogeny ψ and ψ˜ through the SIDH attacks. Instead, we want to modify ˜2 [d] such that φ the proof to reveal diﬀerent bases T2 , V2 ∈ E2 [d] and T˜2 , V˜2 ∈ E  ˜ and φ are parallel with regards to them, but also such that they do not reveal ˜ We thus propose that the prover generates much information about ψ and ψ. four random coeﬃcients w, x, y, z ∈ Zd such that wz − xy = 0 mod d, and computes T2 and V2 as the solution of ψ(T0 ) = [w]T2 + [x]S2 , ψ(V0 ) = [y]T2 + [z]V2 , and similarly for T˜2 and V˜2 . This is then secure, because the basis T2 , V2 is uniformly random. Thus, for a single proof, this change only does not aﬀect the security of the proof since no additional information is revealed. The rest of the proof needs to be modiﬁed to ensure that the process is followed correctly, i.e. we want the prover to reveal the values w, x, y, z together with ψ so that the veriﬁer can verify the correctness of T2 and V2 . The modiﬁed proof is denoted ∗ , and it is represented explicitly in the full version of this paper [2]. by Piso  
  A. Basso  
  Now, if the prover executes the modiﬁed proof of isogeny knowledge for φ and φ˜ in parallel, with the same challenges, and with the same values x, w, y, z, the isogenies φ , φ˜ revealed when chall = 0 are parallel when the isogenies φ, φ˜ are also parallel, as shown in the following lemma. Lemma 9. Let notation be as above. The isogenies φ, φ˜ are parallel if and only if the isogenies φ , φ˜ are also parallel. Proof. Assume the isogeny φ has kernel [k0 ]T0 + [k1 ]V0  and the isogeny φ˜ has kernel [k˜0 ]T˜0 + [k˜1 ]V˜0 . The kernel of φ is the image of the kernel of φ under ψ, i.e. ker φ = ψ(ker φ). Since ker φ = [k0 ]T0 + [k1 ]V0 , it follows that ker φ = [k0 ]ψ(T0 ) + [k1 ]ψ(V0 ) = [wk0 + yk1 ]T2 + [xk0 + zk1 ]V2 . Similarly, we obtain ker φ˜ = [wk˜0 + y k˜1 ]T˜2 + [xk˜0 + z k˜1 ]V˜2 . Since the coeﬃcients w, x, y, z were chosen such that the matrix ( wy xz ) is invertible, we obtain that k0 = k˜0 and k1 = k˜1 holds if and only if wk0 + yk1 =   wk˜0 + y k˜1 ) and xk0 + zk1 = xk˜0 + z k˜1 holds. ∗ We can now use the proof Piso to construct our proof of parallel isogeny knowledge. The prover runs two such proofs in parallel, with the same randomness (w, x, y, z), and responds to the veriﬁer’s challenges with the responses of the individual proofs. The resulting proof is represented explicitly in the full version of this paper [2]. The security proofs follow closely those of the PoIK Piso in Sect. 5.1: correctness of Piso implies correctness of Ppar , while the soundness of Ppar follows from the soundness of Piso and Lemma 9. The argument for zero-knowledge is also similar, but it is based on the hardness of the following problem, which takes into consideration that the two parallel instance partially share the same randomness.  
  Problem 10 (Double DSSP with Torsion (DDSSPwT) problem). Let D0 and D1 be as in Problem 8. Given: ˜0 → E ˜1 , 1. two d-isogenies φ : E0 → E1 , φ˜ : E ˜ ˜ ˜ 2. the points T0 , V0 ∈ E0 [d] and T0 , V0 ∈ E0 [d], ˜0 ∈ E ˜0 [n], where n = fM-SIDH (λ, d), 3. the points P0 , Q0 ∈ E0 [n] and P˜0 , Q distinguish between the following distributions: (E2 , T2 , V2 , P2 , Q2 , E3 , φ ), – D0∗ = ˜2 , T˜2 , V˜2 , P˜2 , Q ˜2, E ˜3 , φ˜ ), , where the curves and the n-torsion (E points follow the D0 -distribution, i.e. we have (E2 , P2 , Q2 , E3 , φ ) ← D0 , and ˜2, E ˜3 , φ˜ ) ← D0 , and moreover ˜2 , P˜2 , Q (E  
  (E2 , T2 , V2 , P2 , Q2 , E3 , φ ), – = ˜2 , T˜2 , V˜2 , P˜2 , Q ˜2, E ˜3 , φ˜ ), , where the curves and the n-torsion (E points follow the D1 -distribution, i.e. we have (E2 , P2 , Q2 , E3 , φ ) ← D1 , and ˜2, E ˜3 , φ˜ ) ← D1 , and moreover the points T2 , V2 and T˜2 , V˜2 form a ˜2 , P˜2 , Q (E ˜2 [d], respectively. random basis of E2 [d] and E D1∗  
  The proof Ppar is a proof of knowledge, and it can be made non-interactive with standards transformations, such as the Fiat-Shamir [11] or the Unruh [23] transform. This is the ﬁrst non-interactive proof of parallelness. We discuss potential optimizations and the concrete cost of such a proof in the full version of this paper [2].  
  7  
  A New OPRF Protocol  
  In this section, we combine the countermeasures presented in Sect. 4, the SIDH countermeasures and the novel proof of isogeny knowledge discussed in Sect. 5, and the non-interactive proof of parallel isogeny introduced in Sect. 6 to obtain a veriﬁable OPRF protocol that is post-quantum secure, round-optimal, and moderately compact. The OPRF protocol is a two-party protocol between a user U and a server S. Let NM , NB , NK be coprime numbers representing the degrees of the message isogeny, the blinding isogeny, and the server’s isogeny, respectively. Let p be a ˜ be prime of the form p = NM NB NK f − 1, for some cofactor f , and let E0 , E two supersingular elliptic curves deﬁned over Fp2 . Moreover, let P, Q be a ﬁxed ˜ be a ﬁxed basis of E[N ˜ K ]. The ﬁrst curve is used basis of E0 [NM ] and let P˜ , Q to compute the PRF, while the second is used within the server’s commitment. At a high-level, to evaluate the OPRF on an input x, the user maps the input to a curve Em according to Algorithm 1 and computes a blinding isogeny φb : Em → Emb . The user then sends the codomain curve, together with torsion images and a proof of their correctness, to the server, which computes a second isogeny φk : Emb → Embk . The torsion information is appropriately masked to avoid the SIDH attacks. The server then responds with the curve Embk , some torsion information, a proof of their correctness, and a proof that it has used the previously-committed secret key. The user then concludes by using the torsion information provided by the server to undo the blinding isogeny and compute the curve Emk . Its j-invariant is then hashed together with the input and the server’s public key to form the PRF output. The protocol is described in Fig. 4, and it realizes the OPRF ideal functionality of Fig. 2, which allows us to state the following theorem. Theorem 11. The protocol described in Fig. 4 realizes the ideal functionality FvOPRF of Fig. 2 in the random oracle model. We sketch a proof in the full version of this paper [2].  
  Preliminaries Traceable Ring Signature  
  In this section, we review the TRS scheme proposed by Fujisaki and Suzuki [22]. Assuming that N is the number of users in the ring, PK = (pk1 , . . . , pkN ) is ring member’s public keys set, issue is a string representing the speciﬁc event of the signature and L = (issue, PK) is the tag of the signature. A TRS scheme consists of ﬁve algorithms TRS = (Setup, KeyGen, Sign, Verify, Trace) described as follows: – pp ← Setup(1λ ): The algorithm run by the trusted authority, which takes as input security parameter λ ∈ N and outputs public parameter pp. – (pk, sk) ← KeyGen(pp): The algorithm run by the ring member, which takes as input public parameter pp and returns public key pk and secret key sk. – σ ← Sign(skπ , L, M ): The algorithm run by the ring member, which takes as input the secret key skπ , a tag L and a message M ∈ {0, 1}∗ , and returns a signature σ. – {accept, reject} ← Verify(L, M, σ): The algorithm run by the signature receiver, which takes as input the tag L, message M and signature σ, and returns either accept or reject. – {indep, linked, pk} ← Trace(L, M, σ, M  , σ  ): The algorithm run by the ring member or trusted authority, which takes as input two traceable ring signatures σ on message M and σ  on message M  with the same tag L, and returns a string that is either indep, linked or an element pk ∈ PK. If σ = Sign(skπ , L, M ) and σ  = Sign(skπ , L, M  ), it holds that : ⎧ ⎪ ⎨indep   Trace(L, M, σ, M , σ ) = linked ⎪ ⎩ pki  
  2.2  
  Restricted Pair of Group Actions  
  The restricted eﬀective group actions (REGA) can be endowed with the properties: one-wayness (OW), weak unpredictability (wU), and weak pseudorandomness (wPR) [1]. The special restricted pair of group actions used in this paper is called “admissible pair of group actions”, which is proposed by Beullens et al. [4]. Definition 1. Given a ﬁnite commutative group G, G 1 and G 2 are two subsets of G. Let S and T be two ﬁnite sets, DS and DT are distributions over two group actions  : G × S → S, G × T → T . For (S0 , T0 ) ∈ S × T , we say that ResPGA = (G, S, T , G 1 , G 2 , DS , DT ) is a ξ-restricted pair of group actions if the following holds: 1. Eﬃcient Group Action: For any g ∈ G 1 ∪ G 2 and (S, T ) ∈ S × T , it is eﬃcient to compute g  S and g  T , and uniquely represent the element of set G, S and T . 2. Eﬃcient Rejection Sampling:  For all g ∈ G 1 , the intersection of all sets G 2 + g is large enough. Let G 3 = g∈G 1 G 2 + g, then |G 3 | = ξ|G 2 |. 3. Eﬃcient Membership Testing: It is eﬃcient to verify that an element z ∈ G 1 , or z ∈ G 2 , or z ∈ G 3 . 4. Given (g  S0 , g  T0 ) for any element g sampled from G 1 uniformly, it is indistinguishable from the elements (S, T ) sampled from S × T uniformly. 5. It is diﬃcult to ﬁnd two elements g, g  ∈ G 2 + G 3 , that satisfy g  S0 = g   S0 and g  T0 = g   T0 . 6. For the element g sampled from set G 1 uniformly, given S = g  S0 and T = g  T0 , it is diﬃcult to ﬁnd g  ∈ G 2 + G 3 such that T = g   T0 .  
  Traceable Ring Signatures from Group Actions  
  Collision-Resistant Hash Function  
  In this paper, the cryptographic primitives used in the TRS scheme, such as pseudo-random number generators (PRG) and commitment schemes, are instantiated by the hash function. Speciﬁcally, we deﬁne ﬁve collision-resistant hash functions: H1 , H2 , H3 , H4 and H5 , where: H1 : {0, 1}∗ → G 1 , H2 : {0, 1}∗ → {0, 1}2λ , Q H3 : {0, 1}∗ → CK ,  
  H4 : {0, 1}∗ → G †1 , H5 : {0, 1}∗ → G †† 1 . For the Fiat-Shamir transform, we deﬁne a hash function H3 to produce an Q , which is a  set of string in {0, 1}Q , such that K unbalanced challenge space CK Q ≥ 2λ1 . bits are 0. The integers Q, K satisfying K 2.5  
  Analysis of Our Traceable Ring Signature Scheme  
  In this section, we analyzed the correctness and security of TRS scheme under isogeny-based instantiation. A detailed proof of the lattice-based TRS scheme is presented in full version of paper. 4.1  
  Correctness  
  The correctness of our TRS scheme ΠISO is composed of completeness and traceability. The completeness can be deduced from the correctness of the main OR sigma protocol. The detailed proof is presented in full version of paper. 4.2  
  Security  
  Implementation and Performance  
  Table 1 presents a detailed performance of our scheme, including signature size and time. Note that the addition of traceability necessitates additional group action computations, which impact the eﬃciency of our TRS scheme, especially for groups with large numbers of members. This explains why our scheme may be less eﬃcient than the original linkable ring signature scheme proposed by Beullens et al. [4]. Table 1. Performance of proposed TRS scheme under diﬀerent instantiations. N TRS_ISO  
  Time KeyGen(ms) Sign(s) Verify(s) Size Public Key(Byte) Secret Key(Byte) Signature(KB)  
  S. Dhooghe and S. Nikova  
  Random Probing Adversary. Consider the set of two functions N = {f0 , f1 } with f0 : F2 → F2 ∪ {⊥} : x → x and f1 : F2 → F2 ∪ {⊥} : x →⊥. Namely, the function which maps a bit to itself and the function which returns nothing (⊥). We consider a Bernoulli distribution over N with mean 0 ≤ ε ≤ 1. Thus, from the set N we draw the function f0 with probability ε and the function f1 with probability 1 − ε. In words, when drawing a random function and evaluating a variable, the adversary has an ε probability to view that variable. We note for clarity that the adversary can precisely target the location of the probes, but the probability for each probe to provide a value is random. In this paper, we only consider random probes over bits. The model is easily generalized to work over larger ﬁelds. However, we note that in practice, the adversary never views the leakage of a large ﬁeld element but, rather, a function of the bit vector (such as its Hamming weight) which was processed. As a result, the link with practice is weaker when a direct generalization of the random probing model is made. Security Model. Consider an algorithm and denote the set of all its variables (excluding the encoding and decoding phases) by V, the set of ε-random probes Bern(ε)  
  on V is the set {F (v)|v ∈ V, F ← N } with V the values the variables V in the algorithm attained with its input and internal randomness, and where for each value an independent random probe is chosen. The random probing security model is the bounded query, left-right security game represented in Fig. 1. The game consists of a challenger picking a random bit b, the challenger then creates an oracle Ob from the algorithm C and provides this to the adversary A. This adversary is computationally unbounded, but it is bounded in the number of queries to the oracle. The adversary provides two secrets k0 , k1 (for a cipher, a secret is the plaintext and the key) and the set of variables V which it wants to probe. The oracle then picks the secret kb , generates its internal randomness, computes the values V on V, and provides the random probing leakage to the adversary. After q queries (for ease, in this work q = 1), the adversary guesses the bit b which was chosen by the challenger.  
  1  
  Adv(A) = | Pr[AO =⊥] − Pr[AO =⊥] | = κ − (1 − (1 − κ)2 )/2 = κ2 /2 . When considering n shares, the probability to change at least one share out of n (due to a set fault) when the sharing has secret zero κ0 or secret one κ1 is n  
  κ0 =  
  Probabilistic Related-Key Statistical Saturation Attack Introducing a Statistical Model into RKSS Cryptanalysis  
  In this subsection, we adopt the notation introduced in Sect. 2. Let qj (resp. qj ) denote the probability that TI (y, z) = j (resp. TI (y, z  ) = j) when iterating over 2t −1 2t −1 all possible values of y ∈ Fs2 . Thus, j=0 qj = 1 and j=0 qj = 1. Note that in the RKSS attack, qj and qj can take various values for diﬀerent wrong key candidates z and z  , while qj = qj holds for any j for a right key guess. Let χ2 (l, λ) represent the noncentral χ2 -distribution with degree of freedom l and noncentrality parameter λ. For an RKSS distinguisher, we can obtain Lemma 1 for both wrong and right key guesses, according to Stuart-Maxwell [24,30] tests for marginal homogeneity. Due to the limit of paper length, proof of Lemma 1 is shown in the full version [20]. Lemma 1. When 2s is suﬃciently large, for a wrong key guess, the statistic 2t −1 (2s qj −2s qj )2 approximately follows χ2 (2t − 1, 0). For the right key γ = j=0 2s qj +2s qj guess, the statistic γ = 0. The only way to reduce the data complexity of an RKSS attack is to reduce the number of y that are chosen. However, the same value distribution property under a right key guess will not hold if we choose some random values for y. The advantage is that we can distinguish a right key guess from a wrong one by constructing a statistic with the information of similar frequencies of each possible output under related-key pairs (z, z  ), if a considerable number of distinct values of plaintexts are reachable. This new kind of RKSS attack with reduced data complexity will be referred to as a probabilistic RKSS attack hereafter. Assume that we have obtained two independent randomly chosen distinct plaintext sets S and S  with the same size N . All plaintexts share the same ﬁxed I. For each y ∈ S (resp. y  ∈ S  ), we can get a t-bit value TI (y, z) (resp. TI (y  , z  )) that is computed under z (resp. z  ). Then we respectively add one to the counter V [j1 ] and V  [j2 ], where j1 = TI (y, z) and j2 = TI (y  , z  ). After traversing all these N values of y and N values of y  , we can construct an eﬃcient distinguisher by investigating the distribution of the following statistic C=  
  t 2 −1  
    Crandom ∼ χ2 2t − 1, 0 .  
  To prove this proposition, we have to recall the following lemma. Lemma 2. (See [12]) Let X = (X1 , X2 , · · · , Xd )T be a d-dimensional statistic vector that follows the multivariate normal distribution with expectation μ and 2 covariance matrix Σ, where Σ is a symmetric  matrix of rank r ≤ d. If Σ = Σ T 2 T and Σμ = μ, we have X X ∼ χ r, μ μ . With Hypothesis 1 and Lemmas 1 and 2, we can prove Proposition 1 as follows. Due to the limit of paper length, we only show the sketch of our proof. Details are shown in the full version [20]. Proof. (Sketch) Recall that when mounting probabilistic RKSS attacks, the counters V [TI (y, z)] and V  [TI (y  , z  )] are generated by encrypting two independently chosen values y and y  under z and z  . Therefore, these two counters are independent of each other. Since we choose distinct values of y (sampling without replacement), the statistic vector (V [0], V [1], · · · , V [2t − 1]) follows a multivariate hypergeometric distribution with parameters (K, 2s , N ) where K = (N q0 , N q1 , · · · , N q2t −1 ). Similarly, the vector (V  [0], V  [1], · · · , V  [2t − 1]) also follows a multivariate hypergeometric distribution however the parameters are (K  , 2s , N ) where K  = (N q0 , N q1 , · · · , N q2 t −1 ). When N is suﬃciently large, both hypergeometric distributions can be approximated into multivariate normal ones. j = V [j] − V  [j]. Then we have that For any 0 ≤ j ≤ 2t − 1, deﬁne X  = (X 0 , X 1 , · · · , X 2t −1 ) also follows a multivariate normal distribution. Since X  expectation of Xj is E(V [j] − V  [j]) = E(V [j]) − E(V  [j]) = N qj − N qj , the  can be obtained. The covariance between X i and X j can be expectation of X     computed by Cov( Xi , Xj ) = Cov(V [i], V [j]) + Cov(V  [i], V [j]). s 2s −N −t   Let Xj = Xj / 2N 2 2s −1 . Then X = X/ 2N 2−t 22s−N −1 also follows a multivariate normal distribution with expectation μ = (μ0 , μ1 , · · · , μ2t −1 ) where 2s − N 2s − N  −t  = (N qj − N qj )/ 2N 2−t s , μj = E(Xj )/ 2N 2 s 2 −1 2 −1 1  
  In our experimental veriﬁcation, s = 12 and it is enough to ensure the validity of this hypothesis, as well as other assumptions used in this paper.  
  −qi qj − qi qj qi (1 − qi ) + qi (1 − qi ) , Σ = . i,j 2 · 2−t 2 · 2−t  
  Due to Hypothesis 1, Σi,i ≈ 1−2−t and Σi,j ≈ −2−t . Notice that Σ is symmetric and its rank is 2t − 1. It is easy to verify that Σ 2 = Σ and Σμ = μ. According to Lemma 2, we can conclude that 2 2t −1 2t −1  2 2s − 1  (V [j] − V  [j]) 2s − 1  N qj − N qj 2 t ∼ χ (2 −1, λ) with λ = s . 2s − N j=0 2N 2−t 2 − N j=0 2N 2−t Under Hypothesis 1, γ in Lemma 1 can be approximated as t 2 −1  
  γ≈  
  2 t While for a wrong key guess, 2N 22s−N −1 λ ∼ χ (2 − 1, 0) according to Lemma 1. Thus, the distribution of Crandom can be obtained with the characteristic func  tions of χ2 -distributions.  
  To decide whether the obtained statistic C is computed from the cipher (a right key guess) or the random permutation (a wrong key guess), we have to perform a statistic test. In this test, we compare C to a threshold value τ . If C ≤ τ , we conclude that C is obtained from the cipher; otherwise, it is from a random permutation. The data complexity needed to perform the statistic test and the threshold value τ can be computed as follows, given error probabilities. Due to the limit of paper length, proof of Corollary 1 is shown in the full version [20]. Corollary 1. Denote α0 as the probability of rejecting the right key and α1 as the probability of accepting a wrong key. Under the assumption of Proposition 1, the number of distinct plaintexts encrypted under a single key is (2t −1)  
  N = 2 − (2 − 1) s  
  The ﬁrst step to mount attacks is to ﬁnd an RKSS distinguisher. As explained in Sect. 2, Li et al. [19] constructed a search algorithm for KDIB distinguishers, and then RKSS distinguishers covering the same rounds can be obtained using Theorem 1. To make our paper self-contained, we brieﬂy recall the principle of this automatic search algorithm. For more details, we refer to [19]. Their search algorithm is based on STP,3 which is a Boolean Satisﬁability Problem (SAT) [8]/Satisﬁability Modulo Theories (SMT) problem [5] solver. The 3  
  S. Hirose and K. Minematsu  
  Fig. 1. Encryption and decryption algorithms of ccAEAD. (Top) our proposal, ECT, (Bottom) Dodis et al.’s method using AEAD [13]. For both cases, enc and dec are encryptment and decryptment algorithms of the encryptment scheme. The ccAEAD decryption algorithms omit the case of veriﬁcation failures. In ECT, EK and DK denote the TBC’s encryption and decryption, where the thick line is tweak input.  
  Compactly Committing Authenticated Encryption Using Encryptment  
  ers, and message franking was out of scope for the lack of opening key needed by ccAEAD. Organization. Section 2 introduces notations and formalizes tweakable block ciphers, ccAEAD, and encryptment. Section 3 describes the generic construction of ccAEAD, called ECT, and conﬁrms its security. Section 4 formalizes RK ccAEAD. Section 5 conﬁrms the security of ECT as RK ccAEAD. Section 6 concludes the paper.  
  2  
  ccAEAD  
  Syntax. ccAEAD [18] is formalized as a tuple of algorithms CAE := (Kg, Enc, Dec, Ver). It is involved with a key space K := Σ n , an associated-data space A ⊆ Σ ∗ , a message space M ⊆ Σ ∗ , a ciphertext space C ⊆ Σ ∗ , an openingkey space L ⊆ Σ  , and a binding-tag space T := Σ τ . The “cc” (compactly committing) property requires that τ = O(n) is small.  
  S. Hirose and K. Minematsu  
  – The key-generation algorithm Kg returns a secret key K ∈ K chosen uniformly at random. – The encryption algorithm Enc takes as input (K, A, M ) ∈ K × A × M and returns (C, B) ∈ C × T . – The decryption algorithm Dec takes as input (K, A, C, B) ∈ K × A × C × T and returns (M, L) ∈ M × L or ⊥ ∈ M × L. – The veriﬁcation algorithm Ver takes as input (A, M, L, B) ∈ A × M × L × T and returns b ∈ Σ. Kg and Enc are randomized algorithms, and Dec and Ver are deterministic algorithms. For every l ∈ N, Σ l ⊆ M or Σ l ∩ M = ∅. For (C, B) ← Enc(K, A, M ), |C| depends only on |M |, and there exists a function clen : N → N such that |C| = clen(|M |). CAE satisﬁes correctness. Namely, for any (K, A, M ) ∈ K×A×M, if (C, B) ← Enc(K, A, M ), then there exists some L ∈ L such that Dec(K, A, C, B) = (M, L) and Ver(A, M, L, B) = 1. Security Requirements. The security requirements of ccAEAD are conﬁdentiality, ciphertext integrity, and binding properties. Confidentiality. The games MO-REAL and MO-RAND shown in Fig. 2 are introduced to formalize the conﬁdentiality as real-or-random indistinguishability in the multi-opening setting. The advantage of an adversary A for conﬁdentiality is   A A   Advmo-ror CAE (A) := Pr[MO-REALCAE = 1] − Pr[MO-RANDCAE = 1] . A is allowed to make queries adaptively to the oracles Enc, Dec, and ChalEnc. In both of the games, Enc and Dec work in the same ways. For each query (A, C, B), Dec returns (M, L) ← Dec(K, A, C, B) only if the query is a previous reply from Enc. Ciphertext Integrity. The game MO-CTXT shown in Fig. 3 is introduced to formalize the ciphertext integrity as unforgeability in the multi-opening setting. The advantage of an adversary A for ciphertext integrity is (A) := Pr[MO-CTXTA Advmo-ctxt CAE = true]. CAE A is allowed to make queries adaptively to the oracles Enc, Dec, and ChalDec. The game outputs true if A asks a query (A, C, B) to ChalDec such that Dec(K, A, C, B) = ⊥ without obtaining it from Enc by a previous query. Binding Properties. Binding properties are deﬁned for a sender and a receiver. Receiver binding describes that a malicious receiver cannot report a non-abusive sender for sending an abusive message. The advantage of an adversary A for receiver binding is      Advr-bind CAE (A) := Pr[((A, M, L), (A , M , L ), B) ← A : (A, M ) = (A , M )  
  ∧ Ver(A, M, L, B) = Ver(A , M  , L , B) = 1].  
  Encryptment  
  Syntax. Encryptment [13] is roughly one-time ccAEAD. It is formalized as a tuple of algorithms EC = (kg, enc, dec, ver). It is involved with a key space Kec := Σ  , an associated-data space A ⊆ Σ ∗ , a message space M ⊆ Σ ∗ , a ciphertext space C ⊆ Σ ∗ , and a binding-tag space T := Σ τ . – The key-generation algorithm kg returns a secret key Kec ∈ Kec chosen uniformly at random. – The encryptment algorithm enc takes as input (Kec , A, M ) ∈ Kec × A × M and returns (C, B) ∈ C × T . – The decryptment algorithm dec takes as input (Kec , A, C, B) ∈ Kec ×A×C×T and returns M ∈ M or ⊥ ∈ M. – The veriﬁcation algorithm ver takes as input (A, M, Kec , B) ∈ A×M×Kec ×T and returns b ∈ Σ. kg is a randomized algorithm, and enc, dec and ver are deterministic algorithms. For (C, B) ← enc(Kec , A, M ), it is assumed that |C| depends only on |M |. EC satisﬁes correctness: For any (Kec , A, M ) ∈ Kec × A × M, if (C, B) ← enc(Kec , A, M ), then dec(Kec , A, C, B) = M and ver(A, M, Kec , B) = 1. A stronger notion of correctness called strong correctness is also introduced: For any (Kec , A, C, B) ∈ Kec ×A×C ×T , if M ← dec(Kec , A, C, B), then enc(Kec , A, M ) = (C, B). Security Requirements. The security requirements of encryptment are conﬁdentiality, second-ciphertext unforgeability, and binding properties. Confidentiality. Two games otREAL and otRAND shown in Fig. 4 are introduced to formalize the conﬁdentiality. In both of the games, an adversary A asks only a single query to the oracle enc. The advantage of A for conﬁdentiality is   A  (A) := Pr[otREALA Advot-ror EC = 1] − Pr[otRANDEC = 1] , EC where “ot-ror” stands for “one-time real-or-random.” Second-Ciphertext Unforgeability. An adversary A asks only a single query (A, M ) ∈ A × M to encKec and gets (C, B) and Kec , where Kec ← kg and (C, B) ← encKec (A, M ). Then, A outputs (A , C  ) ∈ A × C. The advantage of A for second-ciphertext unforgeability is     Advscu EC (A) := Pr[(A, C) = (A , C ) ∧ decKec (A , C , B) = ⊥].  
  Compactly Committing Authenticated Encryption Using Encryptment  
  The advantage of A for strong receiver binding is  Advsr-bind (A) := Pr[((Kec , A, M ), (Kec , A , M  ), B) ← A : EC   (Kec , A, M ) = (Kec , A , M  ) ∧ ver(A, M, Kec , B) = ver(A , M  , Kec , B) = 1].  
  The advantage of an adversary A for sender binding is Advs-bind (A) := Pr[(Kec , A, C, B) ← A, M ← dec(Kec , A, C, B) : EC M = ⊥ ∧ ver(A, M, Kec , B) = 0]. For strongly correct encryptment, Dodis et al. [13] reduced second-ciphertext unforgeability to sender binding and receiver binding. The following proposition shows that it can be reduced only to receiver binding. On the other hand, receiver binding cannot be reduced to second-ciphertext unforgeability. Suppose that EC is secure except that it has a weak key such that receiver binding is broken using the weak key. For second-ciphertext unforgeability, the probability that the weak key is chosen is negligible for a query made by an adversary. Proposition 1. Let EC be a strongly correct encryptment scheme. Then, for any adversary A against EC for second-ciphertext unforgeability, there exists an ˙ and the run time of A ˙ is at ˙ such that Advscu (A) ≤ Advr-bind (A) adversary A EC EC most about that of A. The proof is omitted due to the page limit.  
  3 3.1  
  S. Hirose and K. Minematsu  
  associated-data space, M be its message space, C be its ciphertext space, L := Σ  be its opening-key space, and T := Σ τ be its binding-tag space. Then, for EC, L is its key space, A is its associated-data space, M is its message space, C is its ciphertext space, and T is its binding-tag space. For TBC, its set of keys is K, its set of tweaks is T , and its set of plaintexts or ciphertexts is L. ENC and DEC are shown in Fig. 5. Also refer to Fig. 1 for illustration. They are also depicted in Fig. 1. KG selects a secret key K for TBC from Σ n . VER simply runs ver.  
  Fig. 5. The encryption and decryption algorithms of ECT  
  Security  
  ECT replaces AEAD of the Dodis et al. scheme with TBC. This change does not impact the conﬁdentiality or binding properties. However, it does aﬀect the ciphertext integrity. With ECT, a candidate for the opening key can always be obtained for a ciphertext. Thus, to ensure the ciphertext integrity, it must be intractable to create a new valid ciphertext for the binding tag of the original ciphertext and the opening key candidate. Confidentiality. The conﬁdentiality of ECT is reduced to the conﬁdentiality of EC and the TPRP property of TBC. The proof is omitted due to the page limit. Theorem 1 (Confidentiality). Let A be an adversary against ECT making at most qe , qd , and qc queries to Enc, Dec, and ChalEnc, respectively. Then, ˙ and D such that there exist adversaries A ot-ror ˙ 2 2  Advmo-ror (A) + 2 · Advtprp ECT (A) ≤ qc · AdvEC TBC (D) + (qe + (qe + qc ) )/2 .  
  ˙ and D is at most about that of MO-REALA . D makes at The run time of A ECT most (qe + qc ) queries to its oracle. Ciphertext Integrity. For the ciphertext integrity of ECT, a new notion is introduced to the ciphertext unforgeability of encryptment EC:  
  Definition 1 (Targeted Ciphertext Unforgeability). Let A := (A1 , A2 ) be an adversary acting in two phases. First, A1 takes no input and outputs (B, state), where B ∈ T and state is some state information. Then, A2 takes (B, state) and Kec as input, where Kec ← kg, and outputs (A, C) ∈ A × C. The advantage of A for targeted ciphertext unforgeability is Advtcu EC (A) := Pr[dec(Kec , A, C, B) = ⊥]. It is not diﬃcult to see that the HFC encryptment scheme [13] satisﬁes targeted ciphertext unforgeability in the random oracle model. The ciphertext integrity of ECT is reduced to the second-ciphertext unforgeability and the targeted ciphertext unforgeability of EC and the STPRP property of TBC: Theorem 2 (Ciphertext Integrity). Let A be an adversary against ECT making at most qe , qd , and qc queries to Enc, Dec, and ChalDec, respectively. ˙ A, ¨ and D such that Then, there exist adversaries A, stprp tcu ¨ ˙ Advmo-ctxt (A) ≤ qe · Advscu ECT EC (A) + (qd + qc ) · AdvEC (A) + AdvTBC (D)  
  + (qe + qd + qc )2 /2+1 . ˙ A, ¨ and D is at most about that of MO-CTXTA . D The run time of A, ECT makes at most qe + qd + qc queries to its oracle. Proof. The game MO-CTXTA ECT is shown in Fig. 6. Without loss of generality, it is assumed that A terminates right after win gets true. A The game MO-CTXT-GA 1 in Fig. 7 is diﬀerent from MO-CTXTECT in that the former records all the histories of EK and DK by “P[B, C1 ] ← L” and uses them to answer to queries to Dec and ChalDec. Thus, A Advmo-ctxt (A) = Pr[MO-CTXTA ECT = true] = Pr[MO-CTXTG1 = true]. ECT A The game MO-CTXT-GA 2 in Fig. 8 is diﬀerent from MO-CTXT-G1 in that the former uses a random tweakable permutation  instead of TBC. Let D be an adversary against TBC. D has either (EK , DK ) or (, −1 ) as an oracle and A simulates MO-CTXT-GA 1 or MO-CTXT-G2 with the use of its oracle. Thus,   A A   Advstprp TBC (D) = Pr[MO-CTXT-G1 = true] − Pr[MO-CTXT-G2 = true] .  
  D makes at most qe + qd + qc queries to its oracle, and its run time is at most about that of MO-CTXTA ECT . In the game MO-CTXT-GA 3 shown in Fig. 8, Dec and ChalDec select L uniformly at random from Σ  , while they call −1 in MO-CTXT-GA 2 . As long as no collision is found for L, the games are equivalent to each other. Thus,   A 2 +1  Pr[MO-CTXT-GA . 2 = true]−Pr[MO-CTXT-G3 = true] ≤ (qe +qd +qc ) /2 Now, Pr[MO-CTXT-GA 3 = true] is evaluated. Suppose that win is set true by a query (A∗ , C ∗ , B ∗ ) to ChalDec. Let Win1 , Win2 , and Win3 be the cases that  
  S. Hirose and K. Minematsu  
  1. P[B ∗ , C1∗ ] = ⊥ and P[B ∗ , C1∗ ] is already set by Enc, 2. P[B ∗ , C1∗ ] = ⊥ and P[B ∗ , C1∗ ] is already set by Dec or ChalDec, and 3. P[B ∗ , C1∗ ] = ⊥, respectively, where C1∗ is the least signiﬁcant  bits of C ∗ . Then, Pr[MO-CTXT-GA 3 = true] = Pr[Win1 ] + Pr[Win2 ] + Pr[Win3 ]. ˙ B∗) For Win1 , suppose that Enc sets P[B ∗ , C1∗ ] while computing a reply (C, ∗ ∗ ∗ ˙ C, ˙ B ) ∈ Y and (A∗ , C ∗ , ˙ M˙ ). Then, (A, ˙ C) ˙ = (A , C ) since (A, to a query (A, ∗ ˙ with the oracle enc ˙ against secondB ) ∈ Y. Thus, the following adversary A L ˙ ˙ ciphertext unforgeability is successful. A runs MO-CTXT-GA 3 except that A ∗ ˙ ˙ ˙ ˙ ˙ guesses (A, M ), asks it to encL˙ and gets (C, B ) and L. Finally, A outputs ˙ = Pr[Win1 ]/qe . ˙ A∗ , C ∗ , B ∗ ) = ⊥. Thus, Advscu (A) (A∗ , C ∗ ) satisfying dec(L, EC ¨ ¨ 1, A ¨ 2 ) against tarFor Win2 and Win3 , the following adversary A = (A ¨ geted ciphertext unforgeability is successful. First, A1 runs MO-CTXT-GA 3 and guesses (B ∗ , C1∗ ). It interrupts the execution of MO-CTXT-GA 3 right after it ¨ 2 takes (B ∗ , state ∗ ) and obtains (B ∗ , C1∗ ) and outputs (B ∗ , state ∗ ). Then, A  ¨ L ← ← Σ as input and resumes the execution of MO-CTXT-GA 3 by making ¨ 2 outputs (A∗ , C ∗ ) satisfying dec(L, ¨ A∗ , C ∗ , B ∗ ) = ⊥. use of state ∗ . Finally, A 0 0 ¨   Thus, Advtcu EC (A) = (Pr[Win2 ] + Pr[Win3 ])/(qd + qc ).  
  Fig. 6. Game MO-CTXTA ECT  
  Binding Properties. ECT inherits (strong) receiver binding from EC. ECT also inherits sender binding from EC. Suppose that (K, A, C, B) satisﬁes DEC(K, A, C, B) = ⊥ and VER(A, M, L, B) = 0, where (M, L) ← DEC(K, A, C, B). Then, L = DK (B, C1 ), dec(L, A, C0 , B) = M and M = ⊥, where C = C0 C1 . In addition, ver(A, M, L, B) = 0.  
  Compactly Committing Authenticated Encryption Using Encryptment  
  Remotely Keyed ccAEAD  
  RK ccAEAD is a particular type of ccAEAD. Their diﬀerence is that, for RK ccAEAD, some parts of encryption and decryption are done by a trusted device keeping the secret key. A user or a host performs encryption and/or decryption by making use of the trusted device. The amount of computation for the trusted device is required to be independent of the lengths of a message, associated data, and a ciphertext due to the common case that the computational power of the trusted device is limited. Dodis et al. [14] left it as an open problem to formalize and construct RK ccAEAD schemes. An answer will be given to the problem in this section. 4.1  
  Syntax  
  A A    EC = 1] − Pr[otRAND  EC = 1], (A) := Pr[otREAL  
  It is not diﬃcult to see that the HFC encryptment scheme [13] satisﬁes conﬁdentiality with attachment in the random oracle model. The conﬁdentiality of RK ECT is reduced to the conﬁdentiality of EC with attachment and the STPRP of TBC. The proof is omitted due to the page limit. Theorem 3 (Confidentiality). Let A be an adversary against RK ECT making at most qe , qd , and qc queries to E, D, and ChalEnc, respectively. Then, ˙ and D such that there exist adversaries A ot-ror  
   Advrk-ror ECT (A) ≤ qc · AdvEC  
  Fig. 11. The games for conﬁdentiality of encryptment  
  Ciphertext Integrity. The ciphertext integrity of RK ECT is reduced to the receiver-binding and the targeted ciphertext unforgeability of EC and the STPRP property of TBC. The proof is omitted due to the page limit. Theorem 4 (Ciphertext Integrity). Suppose that the encryptment scheme used for RK ECT satisfies strong correctness. Let A be an adversary against RK ECT making at most qe , qd , and qc queries to E, D, and ChalDec, respectively. ˙ A, ¨ and D such that Then, there exist adversaries A, ˙ + (qd + qc ) · Advtcu (A) ¨ + Advstprp (D) Advrk-ctxt (A) ≤ Advr-bind (A) ECT EC EC TBC + (qe + qd + qc )2 /2 . ˙ A, ¨ and D is at most about that of RK-CTXTA . D makes The run time of A, ECT at most qe + qd + qc queries to its oracles. Binding Properties. To see ECT as RK ccAEAD does not aﬀect the binding properties. Thus, as discussed in Sect. 3.2, RK ECT inherits both (strong) receiver binding and sender binding from EC.  
  6  
  successful key recovery. There are numerous works on reducing oracle queries in the plaintext-checking attack [5,9,15,17,18]. All these attacks aim to fully recover the reused secret with as few queries as possible. Qin et al. [18] gave a systematic approach to ﬁnding the theoretical lower bound of PC-oracle query numbers for all NIST-PQC lattice-based IND-CPA/IND-CCA secure KEMs. The calculation of their lower bounds is essentially the computation of a certain Shannon entropy. Thus, one cannot ﬁnd a better attack with fewer queries on average for full key recovery. Their lower bounds are also conﬁrmed by experiments. Most of the prior works about plaintext-checking attacks to lattice-based KEMs focus on recovering the full reused secret key. However, the adversary may not have enough oracle access to construct a reliable plaintext-checking oracle for recovering the full secret. Thus, compared to recovering the full secret with substantial amounts of oracle queries, one may be interested in the following question: How to analyze the concrete bit security loss of PKE/KEM after a limited number of oracle queries in plaintext-checking attacks?. Some works already analyzed the eﬀects of information leakage on the LWE problem. For example, Dachman-Soled et al. [3] give a general framework to analyze the inﬂuence of side-channel information. They provide four types of side-channel information (“hint”) and analyze the concrete security loss for each type of hint. However, the side-channel information leaked in the plaintextchecking attack (“plaintext-checking hint”) has not been considered. Thus, it remains unclear how to analyze the inﬂuence of plaintext-checking attacks on the hardness of LWE information. Let S = {S0 , S1 , ..., Sn −1 } be the set of all possible values for one coeﬃcient block and its corresponding probabilities {P0 , P1 , ..., Pn−1 }. For a single coeﬃcient block skA [i], Pj = P r(skA [i] = Sj |skA [i] ← S) for j = 0, 1, ..., n − 1. Let H(S) the Shannon entropy for S, Typically, we have H(S|P Chint) ≤ H(S). In other words, each oracle query decreases the Shannon entropy of Alice’s reused secret skA . O returns a bit b depending on whether the plaintext matches or not. Intrinsically, for each coeﬃcient of reused secret key skA , the querying process can be described as a function f of reused secret key skA , plaintext pt, ciphertext ct, in which: f (skA , ct, pt) = b ∈ {0, 1} We deﬁne such type of side-channel information as a plaintext-checking hint. Suppose the adversary A tries to recover the i-th coeﬃcient of skA . Let v be a unit vector with v[i] = 1. Thus it is very natural to express f (skA , ct, pt) as: f (skA , ct, pt) := f (skA , v) f (skA , ct, pt) is a general description of plaintext-checking hint. We ﬁnd a solution to transform plaintext-checking hints to known hints for lattice-based KEMs. Contributions. The main contributions of this paper include:  
  3: t← − St , e ← − SU , f ← − SV 4: randomness comes from coinB ¯ ←t×A+e 5: U 6: V¯ ← t × B + f + encode(pt) ¯) 7: U ← Compress(U 8: V ← Compress(V¯ ) 9: K ← H(ptct = (U, V )) 10: return K 1: function Dec(pp, skA , ct) 2: P arse ct = (U, V ) ¯ ← Decompress(U ) 3: U 4: V¯ ← Decompress(V ) ¯ × skA 5: W ← V¯ − U  6: pt ← decode(W ) 7: K  ← H(pt ct = (U, V )) 8: return K   
  Model of Plaintext-Checking Attack  
  In a plaintext-checking attack, the adversary interacts with plaintext checking oracle O, which works as shown in Algorithm 2. O is a plaintext checking oracle which receives ct and pt, returning one bit showing if ct decrypts to pt. INDCPA secure public key encryption/key encapsulation mechanisms are vulnerable to plaintext-checking attack. The plaintext-checking oracle exists in many cases. In the client-server protocol where the ciphertext is the encryption of some symmetric key k. The adversary can construct faulty ciphertexts that may or may not decode to k and deliver them to the server. Then the adversary can see if secure messaging works and hence simulates a plaintext-checking oracle O. IND-CCA secure KEM may also suﬀer a plaintext-checking attack since the adversary can create oracle O by a test-based template approach as given in [7].  
  Algorithm 2. Plaintext-Checking Attack  1: skA ← AO (pkA )  = skA then 2: if skA 3: return 1 4: else 5: return 0  
  1: ORACLE O(ct = (U, V ), K) 2: K  ← KEM.Dec(ct) 3: if K = K  then 4: return 1 5: else 6: return 0  
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
  Secret Leakage Model in Plaintext-Checking Attack  
  Since many lattice-based IND-CPA secure KEMs use the same meta-structure, they may have similar plaintext-checking attack procedures. Suppose Alice reuses her public key pkA = (A, B). As described in Algorithm 2, the adversary A crafts diﬀerent plaintext and ciphertext ct, pt to recover Alice’s secret key skA with as fewer oracle access as possible. Each coeﬃcient of skA is sampled independently from S ⊂ Ssk . When the adversary A tries to recover the i-th coeﬃcient of skA , each oracle call leaks information about S. Without loss of generality, let S = {S0 , S1 , ..., Sn−1 } be the set of all possible values of the original sparse secret distribution. Let Pj be the probability that skA [i] = Sj where skA [i] is generated from the distribution S, that is, Pj = P r[skA [i] = Sj |skA [i] ← S] for j = 0, 1, ..., n − 1. Denote the new secret distribution after the oracle query as S  after querying plaintext checking oracle O. When the adversary gets a returned value from the Oracle, he can narrow the range of skA [i] from S to S  until the exact value of skA [i] is determined. The change of secret distribution is shown in Fig. 1. As described in Sect. 3.1, block 1 (in a dashed rectangle) represents the multiplication of Abelian groups SA , SB , St , SU , SV (the yellow blocks) before the adversary A queries the PC Oracle O. The blue block in block 1 represents the secret distribution S ⊂ Ssk before the adversary queries the PC Oracle O. The green block in block 2 represents the new secret distribution S  ⊂ Ssk after A queries the PC Oracle O. Other Abelian groups remain unchanged.  
  Fig. 1. Oracle query in Plaintext-Checking Attack and the change of secret distribution.  
  R. Mi et al.  
  In plaintext-checking attack, the adversary A tries to recover the reused secret by accessing oracle O as few as possible. In other words, each oracle query decreases the Shannon entropy of reused secret skA . A tries to reduce the entropy of S as much as possible. Intrinsically, for each coeﬃcient of Alice’s reused secret key skA , the querying process can be described as a function f of reused secret key skA , secret distribution S, ciphertext ct ∈ SU × SV , pt ∈ M. Formally, we can deﬁne the information leakage in plaintext-checking attack as: Definition 5 (Plaintext-Checking Hint). A plaintext-checking hint on the reused secret skA is the crafted plaintext pt, ciphertext ct, such that f (skA , ct, pt) = b ∈ {0, 1}. Let v be a unit vector with v[i] = 1. The expression of plaintext-checking hint f (skA , ct, pt) can be simpliﬁed as f (skA , v) = b ∈ {0, 1} since the adversary tries to recover the i-th coeﬃcient of skA . Most of the prior works consider the Oracle access times for recovering the full reused key. Since the adversary may be prohibited from gathering suﬃcient sidechannel information to build a plaintext-checking oracle, the adversary A does not have suﬃcient access to a plaintext-checking oracle to completely recover the reused secret. For example, PC Oracle constructed by reusing KEM’s public key cannot be accessed when users stop reusing the public key. Thus one may be interested in a more precise analysis of f (skA , ct, pt) to learn security loss after certain times of Oracle access. To investigate security loss after limited times of Oracle queries, one possible way is to express f (skA , ct, pt) in the form that can be integrated into the lattice.  
  4  
  Experiment Results Kyber  
  Figure 2 gives the concrete relationship between the number of queries and bitsecurity for all parameter sets of Kyber in the NIST third-round submission.  
  Bit Security Analysis of Lattice-Based KEMs Under PCA  
  M. Jauch and V. Maram  
  it is enough to simply double the key size of the aﬀected primitives to restore the same level of security as in the classical setting. However, the community quickly realized that it is not suﬃcient to just consider the quantum security of standalone primitives such as block ciphers – since they are rarely used in isolation in practice – but to also consider their associated modes of operation. Such modes are typically designed to provide enhanced security guarantees such as conﬁdentiality, integrity, and authenticity of encrypted messages. In this paper, we will be focusing on modes related to authenticated encryption (AE). Starting with the work of Kaplan et al. [18] which showed how to break the authenticity guarantees of classical AE modes such as GCM and OCB in the quantum setting in polynomial-time (in contrast to the generic quadratic speed up oﬀered by Grover’s algorithm), follow-up works by Bhaumik et al. [4] and Bonnetain et al. [7] improved the quantum attacks against the latter OCB modes, albeit still targeting authenticity. At a high-level, the above attacks fundamentally rely on other well-known quantum algorithms such as Simon’s period ﬁnding algorithm [29] and Deutsch’s algorithm [8]. Subsequently, Maram et al. [22] extended the aforementioned authenticity attacks to also break conﬁdentiality of the OCB modes in the quantum setting; more formally, the authors targeted a quantum security notion called “IND-qCPA security” [5], which diﬀers from the classical IND-CPA security notion in that the adversary is allowed to make quantum encryption queries. It is worth pointing out that the practicality of this model (also generically called “Q2 security” in the literature) where the attacker has quantum access to the secret-keyed encryption functionality is still currently debated in the community (e.g., see [2,4,6,16,18]); however we consider this discussion beyond the scope of our work. Now focusing on the OCB modes, they are one of the most well-studied and widely inﬂuential classical AE modes. OCB has three versions: OCB1 [27], OCB2 [26] and OCB3 [19]. While OCB1 and OCB3 are provably secure AE schemes in the classical setting, a breakthrough result by Inoue et al. [13] showed that OCB2 is classically broken as an AE mode. More speciﬁcally, Inoue and Minematsu [14] ﬁrst came up with classical attacks on the authenticity guarantees of OCB2. After their attacks became public, Poettering [25] and Iwata [15] proceeded to extend them to also break conﬁdentiality of OCB2 in the classical setting. In this context, the aforementioned work of Maram et al. [22] can be seen as translating this strategy to the quantum setting to break (IND-qCPA) conﬁdentiality of the three OCB modes starting with the quantum forgery attacks in [4,7,18]. However at the same time, Inoue et al. [13] observed that their classical attacks on OCB2 do not extend to other popular AE designs based on OCB such as OTR [23] and OPP [11]; this is due to some subtle diﬀerences in the structures of these modes when compared to OCB. In this paper, we analyze if one would observe something similar regarding the eﬀects of quantum insecurity of OCB on the OTR and OPP modes. Starting with OTR [23], it is technically a block cipher mode to realize a nonce-based authenticated encryption with associated data (AEAD) scheme. We speciﬁcally focus on an instantiation of the mode with AES as the underyling block cipher called AES-OTR [24], which also includes an additional way to  
  Quantum Cryptanalysis of OTR and OPP  
  Our Contributions  
  In this paper, we answer the above questions concerning the quantum security of both AES-OTR and OPP modes by presenting tailor-made quantum attacks. Along the way, some of our attacks involve techniques that we believe will be of independent interest to the broader (post-)quantum cryptographic community. Our concrete results are listed below: Attacks on IND-qCPA Security of AES-OTR. In Sect. 3, we present the ﬁrst IND-qCPA attacks against AES-OTR. Speciﬁcally, our quantum attacks are tailored to three diﬀerent settings depending on how the associated data (AD) is processed: namely, the settings with parallel and serial processing of AD, and the setting where no AD is used at all. For the ﬁrst two settings with non-empty AD, our attacks work in the weak adversarial setting where the nonces used by the challenger to answer encryption queries in the IND-qCPA security game are generated uniformly at random (instead of the nonces being chosen by the adversary). On a high level, the attack breaking IND-qCPA security w.r.t. parallel AD processing uses Simon’s algorithm as well as Deutsch’s algorithm to gain raw block cipher access, i.e., the ability to evaluate the underlying block cipher on arbitrary inputs. With this access, it is straightforward to break IND-qCPA security. This attack strategy is  
  M. Jauch and V. Maram  
  similar to that used in [22] to break conﬁdentiality of the OCB modes. However, we need to make an extra assumption for our attack to be eﬃcient: namely that the authentication tags produced by the AES-OTR encryption oracle are not (signiﬁcantly) truncated. It is worth noting that the speciﬁcation of AESOTR [24] recommends parameters with untruncated tags. But interestingly, in the process we were also able to point out a gap in the quantum cryptanalysis of OCB2’s conﬁdentiality (with non-empty AD) in [22], since the corresponding IND-qCPA attack there uses a similar assumption of untruncated tags implicitly. This was later conﬁrmed by one of the authors [21]. Coming to the setting where no AD is used in AES-OTR, our attack assumes a stronger adversarial setting where the adversary is now allowed to adaptively choose the (classical) nonces for its quantum encryption queries in the IND-qCPA security game. This setting is the same as considered in [22, Section 4.4] with respect to their IND-qCPA attack against OCB2 as a “pure” AE (i.e., no AD) scheme. However, what makes our attack a non-trivial extension of the above attack on OCB2 is that for AES-OTR there is an additional formatting function applied to the nonce before it is AES-encrypted. We thus have to perform some additional steps that increase the overall complexity of our attack. The attack is described in detail in the full version of this paper [17]. Quantum Queries over Unequal -Length Data. Our IND-qCPA attack on AES-OTR w.r.t. serial AD processing involves a novel paradigm which, to the best of our knowledge, has never been considered in the (post-)quantum cryptanalytic literature. Note that in the IND-qCPA security deﬁnition, an adversary is allowed to make encryption queries on a quantum superposition of data. However, according to the laws of quantum physics, a superposition is deﬁned only over states with the same number of qubits. Hence in our IND-qCPA scenario, this translates to the seemingly implicit restriction of an adversary only being able to make superposition queries over equal -length data. In our work, we show how to overcome this restriction by modelling the quantum encryption oracle in the IND-qCPA security game in a way which allows an adversary to also make superposition queries over unequal -length data (see Sect. 3.3 for more details). Furthermore, to give evidence of the power of this new quantum cryptanalytic paradigm, we show how an adversary can immediately gain raw block cipher access in our IND-qCPA attack on AES-OTR with serial AD processing using only Simon’s algorithm; this is in contrast to the IND-qCPA attacks against OCB in [22] and against AES-OTR with parallel AD processing discussed above which also require Deutsch’s algorithm to obtain this raw access. It’s also worth pointing out that using this novel paradigm, we no longer have to rely on the above extra assumption of the AES-OTR authentication tags being untruncated in the serial AD case. Finally, our paradigm can also be extended to cryptanalysis in the more realistic post-quantum setting, i.e., where the adversary has quantum access only to public cryptographic oracles (also called “Q1 security” in the literature) such as hash functions in the so-called Quantum Random Oracle Model (QROM).  
  Quantum Cryptanalysis of OTR and OPP  
  A Quantum Key-Recovery Attack on OPP. We present the ﬁrst quantum key-recovery attack on OPP in Sect. 4. Our attack is conducted in the weak adversarial setting similar to our IND-qCPA attacks on AES-OTR, where the nonces are chosen uniformly at random by the challenger. In contrast to AESOTR being based on a block cipher which may only be inverted knowing the key, OPP is built upon an eﬃciently invertible public permutation P . We exploit this speciﬁc property to formulate our key recovery attack. On a high level, we are able to recover a value Ω = P (. . . ||K) using only a single quantum encryption query via an application of Simon’s algorithm, where K is the key used in OPP; hence, applying P −1 to Ω allows us to recover the key K.  
  2  
  M. Jauch and V. Maram  
  Simon’s Algorithm. Simon’s algorithm is a quantum algorithm that is able to solve the following problem referred to as Simon’s problem. This algorithm is the key element of most of our quantum attacks against AES-OTR and OPP. Definition 1. (Simon’s Problem) Given quantum access to a Boolean function f : {0, 1}n → {0, 1}n (called Simon’s function) for which it holds: ∃s ∈ {0, 1}n : ∀x, y ∈ {0, 1}n f (x) = f (y) ⇐⇒ y ∈ {x, x ⊕ s}, the goal is to ﬁnd the period s of f . This problem of course can be solved in a classical setting by searching for collisions in Θ(2n/2 ), when we are given classical access to the function f . However, when we are able to query the function f quantum-mechanically, and we are thus allowed to make queries of arbitrary quantum superpositions of the form |x|0 → |x|f (x), Simon’s algorithm can solve this problem with query complexity O(n). On a high level, Simon’s algorithm is able to recover a random vector y ∈ {0, 1}n in a single quantum query to f that is orthogonal to the period s, i.e. y · s = 0. This subroutine is repeated O(n) times such that one obtains n − 1 independent vectors where each is orthogonal to s with high probability. Therefore s can be recovered by solving the corresponding system of linear equations. For more details on the subroutine, we refer to [18]. Also [18] showed that Simon’s algorithm recovers the hidden period s with O(n) quantum queries even if f has some “unwanted periods” – i.e., values t = s such that f (x) = f (x ⊕ t) holds with probability ≤ 1/2 over a random choice of x. As we will show, this condition is always satisﬁed in our attacks. Deutsch’s Algorithm. Deutsch’s algorithm solves the following problem. Definition 2. Given quantum access to a Boolean function f : {0, 1} → {0, 1}, the goal is to decide whether f is constant, i.e. f (0) = f (1), or f is balanced, i.e. f (0) = f (1). The algorithm can solve this problem with a single quantum query to f with success probability 1; note that any algorithm with classical access to f would need two queries for the same. To be precise, Deutsch’s algorithm solves the above problem by computing the value f (0) ⊕ f (1) using a single quantum query to f . IND-qCPA Security of AEAD Schemes. Below we deﬁne IND-qCPA security for nonce-based authenticated encryption with associated data (AEAD) schemes; the formal deﬁnitions for such nonce-based AEAD schemes are provided in the full version [17]. Definition 3. (IND-qCPA with random nonces) A nonce-based AEAD scheme Π = (Enc, Dec) is indistinguishable under quantum chosen-plaintext attack (IND-qCPA secure) with random nonces, if there is no eﬃcient quantum adversary A that is able to win the following security game, except with probability at most 12 +  where  > 0 is negligible.  
  Quantum Cryptanalysis of OTR and OPP  
  Quantum Key-Recovery Attack on OPP  
  The Oﬀset Public Permutation Mode (OPP) was proposed in [10] and [11] and it essentially tries to generalize OCB3 by replacing the underlying block cipher by a public permutation and a diﬀerent form of masking. In this section we will show that using a public permutation the way OPP does, actually leads to a devastating key recovery attack in the quantum setting. Our attack uses a similar strategy as the IND-qCPA attack against OCB2 in [22, Section 4.4] with adaptively chosen nonces; but instead of choosing a new nonce adaptively to break IND-qCPA security, we are able to recover the value Ω := P (X||K) where P is an eﬃciently invertible public permutation and K is the key (X can be seen as a formatting of the nonce). In contrast to OTR being based on a block cipher that may only be inverted knowing the key, we are here dealing with a public permutation that can be inverted eﬃciently. This is the key issue of OPP and the reason we are able to recover the key knowing Ω. 4.1  
  Specification of OPP  
  = P δ(K, X, (0, 0, 1)) ⊕ 0n ⊕ δ(K, X, (0, 0, 1)) C   = P ϕ2 (Ω) ⊕ ϕ(Ω) ⊕ Ω ⊕ ϕ2 (Ω) ⊕ ϕ(Ω) ⊕ Ω  
  = C0 , i.e. the encryption where Ω = P (X||K) and X = pad0n−κ−k (N ). If now C of the oracle and our manual computation coincide, we can be sure that we recovered the right key K. Else, we can just repeat the attack until the assertion returns to be true. With the included sanity check, we are certain to recover the key K at some point, as step one of our attack involves an application of Simon’s algorithm that already succeeds with high probability thanks to the non-existence of “unwanted periods” as argued before. Finally, we compare our quantum key-recovery attack on OPP with the corresponding attack on the generic Even-Mansour construction of [18] in the full version [17]. Acknowledgements. It is our pleasure to thank Xavier Bonnetain for helpful discussions, and the anonymous reviewers of SAC 2023 for their constructive comments and suggestions.  
  Quantum Cryptanalysis of OTR and OPP  
  S. Deshpande et al.  
  The motivation behind our polynomial multiplication unit is as follows: we represent the non-sparse arbitrary polynomial as arb poly and the sparse ﬁxedweight polynomial by sparse poly. For sparse poly, rather than storing the full polynomial we only store the indices for non-zero values. Then, the multiplication is performed by left shifting arb poly with each index of sparse poly and then performing reduction of the resultant vector in an interleaved fashion. Since the value of n is large in all parameter sets of HQC, we take a sequential approach for performing the left shift. We implement a sequential left shift module similar to one in [12]. The shift module described [12] uses a register based approach and is not scalable when the length of the input is as large as the n value for the HQC parameters (due to a larger resource utilization and complex routing). This issue is circumvented in our design by implementing a block RAM based sequential variable shift module with a dual port BRAM and small barrel rotation unit. The barrel rotation unit and the block RAM widths are used as performance parameter (BW - Block Width) for the shift module and in turn for the whole polynomial multiplication unit. A similar implementation of sequential variable shift module was previously described in [10], however we could not readily use their implementation because the shift module is tightly embedded with the other modules for a diﬀerent application and we re-implemented our version. The hardware design of our polynomial multiplication module (poly mult) is given in Fig. 7b of our extended online version of this work [11]. The arb poly input to the poly mult module is loaded sequentially and the width is of each chunk of arb poly is equal to BW (making total number of chunks in polynomial equal to RAMDEPTH = ceil(n/BW)). We store the least signiﬁcant part of the polynomial at the lowest address of the block RAM and the most signiﬁcant part at the highest address. Since the polynomial length in HQC parameters is equal to n and is not divisible by BW (n is a prime) we pad the most signiﬁcant part of the polynomial with zeros. For sparse poly, one index is loaded at a time. While performing the shift operation we also perform the reduction (X n − 1) in an interleaved fashion. As the result of multiplying two n-bit polynomials could be a 2n-bit polynomial and reduction of 2n-bit polynomial to (X n − 1) in F2 is equivalent to slicing of the 2n-bit polynomial into two parts of n-bit polynomials and then performing a bitwise XOR. As result, when the shift operation is performed on each chunk we also compute the address value (ADDR 2N) (signifying the degree of the resultant polynomial). If we notice that this degree of the resultant polynomial is greater that n we perform XOR of this chunk to the lower chunk by decoding the address based on the value of ADDR 2N. We perform similar operation over all the indices of the sparse poly to achieve the ﬁnal multiplied resultant value. The clock cycles taken by our poly mult module for one polynomial multiplication can be computed using the following formula where WSP ARSE is weight of the sparse polynomial, n is length of the polynomial, BW is the block width, 3 cycles represents the number of pipeline stages and 2 cycles are for the start and done synchronization with interfacing modules. The clock cycles taken for shift and interleaved reduction for one index is (3 +ceil(n/BW)). Our poly mult module is constant time and we achieve that by ﬁxing the WSP ARSE to a speciﬁc value (w and wr ) based on the parameter set.  
  Fast and Eﬃcient Hardware Implementation of HQC  
  HQC Joint Design and Related Work  
  In this section, we present our joint hardware design of a HQC combining our keygen, encap, and decap modules (described in Sect. 2.4, Sect. 2.5, and Sect. 2.6 respectively) into one overall design. Following that we compare our joint design with other HQC combined designs from the literature in Sect. 3.2. In addition to that, we also conduct a comprehensive literature survey focusing on full hardware designs of the other three fourth-round public-key encryption and key-establishment algorithms in NIST’s standardization process: BIKE, Classic McEliece, and SIKE. We also include the CRYSTALS-Kyber, a publickey encryption and key-establishment algorithm selected for standardization at the end of the prior third round. Due to limited space we discuss this part in Appendix 1.A. 3.1  
  HQC Joint Design  
  Encoding Analysis  
  In this section, we give theoretical bounds on the precision loss from encoding and decoding. Proofs of the results can be found in the full version [14]. To understand precision loss due to encoding, as well as translate noise bounds derived in the plaintext space to noise bounds in the message space, we investigate how distance  
  On the Precision Loss in Approximate Homomorphic Encryption  
  Application to Encoding  
  In this section, we apply the results from Sect. 3.1 to produce bounds on the growth of polynomials under encoding and decoding. Our ﬁrst result enables us to produce bounds in the plaintext space given bounds in the message space. Lemma 6. Suppose m ∈ RN and z ∈ CN/2 are such that m = Encode(z, Δ). Then m∞  Δ z∞ + 12 . The result in Theorem 1 enables us to give bounds in the message space given bounds in the plaintext space. Lemma 7. Suppose m ∈ RN has m∞  B. If z = Decode(m, Δ) we have √ I(N )2 +1 that z∞  B, and this bound is the best possible. Δ Due to the fast convergence of I(N ) to 2N π , we can replace this result by its limiting value. We can therefore precisely bound the error introduced during encoding. Corollary 3. Suppose z ∈ CN/2 is encoded under scale factor√Δ. Then the I(N )2 +1 precision lost in each slot as a result of encoding is bounded by , and 2Δ N this bound tends to πΔ as N → ∞. We can also give analogous results for the real and imaginary components alone. Lemma 8. Suppose m ∈ RN has m∞  B. Then if z = Decode(m, Δ) we 2N B, and this bound is the best possible. have that Re(z)∞ , Im(z)∞  πΔ Corollary 4. Suppose z ∈ CN/2 is encoded under scale factor Δ. Then the precision lost on both the real and imaginary components of each slot is bounded N . by πΔ This shows that, perhaps surprisingly, if using a worst case analysis, it is not possible to achieve a tighter analysis of precision loss by considering only the error on the real part of the message. To beneﬁt from restricting our attention to only the real part, we must be able to specify statistical, rather than worst case, behaviour.  
  On the Precision Loss in Approximate Homomorphic Encryption  
  Output Variance  
  Final output bound (CE) √ Fresh = +h+ N · ρfresh · HC (α, N ) √ Add ρ2add = ρ21 + ρ22 N · ρadd · HC (α, N ) √ 2 2 2 2 2 2 2 ρpre-mult = N ρ1 ρ2 + ρ2 m1 2 + ρ1 m2 2 N · ρpre-mult · HC (α, N ) PreMult √  −2  1 2 2 2 2 N · ρks · HC (α, N ) Key-Switch ρks = ρ + 12 P N Q σ + 1P Q (h + 1) √ ρ2 1 Rescale ρ2rs = Δ + (h + 1) N · ρrs · HC (α, N ) 2 12 ρ2fresh  
  5.1  
  A. Costache et al.  
  noise in the ring after each operation. We also (in the rows marked as ‘Real’ and ‘Complex’) report the observed noise in the message space. In Table 5 we report the experimental results for FullRNS-HEAAN [22] in the message space. For the HEAAN v1.0 [24] and FullRNS-HEAAN [22] experiments in the message space, in each trial, we generate a vector of random numbers, encode them, encrypt them, and homomorphically evaluate the circuit as described above. Then, we decrypt and decode and measure the precision loss. The rows marked as ‘Real’ correspond to generating numbers with real part and imaginary part both uniform in [0, 1], encoding and decoding with scale factor Δ, and reporting only the real error on the computation. The rows marks as ‘Complex’ correspond to generating numbers with real part and imaginary part both uniform in [0, 1] and reporting the magnitude of the largest error. While in exact schemes, it is trivial to observe the noise, this is not so straightforward for CKKS. Our methodology was to generate three plaintexts m1 , m2 and m3 , and to run the circuit both in the plaintext space and in the ciphertext space. In other words, the noise reported in Tables 4 and Table 5 is the result of ((m1 + m2 ) · m3 ) − Dec((Enc(m1 ) + Enc(m2 )) · Enc(m3 )) . Results: The plaintext space experiments of Table 4 illustrate that for Textbook CKKS [10], the average case noise approach (CLT) introduced in this work, and our reﬁnements to the prior worst case canonical embedding approach (CE), both improve on the heuristics given in prior work (P-CE), in the sense of predicting a value closer to the observed noise. For CLT compared to P-CE, the heuristicto-practical gap reduces from around 8 bits to less than 1 bit. However, the CLT approach slightly underestimates the maximal noise, and sometimes slightly underestimates the maximum noise (as illustrated in the column gap). The WCR approach leads to a large heuristic-to-practical gap after multiplication, which we also observed in the Complex experiments (in the message space), so we omit it in the FullRNS-HEAAN experiments. For the message space results in Table 4, the addition and multiplication results are similar for both the Real and Complex case. The CLT and CE approaches both underestimate the average and maximum noise by 3 to 7 bits. The WCR approach correctly bounds the noise: tightly for addition, but very loosely after multiplication. The results in Table 5 illustrate that for the RNS variant of [9], the CLT approach and the CE approach both improve on the prior approach (P-CE), in the sense of predicting a value closer to the observed noise. For CLT compared to P-CE, the heuristic-to-practical gap typically reduces from around 6 bits to less than 1 bit. However, we again very frequently observe the CLT giving an underestimate. At log N = 14, a jump is seen in the observed noise values, and in this case the prior approach P-CE gives a tight bound on the noise. Discussion: Our results illustrate that, for the plaintext space for Textbook CKKS, and for the message space in the RNS variant of [9], both the averagecase noise analysis introduced in this work and the reﬁnement of the prior  
  On the Precision Loss in Approximate Homomorphic Encryption  
  for this discrepancy. The authors of [16] compare experimental results of the BGV scheme as implemented in HElib to the theoretical bounds from the work of [29], and observe that the latter also underestimates the noise growth in practice. In contrast, the implementation-speciﬁc analysis of [16] is shown to very closely match the observed noise growth. Our heuristics are not speciﬁc to the implementations in HEAAN v1.0 [24] or Full-RNS HEAAN [22], and assumptions on which the heuristics rely may not hold for each implementation. For example, our experiments indicate that in HEAAN v1.0 [24] (though not in Full-RNS HEAAN [22]), the independence heuristic between coeﬃcients of the noise polynomial fails at encryption. We believe that developing implementationspeciﬁc noise analyses for CKKS is an important direction for future work.  
  and Aleksander Essex  
  Western University, London, Canada {mpratapa,aessex}@uwo.ca Abstract. The number-theoretic literature has long studied the question of distributions of sequences of quadratic residue symbols modulo a prime number. In this paper, we present an eﬃcient algorithm for generating primes containing chosen sequences of quadratic residue symbols and use it as the basis of a method extending the functionality of additively homomorphic cryptosystems. We present an algorithm for encoding a chosen Boolean function into the public key and an eﬃcient two-party protocol for evaluating this function on an encrypted sum. We demonstrate concrete parameters for secure function evaluation on encrypted sums up to eight bits at standard key sizes in the integer factorization setting. Although the approach is limited to applications involving small sums, it is a practical way to extend the functionality of existing secure protocols built on partially homomorphic encryption schemes. Keywords: Secure computation · Additive homomorphic encryption Quadratic residues · Residue symbol sequences  
  1  
  M. Pratapa and A. Essex  
  – A mapping function h : Z → Zp , that maps an input x into h(x) = (αx + β) mod p. Here, {α, β} ∈ Z+ and are given as input parameters to facilitate the application of QR function. We are interested in locating primes p which contain some residue symbol sequences modulo a prime p that imitate the range of f . In other words, given and integers {α, β} we are looking for some p that can compute: QR(h(x), p) = f (x) for 0 ≤ x < t. Approach to Secure Computation. We can homomorphically evaluate f using an additive scheme as follows. Let CS = {Gen, Enc, Dec} be an additively homomorphic public-key cryptosystem that display the following homomorphisms: Enc(x1 ) · Enc(x2 ) = Enc(x1 + x2 mod p) and Enc(x1 )x2 = Enc(x1 x2 mod p). Given an encrypted value Enc(x) in the range 0 ≤ x < t, and an α, β > 0, one can homomorphically compute Enc(h(x)) as follows: Enc(h(x)) = Enc(x)α · Enc(β) = Enc(αx + β mod (p)). Applying the quadratic residue function to the decryption Enc(h(x)) yields. QR(Dec(Enc(h(x))), p) = QR(h(x), p) = f (x). This demonstrates the basic mechanics of the secure evaluation of f . Clearly, however, the decrypter could recover x from seeing h(x), and thus a homomorphic blinding function will be presented later in Sect. 4. 3.2  
  Prime Numbers with Chosen Residue Symbol Sequences  
  Now we show the existence of an integer p implies the existence of a prime p with the same congruences. Since p ≡ p mod Aprod , and therefore p ≡ bx mod ax , then   p = x . ax Finally, since p is relatively prime to Aprod , Dirichlet’s theorem guarantees there  
  are inﬁnitely many primes of the form kAprod + p . Theorem 2. For all t ∈ Z+ and all functions f : Zt → {0, 1} there exists a prime p and two integers 0 < α, β < p such that for all 0 ≤ x < t   αx + β +1 p = f (x) 2   denotes the Legendre symbol of αx + β modulo p. where αx+β p Proof. Let α, β, t be positive integers such that αx + β is prime for all 0 ≤ x < t. The existence of such an α, β is guaranteed for all t > 0 by a theorem due to Green and Tao [20], which proves the primes contain arbitrarily long arithmetic sequences, and, therefore, there exists an α, β for all t > 0 such that αx + β is prime for all 0 ≤ x < t. Given such a linear sequence of (αx + β)’s where all of them are prime valued,3 Theorem 1 guarantees there exists a prime p such that for all 0 ≤ x < t,   p = 2f (x) − 1. αx + β 3  
  Requiring all (αx + β) be prime is only done to facilitate the existence proof. In practice, Algorithm Gen (see Sect. 4.1) can generate suitable keypairs in the presence of composite (αx + β)’s.  
  Our Cryptosystem  
  Let CS = {Gen, Enc, Dec, Add, Smul, Eval} be an additively homomorphic publickey cryptosystem. Let M be the plaintext space and m ∈ M be a message. Without loss of generality and for the sake of a concrete description, we build CS based on the cryptosystem due to Okomoto and Uchiyama [21], which has a message space cardinality |M| = p for a large prime p. Given pre-computed sequence parameters α, β, and a Boolean function f : Zt → {0, 1} we deﬁne CS with the following functionalities: – Gen(1ρ , α, β, f ): Outputs secret key SK = {p, q} and public key PK = {n} where n = p2 q. Here p is chosen such that QR(αx + β, p) = f (x) for 0 ≤ x < t. To facilitate eﬃcient generation for non-trivial values of t, p is generated using the algorithm presented in Sect. 4.1. By contrast, q is randomly chosen using standard methods. Both |p| = |q| = λ, where λ is a standard length at the ρ-bit security level in the integer factorization setting.  
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
  – Enc(PK, m): Encryption function accepting a public key PK, plaintext m and outputting a ciphertext c = m. – Dec(SK, c): Decryption function accepting a private key SK, ciphertext c = m and outputting plaintext m. – Add(c1 , c2 ): Homomorphic addition accepting two encrypted messages c1 = m1  and c2 = m2  and outputting a ciphertext c = (m1 + m2 ) mod p. – Smul(s, c): Scalar homomorphic multiplication accepting a ciphertext c1 = m1  and a scalar m2 , outputting a ciphertext c = (m1 m2 ) mod p. The properties of CS can be further used to securely evaluate f on an encrypted plaintext. Toward that end, a sixth functionality Eval, is deﬁned: – Eval(PK, α, β, c): Secure evaluation of f on an encrypted plaintext c = m. First, a non-zero blinding parameter rc is uniformly sampled from the message space M. Since M = Zp and p is a private value, we sample uniformly from the public interval rc ← [1, 2λ ].4 The function computes: Smul(rc2 , Add(Smul(m, α), Enc(β)) = rc2 · (αm + β) mod p. Note that decrypting (αm + β) mod p directly would reveal m, as α, β are public. Hence, a blinding operation is applied to randomize this value while preserving its residuosity. The result of Eval is the encryption of uniform quadratic residue in Zp if QR(αm + β) = 1, and a uniform non-residue otherwise. The output of Eval can then be decrypted by the private key holder and the quadratic residuosity of the plaintext tested to reveal the outcome of f (m). 4.1  
  Key Generation  
  Correctness of the Evaluation Function  
  The Eval function deﬁned in Sect. 4 is used to homomorphically evaluate f (m) on encrypted plaintext m. Theorem 3. Given c = Enc(m), QR(Dec(SK, Eval(PK, α, β, c)), p) = f (m) in the range 0 ≤ m < t. Proof. Given ciphertext c = Enc(m), a blinding factor rc sampled from the public interval rc ← [1, 2λ ], and public sequence parameters α, β, Eval computes: 2  
  c = Eval(PK, α, β, c) = (cα · β)rc mod n 2  
  information about quadratic residuosity QR(x, p) for t elements, where t is the function size. Due to this reason, the security of our system relies on a slightly weaker assumption than factoring n = p2 q. The alternate hardness assumption we propose here is factoring n = p2 q in the presence of the quadratic residuosity oracle QR(x, p). Although the information provided by such an oracle is highly restricted relative to Zn , whether this information can be exploited to factorize n remains an open question and requires further cryptanalytic eﬀorts.  
  6  
  Protocol  
  Protocol π is conducted between two semi-honest parties PA and PB . Let PA input a vector of plaintexts X = {x1 , . . . , xa } and PB input a vector of plaintexts Y = {y1 , . . . , yb } for xi , yj ∈ M. Let CS = {Gen, Enc, Dec, Add, Smul, Eval} be an additively homomorphic cryptosystem with the functionalities deﬁned in Sect. 4. Let PA be the holder of the private-key SK. Since our protocol is framed as an extension of existing protocols based on additive schemes (e.g., vector addition, weighted sums etc.). We begin by deﬁning a sub-protocol πsub , which capturing the existing protocol conducted on X and Y using the conventional functionalities {Enc, Add, Smul}. Suppose the output of πsub results in PA receiving a single ciphertext m where m represents the nominal outcome of πsub . Protocol π extends πsub with the Eval functionality to homomorphically compute f (m) given m. The full protocol π is presented in Fig. 1.  
  Fig. 1. Secure Function Evaluation Protocol π  
  M. Pratapa and A. Essex   
  Proof. From Theorem 3 we previously established that QR(Dec(SK, c )) returns f (m) as  QR(Dec(SK, c ), p)) = f (m). 6.1  
  Participant Privacy During the Protocol π  
  Proof. The proof of PA ’s privacy is simple because PB only receives the ciphertext c = Enc(x), i.e., mPA = c. To simulate PB ’s view the simulator just needs to sample the messages of the form c ← Z∗n . Due to the semantic security of the CS, it is easy to establish that a ciphertext c = Enc(x) is computationally indistinguishable from an element of Z∗n . SPB can now compute the output using public key PK, c and PB ’s input xPB and computing f (xPA , xPB ) homomorphically,  
  thereby simulating V iewPB . 6.2  
  M. Pratapa and A. Essex  
  the work in [16], the protocol uses Dice coeﬃcient as a similarity metric to perform linkage between two datasets that consist of user names. The records are linked approximately to address any variations or errors in the strings being compared. Accordingly, the records will be considered a match if the Dice coeﬃcient between two strings is above a threshold value. The protocol is performed between two parties PA , PB as shown below. The details for sub-protocols 1 and 2 can be referred from [16]. The diﬀerence with our protocol is during computing the evaluation function, which is in Step 6 of sub-protocol 2 in [16]. Particularly, Protocol 1 in [16] is modiﬁed as follows: – Public parameters: PK, α, β and for a threshold value t maximum set cardinality μ, where μ = t – Private parameters: Party PA holds a list of strings [a1 , . . . , an ] and private keys. Party PB holds a list of strings [b1 , . . . , bn ] – PA , PB produce set intersection cardinalities between the private inputs using sub-protocol 1 from [16] – By modifying the ﬁnal step in sub-protocol 2 in [16] both parties compute threshold dice coeﬃcient dij such that dij = Eval(PK, α, β, θb ) = ((θb )α · 2 β)rc – Output: For all threshold dice coeﬃcient values, if QR(Dec(SK, dij ), p) = 1, PA outputs the index. For a given threshold function, we can compute the dice-coeﬃcient for more precise threshold values compared to [16]. Due to the ability of Gen to generate primes as per the f (x) s range, we can compute any kind of functions securely unlike the approaches in [1,16,30] (see table 3).  
  Table 3. Comparison between secure function evaluation protocols that rely on the runs of quadratic residues. Performance Indicator  
  Conclusion  
  This paper discusses a method to extend the functionality of additively homomorphic schemes in applications where the encrypted sum is below a threshold t  
  Secure Function Extensions to Additively Homomorphic Cryptosystems  
  Abstract. The Implicit Factorization Problem (IFP) was ﬁrst introduced by May and Ritzenhofen at PKC’09, which concerns the factorization of two RSA moduli N1 = p1 q1 and N2 = p2 q2 , where p1 and p2 share a certain consecutive number of least signiﬁcant bits. Since its introduction, many diﬀerent variants of IFP have been considered, such as the cases where p1 and p2 share most signiﬁcant bits or middle bits at the same positions. In this paper, we consider a more generalized case of IFP, in which the shared consecutive bits can be located at any positions in each prime, not necessarily required to be located at the same positions as before. We propose a lattice-based algorithm to solve this problem under speciﬁc conditions, and also provide some experimental results to verify our analysis.  
  Keywords: Implicit Factorization Problem algorithm · Coppersmith’s algorithm  
  both LSBs-MSBs Middle bits General  
  May, Ritzenhofen [15] 2α  
  LSBs  
  Y. Feng et al.  
  seems not easy since the bound for GIFP directly yields a bound for any known √ variant of IFP. Improving the bound for GIFP to the one better than 4α (1 − α) means that we can improve the bound for the variant of IFP sharing the middle bits located in the same position, and improving the bound for GIFP to the one better than 2α − 2α2 means that we can improve the bound for any known variant of IFP. Roadmap. Our paper is structured as follows. Section 2 presents some required background for our approaches. In Sect. 3, we present our analysis of the Generalized Implicit Factorization Problem, which constitutes our main result. Section 4 details the experimental results used to validate our analysis. Finally, we provide a brief conclusion in Sect. 5.  
  2  
  Conclusion and Open Problem  
  In this paper, we considered the Generalized Implicit Factoring Problem (GIFP), where the shared bits are not necessarily required to be located at the same positions. We proposed a lattice-based algorithm that can eﬃciently factor two RSA moduli, N1 = p1 q1 and N2 = p2 q2 , in polynomial time, when the primes share a suﬃcient number of bits. √ Our analysis shows that if p1 and p2 share γn > 4α (1 − α) n consecutive bits, not necessarily at the same positions, then N1 and N2 can be factored in  
  Generalized Implicit Factorization Problem  
  CLAASP: A Cryptographic Library for the Automated Analysis of Symmetric Primitives Emanuele Bellini , David Gerault , Juan Grados(B) , Yun Ju Huang , Rusydi Makarim , Mohamed Rachidi , and Sharwan Tiwari Cryptography Research Center, Technology Innovation Institute, Abu Dhabi, UAE {emanuele.bellini,david.gerault,juan.grados,yunju.huang,mohamed.rachidi, sharwan.tiwari}@tii.ae  
  Abstract. This paper introduces CLAASP, a Cryptographic Library for the Automated Analysis of Symmetric Primitives. The library is designed to be modular, extendable, easy to use, generic, eﬃcient and fully automated. It is an extensive toolbox gathering state-of-the-art techniques aimed at simplifying the manual tasks of symmetric primitive designers and analysts. CLAASP is built on top of Sagemath and is open-source under the GPLv3 license. The central input of CLAASP is the description of a cryptographic primitive as a list of connected components in the form of a directed acyclic graph. From this representation, the library can automatically: (1) generate the Python or C code of the primitive evaluation function, (2) execute a wide range of statistical and avalanche tests on the primitive, (3) generate SAT, SMT, CP and MILP models to search, for example, diﬀerential and linear trails, (4) measure algebraic properties of the primitive, (5) test neural-based distinguishers. We demonstrate that CLAASP can reproduce many of the results that were obtained in the literature and even produce new results. In this work, we also present a comprehensive survey and comparison of other software libraries aiming at similar goals as CLAASP. Keywords: Cryptographic library primitives  
  1  
  E. Bellini et al.  
  Regarding linear cryptanalysis, we obtain a linear trail of Salsa with better correlation than the one reported in [17]. This discovery has the potential to enhance the correlation of the diﬀerential-linear distinguisher against Salsa reduced to 8 rounds presented in the aforementioned paper. Finally, in terms of neural cryptanalysis, CLAASP implements (and can reproduce the results of) [10], in addition to the seminal results of [26]. In addition, researchers willing to apply neural cryptanalysis to new ciphers using the techniques from [10] can do so in a straight-forward manner using the library functions. Besides the presentation of the library, important contributions of this work are a survey and a comparison (where possible) of the main software tools trying to achieve the same goals as CLAASP.  
  2  
  E. Bellini et al.  
  Truncated Diﬀerential Search. This module oﬀers a range of features, including the ability to easily discover truncated diﬀerentials with only one active bit in both the input and output states. Such diﬀerentials, when paired with linear approximations, can be very useful for increasing the correlations of diﬀerentiallinear distinguishers. For instance, we successfully used this module to rediscover the truncated diﬀerential outlined in [3], which has been cited and studied extensively in various papers (such as [22]). To view the script we used to rediscover this diﬀerential, you can refer to the accompanying repository for this paper.  
  Fig. 5. Timings of the avalanche tests for ﬁve rounds of popular ciphers  
  Table 1, and our diﬀerential distinguishers are the longest ones among existing ones. It is important to note that while the previous attacks may have been adjusted for key recovery, identifying the longest distinguisher is very important to deeply comprehend the structural properties of these primitives as pseudo random permutations. These results demonstrate that the proposed framework is eﬀective for evaluating the tight diﬀerential bounds. Diﬀerence in Behavior of Clustering Eﬀect Between PRINCE and QARMA. We look into the diﬀerence between PRINCE and QARMA in the behavior of a differential. Our experiments observe that the gaps in the probability between a diﬀerential characteristic and a diﬀerential can be large in QARMA under the SK setting compared to that in PRINCE. Speciﬁcally, QARMA under the single-key (SK) setting has a large impact on the clustering eﬀect, and the case reported by K¨ olbl and Roy [19] can occur in QARMA under the SK setting. A detailed investigation of such gaps reveals that they are inﬂuenced by diﬀerent design strategies for the linear layers (i.e., matrices). After conducting the additional experiments using four types of matrices with diﬀerent properties, we ﬁnd that the target cipher has a good resistance to a clustering eﬀect when each output bit of the round function depends on as many input bits of the round function  
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
  Preliminaries Deﬁnitions of Diﬀerential Characteristic and Diﬀerential  
  We frequently use terms diﬀerential characteristic and diﬀerential throughout this paper. To avoid mixing these terms, we specify their deﬁnitions and how to calculate their probabilities. Further, we provide the deﬁnition of weight that is also frequently used in this paper. Notably, we explain a diﬀerential characteristic and diﬀerential over an r-round iterated block cipher E(·) = fr (·) ◦ · · · ◦ f1 (·). Deﬁnition 1 (Diﬀerential characteristic). A diﬀerential characteristic is a sequence of diﬀerences over E deﬁned as follows: f1  
  f2  
  SAT-Based Automatic Search for Diﬀerential Characteristics  
  SAT. When a formula consists of only AND (∧), OR (∨), and NOT (·) operations based on Boolean variables, we refer to it as a Boolean formula. In a SAT problem, a SAT solver checks whether there is an assignment of Boolean variables that can validate a Boolean formula or not. If such an assignment exists, a SAT solver returns satisﬁable or “SAT”. Generally, a SAT problem is an NPcomplete [13]. However, owing to numerous eﬀorts for SAT problems, nowadays, there are numerous excellent SAT solvers that can solve a SAT problem very eﬃciently, such as CaDiCaL, Kissat, and CryptoMiniSat5. A Boolean formula can be converted into a Conjunctive Normal Form (CNF), which is expressed by the conjunction (∧) of the disjunction (∨) on (possibly ja i ci,j ), where ci,j is a Boolean negated) Boolean variables, such as a=0 ( b=0 ja variable. We call each disjunction b=0 ci,j in a Boolean formula a clause. SAT-Based Automatic Tools. SAT-based automatic tools are known as a valid approach to ﬁnd optimal diﬀerential/linear characteristics and are more powerful than MILP-based ones as shown in [27]. To implement its approach with the SAT method, the diﬀerential/linear propagation over all operations in a primitive must be converted into a CNF, and then we check if there exists a diﬀerential/linear characteristic along with a speciﬁed weight as a SAT problem. We can know the optimal diﬀerential/linear characteristics by solving some SAT problems by changing the number of speciﬁed weights. SAT Models for Basic Operations. Our framework is based on a pure-SAT model proposed by Sun et al. [26,27]. Due to the page limitation, we do not give the detailed modeling method (for more information, please refer to Sun et al.’s work). Herein, we specify some basic notations that are used in this study to construct a whole SAT model as follows: MSAT : A whole SAT model that we solve. Mcla.operations : Clauses to express the propagation of diﬀerences in a certain operation. These clauses also contain variables to express a weight corresponding to the propagation of diﬀerences in a probabilistic operation. Mvar : Variables to construct clauses. In this study, we use Mcla.xor , Mcla.matrix , and Mcla.sbox as clauses to express the propagation of diﬀerences in PRINCE and QARMA. In addition, we also use Mcla.input and Mcla.sec(B) to evaluate a minimum weight. These clauses play a role as follows: Mcla.input : Clauses to avoid a trivial diﬀerential propagation, such as all input diﬀerences being zero at the same time. Mcla.sec(B) : Clauses to count the total weight of a primitive. More speciﬁcally, j the constraint of i=0 pi ≤ B can be added, where pi is a Boolean variable to express a weight and j is the total number of pi . There are several methods to realize such a constraint in a Boolean formula [4,25]. Among these, we employ Sequential Encoding Method [25] that was used in numerous works.  
  Parallel SAT Framework to Find Clustering of Diﬀerential Characteristics  
  Finding Diﬀerential Characteristics with Minimum Weight. With the clauses and variables introduced in this section, we construct a whole SAT model as follows: MSAT ← (Mcla.matrix , Mcla.sbox , Mcla.sec , Mcla.input ). Now, we are ready to ﬁnd a diﬀerential characteristic with the minimum weight by feeding MSAT and Mvar to a SAT solver. If a SAT solver returns “UNSAT”, there is no diﬀerential characteristic with a weight of ≤ B. In that case, we increment B and repeat it until a SAT solver returns “SAT”. This means that we obtain a diﬀerential characteristic with the minimum weight of B. Modeling for a Clustering Eﬀect. To take a clustering eﬀect into account, we must solve a SAT problem multiple times with the same input and output differences, while the identical internal diﬀerential propagation is deleted from the solution space of the initial SAT problem. To realize this procedure, we introduce the following clauses: Mcla.clust : Clauses to ﬁx the input and output diﬀerences to ﬁnd multiple differential characteristics with the same input and output diﬀerences. Mcla.clust : Clauses to remove the internal diﬀerential propagation from a SAT model. These will be repeatably added to a SAT model whenever another internal diﬀerential propagation is found. When evaluating a clustering eﬀect, we attempt to ﬁnd a diﬀerential characteristic with the weight of B, not the weight of ≤ B so as to calculate the exact probr·i−1 ability of a diﬀerential due to the same reason mentioned in [26]. j=0 pj = B r·i−1 r·i−1 can be obtained by applying both j=0 pj ≤ B and j=0 pj ≥ B. The ﬁrst constraint is already given above, and the second one can be easily obtained from r·i−1 p ≤ B with a small change. More information is provided in the previous j j=0 r·i−1 study [26]. Hereafter, Mcla.sec(B) denotes the clauses to express j=0 pj ≥ B. The detailed comprehensive algorithm for ﬁnding diﬀerential characteristics and evaluating the clustering eﬀect will be given in the following section.  
  3  
  A New SAT Framework to Find the Best Diﬀerential  
  In this section, we propose a new generic SAT-based automatic search framework to ﬁnd a diﬀerential with a higher probability under a speciﬁed condition (we refer to it as a good diﬀerential in this paper). Speciﬁcally, our framework can eﬃciently investigate the clustering eﬀect of all diﬀerential characteristics having diﬀerent (c0 , cr ) with a speciﬁed range of probability and identify a good diﬀerential. Our framework leverages a method to solve incremental SAT problems in parallel using a multi-threading technique, leading to an eﬃcient search for all diﬀerentials under the speciﬁed condition. Speciﬁcally, the unique features of our framework are listed as follows:  
  Finding a Good Diﬀerential  
  We present a new method to ﬁnd a good diﬀerential under a speciﬁed condition. Our method requires several basic algorithms to ﬁnd diﬀerential characteristics, such as the ones presented in [27]. Due to the page limitation, We leave a detailed explanation of them in the full version of this paper.3 The idea of our method is to investigate a clustering eﬀect about all diﬀerential characteristics having diﬀerent (c0 , cr ) with not only the minimum weight, but also a speciﬁed range of weight, and then identify a good diﬀerential. Before giving a detailed algorithm of our method, we explain the procedure of this method step by step as follows: Step 1: Identify the weight Wmin of the r-round optimal diﬀerential characteristic by SATdiff.min (). Step 2: Obtain all diﬀerential characteristics having diﬀerent (c0 , cr ) with the weight from Wmin to Wmin + α by SATdiff.all (). 1 2 3  
  A. Ebrahimi et al.  
  used to provide encryption for data transmission, ensuring that the content remains conﬁdential and secure from unauthorized access. However, the eﬀectiveness of block ciphers is always being tested, and researchers are continually exploring new ways to improve their resilience against attacks like diﬀerential [7], linear [19], algebraic attacks [3], etc. Among several cryptanalysis techniques, Rotational-XOR (RX) cryptanalysis has emerged as a powerful method to evaluate the security of block ciphers, particularly ARX and AND-RX ciphers such as Speck, Simon, and Simeck [1]. In recent years, artiﬁcial intelligence (AI) and deep learning have shown great potential in a variety of applications, including cryptanalysis. Their ability to analyze complex patterns and relationships in large datasets has motivated researchers to explore new techniques for breaking cryptographic algorithms [6, 10,20]. This paper aims to investigate the application of deep learning in the RX cryptanalysis of AND-RX block ciphers, with a focus on Simon and Simeck, and proposes an approach to see the impact of diﬀusion layers in these ciphers. The conventional cryptographic analysis techniques utilized in RX cryptanalysis commonly depend on weak-key models, wherein statistical methods are utilized to detect distinguishers and possible vulnerabilities. Nevertheless, these methods are constrained, as achieving a good distinction with a limited weakkey model may not be feasible. In this context, deep learning has been proposed as an alternative technique, oﬀering the possibility of improved results in cryptanalysis tasks. Our proposed method has enabled us to acquire distinguishers for full-key classes concerning Simeck and Simon ciphers. In addition to assessing the security of ciphers, ﬁnding the best parameters for diﬀusion layers is a crucial aspect of cipher design. The diﬀusion layer plays a signiﬁcant role in ensuring that minimal alterations in plaintext or key inputs lead to substantial changes in the ciphertext output, making it challenging for adversaries to decipher the original data. In this paper, we propose a new approach that involves using a modiﬁed version of the optimizer in [5] to determine the best RX diﬀerential inputs and the optimal shift parameter for ﬁnding the longest round distinguisher with the aid of deep learning classiﬁers. Furthermore, we use this optimizer to identify the best set of rotations in the diﬀusion layer that works against deep learning optimizers, speciﬁcally for Simeck-like ciphers. Our approach ensures that deep learning distinguishers cannot ﬁnd the optimal distinguishers, thereby enhancing the overall security of the ciphers. Therefore, our method demonstrates the potential for improving the security of ciphers while also enhancing the eﬃciency of the design process by utilizing deep learning classiﬁers in combination with an optimizer. Our ﬁndings contribute to the ongoing eﬀorts to enhance the security of AND-RX block ciphers and highlight the potential of AI applications in Rotational-XOR cryptanalysis. Our Deep Learning (DL)-based distinguishers demonstrate superior performance on the Simeck cipher compared to the Simon cipher. In order to juxtapose our achieved Deep Learning (DL)-based distinguishers with other relatedkey DL-based distinguishers for the Simeck cipher, the results are presented in Table 1. It’s important to note that our distinguisher was trained exclusively on  
  Deep Learning-Based Rotational-XOR Cryptanalysis  
  A. Ebrahimi et al.  
  Gohr’s neural distinguisher. This allowed for full interpretability of the distinguisher and contributed towards the interpretability of deep neural networks. In [5], researchers presented a novel tool for neural cryptanalysis that comprises two components. Firstly, an evolutionary algorithm is proposed for the search of single-key and related-key input diﬀerences that are eﬀective with neural distinguishers, thereby enabling the search for larger ciphers while eliminating the dependence on machine learning and prioritizing cryptanalytic methods. Secondly, DBitNet, a neural distinguisher architecture independent of the cipher structure, is introduced and demonstrated to outperform current state-of-the-art architectures. Using their tool, the researchers improved upon the state-of-theart neural distinguishers for various ciphers and provided new neural distinguishers for others. The paper also provides a comparative review of the current state-of-the-art in neural cryptanalysis. 1.2  
  Our Contribution  
  In this paper, we present several contributions to the rotational-XOR cryptanalysis of AND-RX block ciphers such as Simon and Simeck. Our research advances the understanding of these ciphers and their resistance to attacks by incorporating deep learning techniques. Our main contributions are as follows: 1. We propose the ﬁrst study of neural-assisted RX cryptanalysis. We modiﬁed the evolutionary algorithm presented in the work of Bellini et al. [5] to determine the optimal values for the translation parameter, denoted as δ, and the rotation oﬀset parameter, represented as γ, for RX pairs, and by doing so we were able to ﬁnd new RX distinguishers for Simon and Simeck ciphers 2. Our research successfully identiﬁes RX distinguishers using deep learning techniques for diﬀerent versions of Simon and Simeck in the related-key scenario, as opposed to the traditional weak-key model. 3. Our work contributes to ﬁnding the best parameters for diﬀusion layer for Simeck-like ciphers. The practical implications of these ﬁndings oﬀer insights for designing secure AND-RX block ciphers and improving their resistance to RX cryptanalysis. 1.3  
  Outline  
  The current paper’s structure is outlined as follows. The background concepts relevant to AND-RX ciphers, RX cryptanalysis, and deep learning-based cryptanalysis are discussed in Sect. 2. The methodology employed, which includes a modiﬁed evolutionary algorithm, is presented in Sect. 3. In Sect. 4, we report new distinguishes that have been discovered for Simon and Simeck. Section 5 proposes a technique for identifying the optimal permutation parameters for the diﬀusion layer of AND-RX block ciphers against DL-based attacks. Finally, Sect. 6 provides a concluding remark for the paper.  
  Deep Learning-Based Rotational-XOR Cryptanalysis  
  Preliminaries  
  In this section, we provide an overview of the key concepts and terms related to AND-RX ciphers, Rotational-XOR (RX) cryptanalysis, and deep learning techniques for cryptanalysis. Understanding these foundational concepts is essential for comprehending the methods and results presented in this paper. 2.1  
  AND-RX Ciphers  
  Author Index  
  Author Index  
