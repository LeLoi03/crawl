 Collaborative Computing: Networking, Applications and Worksharing  
 19th EAI International Conference, CollaborateCom 2023, Corfu Island, Greece, October 4-6, 2023, Proceedings, Part III  
 Editors: Honghao Gao, Xinheng Wang, Nikolaos Voros   
 Multi-agent Reinforcement Learning Based Collaborative Multi-task Scheduling for Vehicular Edge Computing  
  Abstract   
 Nowadays, connected vehicles equipped with advanced computing and communication capabilities are increasingly viewed as mobile computing platforms capable of offering various in-vehicle services, including but not limited to autonomous driving, collision avoidance, and parking assistance. However, providing these time-sensitive services requires the fusion of multi-task processing results from multiple sensors in connected vehicles, which poses a significant challenge to designing an effective task scheduling strategy that can minimize service requests’ completion time and reduce vehicles’ energy consumption. In this paper, a multi-agent reinforcement learning-based collaborative multi-task scheduling method is proposed to achieve a joint optimization on completion time and energy consumption. Firstly, the reinforcement learning-based scheduling method can allocate multiple tasks dynamically according to the dynamic-changing environment. Then, a cloud-edge-end collaboration scheme is designed to complete the tasks efficiently. Furthermore, the transmission power can be adjusted based on the position and mobility of vehicles to reduce energy consumption. The experimental results demonstrate that the designed task scheduling method outperforms benchmark methods in terms of comprehensive performance.   
 Peisong Li, Ziren Xiao, Xinheng Wang, Kaizhu Huang, Yi Huang, Andrei Tchernykh   
 A Novel Topology Metric for Indoor Point Cloud SLAM Based on Plane Detection Optimization  
  Abstract   
 Accurate self-localization and navigation in complex indoor environments are essential functions for the intelligent robots. However, the existing SLAM algorithms rely heavily on differential GPS or additional measuring devices (such as expensive laser tracker), which not only increase research costs but also limit the deployment of algorithms in specific scenarios. In recent years, reference-free pose estimation methods based on the topological structure of point cloud maps have gained popularity, especially in indoor artificial scenes where rich planar information is available. Some existing algorithms suffer from inaccuracies in spatial point cloud plane segmentation and normal estimation, leading to the introduction of evaluation errors. This paper introduces the optimization of plane segmentation results by incorporating deep learning-based point cloud semantic segmentation and proposes measurement indicators based on the Plane Normals Entropy (PNE) and Co-Plane Variance (CPV) to estimate the rotation and translation components of SLAM poses. Furthermore, we introduce a ternary correlation measure to analyze the relationship between noise, relative pose estimation, and the two proposed measures, building upon the conventional binary correlation measure. Our proposed PNE and CPV metrics were quantitatively evaluated on two different scenarios of LiDAR point cloud data in Gazebo simulator, and the results demonstrate that these metrics exhibit superior binary and triple correlation and computational efficiency, making them a promising solution for accurate self-localization and navigation in complex indoor environments.   
 Zhenchao Ouyang, Jiahe Cui, Yunxiang He, Dongyu Li, Qinglei Hu, Changjie Zhang   
 FederatedMesh: Collaborative Federated Learning for Medical Data Sharing in Mesh Networks  
  Abstract   
 Edge computing is a paradigm that involves performing local processing on lightweight devices at the edge of networks to improve response times and reduce bandwidth consumption. While machine learning (ML) models can run on smaller computing devices at the edge, training ML models presents challenges for low-capacity devices. This paper aimed to evaluate the performance of Federated Learning (FL) - a distributed ML framework, when training a medical dataset using Raspberry Pi devices as client nodes. The testing accuracy, CPU usage, RAM memory usage and network performance were measured for different number of clients and epochs. The results showed that increasing the number of devices generally improved the testing accuracy, with the greatest improvement observed in the earlier epochs. However, increasing the number of devices also increased the CPU usage, with a significant increase observed in the later epochs. Additionally, the RAM memory usage increased slightly as the number of clients and epochs increased. The findings suggest that FL can be an effective way to train medical models using distributed devices, but careful consideration must be given to the trade-off between accuracy and computational resources.   
 Lamir Shkurti, Mennan Selimi, Adrian Besimi   
 Enhancing Session-Based Recommendation with Multi-granularity User Interest-Aware Graph Neural Networks  
  Abstract   
 Session-based recommendation aims at predicting the next interaction based on short-term behaviors within an anonymous session. Conventional session-based recommendation methods primarily focus on studying the sequential relationships of items in a session while often failing to adequately consider the impact of user interest on the next interaction item. This paper proposes the M  ulti-granularity U  ser I  nterest-aware G  raph N  eural N  etworks (MUI-GNN) model, which leverages item attributes and global context information to capture users’ multi-granularity interest. Specifically, in addition to capturing the sequential information within sessions, our model incorporates individual and group interest of users at item and global granularity, respectively, enabling more accurate item representations. In MUI-GNN, a session graph utilizes the sequential relationships between different interactions to infer the scenario of the session. An item graph explores individual user interest by searching items with similar attributes, while a global graph mines similar behavior patterns between different sessions to uncover group interest among users. We apply contrastive learning to reduce noise interference during the graph construction process and help the model obtain more contextual information. Extensive experiments conducted on three real-world datasets have demonstrated that the proposed MUI-GNN outperforms state-of-the-art session-based recommendation models.   
 Cairong Yan, Yiwei Zhang, Xiangyang Feng, Yanglan Gan   
 An Evolving Transformer Network Based on Hybrid Dilated Convolution for Traffic Flow Prediction  
  Abstract   
 Decision making based on predictive traffic flow is one of effective solutions to relieve road congestion. Capturing and modeling the dynamic temporal relationships in global data is an important part of the traffic flow prediction problem. Transformer network has been proven to have powerful capabilities in capturing long-range dependencies and interactions in sequences, making it widely used in traffic flow prediction tasks. However, existing transformer-based models still have limitations. On the one hand, they ignore the dynamism and local relevance of traffic flow time series due to static embedding of input data. On the other hand, they do not take into account the inheritance of attention patterns due to the attention scores of each layer’s are learned separately. To address these two issues, we propose an evolving transformer network based on hybrid dilated convolution, namely HDCformer. First, a novel sequence embedding layer based on dilated convolution can dynamically learn the local relevance of traffic flow time series. Secondly, we add residual connections between attention modules of adjacent layers to fully capture the evolution trend of attention patterns between layers. Our HDCformer is evaluated on two real-world datasets and the results show that our model outperforms state-of-the-art baselines in terms of MAE, RMSE, and MAPE.   
 Qi Yu, Weilong Ding, Maoxiang Sun, Jihai Huang   
 DT-MUSA: Dual Transfer Driven Multi-source Domain Adaptation for WEEE Reverse Logistics Return Prediction  
  Abstract   
 Reverse logistics (RL) return prediction for Waste Electrical and Electronic Equipment (WEEE) has gained attention due to its potential to improve operational efficiency in the recycling industry. However, in data-scarce regions, commonly used deep learning models perform poorly. Existing multi-source cross-domain transfer learning models can partially overcome data scarcity by using historical data from multiple sources. However, these models aggregate multi-source domain data into a single-source domain in transfer, ignoring the differences in time series features among source domains. Additionally, the lack of historical data in the target domain makes fine-tuning the prediction model inoperative. To address these issues, we propose Dual Transfer Driven Multi-Source domain Adaptation (DT-MUSA) for WEEE RL return prediction. DT-MUSA includes a dual transfer model that combines sample transfer and model transfer and a basic prediction model MUCAN (Multi-time Scale CNN-Attention Network). It employs a multi-task learning to aggregate predictors from multiple regions and avoids negative transfer learning. The dual transfer model enables fine-tuning of the base model MUCAN by generating long-term time series data through sample transfer. We applied DT-MUSA to real cases of an RL recycling company and conducted extensive experiments. The results show that DT-MUSA outperforms baseline prediction models significantly.   
 Ruiqi Liu, Min Gao, Yujiang Wu, Jie Zeng, Jia Zhang, Jinyong Gao   
 A Synchronous Parallel Method with Parameters Communication Prediction for Distributed Machine Learning  
  Abstract   
 With the development of machine learning technology in various fields, such as medical care, smart manufacturing, etc., the data has exploded. It is a challenge to train a deep learning model for different application domains with large-scale data and limited resources of a single device. The distributed machine-learning technology, which uses a parameter server and multiple clients to train a model collaboratively, is an excellent method to solve this problem. However, it needs much communication between different devices with limited communication resources. The stale synchronous parallel method is a mainstream communication method to solve this problem, but it always leads to high synchronization delay and low computing efficiency as the inappropriate delay threshold value set by the user based on experience. This paper proposes a synchronous parallel method with parameters communication prediction for distributed machine learning. It predicts the optimal timing for synchronization, which can solve the problem of long synchronization waiting time caused by the inappropriate threshold settings in the stale synchronous parallel method. Moreover, it allows fast nodes to continue local training while performing global synchronization, which can improve the resource utilization of work nodes. Experimental results show that compared with the delayed synchronous parallel method, the training time and quality, and resource usage of our method are both significantly improved.   
 Yanguo Zeng, Meiting Xue, Peiran Xu, Yukun Shi, Kaisheng Zeng, Jilin Zhang, Lupeng Yue   
