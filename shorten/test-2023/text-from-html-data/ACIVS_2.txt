 nach oben    
 2023 | Buch  
 Kapitel lesen  Erstes Kapitel lesen     
 Advanced Concepts for Intelligent Vision Systems  
 21st International Conference, ACIVS 2023 Kumamoto, Japan, August 21–23, 2023 Proceedings  
 herausgegeben von: Jaques Blanc-Talon, Patrice Delmas, Wilfried Philips, Paul Scheunders   
 Über dieses Buch  
 This book constitutes the proceedings of the 21st International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2023, held in Kumamoto, Japan, during August 2023.  
 Semi-supervised Classification and Segmentation of Forest Fire Using Autoencoders  
  Forests play a crucial role in sustaining life on earth by providing vital ecosystem services and supporting a wide range of species. The unprecedented increase in forest fires aka ‘infernos’ due to global warming i.e. rising temperatures and changing weather patterns, is quite alarming. Recently, machine learning and computer vision-based techniques are leveraged to proactively analyze forest fire events. To this end, we propose novel semi-supervised classification and segmentation techniques using autoencoders to analyse forest fires, that require significantly less labelling effort in contrast to the fully-supervised methods. In particular, semi-supervised classification of forest fire using Convolutional autoencoders is proposed. Further, Class Activation Map-based techniques and patch-wise extraction methods are envisaged for the segmentation task. Extensive experiments are carried out on two publicly available large datasets i.e. FLAME and Corsican datasets. The proposed models are found to be outperforming the state-of-the-art approaches.  
 Akash Koottungal, Shailesh Pandey, Athira Nambiar   
 YOLOPoint: Joint Keypoint and Object Detection  
  Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.  
 Anton Backhaus, Thorsten Luettel, Hans-Joachim Wuensche   
 Segmentation of Range-Azimuth Maps of FMCW Radars with a Deep Convolutional Neural Network  
  In this paper, we propose a novel deep convolutional neural network for the segmentation of Range-Azimuth (RA) maps produced by a low-power Frequency Modulated Continuous Wave (FMCW) radar to facilitate autonomous operation on a moving platform in a dense, feature-rich environment such as warehouses, storage and industrial sites. Our key contribution is a compact neural network architecture that estimates the extent of free space with high distance and azimuth resolution, yielding high-quality navigation cues at a much lower computational cost than full 2D space segmentation techniques. We demonstrate our method on a unmanned aerial vehicle (UAV) in an industrial warehouse environment.  
 Pieter Meiresone, David Van Hamme, Wilfried Philips   
 Upsampling Data Challenge: Object-Aware Approach for 3D Object Detection in Rain  
  Lidar-based 3D object detection has been widely adopted for autonomous vehicles. However, adverse weather conditions, such as rain, pose significant challenges by reducing both detection distance and accuracy. Intuitively, one could adopt upsampling to improve detection accuracy. Nevertheless, the task of increasing the number of target points, especially the key detection points crucial for object detection, remains an open issue. In this paper, we explore how an additional data upsampling pre-processing stage to increase the density of the point cloud can potentially benefit deep-learning object detection. Unlike the state-of-the-art upsampling approaches which aim to improve point cloud appearance and uniformity, we are interested in object detection. The object of interest, rather than full scenarios or small patches, is used to train the network - we call it object-aware learning. Additionally, data collection and labelling are time-consuming and expensive, especially for rain scenarios. To tackle this challenge, we propose a semi-supervised upsampling network that can be trained using a relatively small number of labelled simulated objects. Lastly, we verify a well-established sensor/rain simulator, using a publicly available database. The experimental results on a database generated by this simulator are promising and have shown that our object-aware networks can extend the detection range in rainy scenarios and can achieve improvements in Bird’s-eye-View Intersection-over-Union (BEV IoU) detection accuracy.  
 Richard Capraru, Jian-Gang Wang, Boon Hee Soong   
 A Single Image Neuro-Geometric Depth Estimation  
  This paper introduces a novel neuro-geometric methodology for single image object depth estimation, abbreviated as NGDE. The proposed methodology can be described as a hybrid methodology since it combines a geometrical and a deep learning component. NGDE leverages the geometric camera model to initially estimate a set of probable depth values between the camera and the object. Then, these probable depth values along with the pixel coordinates that define the boundaries of an object, are propagated to a deep learning component, appropriately trained to output the final object depth estimation. Unlike previous approaches, NGDE does not require any prior information about the scene, such as the horizon line or reference objects. Instead, NGDE uses a virtual 3D point cloud projected to the 2D image plane that is used to estimate probable depth values indicated by 3D-2D point correspondences. Then by leveraging a multilayer perceptron (MLP) that considers both the probable depth values and the 2D bounding box of the object, NGDE is capable of accurately estimating the depth of an object. A major advantage of NGDE over the state-of-the-art deep learning-based methods is that it utilizes a simple MLP instead of computationally complex Convolutional Neural Networks (CNNs). The evaluation of NGDE on KITTI indicates its advantageous performance over relevant state-of-the-art approaches.  
 George Dimas, Panagiota Gatoula, Dimitris K. Iakovidis   
 IRIS Segmentation Technique Using IRIS-UNet Method  
  Precise segmentation of the iris from the eye images is an essential task in iris diagnosis. Most of the predictions fail due to improper segmentation of iris images, which results in false predictions of the patient’s disease. However, the traditional methods for segmenting the iris are not suitable for iris diagnosis applications. In the field of medical purposes. The iris of the eye could be separated from the eye using deep learning techniques. To overcome this issue, we design a model called Iris-UNet which can effectively segment the limbic and pupillary boundary from the eye images. Using the Iris-UNet model, high-level features are extracted in the encoder path, and segmentation of limbic and pupillary boundaries takes place in the decoder path. We have evaluated our Iris-UNet model on real patient datasets: CASIA, MMU, and PEC datasets. Our Iris-UNet model shows an outperforming solution through the experimental results compared with other traditional methods.  
 M. Poovayar Priya, M. Ezhilarasan   
 Quality Assessment for High Dynamic Range Stereoscopic Omnidirectional Image System  
  This paper focuses on visual experience of high dynamic range (HDR) stereoscopic omnidirectional image (HSOI) system, which includes such as HSOI generation, encoding/decoding, tone mapping (TM) and terminal visualization. From the perspective of quantifying coding distortion and TM distortion in HSOI system, a “no-reference (NR) plus reduced-reference (RR)” HSOI quality assessment method is proposed by combining Retinex theory and two-layer distortion simulation of HSOI system. The NR module quantizes coding distortion for HDR images only with coding distortion. The RR module mainly measures the effect of TM operator based on the HDR image only with coding distortion and the mixed distorted image after TM. Experimental results show that the objective prediction of the proposed method is better compared some representative method and more consistent with users’ visual perception.  
 Liuyan Cao, Hao Jiang, Zhidi Jiang, Jihao You, Mei Yu, Gangyi Jiang   
 Multimodal Representations for Teacher-Guided Compositional Visual Reasoning  
  Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving training efficiency and final performance. We demonstrate that by incorporating cross-modal features and employing more effective training techniques for NMN, we achieve a favorable balance between performance and transparency in the reasoning process.  
 Wafa Aissa, Marin Ferecatu, Michel Crucianu   
  Paul Scheunders  
 Copyright-Jahr  2023    
 Verlag  Springer Nature Switzerland     
