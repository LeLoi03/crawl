1. IWDW_0 conference:
JavaScript must be enabled to use the system

2. WG_0 conference:
WG 2023  
 June 28-30, 2023, Fribourg, Switzerland  
  Menu  Skip to content  WG 2023 
  Accepted Papers 
  Schedule 
  Social Activities 
  Committees 
  Practical Information 
    
 Search for:       

 49th International Workshop on Graph-Theoretic Concepts in Computer Science  

 WG conferences aim to connect theory and applications by demonstrating how graph-theoretic concepts can be applied in various areas of computer science. The goal is to present recent results and to identify and explore directions for future research. The 49th edition is organised by the University of Fribourg, Switzerland and will be held on June 28-30, 2023.  
 WG 2024 will take place in Gozd Martuljek, Slovenia. More information can be found here  .  
     
 The proceedings of WG 2023 can be found here  .  
 Aims and Scopes  
 WG is mainly concerned with efficient algorithms of various types (e.g. sequential, parallel, distributed, randomized, parameterized) for problems on graphs and networks. The goal is to present recent results and to identify and explore directions for future research. Submitted papers should describe original results in any aspects of graph theory related to computer science, including but not restricted to:  
 design and analysis of sequential, parallel, randomized, parameterized algorithms 
  distributed graph and network algorithms 
  structural graph theory with algorithmic or complexity applications 
  computational complexity of graph and network problems 
  graph grammars, graph rewriting systems and graph modeling 
  graph drawing and layouts 
  computational geometry 
  computational biology 
  graph mining 
  random graphs and models of the web and scale-free networks 
  support of the above concepts by suitable implementations and applications. 

 Call for Papers  
 The call for papers can be found here  .  
 Important Dates  
 Abstract submission deadline: February 13, 2023 AoE 
  Paper submission deadline: February 20, 2023 AoE 
  Notification of paper acceptance: April 26, 2023 
  Symposium: June 28-30, 2023 
  Final versions due: July 21, 2023 
    
 Invited Speakers  
 Flavia Bonomo | , University of Buenos Aires, Argentina. | Title: Generalized list matrix partition problems on chordal graphs, parameterized by leafage, | Abstract 
  Eunjung Kim | , LAMSADE, Paris-Dauphine University, France. | Title: Twin-width, graph classes and a bit of logic, | Abstract 
  Nicolas Trotignon | , CNRS, École Normale Supérieure de Lyon, France. | Title: Triangle-free graphs of large chromatic number, | Abstract 
  WG Test of Time Award  
 The third WG Test of Time Award, given for a highly influential paper presented at a previous WG conference, has been given to Alistair Sinclair and Mark Jerrum for their paper “Approximate Counting, Uniform Generation and Rapidly Mixing Markov Chains” from WG 1987. The WG Test of Time Award lecture will be given by:  
   
  Mark Jerrum  , Queen Mary, University of London, United Kingdom  
  Title: 35 years of counting, sampling and mixing, Abstract   
 Best Paper and Best Student Paper  
 The WG 2023 Best Paper award has been given to Paul Jungeblut, Samuel Schneider and Torsten Ueckerdt for their paper “Cops and Robber – When Capturing is not Surrounding” and the WG 2023 Best Student Paper Award has been given to Falko Hegerfeld and Stefan Kratsch for their paper “Tight Algorithms for Connectivity Problems Parameterized by Modular-Treewidth”. Both awards are sponsored by Springer.  

 We thank our sponsors for their support  

 Previous Editions  
 48th WG in Tübingen, Germany (2022)  LNCS vol. 13453  
  47th WG in Warsaw, Poland, online (2021)  LNCS vol. 12911  
  46th WG in Leeds, United Kingdom (2020)  LNCS vol. 12301  
  45th WG in Vall de Núria, Catalonia, Spain (2019)  LNCS vol. 11789  
  44th WG in Lübbenau, Germany (2018)  LNCS vol. 11159  
  43th WG in Eindhoven, The Netherlands (2017)  LNCS vol. 10520  
  42th WG in Istanbul, Turkey (2016)  LNCS vol. 9941  
  41th WG in Garching, Germany (2015)  LNCS vol. 9224  
  40th WG in Nouan-le-Fuzelier, France (2014)  LNCS vol. 8747  
  39th WG in Lübeck, Germany (2013) LNCS vol. 8165  
  38th WG in Jerusalem, Israel (2012)  LNCS vol. 7551  
  37th WG in Teplá Monastery, West Bohemia, Czech Republic (2011)  LNCS vol. 6986  
  36th WG in Zaros, Crete, Greece (2010)  LNCS vol. 6410  
  35th WG in Montpellier, France (2009)  LNCS vol. 5911  
  34th WG in Durham, U.K. (2008) LNCS vol. 5344  
  33rd WG in Dornburg, Germany (2007) LNCS vol. 4769  
  32nd WG in Bergen, Norway (2006) LNCS vol. 4271  
  31st WG in Metz, France (2005)  LNCS vol. 3787  
  30th WG in Bonn, Germany (2004) LNCS vol. 3353  
  29th WG in Elspeet, the Netherlands (2003)  LNCS vol. 2880  
  28th WG in Český Krumlov, Czech Republic (2002) LNCS vol. 2573  
  27th WG in Boltenhagen near Rostock, Germany (2001) LNCS vol. 2204  
  26th WG in Konstanz, Germany (2000) LNCS vol. 1928  
  25th WG in Ascona, Switzerland (1999) LNCS vol. 1665  
  24th WG in Smolenice, Slovak Republic (1998) LNCS vol. 1517  
  23rd WG in Berlin, Germany (1997) LNCS vol. 1335  
  22nd WG in Como, Italy (1996) LNCS vol. 1197  
  21st WG in Aachen, Germany (1995) LNCS vol. 1017  
  20th WG in Herrsching, Germany (1994) LNCS vol. 903  
  19th WG in Utrecht, The Netherlands (1993) LNCS vol. 790  
  18th WG in Wiesbaden-Naurod, Germany (1992) LNCS vol. 657  
  17th WG in Fischbachau, Germany (1991) LNCS vol. 570  
  16th WG in Berlin, Germany (1990) LNCS vol. 484  
  15th WG in Castle Rolduc, The Netherlands (1989) LNCS vol. 411  
  14th WG in Amsterdam, The Netherlands (1988) LNCS vol. 344  
  13th WG in Kloster Banz/Staffelstein, Germany (1987) LNCS vol. 314  
  12th WG in Bernried, Germany (1986) LNCS vol. 246  
  11th WG in Castle Schwanberg, Germany (1985), ISBN 3-853-20357-4  
  10th WG in Berlin, Germany (1984), ISBN 3-853-20334-5  
  9th WG in Haus Ohrbeck, Germany (1983), ISBN 3-853-20311-6  
  8th WG in Neuenkirchen, Germany (1982), ISBN 3-446-13778-5  
  7th WG in Linz, Austria (1981), ISBN 3-446-13538-3  
  6th WG in Bad Honnef, Germany (1980) LNCS vol. 100  
  5th WG in Berlin, Germany (1979), ISBN 3-446-13135-3  
  4th WG in Castle Feuerstein, Germany (1978), ISBN 3-446-12748-3  
  3rd WG in Linz, Austria (1977), ISBN 3-446-12526-3  
  2nd WG in Göttingen, Germany (1976), ISBN 3-446-12330-4  
  1st WG in Berlin, Germany (1975), ISBN 3-446-12215-X  

 49th International Workshop on Graph-Theoretic Concepts in Computer Science  
 June 28-30, 2023, Fribourg, Switzerland  
 For any information, please contact: wg2023@unifr.ch   

 Proudly powered by WordPress

3. HLPP_0 conference:
HLPP 2023: 16th International Symposium on High-Level Parallel Programming and Applications    
 Cluj-Napoca, Romania, June 29-30, 2023   
   
 Menu  Aims & Scope 
  News 
  CFP 
  Important Dates 
  Submission 
  Registration 
  Program 
  Organization 
  Local Info 
  Contact 

 Aims and Scope   
 As processor and system manufacturers adjust their roadmaps towards increasing levels of both inter and intra-chip parallelism, so the urgency of reorienting the mainstream software industry towards these architectures grows.  
 At present, popular parallel and distributed programming methodologies are dominated by low-level techniques such as send/receive message passing, or equivalently unstructured shared memory mechanisms.  
 Higher-level, structured approaches offer many possible advantages and have a key role to play in the scalable exploitation of ubiquitous parallelism.  
 HLPP symposia provide a forum for discussion and research about such high-level approaches to parallel and distributed programming.  
 Topics  
 HLPP 2023 invites papers on all topics in high-level parallel programming, its tools and applications including, but not limited to, the following aspects:  
 High-level parallel programming and performance models (e.g. BSP, CGM, LogP, MPM, etc.) and tools 
  Declarative parallel and distributed programming methodologies based on functional, logical, data-flow, actor, and other paradigms 
  Algorithmic skeletons, patterns, etc. and constructive methods 
  High-level parallelism in programming languages and libraries (e.g, Haskell, Scala, C++, etc.): semantics and implementation 
  Verification of declarative parallel and distributed programs 
  Efficient code generation, auto-tuning and optimization for parallel and distributed programs 
  Model-driven software engineering for parallel and distributed systems 
  Domain-specific languages: design, implementation and applications 
  High-level programming models for heterogeneous/hierarchical platforms with accelerators, e.g., GPU, Many-core, DSP, VPU, FPGA, etc. 
  High-level parallel methods for large structured and semi-structured datasets 
  Applications of parallel and distributed systems using high-level languages and tools 
  Teaching experience with high-level tools and methods for parallel and distributed computing 
  HLPP Symposia Series  
 http://hlpp.net   

 Sponsors of the Symposium:   
 HUAWEI Technologies France SASU  Robert Bosch SRL  Babeș-Bolyai University   

 Babeș-Bolyai University 
  Faculty of Mathematics and Computer Science

4. HIPS_0 conference:
HIPS 2023   
 The 28th HIPS workshop, held in conjunction with IPDPS 2023.  
 View My GitHub Profile   

 28th International Workshop on High-Level Parallel Programming Models and Supportive Environments  
 Overview  
 The 28th HIPS workshop, proposed as a full-day meeting at the IEEE IPDPS 2023 conference in St. Petersburg, Florida, USA, focuses on high-level programming of multiprocessors, compute clusters, and massively parallel machines. Like previous workshops in the series, which was established in 1996 this event will serve as a forum for research in the areas of parallel applications, language design, compilers, runtime systems, and programming tools. It provides a timely forum for scientists and engineers to present the latest ideas and findings in these rapidly changing fields. In our call for papers, we especially encouraged innovative approaches in the areas of emerging programming models for large-scale parallel systems and many-core architectures.  
   Program  
 May 15th, 2023  
  08:55 - 17:05 EDT  
 Welcome Remarks  
 08:55 - 09:00 EDT  
 Keynote 1  
 09:00 - 10:00 EDT  
 Title: Three Insights Learned in Optimizing Deep Learning   
  Prof. Xipeng Shen   
 Abstract:  
  Computing efficiency is crucial for Deep Learning. This talk summarizes three-fold key insights that Prof. Shen’s group has attained in their years of research on high performance machine learning and real-time AI. In particular, Prof. Shen will draw on DNN and GNN optimization examples to explain the top factors to consider in ML compilation, the surprisingly large potential in approximation-based optimization, and how model-code co-optimization goes a long way in unleashing the performance potential of deep learning.  
 Biography:  
  Xipeng Shen is a Professor in the Computer Science Department at North Carolina State University. His primary research work lies in the field of programming systems and intelligent computing, with an emphasis on inter-disciplinary problems and cross-cutting approaches. His research has influenced the development of modern programming systems in multicore and heterogeneous computing as well as ML systems. He is a recipient of the DOE Early Career Award, NSF CAREER Award, Google Faculty Research Award, IBM CAS Faculty Fellow Award, and the NCSU University Faculty Scholars Award. He is an ACM Distinguished Member, ACM Distinguished Speaker, and a senior member of IEEE.  
 Session One: Performance Portability  
 10:30 - 11:30 EDT  
 Understanding Performance Portability of SYCL Kernels: A Case Study with the All-Pairs Distance Calculation in Bioinformatics on GPUs   
  Zheming Jin, Jeffrey Vetter  
 Evaluating performance and portability of high-level programming models: Julia, Python/Numba, and Kokkos on exascale nodes   
  William Godoy, Pedro Valero-Lara, Elise Dettling, Christian Trefftz, Ian Jorquera, Thomas Sheehy, Ross Miller, Marc Gonzalez-Tallada, Jeffrey Vetter, Valentin Churavy  
 Session Two: Memory Subsystem  
 13:00 - 14:00 EDT  
 Evaluating Functional Memory-Managed Parallel Languages for HPC using the NAS Parallel Benchmarks   
  Michael Wilkins, Garrett Weil, Luke Arnold, Nikos Hardavellas, Peter Dinda  
 Memory Traffic and Complete Application Profiling with PAPI Multi-Component Measurements   
  Daniel Barry, Heike Jagode, Anthony Danalis, Jack Dongarra  
 Keynote 2  
 14:00 - 15:00 EDT  
 Compiler Optimization for Tensor Computations: Challenges and Opportunities   
  Prof. Ponnuswamy Sadayappan   
 Abstract:  
  Compiler technology today is very advanced with respect to lowering programs from high-level languages to low-level instruction sets so as to optimize the number of executed instructions. However, the fundamental bottleneck in computers today is not the cost of executed arithmetic/logic instructions but the cost of data access and movement, whether it be between processors in a parallel system or through the memory hierarchy at each processor. Although many compiler optimization techniques such as loop tiling/fusion and data layout transformations have been devised to address this critical bottleneck, the achievable performance from automatic compiler optimization today for key matrix/tensor computations is not comparable to that achieved by manually optimized libraries or auto-tuning frameworks like TVM. This talk will elaborate on some of the key challenges/opportunities for compilers, including design space exploration, effective performance modeling, and algorithm-architecture co-design.  
 Biography:  
  Sadayappan is a Professor in the School of Computing at the University of Utah, with a joint appointment at Pacific Northwest National Laboratory. His primary research interests center around compiler/runtime optimization for high-performance computing, with an emphasis on matrix/tensor computations. He collaborates closely with computational scientists and data scientists in developing high-performance domain-specific frameworks and applications. Sadayappan is an IEEE Fellow.  
 Session Three: Tuning and Analysis  
 15:30 - 17:00 EDT  
 Runtime-Adaptable Selective Performance Instrumentation   
  Sebastian Kreutzer, Christian Iwainsky, Marta Garcia-Gasulla, Victor Lopez, Christian Bischof  
 Designing secure performance metrics for last level cache   
  Probir Roy, Birhanu Eshete, Pengfei Su  
 OptiCPD: Optimization For The Canonical Polyadic Decomposition Algorithm on GPUs   
  Srinivasan Subramaniyan, Xiaorui Wang  
 Closing Remarks  
 17:00 - 17:05 EDT  
   Committees  
 Workshop Co-chairs  
 Harshitha Menon (Lawrence Livermore National Laboratory, USA), gopalakrishn1 at llnl.gov 
  Jens Domke (RIKEN Center for Computational Science, Japan), jens.domke at riken.jp 
  Steering Committee  
 Rudolf Eigenmann, University of Delaware, USA 
  Michael Gerndt, Technische Universität München, Germany 
  Frank Mueller, North Carolina State University, USA 
  Martin Schulz, Technische Universität München, Germany 
  Program Committee  
 Florina M. Ciorba, Universität Basel, Switzerland 
  Miwako Tsuji, RIKEN Center for Computational Science, Japan 
  Nikela Papadopoulou, Chalmers University of Technology, Sweden 
  Qijing Jenny Huang, NVIDIA, USA 
  Alexander Weinert, Deutsches Zentrum für Luft- und Raumfahrt (DLR), Germany 
  Balazs Gerofi, Intel, USA 
  Christian Terboven, RWTH Aachen University, Germany 
  Giorgis Georgakoudis, Lawrence Livermore National Laboratory, USA 
  James Lin, Shanghai Jiao Tong University, China 
  Jeffrey Young, Georgia Institute of Technology, USA 
  Konstantinos Parasyris, Lawrence Livermore National Laboratory, USA 
  Seyong Lee, Oak Ridge National Laboratory, USA 
  Tomohiro Ueno, RIKEN Center for Computational Science, Japan 
  William Moses, Massachusetts Institute of Technology, USA 
  Melih Elibol, NVIDIA, USA 
    Registration  
 Attendance at this workshop is part of the registration for IPDPS 2023. See here  to register.  
 Topics of Interest  
 Topics of interest to the HIPS workshop include but are not limited to:  
 High-level and domain-specific programming systems 
  Languages and compilers for post-Moore’s-Law (or Post Von Neumann) 
  Language/compiler support for AI/ML and Cybersecurity/Privacy (e.g., ML-based auto-tuning) 
  Task-based programming systems 
  (Scalable) programming tools and tools for power & performance analysis, modeling, monitoring, and debugging and core correctness 
  Compiler analysis and optimization techniques 
  OS and architectural support for parallel programming and debugging 
  Software and system support for extreme scalability including fault tolerance and power-aware HPC 
  Programming environments for heterogeneous multicore systems and accelerators such as GPUs and FPGAs 
  Solutions for programming paradigms for GPUs from different hardware vendors 
  Dynamism in applications and system resources 
  Performance portability 
  Efforts for improving the sustainability of scientific software 
  Languages and runtime support for multi-science/coupled codes, including but not limited to ensemble computing and uncertainty quantification 
  New programming languages and constructs for exploiting parallelism and locality 
  Higher-level programming support for quantum computing/quantum simulation 
  Important Deadlines  
 Submission due date: January 19th  February 3rd, 2023 Anywhere on Earth (AoE)  
 Author notification: February 21th, 2023 AoE  
 Camera-ready papers: March 7th, 2023 AoE  
 Submission  
 The HIPS paper style is identical to the IPDPS paper style.  
 Full papers  may not exceed 10 single-spaced double-column pages using 10-point size font on 8.5x11 inch pages (IEEE conference style), including figures, tables, and references.  
 Short papers  may not exceed 4 single-spaced double-column pages using 10-point size font on 8.5x11 inch pages (IEEE conference style), including figures, tables, and references.  
 IPDPS 2023 Call for Papers   
 Submission Website   
   History  
  
 Workshop | Date | Location 
 27th HIPS 2021 | May 30th 2022 | Virtual 
 26th HIPS 2021 | May 17th 2021 | Virtual 
 25th HIPS 2020 | May 18th 2020 | New Orleans, Louisiana, USA 
 24th HIPS 2019 | May 20th 2019 | Rio de Janeiro, Brazil 
 23rd HIPS 2018 | May 21st 2018 | Vancouver, British Columbia, Canada 
 22nd HIPS 2017 | May 29th 2017 | Orlando, FL, USA 
 21st HIPS 2016 | May 23rd 2016 | Chicago, IL, USA 
 20th HIPS 2015 | May 25th 2015 | Hyderabad, India 
 19th HIPS 2014 | May 19th 2014 | Phoenix, AZ, USA 
 18th HIPS 2013 | May 20th 2013 | Boston, MA, USA 
 17th HIPS 2012 | May 21st 2012 | Shanghai, China 
 16th HIPS 2011 | May 20th 2011 | Anchorage, Alaska, USA 
 15th HIPS 2010 | April 19th 2010 | Atlanta, GA, USA 
 14th HIPS 2009 | May 25th 2009 | Rome, Italy 
 13th HIPS 2008 | April 14th 2008 | Miami, FL, USA 
 12th HIPS 2007 | March 26th 2007 | Long Beach, California, USA 
 11th HIPS 2006 | April 25th 2006 | Rhodes Island, Greece 
 10th HIPS 2005 | April 4th 2005 | Denver, Colorado, USA 
 9th HIPS 2004 | April 26th 2004 | Santa Fe, New Mexico, USA 
 8th HIPS 2003 | April 22nd 2003 | Nice, France 
 7th HIPS 2002 | April 15th 2002 | Fort Lauderdale, FL, USA 
 6th HIPS 2001 | April 23rd 2001 | San Francisco, CA, USA 
 5th HIPS 2000 | May 1st 2000 | Cancun, Mexico 
 4th HIPS 1999 | April 12th 1999 | San Juan, Puerto Rico, USA 
 3rd HIPS 1998 | March 30th 1998 | Orlando, FL, USA 
 2nd HIPS 1997 | April 1st 1997 | Geneva, Switzerland 
 1st HIPS 1996 | April 16th 1996 | Honolulu, HI, USA 

 Hosted on GitHub Pages — Theme by orderedlist

5. WG_1 conference:
WG 2023  
 June 28-30, 2023, Fribourg, Switzerland  
  Menu  Skip to content  WG 2023 
  Accepted Papers 
  Schedule 
  Social Activities 
  Committees 
  Practical Information 
    
 Search for:       

 Call for Papers  
  
 We invite authors to submit papers describing original research of theoretical or practical significance to the 49th International Workshop on Graph-Theoretic Concepts in Computer Science (WG 2023). The WG 2023 conference is the 49th edition of the WG series and will take place from Wednesday 28th June to Friday 30th June 2023 in Fribourg, Switzerland.  
 Aims and Scope  
 WG conferences aim to connect theory and applications by demonstrating how graph-theoretic concepts can be applied in various areas of computer science. The goal is to present recent results and to identify and explore directions for future research. Submitted papers should describe original results in any aspects of graph theory related to computer science, including but not restricted to:  
 design and analysis of sequential, parallel, randomized, parameterized, and distributed graph and network algorithms, 
  structural graph theory with algorithmic or complexity applications, 
  computational complexity of graph and network problems, 
  graph grammars, graph rewriting systems and graph modelling, 
  graph drawing and layouts, 
  computational geometry, 
  computational biology, 
  random graphs and models of the web and scale-free networks, and support of these concepts by suitable implementations and applications. 
  Important dates  
 Abstract submission deadline: February 13, 2023 AoE 
  Paper submission deadline: February 20, 2023 AoE 
  Notification of paper acceptance: April 26, 2023 
  Symposium: June 28-30, 2023 
  Final versions due: July 21, 2023 
    
 Submissions And Proceedings  
 Contributors are invited to submit an extended abstract of at most 12 pages Springer LNCS format  including title and abstract, but excluding references. Proofs omitted due to space restrictions must be placed in an appendix, to be read by program committee members at their discretion. All papers must be original and not simultaneously submitted to another journal or conference. Accepted contributions will be published in the conference post-proceedings in the Lecture Notes in Computer Science (ARCoSS/LNCS) series of Springer-Verlag. At least one author of each accepted paper will be expected to register and present the paper in person at the conference.  
 The submission server can be found here  .  
 Awards  
 WG 2023 will offer awards for the best paper and the best student paper. The awards will be decided by the program committee. The committee can decide to split the awards over multiple papers, or not to offer an award. Papers eligible for the best student paper can have non-student co-authors, but the main work in a paper that is a candidate for the best student paper award must be done by co-authors that were students at the time of submission, and the award can be received only by such co-authors. It must be indicated at the time of submission whether a paper is a candidate for this award.  

 49th International Workshop on Graph-Theoretic Concepts in Computer Science  
 June 28-30, 2023, Fribourg, Switzerland  
 For any information, please contact: wg2023@unifr.ch   

 Proudly powered by WordPress

6. HLPP_1 conference:
HLPP 2023: 16th International Symposium on High-Level Parallel Programming and Applications     
 Cluj-Napoca, Romania, June 29-30, 2023    
   
 Menu  Aims & Scope 
  News 
  CFP 
  Important Dates 
  Submission 
  Registration 
  Program 
  Organization 
  Local Info 
  Contact 

 CFP   
 CALL FOR PAPERS  
 HLPP 2023  
  16 th  International Symposium on International Symposium on  
  High-Level Parallel Programming and Applications   
 Cluj-Napoca, Romania   
  June 29-30, 2023   
  On-site Event   
  https://www.cs.ubbcluj.ro/hlpp2023   
  HLPP2023 CFP in pdf format   
 Aims and Scope  
 As processor and system manufacturers adjust their roadmaps towards increasing levels of both inter and intra-chip parallelism, so the urgency of reorienting the mainstream software industry towards these architectures grows.  
 At present, popular parallel and distributed programming methodologies are dominated by low-level techniques such as send/receive message passing, or equivalently unstructured shared memory mechanisms.  
 Higher-level, structured approaches offer many possible advantages and have a key role to play in the scalable exploitation of ubiquitous parallelism.  
 HLPP symposia provide a forum for discussion and research about such high-level approaches to parallel and distributed programming.  
 Topics  
 HLPP 2023 invites papers on all topics in high-level parallel programming, its tools and applications including, but not limited to, the following aspects:  
 High-level parallel programming and performance models (e.g. BSP, CGM, LogP, MPM, etc.) and tools 
  Declarative parallel and distributed programming methodologies based on functional, logical, data-flow, actor, and other paradigms 
  Algorithmic skeletons, patterns, etc. and constructive methods 
  High-level parallelism in programming languages and libraries (e.g, OCaml, Haskell, Scala, C++, etc.): semantics and implementation 
  Verification of declarative parallel and distributed programs 
  Efficient code generation, auto-tuning, and optimization for parallel and distributed programs 
  Model-driven software engineering for parallel and distributed systems 
  Domain-specific languages: design, implementation, and applications 
  High-level programming models for heterogeneous/hierarchical platforms with accelerators, e.g., GPU, Many-core, DSP, VPU, FPGA, etc. 
  High-level parallel methods for large structured and semi-structured datasets 
  Applications of parallel and distributed systems using high-level languages and tools 
  Teaching experience with high-level tools and methods for parallel and distributed computing 
  HLPP Symposia Series  
 http://hlpp.net   
 Submission  
 Papers submitted to HLPP 2023 must describe original research results and must not have been published or simultaneously submitted anywhere else. As it is traditional for HLPP symposia, the accepted papers will be distributed at the symposium in draft proceedings, and revised papers will be published in a special issue of the International Journal of Parallel Programming.  
 Each paper will receive a minimum of three reviews by members of the international technical program committee. Papers will be selected based on their originality, relevance, technical clarity, and quality of presentation.  
 Manuscripts must be prepared with the Springer IJSS latex macro package using the single column option (\documentclass[smallextended]{svjour3}) and submitted via the EasyChair Conference Management System as one pdf file. The strict page limit for initial submission and camera-ready version is 20 pages in the aforementioned format.  
 At least one author of each accepted paper must register for the HLPP 2023 symposium and present the paper.  
 After the symposium the authors of the papers will have ample time to revise their papers and to incorporate both reviewer feedback and presentation feedback from the whole audience during the symposium.  
 Important Dates  
 Abstract submission: | April 3, 2023 | , | April 17, 2023 | Extended: April 21, 2023 
  Paper submission: | April 7, 2023 | , | April 21, 2023 | Extended: April 28, 2023 
  Notification: | May 15, 2023, | May 19, 2023 
  Draft proceedings version: June 9, 2023 
  Author registration deadline: June 9, 2023 
  Non-author registration deadline: June 20, 2023 
  Conference: June 29-30, 2023 
  Journal paper submission: September 2023 
  Organization  
 Virginia Niculescu, Babeș-Bolyai University of Cluj-Napoca, Romania 
  Adrian Sterca, Babeș-Bolyai University of Cluj-Napoca, Romania 
  Darius Bufnea, Babeș-Bolyai University of Cluj-Napoca, Romania 
  Steering Committee  
 Gaétan Hains (Université Paris-Est Créteil, France) 
  Clemens Grelck (Universiteit van Amsterdam, Netherlands) 
  Kiminori Matsuzaki (Kochi University of Technology, Japan) 
  Christoph W. Kessler (Linköping University, Sweden) 
  Herbert Kuchen (University of Münster | , | Germany) 
  Marco Danelutto (University of Pisa, Italy) 
  Ines Dutra (University of Porto, Portugal) 
  Arturo Gonzalez-Escribano (Universidad de Valladolid, Spain) 
  Frédéric Dabrowski (Université d’Orléans, France) 
  Virginia Niculescu (Babeș-Bolyai University, Romania) 

 Sponsors of the Symposium:   
 HUAWEI Technologies France SASU  Robert Bosch SRL  Babeș-Bolyai University   

 Babeș-Bolyai University 
  Faculty of Mathematics and Computer Science

7. HIPS_1 conference:
IPDPS 2023 Conference  
 Home 
  Advance Program 
  Registration 
  Author Resources 
  PhD Forum 
  Workshops 
  Industry 
  Student Travel 
  Hotel & Travel Tips 
  Organization 
  Program Committee 
  Call for Papers | (CLOSED) 
  General IPDPS Info  
 About IPDPS 
  Conference Archive 
  Babbage Award 
  Proceedings Library 
  Steering Committee 
  Call for Volunteers 
  Call for Future Hosts 
  Contact IPDPS 
  Sponsors  

 IN COOPERATION WITH  

 and  

 INDUSTRY PARTNERS  
      
 Silver Level Partner   

 COMPSYS IPDPS 2023    
 Workshop Support | IPDPS 2023 Workshops 
 WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | IPDPS 2023 Workshops | WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 IPDPS 2023 Workshops 
 WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | IPDPS 2023 Workshops 
 WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | IPDPS 2023 Workshops | WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science | Search IPDPS  

 Follow IPDPS  
       
 IPDPS 2022 Report  

  36th IEEE International Parallel & Distributed Processing Symposium   
  May 30 – June 3, 2022  
   (Lyon, France)  
  HELD VIRTUALLY   
 REPORT ON IPDPS 2022 
 IPDPS 2023 Workshops 
 WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | IPDPS 2023 Workshops | WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 IPDPS 2023 Workshops 
 WORKSHOPS CHAIR   
  Ananth Kalyanaraman (Washington State University, USA)  
 WORKSHOPS VICE CHAIR   
  Suren Byna (Ohio State University, USA)  
  workshops@ipdps.org    
 ABOUT IPDPS WORKSHOPS   
 Usually held on the first and last day of the conference, IPDPS workshops provide an extended forum that allows the IPDPS community an opportunity to fully explore special topics and to present work that is more preliminary and cutting-edge or that has more practical content than the more mature research presented in the main symposium. Each workshop has its own requirements and schedule for submissions. Workshop paper submission dates typically fall after the notification date for the main symposium papers. Proceedings of the workshops are distributed at the conference and are submitted for inclusion in the IEEE Xplore Digital Library after the conference.  
 Below is the list of 21 workshops planned for 2023 in St. Petersburg, Florida. For the most up to date information on each workshop, follow the link from this page.  
 IPDPS 2023 MONDAY WORKSHOPS – 15 MAY   
  
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 

   IPDPS 2023 FRIDAY WORKSHOPS – 19 MAY   
  
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science | 1 | HCW | Heterogeneity in Computing Workshop | 2 | RAW | Reconfigurable Architectures Workshop | 3 | HiCOMB | High Performance Computational Biology | 4 | GrAPL | Graphs, Architectures, Programming, and Learning | 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education | 6 | APDCM | Advances in Parallel and Distributed Computational Models | 7 | HIPS | High-level Parallel Programming Models and Supportive Environments | 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC | 9 | ADOPT | AI for Datacenter Operations | 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications | 11 | AsHES* | Accelerators and Hybrid Emerging Systems | 12 | ESSA* | Extreme-Scale Storage and Analysis | 13 | JSSPP | Job Scheduling Strategies for Parallel Processing | 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing | 15 | iWAPT | Automatic Performance Tuning | 16 | PAISE | Parallel AI and Systems for the Edge | 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures | 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems | 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization | 20 | COMPSYS* | Composable Systems | 21 | ExSAIS* | Extreme Scaling of AI for Science 
 1 | HCW | Heterogeneity in Computing Workshop 
 2 | RAW | Reconfigurable Architectures Workshop 
 3 | HiCOMB | High Performance Computational Biology 
 4 | GrAPL | Graphs, Architectures, Programming, and Learning 
 5 | EduPar | NSF/TCPP Workshop on Parallel and Distributed Computing Education 
 6 | APDCM | Advances in Parallel and Distributed Computational Models 
 7 | HIPS | High-level Parallel Programming Models and Supportive Environments 
 8 | CGRA4HPC | Coarse-Grained Reconfigurable Architectures for HPC 
 9 | ADOPT | AI for Datacenter Operations 
 10 | Q-CASA | Quantum Computing Algorithms, Systems, and Applications 
 11 | AsHES* | Accelerators and Hybrid Emerging Systems 
 12 | ESSA* | Extreme-Scale Storage and Analysis 
 13 | JSSPP | Job Scheduling Strategies for Parallel Processing 
 14 | PDSEC | Parallel and Distributed Scientific and Engineering Computing 
 15 | iWAPT | Automatic Performance Tuning 
 16 | PAISE | Parallel AI and Systems for the Edge 
 17 | ScaDL | Scalable Deep Learning over Parallel And Distributed Infrastructures 
 18 | ParSocial | Parallel and Distributed Processing for Computational Social Systems 
 19 | PDCO* | Parallel / Distributed Combinatorics and Optimization 
 20 | COMPSYS* | Composable Systems 
 21 | ExSAIS* | Extreme Scaling of AI for Science 
 Copyright © IPDPS. All rights reserved.

8. WG_2 conference:
We're sorry but SNF Core Figures doesn't work properly without JavaScript enabled. Please enable it to continue.

9. HLPP_2 conference:
Enable JavaScript and cookies to continue

10. WG_3 conference:
Home 
  Search 
  Katalog & Koleksi | Katalog 
  Informasi | Akses Eikon Refinitiv 
  Tata Cara Approval Laporan Magang & KP 
  Tata Cara Upload Laporan Magang & KP 
  Fitur Mobile App: Layanan Book Delivery 
  Pemilihan Jurnal untuk Publikasi Ilmiah 
  Peraturan Tel-U Open Library 
  Sumber Daya Informasi Pendukung Kegiatan Penelitian 
  Surat Bebas Kewajiban Perpustakaan (SBKP) 
  Layanan Assistive Technology 
  Fasilitas Cek Similarity, iThenticate dan Turnitin 
  Tentang Kami 
  Tahun Terbit 

 Graph-Theoretic Concepts in Computer Science: 49th International Workshop, WG 2023, Fribourg, Switzerland, June 28–30, 2023, Revised Selected Papers  
 Daniël Paulusma, Bernard Ries  

 Informasi Dasar  

 Dilihat  31 kali   

 No. Katalog  24.21.1974   

 Klasifikasi  006.6   

 Jenis katalog  Buku - Elektronik (E-Book)   

 No. Rak  4   

 Abstraksi  This volume constitutes the thoroughly refereed proceedings of the 49th International Workshop on Graph-Theoretic Concepts in Computer Science, WG 2023. The 33 full papers presented in this volume were carefully reviewed and selected from a total of 116 submissions. The WG 2022 workshop aims to merge theory and practice by demonstrating how concepts from graph theory can be applied to various areas in computer science, or by extracting new graph theoretic problems from applications. Keywords  

  Subjek  
 Subjek utama  COMPUTER GRAPHICS   

 Subjek tambahan    

  Katalog  
 Judul  Graph-Theoretic Concepts in Computer Science: 49th International Workshop, WG 2023, Fribourg, Switzerland, June 28–30, 2023, Revised Selected Papers   

 ISBN  978-3-031-43380-1   

 Kolasi  478p.: pdf file.; 10 MB   

 Bahasa  Inggris   

  Sirkulasi  
 Harga pinjam  Rp. 0   

 Biaya denda  Rp. 1.000   

 Sirkulasi  Tidak   

 Pengarang  
 Nama  Daniël Paulusma, Bernard Ries   

 Jenis  Perorangan   

 Penyunting/  
  Pembimbing    

 Alih bahasa    

  Penerbit  
 Nama  Springer Nature Switzerland   

 Kota  Switzerland   

 Tahun  2023   

  Koleksi  
 Total  1 Koleksi    

 Tersedia  1 Koleksi    

 Kompetensi  
 Tidak ada    

  Download / Flippingbook  
 Link file  E - Book (ebook.pdf)    
  belum pernah diunduh    

 Rekomendasi    

  Ulasan  
 Belum ada ulasan yang diberikan   
 anda harus sign-in untuk memberikan ulasan ke katalog ini   

 Kembali 

 Copyright © 2011 - Telkom Open Library  
   
  NPP : 3204122D0000002

11. HIPS_2 conference:
[hpc-announce] [IPDPS] CFP: EXTENDED DEADLINE - HIPS 2023 - 28th International Workshop on High-Level Parallel Programming Models and Supportive Environments  
 Gopalakrishnan Menon, Harshitha  gopalakrishn1 at llnl.gov   
  Tue Jan 17 13:35:09 CST 2023   
 Previous message (by thread): | [hpc-announce] (Extended!) Call for Contributions: The 14th ACM/SPEC International Conference on Performance Engineering (ICPE 2023) 
  Next message (by thread): | [hpc-announce] (Deadline Extended) Call for Papers, PDSEC-23 Workshop, May 2023 
  Messages sorted by: | [ date ] | [ thread ] | [ subject ] | [ author ] 
   [Apologies if you receive multiple copies of this CFP] ================================================================= HIPS 2023 CALL FOR WORKSHOP PAPERS: FULL and SHORT ================================================================= The 28th HIPS workshop ( https://hips2023.github.io/  ) will be held in St. Petersburg, Florida, USA as a full-day workshop at the IEEE IPDPS 2023 conference (May 15-19, 2023). The goal of the HIPS workshop series is to provide attendees with a focused and in-depth platform for presentations, discussion and interaction in a particular subject area. We are inviting workshop papers---FULL and SHORT---on topics related to all aspects of research, development, and application of high-performance systems with a focus on high-level programming of multiprocessors, compute clusters, and massively parallel machines. The full list of "Topics of Interest" is provided on our website, and submissions within the scope of this workshop series are highly encouraged (including unlisted, but adjunct, topic areas). IMPORTANT DAYS and DEADLINES * Submission due date: January 26th, 2023 Anywhere on Earth (AoE) * Author notification: February 21st, 2023 AoE * Camera-ready papers: March 7th, 2023 AoE * Workshop day: May 15th, 2023 SUBMISSIONS and REVIEW PROCESS * Full papers: Up to 10 pages incl. figures, tables, and references * Short papers: Up to 4 pages incl. figures, etc. * HIPS paper style is identical to the IPDPS paper style * Each (single-blind) paper will be peer-reviewed by three reviewers * Submission website: https://ssl.linklings.net/conferences/ipdps/?page=Submit&id=HIPSWorkshopFullSubmission&site=ipdps2023  COMMITTEES Workshop Co-chairs * Jens Domke (RIKEN Center for Computational Science, Japan) - jens.domke at riken.jp  * Harshitha Menon (Lawrence Livermore National Laboratory, USA) - harshitha at llnl.gov  Program Committee * see confirmed/tentative list on https://hips2023.github.io/  Steering Committee * Rudolf Eigenmann, University of Delaware, USA * Michael Gerndt, Technische Universität München, Germany * Frank Mueller, North Carolina State University, USA * Martin Schulz, Technische Universität München, Germany -- Harshitha Menon Center for Applied Scientific Computing Lawrence Livermore National Laboratory harshitha at llnl.gov  , http://harshithamenon.com/     
 Previous message (by thread): | [hpc-announce] (Extended!) Call for Contributions: The 14th ACM/SPEC International Conference on Performance Engineering (ICPE 2023) 
  Next message (by thread): | [hpc-announce] (Deadline Extended) Call for Papers, PDSEC-23 Workshop, May 2023 
  Messages sorted by: | [ date ] | [ thread ] | [ subject ] | [ author ] 
   More information about the hpc-announce mailing list

12. HLPP_3 conference:
[hpc-announce] Call for participation - HLPP'2023 High-Level Parallel Programming and Applications  
 VIRGINIA NICULESCU  virginia.niculescu at ubbcluj.ro   
  Wed Jun 7 11:15:29 CDT 2023   
 Previous message (by thread): | [hpc-announce] Call for Papers: The Third International Workshop on Integrating High-Performance and Quantum Computing (at IEEE Quantum Week) 
  Next message (by thread): | [hpc-announce] CFP: Second Annual Workshop on Cyber Security in High Performance Computing (S-HPC 2023) 
  Messages sorted by: | [ date ] | [ thread ] | [ subject ] | [ author ] 
   [Apologies if you receive multiple copies of this Call for Participation] CALL FOR PARTICIPATION >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> HLPP 2023 16th International Symposium on International Symposium on High-Level Parallel Programming and Applications Cluj-Napoca, Romania June 29-30, 2023 https://www.cs.ubbcluj.ro/hlpp2023  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  Aims and Scope As processor and system manufacturers adjust their roadmaps towards increasing levels of both inter and intra-chip parallelism, so the urgency of reorienting the mainstream software industry towards these architectures grows. At present, popular parallel and distributed programming methodologies are dominated by low-level techniques such as send/receive message passing, or equivalently unstructured shared memory mechanisms. Higher-level, structured approaches offer many possible advantages and have a key role to play in the scalable exploitation of ubiquitous parallelism. HLPP symposia provide a forum for discussion and research about such high-level approaches to parallel and distributed programming. Topics * High-level parallel programming and performance models (e.g. BSP, CGM, LogP, MPM, etc.) and tools * Declarative parallel and distributed programming methodologies based on functional, logical, data-flow, actor, and other paradigms * Algorithmic skeletons, patterns, etc. and constructive methods * High-level parallelism in programming languages and libraries (e.g, OCaml, Haskell, Scala, C++, etc.): semantics and implementation * Verification of declarative parallel and distributed programs * Efficient code generation, auto-tuning, and optimization for parallel and distributed programs * Model-driven software engineering for parallel and distributed systems * Domain-specific languages: design, implementation, and applications * High-level programming models for heterogeneous/hierarchical platforms with accelerators, e.g., GPU, Many-core, DSP, VPU, FPGA, etc. * High-level parallel methods for large structured and semi-structured datasets * Applications of parallel and distributed systems using high-level languages and tools * Teaching experience with high-level tools and methods for parallel and distributed computing Past HLPP Symposia: https://hlpp.eu<https://hlpp.eu/  > >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  Invited Keynote Dana Petcu, West University of Timișoara, Romania: "Extreme Data Processing in Exascale Systems< https://www.cs.ubbcluj.ro/hlpp2023/wp-content/uploads/Dana-Petcu-HLPP-2023-keynote-abstract.pdf  >" >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Organization * Virginia Niculescu, “Babeş-Bolyai” University of Cluj-Napoca, Romania * Adrian Sterca, “Babeş-Bolyai” University of Cluj-Napoca, Romania * Darius Bufnea, “Babeş-Bolyai” University of Cluj-Napoca, Romania Steering Committee * Gaétan Hains (Université Paris-Est Créteil, France) * Clemens Grelck (Universiteit van Amsterdam, Netherlands) * Kiminori Matsuzaki (Kochi University of Technology, Japan) * Christoph W. Kessler (Linköping University, Sweden) * Herbert Kuchen (University of Münster, Germany) * Marco Danelutto (University of Pisa, Italy) * Ines Dutra (University of Porto, Portugal) * Arturo Gonzalez-Escribano (Universidad de Valladolid, Spain) * Frédéric Dabrowski, Université d'Orléans, France) * Virginia Niculescu (“Babes-Bolyai” University, Romania) We are looking forward to seeing you in Cluj-Napoca, Romania in June 2022. Virginia Niculescu ---------------------------------------------- Dr. Virginia Niculescu Faculty of Mathematics and Computer Science "Babes-Bolyai" University E-mail: virginia.niculescu at ubbcluj.ro  , vniculescu at cs.ubbcluj.ro  <mailto: vniculescu at cs.ubbcluj.ro  > Web: http://www.cs.ubbcluj.ro/~vniculescu  DISCLAIMER Prezentul mesaj și orice documente atașate pot conține informații confidențiale sau sensibile care aparțin Universității Babeș-Bolyai din Cluj-Napoca. Conținutul mesajului nu poate fi dezvăluit sau utilizat de către altcineva decât destinatarul, persoană fizică sau juridică. Dacă ați primit acest mesaj dintr-o eroare, vă rugăm să ne anunțati imediat, ca răspuns la mesajul de față, și să ștergeți apoi din sistemul dvs. mesajul fără a-l copia sau deschide. Prin prezenta vi se notifică faptul că orice dezvaluire, copiere, distribuire sau inițierea/omiterea unor acțiuni pe baza prezentelor informații sunt strict interzise și atrag răspunderea civilă și/sau penală (art. 302 Noul Cod Penal). UBB nu este răspunzătoare pentru modificările care pot fi aduse prezentului mesaj și nici pentru întârzierile care pot surveni în recepționarea acestuia. De asemenea, nu putem garanta integritatea mesajului și nici că acesta este lipsit de viruși, interceptări sau interferențe de orice natură. ================= The contents of this email message and any attachments may contain confidential and/or privileged information belonging to Babeș-Bolyai University of Cluj-Napoca. This information is intended solely for the addressee, natural or legal person, and is legally protected from disclosure and may not be used by anyone other than the intended recipient. If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately alert the sender by reply email and then delete this message and any attachments. You are hereby notified that any use, dissemination, copying, or storage of this message or its attachments as well as taking any action in reliance on the contents of this information is strictly prohibited and entails civil and/or criminal liability (under Art. 302 of the New Criminal Code). UBB accepts no liability for any errors or omissions in the contents of this message. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. UBBCLUJ < https://www.ubbcluj.ro/ro/politici/  >    
 Previous message (by thread): | [hpc-announce] Call for Papers: The Third International Workshop on Integrating High-Performance and Quantum Computing (at IEEE Quantum Week) 
  Next message (by thread): | [hpc-announce] CFP: Second Annual Workshop on Cyber Security in High Performance Computing (S-HPC 2023) 
  Messages sorted by: | [ date ] | [ thread ] | [ subject ] | [ author ] 
   More information about the hpc-announce mailing list

13. HIPS_3 conference:
The requested URL was rejected. Please consult with your administrator.  
   
  Your support ID is: < 1064046693045588917>  
   
  [Go Back]

14. DAS_0 conference:
Home 
  Program | Program overview 
  Program interactive/eventee 
  Program booklet 
  Keynote Speakers | Keynote 1 / Prof Adam Jatowt 
  Keynote 2 / Prof Andreas Dengel 
  Tutorial 
  Industrial track 
  Awards 
  Welcome reception 
  Social event and Banquet 
  Proceedings | DAS2022 Proceedings 
  Short Paper Booklet 
  Photos 
  Special session CSAWA 
  People | Committees 
  List of reviewers 
  Local organizing committee 
  Contact us 
  More | Attendance | On-site participation | On-site participation 
  Poster Printing Service 
  Accommodation 
  DAS2022 venues interactive map 
  Online participation 
  Instructions for speakers 
  Registration | Registration 
  Financial Assistance 
  Paper submission process | Topics of interest 
  Call For Papers 
  Paper Submission | Author instructions 
  Submit your paper 
  Camera-ready Full-Paper 
  Camera-ready Short-Paper 
  Instructions for speakers 
  List of accepted Full-Papers 
  List of accepted Short-Papers 
  Important dates 
  Tutorials submission process | Call for Tutorials 
  Important dates 
  List of accepted tutorials 
  Previous editions 

 Aller au contenu (Pressez Entrée)      
 DAS 2022   
 15th IAPR International Workshop on Document Analysis Systems  

 Home 
  Program | Program overview 
  Program interactive/eventee 
  Program booklet 
  Keynote Speakers | Keynote 1 / Prof Adam Jatowt 
  Keynote 2 / Prof Andreas Dengel 
  Tutorial 
  Industrial track 
  Awards 
  Welcome reception 
  Social event and Banquet 
  Proceedings | DAS2022 Proceedings 
  Short Paper Booklet 
  Photos 
  Special session CSAWA 
  People | Committees 
  List of reviewers 
  Local organizing committee 
  Contact us 
  More | Attendance | On-site participation | On-site participation 
  Poster Printing Service 
  Accommodation 
  DAS2022 venues interactive map 
  Online participation 
  Instructions for speakers 
  Registration | Registration 
  Financial Assistance 
  Paper submission process | Topics of interest 
  Call For Papers 
  Paper Submission | Author instructions 
  Submit your paper 
  Camera-ready Full-Paper 
  Camera-ready Short-Paper 
  Instructions for speakers 
  List of accepted Full-Papers 
  List of accepted Short-Papers 
  Important dates 
  Tutorials submission process | Call for Tutorials 
  Important dates 
  List of accepted tutorials 
  Previous editions 

  15TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS  
 May 22-25, La Rochelle, France   
  DAS 2022 is planned as a physical conference with support for remote attendance   
   
 DAS2022 Proceedings  Photos    

 DAS 2022 is the 15th edition of the IAPR sponsored workshop focusing on system-level issues and approaches in document analysis and recognition.The workshop comprises invited keynote speeches, oral and poster presentations, tutorials, and working group discussions. Proceedings (full papers) will be published by Springer.  
  
 DAS 2022 will be hosted by La Rochelle University (France) on May 22-25, 2022. The city of La Rochelle, situated on the central west coast of France, possesses a rich historical fabric, including the Saint-Nicholas tower, and urban heritage. In the early 21st century, the city has consistently been ranked among France’s most liveable cities.  
  
 La Rochelle University, founded in 1993, is one of the youngest Universities in France with more than 8500 students. Specialized in Sustainable Smart Urban Coastal studies, and currently lead one of the first European Universities named EU-CONEXUS.  
  
 DAS 2022 will include oral presentations, posters of research or prototype systems, and keynote speeches addressing key techniques of Document Analysis. Tutorials will be organized as well. The tutorials will be held on the day before the three-day main workshop.  
  
  Platinum Sponsors   

  Gold Sponsor  s  

  Silver Sponsor  s  

  Academic Sponsors   

   With the support of   

   DAS 2022 Proceedings will be published in LNCS  

 Group Photos  

  IMG_7291b   

 IMG_7274   

 IMG_7272   

 IMG_7283   

 IMG_7280   

 IMG_7291   

 Awards  

  IMG_7373   

 1   

 IMG_7376   

 IMG_7377   

 IMG_7379   

 IMG_7380   

 IMG_7383   

 1   

 IMG_7387   

 IMG_7388   

 IMG_7392   

 IMG_7394   

 1   

 DAS 2022    Follow   
 15th IAPR International Workshop on Document Analysis System  

   DAS 2022  @DAS_2022  ·  5 Juin    

 We have published some photos of #DAS2022 on the website. Feel free to download the photos and share yours with us with #DAS2022  
   
  👉 https://bit.ly/3Q1XA1X  
      
    Reply on Twitter 1533362566977708037     Retweet on Twitter 1533362566977708037       Like on Twitter 1533362566977708037  5   Twitter 1533362566977708037     
   
   DAS 2022  @DAS_2022  ·  1 Juin    

 The #DAS2022 organising committee would like to thank all of its sponsors who made it possible for the conference to take place under very good conditions.  
   
  @ARIADNEXT @EskerFrance #IMDS @GoodNotesApp @Yooz_Demat @myscript @_Teklia_ @VialinkFR @ITESOFT @AggloLR @NvelleAquitaine  
   
    Reply on Twitter 1531906141277589506     Retweet on Twitter 1531906141277589506       Like on Twitter 1531906141277589506  10   Twitter 1531906141277589506     

 Load More...     

 © Copyright 2022 DAS 2022  . Tous Droits Réservés.  The Conference | Développé par Rara Theme   Propulsé par WordPress  .

15. DAS_1 conference:
George  Retsinas  Toggle navigation      about 
  publications (current) 
  projects 
  cv (current) 

 publications  

 Conference Articles  
 2024  
 3D Facial Expressions through Analysis-by-Neural-Synthesis  G. Retsinas  , P. P. Filntisis, R. Danecek, V. F. Abrevaya, A. Roussos, T. Bolkart, and P. Maragos  In Conference on Computer Vision and Pattern Recognition (CVPR)  , 2024 
  2023  
 Feather: An Elegant Solution to Effective DNN Sparsification  A.G. Georgoulakis, G. Retsinas  , and P. Maragos  In British Machine Vision Conference (BMVC)  , 2023 
  Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos  P. P. Filntisis, G. Retsinas  , F. Paraperas-Papantoniou, A. Katsamanis, A. Roussos, and P. Maragos  In 4th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) at CVPR  , 2023 
  Mushroom Segmentation and 3D Pose Estimation from Point Clouds using Fully Convolutional Geometric Features and Implicit Pose Encoding  G. Retsinas  , N. Efthymiou, and P. Maragos  In Agriculture-Vision: 4th International Workshop at CVPR  , 2023 
  A Realistic Synthetic Mushroom Scenes Dataset  D. Anagnostopoulou, G. Retsinas  , N. Efthymiou, P. Filntisis, and P. Maragos  In Agriculture-Vision: 4th International Workshop at CVPR  , 2023 
  Enhancing Action Recognition in Vehicle Environments with Human Pose Information  M. Konstantinou, G. Retsinas  , and P. Maragos  In PErvasive Technologies Related to Assistive Environments (PETRA)  , 2023 
  Keyword Spotting Simplified: A Segmentation-Free Approach using Character Counting and CTC re-scoring  G. Retsinas  , G. Sfikas, and C. Nikou  In International Conference on Document Analysis and Recognition  , 2023 
  Shared-Operation Hypercomplex Networks for Handwritten Text Recognition  G. Sfikas, G. Retsinas  , P. Dimitrakopoulos, B. Gatos, and C. Nikou  In International Conference on Document Analysis and Recognition  , 2023 
  Binarization with Quaternionic Double Discriminator Generative Adversarial Network  G. Sfikas, G. Retsinas  , and B. Gatos  In Workshop on Machine Learning, International Conference on Document Analysis and Recognition  , 2023 
  Styled Text-to-Text-Content-Image Generation with Latent Diffusion Models  K. Nikolaidou, G. Retsinas  , V. Christlein, M. Seuret, G. Sfikas, E. Barney Smith, H. Mokayed, and 1 more author   In International Conference on Document Analysis and Recognition  , 2023 
  From Digital Phenotype Identification to Detection of Psychotic Relapses  N. Efthymiou, G. Retsinas  , P. Filntisis, C. Garoufis, A. Zlatintsi, E. Kalisperakis, V. Garyfalli, and 4 more authors   In IEEE International Conference on Healthcare Informatics (ICHI)  , 2023 
  E-PREVENTION: THE ICASSP-2023 CHALLENGE ON PERSON IDENTIFICATION AND RELAPSE DETECTION FROM CONTINUOUS RECORDINGS OF BIOSIGNALS  A. Zlatintsi, P. Filntisis, N. Efthymiou, C. Garoufis, G. Retsinas  , T. Sounapoglou, I. Maglogiannis, and 3 more authors   In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)  , 2023 
  Newton-based Trainable Learning Rate  G. Retsinas  , G. Sfikas, P. P. Filntisis, and P. Maragos  In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)  , 2023 
  2022  
 Attribute-based Gesture Recognition: Generalization to Unseen Classes  G. Retsinas  , P. P. Filntisis, N. Kardaris, and P. Maragos  In IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)  , 2022 
  Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes  P. Misiakos, G. Smyrnis, G. Retsinas  , and P. Maragos  In International Conference on Learning Representations (ICLR)  , 2022 
  Hypercomplex Generative Adversarial Networks for Lightweight Semantic Labeling  G. Sfikas, G. Retsinas  , B. Gatos, and C. Nikou  In International Conference on Pattern Recognition and Artificial Intelligence (ICPRAI)  , 2022 
  Best Practices for a Handwritten Text Recognition system  G. Retsinas  , G. Sfikas, B. Gatos, and C. Nikou  In International Workshop on Document Analysis Systems (DAS)  , 2022 
  On-The-Fly Deformations for Keyword Spotting  G. Retsinas  , G. Sfikas, B. Gatos, and C. Nikou  In International Workshop on Document Analysis Systems (DAS)  , 2022 
  Exploring uses of Normalizing Flows for Document Image Processing: Text Super-resolution and Binarization  G. Sfikas, G. Retsinas  , and B. Gatos  In International Workshop on Document Analysis and Systems (DAS)  , 2022 
  Keyword Spotting with Quaternionic ResNet: Application to Spotting in Greek Manuscripts  G. Sfikas, G. Retsinas  , A. Giotis, B. Gatos, and C. Nikou  In International Workshop on Document Analysis Systems (DAS)  , 2022 
  2021  
 Online Weight Pruning Via Adaptive Sparsity Loss  G. Retsinas  , A. Elafrou, G. Goumas, and P. Maragos  In IEEE International Conference on Image Processing (ICIP)  , 2021 
  Deformation-Invariant Networks For Handwritten Text Recognition  G. Retsinas  , G. Sfikas, C. Nikou, and P. Maragos  In IEEE International Conference on Image Processing (ICIP)  , 2021 
  From Seq2Seq Recognition to Handwritten Word Embeddings  G. Retsinas  , G. Sfikas, C. Nikou, and P. Maragos  In British Machine Vision Conference (BMVC)  , 2021 
  Iterative Weighted Transductive Learning for Handwriting Recognition  G. Retsinas  , G. Sfikas, and C. Nikou  In International Conference on Document Analysis and Recognition (ICDAR)  , 2021 
  Quaternion Generative Adversarial Networks for Inscription Detection in Byzantine Monuments  G. Sfikas, A.P. Giotis, G. Retsinas  , and C. Nikou  In 2nd workshop on Pattern Recognition for Cultural Heritage (PatReCH 2020), held in conjunction with the 25th International Conference on Pattern Recognition (ICPR)  , 2021 
  Enhancing Handwritten Text Recognition with N-gram sequence decomposition and Multitask Learning  V. Tassopoulou, G. Retsinas  , and P. Maragos  In 25th International Conference on Pattern Recognition (ICPR)  , 2021 
  2020  
 How to track your dragon: A Multi-Attentional Framework for real-time RGB-D 6DOF Object Pose Tracking  I. Marougkas, P. Koutras, N. Kardaris, G. Retsinas  , G. Chalvatzaki, and P. Maragos  In 6th Workshop on Recovering 6D Object Pose (R6D2020) of the European Conference on Computer Vision (ECCV)  , 2020 
  An intelligent cloud-based platform for effective monitoring of patients with psychotic disorders  I. Maglogiannis, A. Zlatintsi, A. Menychtas, D. Papadimatos, P.P. Filntisis, N. Efthymiou, G. Retsinas  , and 2 more authors   In International Conference on Artificial Intelligence Applications and Innovation (AIAI-2020)  , 2020 
  Person Identification using Deep Convolutional Neural Networks on Short-Term Signals from Wearable Sensors  G. Retsinas  , P. P. Filntisis, N. Efthymiou, E. Theodosis, A. Zlatintsi, and P. Maragos  In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)  , 2020 
  Maxpolynomial Division with Application to Neural Network Simplification  G. Smyrnis, G. Retsinas  , and P. Maragos  In 45th International Conference on Acoustics, Speech, and Signal Processing (ICASSP)  , 2020 
  2019  
 An Alternative Deep Feature Approach to Line Level Keyword Spotting  G. Retsinas  , G. Louloudis, N. Stamatopoulos, G. Sfikas, and B. Gatos  In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  , 2019 
  RecNets: Depthwise-Recurrent Convolutional Neural Networks  G. Retsinas  , A. Elafrou, G. Goumas, and P. Maragos  In British Machine Vision Conference (BMVC)  , 2019 
  2018  
 Compact Deep Decriptors for Keyword Spotting  G. Retsinas  , G. Sfikas, G. Louloudis, N. Stamatopoulos, and B. Gatos  In 16th International Conference on Frontiers in Handwriting Recognition (ICFHR’18)  , 2018 
  Exploring critical aspects of CNN-based Keyword Spotting. A PHOCNet study  G. Retsinas  , G. Sfikas, N. Stamatopoulos, G. Louloudis, and B. Gatos  In 13th IAPR International Workshop on Document Analysis Systems (DAS’18)  , 2018 
  2017  
 Transferable deep features for keyword spotting  G. Retsinas  , G. Sfikas, and B. Gatos  In IWCIM, in conjuction with EUSIPCO  , 2017 
  A PHOC Decoder for Lexicon-Free Handwritten Word Recognition  G. Sfikas, G. Retsinas  , and B. Gatos  In 14th International Conference on Document Analysis and Recognition (ICDAR’17)  , 2017 
  Nonlinear Manifold Embedding on Keyword Spotting using t-SNE  G. Retsinas  , N. Stamatopoulos, G. Louloudis, G. Sfikas, and B. Gatos  In 14th International Conference on Document Analysis and Recognition (ICDAR)  , 2017 
  2016  
 Zoning Aggregated Hypercolumns for Keyword Spotting  G. Sfikas, G. Retsinas  , and B. Gatos  In 15th International Conference on Frontiers in Handwriting Recognition (ICFHR’16)  , 2016 
  Keyword Spotting in Handwritten Documents using Projections of Oriented Gradients  G. Retsinas  , G. Louloudis, N. Stamatopoulos, and B. Gatos  In 12th Workshop on Document Analysis Systems (DAS)  , 2016 
  Efficient Document Image Segmentation Representation by Approximating Minimum-Link Polygons  G. Retsinas  , G. Louloudis, N. Stamatopoulos, and B. Gatos  In 12th Workshop on Document Analysis Systems (DAS’16)  , 2016 
  2015  
 GRPOLY-DB: An Old Greek Polytonic Document Image Database  B. Gatos, N. Stamatopoulos, G. Louloudis, G. Sfikas, G. Retsinas  , V. Papavassiliou, F. Simistira, and 1 more author   In 13th International Conference on Document Analysis and Recognition (ICDAR’15)  , 2015 
  Isolated Character Recognition using Projections of Oriented Gradients  G. Retsinas  , B. Gatos, N. Stamatopoulos, and G. Louloudis  In 13th International Conference on Document Analysis and Recognition (ICDAR’15)  , 2015 
  Historical Typewritten Document Recognition Using Minimal User Interaction  G. Retsinas  , B. Gatos, A. Antonacopoulos, G. Louloudis, and N. Stamatopoulos  In 3rd International Workshop on Historical Document Imaging and Processing (HIP’15)  , 2015 
  Book Chapters  
 2020  
 OldDocPro: Old Greek Document Recognition  B. Gatos, G. Louloudis, N. Stamatopoulos, G. Retsinas  , G. Sfikas, A. Giotis, F. Simistira Liwicki, and 2 more authors   In Handwritten Historical Document Analysis, Recognition, and Retrieval — State of the Art and Future Trends  , 2020 
  Journal Articles  
 2023  
 Mushroom Detection and 3D Pose Estimation from Multi-View Point Clouds  G. Retsinas  , N. Efthymiou, D. Anagnostopoulou, and P. Maragos  MDPI Sensors  , 2023 
  2019  
 Transforming Scholarship in the Archives Through Handwritten Text Recognition: Transkribus as a Case Study  G. Mühlberger, L. Seaward, G. Retsinas  , H. Wurster, and K. Zagoris  Journal of Documentation  , 2019 
  2018  
 Efficient Learning-Free Keyword Spotting  G. Retsinas  , G. Louloudis, N. Stamatopoulos, and B. Gatos  IEEE Transactions on Pattern Analysis and Machine Intelligence  , 2018 

 © Copyright 2024 George Retsinas. Powered by Jekyll  with al-folio  theme. Hosted by GitHub Pages  .

16. WISA_0 conference:
WISA 2024    

 August 21-23, 2024  
  MAISON GLAD, Jeju Island, Korea  

 Menu   
 General | Committees 
  Papers | Paper Submission 
  Call for Papers 
  Call for Posters 
  Accepted Papers 
  Accepted Posters 
  Publications 
  Best Paper Award 
  Programs | Main Program 
  Poster Program 
  Special Session 
  Keynotes and Invited Talks | Keynotes 
  Invited Talks 
  Workshop 
  Registration | Registration 
  Venue 
  Travel | Jeju & Visa 
  Accommodation 
  Online Conference / Proceeding 
  History | Past Conferences 

 The 25th World Conference on  
  Information Security Applications | WISA is the premier security research venue co-hosted by the Korea Institute of Information Security and Cryptology (KIISC) and the Electronics & Telecommunications Research Institute (ETRI) and co-sponsored by the Ministry of Science and ICT (MSIT), the Korea Internet & Security Agency (KISA), and the National Security Research Institute (NSR). This year, WISA will be exploring the innovative possibilities of various technologies in enhancing (and threatening) cyber security: artificial intelligence (AI) and blockchain-driven security, hardware cryptography, and all other technical and practical aspects of security applications. We invite participants from a diverse group of researchers and practitioners who are passionate about advancing the state of the art, as well as addressing fundamental security challenges. Join us at WISA 2024 and be part of the conversion that shapes the future of information security. 

 General  
 Committees 

 Papers  
 Paper Submission 
  Call for papers 
  Call for Posters 
  Accepted Papers 
  Accepted Posters 
  Publications 
  Best Paper Award 

 Programs  
 Main Program 
  Poster Program 

 Keynotes & Invited Talks  
 Keynotes 
  Invited Talks 

 Registration  
 Registration 
  Venue 

 Travel  
 Jeju & Visa 
  Accommodation 

 Proceeding  
 Proceeding 

 History  
 Past Conferences 

 Contact info  
 +82-2-564-9333 
  +82-2-564-9226 
  kiisc@kiisc.or.kr 
  Seongji Heights 3-Cha Bldg., Room 909 | 507, Nonhyeon-ro, Gangnam-gu, | Seoul 06132, | Korea 

 Organization  
 CO-HOSTED BY 
  Sponsored by 

 Copyright  Korea Institute of Information Security and Cryptology  .   
   
  Terms and conditions  / Privacy policy

17. WISA_1 conference:
WISA 2024    

 August 21-23, 2024  
  MAISON GLAD, Jeju Island, Korea  

 Menu   
 General | Committees 
  Papers | Paper Submission 
  Call for Papers 
  Call for Posters 
  Accepted Papers 
  Accepted Posters 
  Publications 
  Best Paper Award 
  Programs | Main Program 
  Poster Program 
  Special Session 
  Keynotes and Invited Talks | Keynotes 
  Invited Talks 
  Workshop 
  Registration | Registration 
  Venue 
  Travel | Jeju & Visa 
  Accommodation 
  Online Conference / Proceeding 
  History | Past Conferences 

 Accepted Papers  

 # | Authors | Title 
 4 | Dedy Septono Catur Putranto, Rini Wisnu Wardhani, Jo Jaehan and Howon Kim | ECPM Cryptanalysis Resource Estimation 
 16 | Gyeongsup Lim, Wonjun Oh and Junbeom Hur | VoteGAN: Generalized Membership Inference Attack Against Generative Models by Multiple Discriminators 
 18 | Yujin Oh, Kyungbae Jang and Hwajeong Seo | Quantum Implementation of LSH 
 19 | Manu Jo Varghese, Adnan Anwar, Robin Doss, Frank Jiang and Abdur Rakib | Reverse Engineering-Guided Fuzzing for CAN Bus Vulnerability Detection 
 20 | Naufal Suryanto, Andro Aprila Adiputra, Ahmada Yusril Kadiptya, Yongsu Kim and Howon Kim | Adversarial Manhole: Challenging Monocular Depth Estimation and Semantic Segmentation Models with Patch Attack 
 26 | Norihiro Okui, Shotaro Fukushima, Ayumu Kubota and Takuya Yoshida | Unsupervised Contextual Anomalous Communication Detection Using VQ Tokenization with Flow Data 
 27 | Arpita Dinesh Sarang, Sang-hoon Choi and Ki-woong Park | Plotting OSS-based Supply Chain attack strategies and the defense failure 
 28 | Mai Bui, Thien-Phuc Doan, Kihun Hong and Souhwan Jung | Boosting Black-Box Transferability of Weak Audio Adversarial Attacks with Random Masking 
 29 | Yu-Tse Shih, Shih-Ming Huang and Chun-I Fan | PUF-Based Authentication and Authorization Protocol for IoT 
 31 | Kibeom Park and Huy Kang Kim | Field Testing and Detection of Camera Interference for Autonomous Driving 
 34 | Byunggeon Choi, Hongjoo Jin, Dong Hoon Lee and Wonsuk Choi | ChatDEOB: An Effective Deobfuscation Method based on Large Language Model 
 41 | En-Wei Zhang, Luo-Fan Wu and Chun-Wei Tsai | An effective ensemble algorithm for short-term load forecasting 
 42 | Keun Young Lee and Ji-yeon Yoo | A Proposal of a Supply Chain Security Model for Generative AI 
 43 | Ahod Alghuried and David Mohaisen | Simple Perturbations Subvert Ethereum Phishing Transactions Detection: An Emperical Analysis 
 44 | Xiaofeng Lu and Tianze Cheng | A Black-box Adversarial Attack Against Graph Neural Network Based on Random Walk 
 45 | Dong Young Kim and Huy Kang Kim | Who ruins the game?: unveiling cheating users in the “Battlefield" game 
 50 | Rion Matsuzaki and Masaya Sato | Detecting Phishing-targeted Web Push Notifications through Image Similarity Analysis 
 53 | Ilhwan Ji, Changjin Choi, Cheolho Lee, Seungho Jeon and Jung Taek Seo | One-class Classification-based Position Falsification Detection System in C-ITS Communication Network 
 54 | Naoki Shibayama and Yasutaka Igarashi | Integral Attack with Bit-based Division Property on the Lightweight Block Cipher LBC 
 57 | Oğuz Yayla and Yunus Emre Yılmaz | An AIS-20/31 Compliant and Improved for Acceleration PLL-TRNG Implementation 
 60 | Ayodeji Adeniran, Kieran Human and David Mohaisen | Dissecting the Infrastructure Used in Web-based Cryptojacking: A Measurement Perspective 
 62 | Bogeum Kim, Hyejin Sim, Jiwon Yun, Howon Kim and Jaehan Cho | LLM Guardrail Framework: A Novel Approach for Implementing Zero Trust Architecture 
 66 | Taeyang Lee and Jong-Hyouk Lee | Privacy Enhanced P2P Power Trading Using DID-based Identity Verification in a Microgrid 
 68 | Jihye Kim, Jaehyoung Park and Jong-Hyouk Lee | ARP Spoofing Mitigation for the E2 Interface in Open RAN: An xApp Approach 
 72 | Mariama Mbow, Rodrigo Román, Ayan Seal, Sraban Kumar Mohanty, Kevin I-Kai Wang and Kouichi Sakurai | Investigating the Transferability of Evasion Attacks in Network Intrusion Detection Systems Considering Domain-Specific Constraints 
 78 | Yeon-Jin Kim, Na-Eun Park and Il-Gu Lee | Air-Fuzz: Feasibility analysis of fuzzing-based side-channel information leakage attack in air-gapped networks 
 83 | Jaehyoung Park, Jihye Kim and Jong-Hyouk Lee | Security Function for 5G-Advanced and 6G: Malicious UE Detection in Network Slicing 
 85 | Jung Yup Rhee, Leo Hyun Park and Taekyoung Kwon | Enhancing Robustness in NIDS via Coverage Guided Fuzzing and Adversarial Training 
 87 | Gyuhyun Jeon, Seungho Jeon and Jung Taek Seo | A Survey on Attack Cases with VBS Malware in Windows 
 88 | Cheng-Han Shie, Pin-Huang Fang and Chun-I Fan | Fuzzing JavaScript Engines with Diversified Mutation Strategies 

 General  
 Committees 

 Papers  
 Paper Submission 
  Call for papers 
  Call for Posters 
  Accepted Papers 
  Accepted Posters 
  Publications 
  Best Paper Award 

 Programs  
 Main Program 
  Poster Program 

 Keynotes & Invited Talks  
 Keynotes 
  Invited Talks 

 Registration  
 Registration 
  Venue 

 Travel  
 Jeju & Visa 
  Accommodation 

 Proceeding  
 Proceeding 

 History  
 Past Conferences 

 Contact info  
 +82-2-564-9333 
  +82-2-564-9226 
  kiisc@kiisc.or.kr 
  Seongji Heights 3-Cha Bldg., Room 909 | 507, Nonhyeon-ro, Gangnam-gu, | Seoul 06132, | Korea 

 Organization  
 CO-HOSTED BY 
  Sponsored by 

 Copyright  Korea Institute of Information Security and Cryptology  .   
   
  Terms and conditions  / Privacy policy

18. MABS_0 conference:
MABS 2023 - The 24th International Workshop on Multi-Agent-Based Simulation       Committees  Call for Papers  Accepted Papers  Registration  Invited Talk  Program  Post-Proceedings  MABS Series    

  The MABS 2023 post-proceedings volume in Springer Multi-Agent-Based Simulation book series  has been published and can be accessed via Springer Link (LNCS 14558)    
   Best Paper Award  
 Active Sensing for Epidemic State Estimation using ABM-guided Machine Learning  [ PDF  ] [ Presentation  ]  
  Sami Saliba, Faraz Dadgostari, Stefan Hoops, Henning S. Mortveit, Samarth Swarup   
  The 2023 Multi-Agent-Based Simulation (MABS) workshop is the 24th of the MABS series that began in 1998  . Its scientific focus lies in the confluence of social sciences and multi-agent systems, with a strong application/empirical vein, and its emphasis is stressed on (i) exploratory agent based simulation as a principled way of undertaking scientific research in the social sciences and (ii) using social theories as an inspiration to new frameworks and developments in multi-agent systems.  
 The excellent quality level of this workshop has been recognized since its inception and its proceedings have been regularly published in Springer’s Lecture Notes series. MABS 2023 will be hosted at AAMAS 2023 (22nd International Conference on Autonomous Agents and Multiagent Systems)  , which will take place at ExCel London  in London, UK on May 29-June 02, 2023.  
 Important Dates  
 Submission deadline: | January 30, 2023  February 6, 2023 | February 15, 2023 AOE (Extended) 
  Authors Notification: | March 13, 2023 | March 18, 2023 
  Camera Ready: | March 20, 2023 | March 28, 2023 AOE 
  Preliminary Program Published: April 10, 2023 
  MABS 2023 will take place: | May 30, 2023 (Confirmed) 
  Preparation of post-proceedings (Springer LNAI): 2nd half of 2023 
  Program chairs  
 Luis Gustavo Nardin and Sara Mehryar  

  MABS2023 
  mabs2023@easychair.org 
    
 MABS 2023 – The 24th International Workshop on Multi-Agent-Based Simulation

19. DAS_2 conference:
Error establishing a database connection

20. MISE_0 conference:
JavaScript must be enabled to use the system

21. WISA_2 conference:
Conference Partner   Home 
  Conferences 
  Journals 
  Proofreading 
  Login 

  中文  |  English  |  Español  |  日本語     

 Conference Partner  » Conferences  » WISA    
  Conference Information   
   
 WISA 2023: World Conference on Information Security Applications  
 https://wisa.or.kr/   
   
 Submission Date: | 2023-06-16 Extended 
 Notification Date: | 2023-07-04 
 Conference Date: | 2023-08-23 
 Location: | Jeju Island, Korea 
 Years: | 24 
  
 Viewed: 10844  Tracked: 2  Attend: 1    

  Call For Papers   
   
 The areas of interest include, but are not limited to: ● AI Security ● Analysis of network and security protocols ● Anonymity and censorship-resistant technologies ● Applications of cryptographic techniques ● Authentication and authorization ● Automated tools for source code/binary analysis ● Automobile security ● Botnet defense ● Blockchain security ● Critical infrastructure security ● DoS attacks and countermeasures ● Digital Forensics ● Embedded systems security ● Exploit techniques and automation ● Hardware and physical security ● HCI security and privacy ● Intrusion detection and prevention ● Malware analysis ● Mobile/wireless/cellular system security ● Network-based attacks ● Network infrastructure security ● Operating system security ● Practical cryptanalysis (hardware, DRM, etc.) ● Quantum cryptanalysis ● Security policy ● Side channel attacks and countermeasures ● Storage and file systems security ● Techniques for developing secure systems ● Trustworthy computing ● Trusted execution environments (Intel SGX, ARM Trustzone, etc.) ● Unmanned System Security for Vehicle/Drone/Ship Systems ● Vulnerability research ● Web security  Last updated by Dou Sun  in 2023-06-11   

  Related Conferences   

 CCF | CORE | QUALIS | Short | Full Name | Submission | Notification | Conference 
 CISCE | International Conference on Communications, Information System and Computer Engineering | 2021-04-07 | 2021-05-14 
 b4 | 3PGCIC | International Conference on P2P, Parallel, Grid, Cloud and Internet Computing | 2015-07-31 | 2015-08-25 | 2015-11-04 
 EMSA | International Conference on Embedded Systems and Applications | 2023-03-04 | 2023-03-11 | 2023-03-18 
 b3 | FGCN | International Conference on Future Generation Communication and Networking | 2015-06-30 | 2015-07-15 | 2015-11-25 
 ICDMM | International Conference on Design, Manufacturing and Mechatronics | 2017-05-19 | 2017-05-26 
 CloudNet | International Conference on Cloud Networking | 2024-08-15 | 2024-09-16 | 2024-11-27 
 a | WINE | Conference on Web and Internet Economics | 2024-07-15 | 2024-09-16 | 2024-12-02 
 CVDL | International Conference on Computer Vision and Deep Learning | 2023-12-10 | 2024-01-20 
 c | b2 | ICT | International Conference on Telecommunications | 2021-03-15 | 2021-04-05 | 2021-06-01 
 WIVEC | International Symposium on Wireless Vehicular Communications | 2012-11-20 | 2013-01-11 | 2013-06-02 
  
 4054  240  3646  1026  2095  1229  1038  4709  1220  1157    

 Short | Full Name | Submission | Conference 
 CISCE | International Conference on Communications, Information System and Computer Engineering | 2021-04-07 | 2021-05-14 
 3PGCIC | International Conference on P2P, Parallel, Grid, Cloud and Internet Computing | 2015-07-31 | 2015-11-04 
 EMSA | International Conference on Embedded Systems and Applications | 2023-03-04 | 2023-03-18 
 FGCN | International Conference on Future Generation Communication and Networking | 2015-06-30 | 2015-11-25 
 ICDMM | International Conference on Design, Manufacturing and Mechatronics | 2017-05-19 | 2017-05-26 
 CloudNet | International Conference on Cloud Networking | 2024-08-15 | 2024-11-27 
 WINE | Conference on Web and Internet Economics | 2024-07-15 | 2024-12-02 
 CVDL | International Conference on Computer Vision and Deep Learning | 2023-12-10 | 2024-01-20 
 ICT | International Conference on Telecommunications | 2021-03-15 | 2021-06-01 
 WIVEC | International Symposium on Wireless Vehicular Communications | 2012-11-20 | 2013-06-02 
  
 4054  240  3646  1026  2095  1229  1038  4709  1220  1157    

  Related Journals   

 CCF | Full Name | Impact Factor | Publisher | ISSN 
 Sustainable Cities and Society | 10.50 | Elsevier | 2210-6707 
 c | Personal and Ubiquitous Computing | Springer | 1617-4909 
 Mechanical Systems and Signal Processing | 7.900 | Elsevier | 0888-3270 
 c | Intelligent Data Analysis | 0.900 | IOS Press | 1088-467X 
 International Journal of Advanced Mechanical Engineering & Applications | AR Publication | 0000-0000 
 Computational and Structural Biotechnology Journal | 4.400 | Elsevier | 2001-0370 
 Journal of Information Science and Engineering | 1.100 | Institute of Information Science | 0000-0000 
 Bulletin of Electrical Engineering and Informatics | IAES | 2089-3191 
 Information Processing and Management | 7.400 | Elsevier | 0306-4573 
 Journal of Sensors | 1.400 | Hindawi | 1687-725X 
  
 668  528  667  697  557  738  605  878  739  657    

 Full Name | Impact Factor | Publisher 
 Sustainable Cities and Society | 10.50 | Elsevier 
 Personal and Ubiquitous Computing | Springer 
 Mechanical Systems and Signal Processing | 7.900 | Elsevier 
 Intelligent Data Analysis | 0.900 | IOS Press 
 International Journal of Advanced Mechanical Engineering & Applications | AR Publication 
 Computational and Structural Biotechnology Journal | 4.400 | Elsevier 
 Journal of Information Science and Engineering | 1.100 | Institute of Information Science 
 Bulletin of Electrical Engineering and Informatics | IAES 
 Information Processing and Management | 7.400 | Elsevier 
 Journal of Sensors | 1.400 | Hindawi 
  
 668  528  667  697  557  738  605  878  739  657    

  Recommendation   

 Track It 2 
  Attend It 1 
  Edit CFP 

 Tracker 
 Caixia Ma (120) 
 Li Zhang (477) 
  
 47912  34499    

 Attender | Year 
 Haohang Sun (1) | 2023 
  
 60619    
   
  Advertisment   

  4,945  Conferences | 1,179  Journals | 69,529  Researchers | 383,507,493 PV  
  Copyright © 2011-2024 myhuiban.com. All Rights Reserved. About Us  | Facebook  | X  | Post CFP or Contact Us  | Promotion

22. MABS_1 conference:
CoMSES Network     
   About 
  Model Library 
  Education 
  Events 
  Jobs 
  Resources 
  Forums 
  Sign In 

 Community Events  

  Javascript is disabled in your browser. Some of this site's functionality will not work  or be heavily degraded without it.   
   
 Home 
  Community Events 
  MABS 2023 - The 24th International Workshop on Multi-Agent-Based Simulation 
   
 MABS 2023 - The 24th International Workshop on Multi-Agent-Based Simulation  
 multiagent systems  agent based modeling and simulation    
  ABOUT MABS  
 MABS 2023 is part of the AAMAS 2023 conference taking place in London, UK on  
  May 30, 2023.  
 The meeting of researchers from Multi-Agent Systems (MAS) engineering,  
  simulation, and the social, economic, and organizational sciences is extensively  
  recognised for its role in cross-fertilization. The synergy among researchers  
  from these fields has undoubtedly been an important source of inspiration for  
  the body of knowledge that has been produced in the area.  
 The MABS workshop series continues with its goal to bring together researchers  
  interested in MAS engineering and simulation, with researchers focused on  
  understanding and finding efficient solutions to model complex social and  
  socio-technical systems, in areas such as economics, management, organizational  
  and social sciences in general.  
 In all these areas, agent theories, metaphors, models, analysis, experimental  
  designs, empirical studies, and methodological principles, all converge into  
  simulation as a way of achieving explanations and predictions, conducting  
  exploration and testing of hypotheses, investigating better designs and systems.  
 TOPICS OF INTEREST  
 We welcome paper submissions from all areas of multi-agent-based simulation and  
  particularly also of work in early stages, which can benefit from active  
  discussions!  
 The range of technical issues that MABS has dealt with, and continues to deal  
  with, is quite diverse and extensive. Topics relevant to this workshop include,  
  but are not limited to:  
  *   Simulation   methodologies   and   tools   -   Standards   for   MABS   -   Methodologies   and   modeling   formalism   for   MABS   -   Methodologies   to   combine   MABS   with   other   modeling   approaches   -   Simulation   languages  ,   platforms   and   tools   for   MABS   -   Large  -  scale   and   distributed   MABS   -   Scalability   and   robustness   in   MABS   -   Future   challenges   in   MABS  *   Simulation   of   Social   and   Intelligent   behavior   -   Formal   and   agent   models   of   social   behavior   -   Cognitive   modeling   and   simulation   -   Game   theory   and   simulation   -   Social   structure  ,   social   networks   and   simulating   organizations   -   Simulating   social   complexity   (  e  .  g  .   structures   and   norms  ,   social   order  ,   emergence   of   cooperation   and   coordinated   action  ,   self  -  organization  ,   the   micro  -  macro   link  )  *   Applications   and   Empirical   Work   -   MABS   for   socio  -  ecological   systems   -   MABS   for   socio  -  technical   systems   -   Agent  -  based   experimental   economics   -   Participatory   and   Human  -  in  -  the  -  Loop   simulation   -   MABS   and   games   -   MABS   and   cloud   computing   -   MABS   in   governance   and   policy  -  making  *   Simulation   Analytics   -   Visualization   and   analytic   tools   for   MABS   -   Experimental   design   for   MABS   -   Statistical   and   data   analysis   methods   for   MABS   -   Data   mining   and   machine   learning   methods   for   MABS      
 MABS is a powerful tool that is being used to inform policy or decisions in  
  many fields of practical importance. The use of models in general, and MABS  
  in particular, during the COVID-19 pandemic has evidenced some of their  
  pitfalls to inform intervention policies. Thus, we propose for 2023 to  
  prioritize submissions of papers that explore how to tackle the pitfalls in  
  using MABS to inform policy making.  
 TARGET AUDIENCE  
 The workshop will provide a forum for social scientists, policy-makers, and  
  AI, MAS and simulation researchers and developers, to assess the current  
  state of the art in the agent-based modeling and simulation of social and  
  socio-technical systems, to identify where and discuss how existing  
  approaches can be successfully applied, to learn about new approaches and  
  explore future research challenges, and to exchange ideas and knowledge in  
  an interdisciplinary environment.  
 The workshop will be of interest to researchers engaged in modeling and  
  analyzing multi-agent systems, and those interested in applying agent-based  
  simulation techniques to real-world problems. In addition, it will attract  
  researchers committed to cross-cutting research that is complementary to  
  more orthodox modeling approaches.  
 MABS intends to be a place where researchers can exchange ideas about their  
  work and we are looking forward to the community meeting once again in  
  person.  
 SUBMISSION GUIDELINES  
 Submissions are limited to 12 pages including references formatted according to the Springer LNCS  
  style and must be electronically submitted before the submission deadline  
  through the workshop conference system, which is available at  
  https://easychair.org/conferences/?conf=mabs2023  .  
 All contributions will be peer-reviewed by at least two independent PC  
  members. The evaluation criteria of contributions will be based on  
  originality, quality, clarity, and its relevance to the workshop.  
 COMMITTEES  
 PROGRAM CHAIRS  
  - Luis Gustavo Nardin (Mines Saint-Étienne, France)  
  - Sara Mehryar (London School of Economics, UK)  
 PROGRAM COMMITTEE  
  - Diana Adamatti, Federal University of Rio Grande, Brazil  
  - Shah Jamal Alam, Habib University, Pakistan  
  - Frederic Amblard, Toulouse 1 Capitole University, France  
  - Luis Antunes, University of Lisbon, Portugal  
  - Robert Axtell, George Mason University, USA  
  - Cristiano Castelfranchi, Institute of Cognitive Sciences and Technologies, Italy  
  - Emile Chappin, Delft University of Technology, The Netherlands  
  - Sung-Bae Cho, Yonsei University, South Korea  
  - Paul Davidsson, Malmö University, Sweden  
  - Frank Dignum, Umeå University, Sweden  
  - Bruce Edmonds, Manchester Metropolitan University, UK  
  - Benoit Gaudou, University of Toulouse, France  
  - Nigel Gilbert, University of Surrey, UK  
  - Nick Gotts, Independent Researcher, UK  
  - Rainer Hegselmann, Bayreuth University, Germany  
  - Marco Janssen, Arizona State University, USA  
  - Fabian Lorig, Malmö University, Sweden  
  - Gustavo Giménez Lugo, Federal University of Technology – Parana, Brazil  
  - Ruth Meyer, Centre for Policy Modelling, UK  
  - Emma Norling, University of Sheffield, UK  
  - Paulo Novais, Universidade do Minho, Portugal  
  - Mario Paolucci, Institute of Cognitive Sciences and Technologies, Italy  
  - Gary Polhill, The James Hutton Institute, UK  
  - Jeffrey Schank, University of California – Davis, USA  
  - Elizabeth Sklar, University of Lincoln, UK  
  - Samarth Swarup, University of Virginia, USA  
  - Keiki Takadama, The University of Electro-Communications, Japan  
  - Takao Terano, Tokyo Institute of Technology, Japan  
  - Klaus Troitzsch, University of Koblenz-Landau, Germany  
  - Nicolas Verstaevel, Toulouse 1 Capitole University, France  
  - Liu Yang, Southeast University, China  
  - Neil Yorke-Smith, Delft University of Technology, The Netherlands  
 MABS STEERING COMMITTEE  
  - Frédéric Amblard (University of Toulouse, France)  
  - Luis Antunes (University of Lisbon, Portugal)  
  - Paul Davidsson (Malmö University, Sweden)  
  - Emma Norling (University of Sheffield, UK)  
  - Mario Paolucci (National Research Council, Italy)  
  - Jaime Simão Sichman (University of São Paulo, Brazil)  
  - Samarth Swarup (University of Virginia, USA)  
  - Takao Terano (Tokyo Institute of Technology, Japan)  
  - Harko Verhagen (Stockholm University, Sweden)  

 Submitter  Luis Gustavo Nardin   
 Last updated  Thu, Feb 09, 2023 at 09:36 PM  
 When  Monday, May 29, 2023 - Tuesday, May 30, 2023  
 Where  London, UK  
 More information  https://mabsworkshop.github.io/    
 Early registration deadline  None  
 Registration deadline  None  
 Submission deadline  Tuesday, February 14, 2023  

 Discussion  

 This website uses cookies and Google Analytics to help us track user engagement and improve our site. If you'd like to know more information about what data we collect and why, please see our data privacy policy  . If you continue to use this site, you consent to our use of cookies.   
 Accept    

 Quick Links   
 Join CoMSES 
  Contact Us 
  Privacy Policy 
  FAQ 
    
 Follow Us   
 YouTube 
  Twitter 
  RSS Feed 

 © 2007 - 2024 CoMSES Net  |  v2024.07-84-g9af9

23. MISE_1 conference:
MODELS 2023">           MODELS 2023">          MODELS 2023">  Modeling Language Engineering (MLE)    About 
  Editions | MLE2024  MLE2023  MLE2022  MLE2021  MLE2020  MLE2019 
  Steering Committee 

 MLE 2023  
  5th International Workshop on Modeling Language Engineering, October 1, 2023  
  Co-located with MODELS 2023     

 About  | Call for papers  | Dates  | Program committee   
 About  
 Software-intensive systems are complicated, driven by the need to integrate across multiple concerns. Consequently, the development of such systems requires the integration of different concerns and skills. These concerns can be covered by different domain-specific modeling languages, with specific concepts, technologies, and abstraction levels. This multiplication of languages eases the development related to each individual specific concern but raises language and technology integration problems at the different stages of the software life cycle. To reason about the global system as a whole, it is necessary to explicitly describe the different kinds of relationships that exist between the different languages used in its development. To support effective language integration, there is a pressing need to reify and classify these relationships, as well as the language interactions that the relationships enable. Equally, the proliferation of domain-specific modeling languages required increases the need for effective and efficient techniques for engineering languages and their support infrastructures (transformations, analysis tools, editors, execution infrastructure, debuggers, …).  
 The Modeling Language Engineering (MLE) workshop aims at bringing together researchers and practitioners working on modeling-language and software-language engineering. It is a meeting opportunity for Software Language Engineering (SLE) enthusiasts within the software-modeling community.  
 Keynote  
 Manuel Wimmer  
 Title  : Out-of-the-Box Testing and Debugging Techniques for Domain-Specific Languages  
 Developing software-intensive systems is still a major challenge as current systems have to incorporate complex domain knowledge, e.g., see Digital Twins as one prominent example. Model-Driven Engineering aims at reducing the accidental complexity associated with the development of such systems through the use of Domain-Specific Languages (DSLs). Such languages allow domain experts the design of systems by providing dedicated modeling concepts and editors. However, to realize the full potential of DSLs, domain experts have to be kept in the loop in later phases of the system life-cycle such as testing and debugging. For this, additional tool support is required that goes beyond what is currently provided by modeling editors.  
 To improve this situation, we propose the automated generation of generic testing and debugging tools for DSLs based on existing language engineering techniques which also incorporate the operational semantics of DSLs. In particular, we have worked on model coverage computation to assess the quality of test suites for domain-specific models and fault localization mechanisms to trace back incorrect behaviour to those model elements that cause the fault. In my talk, I will give an overview on the foundations for such techniques and give a tour on tools we have developed with our international collaboration partners for the GEMOC Studio.  
 Program  
 Date  : Sunday the 1st of October 2023  
 Session 1 – Domain-Specific Modeling (9:25 - 11:00)  
 9:25 – Workshop introduction  
 9:30 – User-Centric Model-Aware Recommendations for Industrial Domain-Specific Modelling Languages. Rohit Gupta, Nico Jansen, Nikolaus Regnat and Bernhard Rumpe  
 10:00 – Enhancing Gameful Systems with a Domain Specific Language for Rules Lifecycle Management. Antonio Bucchiarone, Stefano Martella, Mario Fusco and Henry Muccini  
 10:30 – On the Suitability of LSP and DAP for Domain-Specific Languages. Josselin Enet, Erwan Bousse, Massimo Tisi and Gerson Sunyé  
 Coffee break (11:00 − 11:30)  
 Session 2 – Panel (11:30 − 13:00)  
 Title  : “Engineering and Implementing SysML v2”  
 Moderator  : Ed Seidewitz, Model Driven Solutions  
 Panelists  :  
 Jérémie Tatibouet, CEA 
  Maxime Savary-LeBlanc, Ansys 
  Patrick Ollerton, PTC 
  Ákos Horváth, IncQuery Labs 
  Lunch break (13:00 − 14:30)  
 Session 3 – Keynote (14:30 - 15:30)  
 14:30 – Keynote: Manuel Wimmer  
 Session 4 (1) – Modeling Language Tooling (15:30 - 16:00)  
 15:30 – Collaborative Live Modelling by Language-Agnostic Versioning. Joeri Exelmans, Ciprian Teodorov, Robert Heinrich, Alexander Egyed and Hans Vangheluwe  
 Coffee break (16:00 - 16:30)  
 Session 4 (2) – Modeling Language Tooling (16:30 - 18:00)  
 16:30 – Monitoring Association Constraints in Model-Oriented Programming. Sylvain Guerin, Joel Champeau, Antoine Beugnard and Salvador Martínez  
 17:00 – Towards an End-to-End Metamodeling Approach using Rust. Léo Olivier, Marcos Didonet Del Fabro, Chokri Mraidha and Sébatien Gérard  
 17:30 – Discussions  
 Call for papers  
 Topics  
 The topics of interest for MLE 2023 include:  
 Methodologies, languages, techniques, and methods for designing and implementing modeling languages 
  Composition, extension, and reuse of modeling languages and model execution tools 
  Heterogeneous modeling, simulation, and execution 
  Customization of modeling languages 
  Integration of modeling languages and programming languages 
  Semantics-aware model transformations and code generation 
  Scalability of model execution and execution-based model analysis 
  Execution of partial and underspecified models 
  Model execution in the presence of non-determinism and concurrency 
  Tracing model executions and analyzing model execution traces 
  Model execution tools for the (dynamic) validation, verification, and testing of systems (e.g., model animation, debugging, simulation, trace exploration, model checking, symbolic execution) 
  Live modeling and exploratory modeling techniques 
  Automation techniques for the development of modeling and model execution tools 
  Evolution in the context of executable modeling (e.g.} evolution of executable modeling languages, execution semantics, executable models, model execution tools) 
  Verification of semantic conformance (e.g., among executable modeling languages, executable models, model execution tools) 
  Integration challenges for languages, from requirements to design, for analysis and simulation, during runtime, etc. 
  Case studies and experience reports on the successful or failed adoption of modeling in different application domains and application contexts 
  Surveys and benchmarks of different approaches for the development of modeling languages, model execution, and execution-based model analysis 
  Submissions describing practical and industrial experience related to the use of modeling languages are also encouraged, particularly in the following application domains: Cyber-Physical Systems, Smart Manufacturing, Industry 4.0; Internet of Services, Internet of Things; Smart City, Smart Building, Home automation; Smart and Learning systems.  
 Workshop Format  
 This full-day workshop will prioritize discussions over presentations. We plan to open with a keynote in the morning, followed by paper presentations. The afternoon will then be spent primarily in discussions inspired by topics raised by the keynotes and paper presentations. Where there is sufficient divergence in the topics raised, we will create break-out groups of participants interested in each sub-topic. The goal of these discussions is to identify commonalities and connections between different topics, support research networking, cross-pollination, and informal knowledge transfer. The final session of the workshop will be focused on summarising the key topics and ideas discussed at the workshop to help identify the next steps that may be followed up by workshop participants.  
 Submission  
 We expect early research results about the aforementioned topics, descriptions of problems, case studies, experience reports, or solutions related to the topics of interest.  
 Each contribution must be described in a short paper of 5 pages or a full paper of 10 pages, in IEEE format ( IEEEtran  ). Two more pages containing only references are permitted.  
 Papers that describe use cases or novel approaches can be accompanied by concrete artifacts, such as models (requirements, design, analysis, transformation, composition, etc.), stored in a public repository. Artifacts should illustrate any experience with the conjoint use of different modeling languages.  
 All submissions have to follow the IEEE format ( IEEEtran  ) and must be submitted electronically in PDF format via Easychair  . They will be evaluated by at least three members of the program committee regarding novelty, correctness, significance, readability, and alignment with the workshop call. Furthermore, all submissions must be original work and must not have been previously published or being under review elsewhere. The accepted papers will be included in the joint workshop proceedings published by the IEEE.  
 Dates  
 Paper Abstract submission deadline | : July 17, 2023, AOE 
  Paper submission deadline | : July 20, 2023, AOE 
  Notification of acceptance | : August 15, 2023, AOE 
  Camera-ready deadline | : August 22, 2023, AOE 
  Workshop | : October 1, 2023, full day 
  Organizing committee  
 Ed Seidewitz (Model Driven Solutions, USA) 
  Arnaud Blouin (INSA Rennes, France) 
  Jérôme Pfeiffer (Universität Stuttgart, Germany) 
  Program committee  
 Peter J.Clarke, Florida International University , USA 
  Davide Di Ruscio, University of L’Aquila, Italy 
  Juergen Dingel, Queen’s University, Canada 
  Djamel Eddine Khelladi, CNRS, France 
  Faezeh Khoram, IMT Atlantique, France 
  Zoltán Micskei, Budapest University of Technology and Economics, Hungary 
  Jean-Marie Mottu, Nantes University, France 
  Cortland Starrett, OneFact, USA 
  Massimo Tisi, IMT Atlantique, France 
  Juha-Pekka Tolvanen, MetaCase, Finland 
  Federico Tomassetti, Strumenta, Italia 
  Ernesto Posse, Zeligsoft, Canada 
  Manuel Wimmer, Johannes Kepler University Linz, Austria 
  Andreas Wortmann, RWTH Aachen University, Germany 

 Email me 
  Twitter 
  2024  
 Powered by Beautiful Jekyll

24. EOMAS_0 conference:
Academia.edu no longer supports Internet Explorer.  
 To browse Academia.edu and the wider internet faster and more securely, please take a few seconds to upgrade your browser  .  
   
 ×  Close   Log In   
   
   Log in  with Facebook     
    Log in  with Google     

 or    

   Email     
 Password     
   Remember me on this computer     

  or reset password    
    Enter the email address you signed up with and we'll email you a reset link.  

 Need an account? Click here to sign up     

 Log In  Sign Up    

 Log In 
  Sign Up 
  more 
  About 
  Press 
  Blog 
  Papers 
  Terms 
  Privacy 
  Copyright 
  We're Hiring! 
  Help Center 
  less 
    
   download  Download Free PDF  
   
 Download Free PDF   

 Enterprise and Organizational Modeling and Simulation  
  Vojtěch Merunka    
 2018, Lecture notes in business information processing  
   
 See full PDF  download  Download PDF    

 Related papers  
 Book EnterpriseRiskManagementModels  leonardo pacheco    
 download  Download free PDF   View PDF  chevron_right     
   
 The Practice of Enterprise Modeling  Jelena Zdravkovic    
 Lecture Notes in Business Information Processing, 2016  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise, Business-Process and Information Systems Modeling  Terry Halpin    
 Lecture Notes in Business Information Processing, 2009  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise, Business-Process and Information Systems Modeling (Paperback): 10th International Worksh  Terry Halpin    
 2009  
 download  Download free PDF   View PDF  chevron_right     
   
 Modelling and Simulation  Gilbert Arbez    
 Simulation foundations, methods and applications, 2019  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise Modeling: Practices and Perspectives  Brage Johansen    
 ASME 1995 15th International Computers in Engineering Conference and the ASME 1995 9th Annual Engineering Database Symposium  
 This paper presents an overview of various approaches to enterprise modeling, illustrated by present and future applications of enterprise modeling technology. A taxonomy derived from different objectives of enterprise modeling is proposed. Preliminary experiences from a large-scale enterprise modeling and organizational restructuring project are reported. The project was conducted at a natural gas process plant operated by the Norwegian oil company Statoil. We argue that the potential of enterprise modeling in business process improvements only can be utilized when the methodology is brought to the heads and hands of the inhabitants of the enterprise. Finally, a coordination environment denoted “the control room metaphor” is presented as a futuristic view of enterprise model development and application.  
 download  Download free PDF   View PDF  chevron_right     
   
 From Expert Discipline to Common Practice: A Vision and Research Agenda for Extending the Reach of Enterprise Modeling  Stijn Hoppenbrouwers    
 Business & Information Systems Engineering  
 download  Download free PDF   View PDF  chevron_right     
   
 SIMULATION MODELING IN ORGANIZATIONAL AND MANAGEMENT RESEARCH  Zhiang Lin    
 Academy of Management Review, 2007  
 download  Download free PDF   View PDF  chevron_right     
   
 An Introduction to the Symposium on the Use of Simulation in Applied Industrial Organization  Luke Froeb    
 International Journal of the Economics of Business, 2000  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise Modelling for the Masses – From Elitist Discipline to Common Practice  Stijn Hoppenbrouwers    
 The Practice of Enterprise Modeling, 2016  
 download  Download free PDF   View PDF  chevron_right     

 See full PDF  download  Download PDF    

  Loading Preview  

 Sorry, preview is currently unavailable. You can download the paper by clicking the button above.  

 Related papers  
 A structured methodology for enterprise modeling: a case study for modeling the operation of a british organization  Nashwan Dawood    
 2011  
 download  Download free PDF   View PDF  chevron_right     
   
 Simulation Modeling as a Tool for Ensuring Sustainable Development and Competitiveness of an Enterprise  Zaneta Simanaviciene    
 Proceedings of the Second Conference on Sustainable Development: Industrial Future of Territories (IFT 2021), 2021  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise and its Business Environment  Julie McFarlane    
 2016  
 download  Download free PDF   View PDF  chevron_right     
   
 An Agent-Based Simulation Model for Organizational Analysis  Krishna Pattipati    
 2006  
 download  Download free PDF   View PDF  chevron_right     
   
 The Review of Simulation for Business Organizations’ Problems and Applications  Dr. Waled Khaled    
 International journal of business and economics, 2015  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise Modelling in Support of Organisation Design and Change  Joseph Ajaefobi    
 Concepts, Methodologies, Tools and Applications  
 download  Download free PDF   View PDF  chevron_right     
   
 Literature Overview of Approaches for Enterprise-wide Modelling, Simulation and Optimisation  Elena Fitkov-Norris    
 2010  
 download  Download free PDF   View PDF  chevron_right     
   
 simulation modelling and business strategy research  Daniel Levinthal    
 2015  
 download  Download free PDF   View PDF  chevron_right     
   
 Organizational Systems  Raul Espejo    
 Springer eBooks, 2011  
 download  Download free PDF   View PDF  chevron_right     
   
 Business Modeling  Marisa A Sanchez    
 download  Download free PDF   View PDF  chevron_right     
   
 Data-driven simulation of the extended enterprise  Richard Farr    
 18th International Conference on …, 2005  
 download  Download free PDF   View PDF  chevron_right     
   
 Guest editorial to the Theme Section on enterprise modelling  Balbir Barn    
 Software & Systems Modeling, 2013  
 download  Download free PDF   View PDF  chevron_right     
   
 Toward Structured Simulation of Enterprise Models  Vinay Kulkarni  ,  Hemant Rathod    
 2014 IEEE 18th International Enterprise Distributed Object Computing Conference Workshops and Demonstrations, 2014  
 download  Download free PDF   View PDF  chevron_right     
   
 An Introduction to the JET Special Issue ofJournal of Enterprise Transformation: Enterprise Modeling  Robert Cloutier    
 Journal of Enterprise Transformation, 2011  
 download  Download free PDF   View PDF  chevron_right     
   
 THE CONCEPT OF SIMULATION IN ORGANIZATIONAL RESEARCH  arief hidayat    
 download  Download free PDF   View PDF  chevron_right     
   
 Research and Practical Issues of Enterprise Information Systems II  Antonin Pavlicek    
 2007  
 download  Download free PDF   View PDF  chevron_right     
   
 General purpose enterprise simulation with master  Willi Bernhard  ,  Marco Bettoni    
 Proceedings of the 25th conference on Winter simulation - WSC '93, 1993  
 download  Download free PDF   View PDF  chevron_right     
   
 Enterprise Modelling: A Declarative Approach for  Yun-heh Chen-burger    
 FBPML'European Conference on …, 2002  
 download  Download free PDF   View PDF  chevron_right     
   
 The using of Simulated Models in Management  G. Cavory    
 Analele Universitatii, 2006  
 download  Download free PDF   View PDF  chevron_right     

 Related topics  
 Business  Computer Science    

 About 
  Press 
  Blog 
  Papers 
  Topics 
  We're Hiring! 
  Help Center 
  Find new research papers in: 
  Physics 
  Chemistry 
  Biology 
  Health Sciences 
  Ecology 
  Earth Sciences 
  Cognitive Science 
  Mathematics 
  Computer Science 
  Terms 
  Privacy 
  Copyright 
  Academia ©2024

25. WISA_3 conference:
JavaScript must be enabled to use the system

26. MMSP_0 conference:
Skip to content  IEEE.org 
  IEEE Xplore  Digital Library 
  IEEE Standards 
  IEEE Spectrum 
  More Sites 

  MENU     
     
 Email address   What would you like to search for?    

 SEARCH      

   About 
  For Authors | Instructions for Authors 
  Important Dates 
  Présentation Guidelines 
  CALLS | Call for papers 
  Call for Special Sessions 
  Call for Industry Demos 
  Call for Grand Challenges 
  Program | Technical Program 
  Special sessions 
  Keynote Speakers 
  Registration 
  Travel Grant 
  Organization | Organizing committee 
  Venue | Conference Venue 
  Accommodation 
  Travel connections 
  Social Program 
  Sponsors 

 The IEEE 25th International Workshop on Multimedia Signal Processing  
 September 27-29, 2023 | Poitiers, France  
 Submit Now    

 The IEEE 25th International Workshop on Multimedia Signal Processing  
 September 27-29, 2023 | Poitiers, France  

 The IEEE 25th International Workshop on Multimedia Signal Processing  
 September 27-29, 2023 | Poitiers, France  
 Submit Now    

 Multimedia Signal Processing in the Metaverse Era   
 IEEE MMSP 2023 is held on Spetember 27–29, 2023 in Poitiers, France. It is 25th in the series, organized by the Multimedia Signal Processing Technical Committee of the IEEE Signal Processing Society (SPS), with the aim to bring together researchers and practitioners from academia and industry, passionate about multimedia signal processing, to share their knowledge, exchange ideas, explore future research directions and network.  

 News  
     
 Technical Program announced   
  Check details !!   

 Accepted special sessions  :  
 Quality Assessment and Enhancement for Multimedia Visual Signals, | Wei Zhou, Guanghui Yue, Xiongkuo Min, Jesús Gutiérrez 
  Recent advances in movement studies | , Baptiste Magnier 
  Emerging Computational Imaging and Multimedia Signal Processing Techniques for the Metaverse | , You Yang, Kejun Wu, Giuseppe Valenzise 

 Industry Demos   
 4 Industry demos have been accepted 

 Keynotes  

 Marc  Antonini    
 Research Director - I3S / CNRS - France   
 Synthetic DNA as the future of data archiving - Technological advances and challenges   

 Laura  Toni    
 Associate Professor - University College London - UK   
 Graph-based machine learning in the metaverse era   

 Moncef  Gabbouj    
 Professor, Department of Computing Sciences, Tampere University   
 Advanced blind signal restoration   

 Sponsored by  

 Organized by  

 Home 
  Sitemap 
  Accessibility 
  Nondiscrimination Policy 
  IEEE Ethics Reporting 
  IEEE Privacy Policy 
  Terms 

 © Copyright 2024 IEEE – All rights reserved. Use of this website signifies your agreement to the IEEE Terms and Conditions.  
  A not-for-profit organization, IEEE is the world’s largest technical professional organization dedicated to advancing technology for the benefit of humanity.  

  Back to Top

27. MABS_2 conference:
Multi-Agent-Based Simulation (MABS)    
 The International Workshop Series      

 About MABS        
 The next MABS Workshop      
 MABS 2024 
  The previous MABS Workshops      
 MABS 2023 
  MABS 2022 
  MABS 2021 
  MABS 2020 
  MABS 2019 
  MABS 2018 
  MABS 2017 
  MABS 2016 
  MABS 2015 
  MABS 2014 
  MABS 2013 
  MABS 2012 
  MABS 2011 
  MABS 2010 
  MABS 2009 
  MABS 2008 
  MABS 2007 
  MABS 2006 
  MABS 2005 
  MABS 2004 
  MABS 2003 
  MABS 2002 
  MABS 2000 
  MABS 1998 
  The Multi-Agent-Based Simulation  book series        
 The MABS Steering Committee | Multi-agent systems   (MAS) is one of the most interesting technologies that have emerged in computer science in the last 20 years. As it is said in the  ATAL   home page, one of the most important workshops on the area in the late 90's, " Agents are autonomous computer programs, capable of independent action in environments that are typically dynamic and unpredictable. Agents have proven to be of interest in many important application areas, such as electronic commerce on the Internet, the control of space probes on missions to the outer planets, the design of user interfaces, to industrial process control  ". Within the computer science community, this technology was used specially in problem solving  . On the other hand, the models, software architectures and inplementations issued from the field could be very useful to another scientific discipline: social simulation  . Social scientists usually need some computational testbeds to test their theories about social interaction or emergence of conventions among others. The MABS workshop series aims to bring together researchers from artificial intelligence, computer science and social sciences interested in using multi-agent models and technology in social simulation.    
 MABS has been held every two years from 1998 to 2002, and annually since then. The workshop is now one of the most successful international forum for presenting and publishing research on the theory and practice of multi-agent-based simulation. 

  Send questions and comments to Jaime Simão Sichman  at jaime.sichman@poli.usp.br   
   
  Last Update: June 16, 2024

28. MISE_2 conference:


29. EOMAS_1 conference:
Skip to main content    
  
 eBay Home | Shop by category   Shop by category | Enter your search keyword | All Categories | Enter your search keyword | All Categories | Advanced |  
 Enter your search keyword | All Categories 
  
 Hi ( Sign in  to bid or buy) | Hi! Sign in  or register 
  Daily Deals 
  Help & Contact 
  Ship to     Loading...    Error: Try Again  Ok 
  Sell 
  Watchlist   Expand Watch List      Loading...   Sign in  to see your user information 
  My eBay   Expand My eBay  Summary 
  Recently Viewed 
  Bids/Offers 
  Watchlist 
  Purchase History 
  Buy Again 
  Selling 
  Saved Searches 
  Saved Sellers 
  Messages 
   Expand Cart      Loading...   Something went wrong. View cart for details. 
    
 {"delay":300}   

   breadcrumb  
 eBay 
  Books, Movies & Music 
  Books & Magazines 
  Books, Movies & Music 
  Books & Magazines 
  Textbooks, Education & Reference 
  Textbooks 

 CURRENTLY SOLD OUT   
 Enterprise and Organizational Modeling and Simulation: 15th International Workshop, EOMAS 2019, Held at CAiSE 2019, Rome, Italy, June 3-4, 2019, Selected Papers by Pavel Malyzhenkov, Robert Pergl, Eduard Babkin, Vojtech Merunka, Russell Lock (Paperback, 2019)   
 About this product     

 About this product  
 Product Information  
 This book constitutes the refereed proceedings of the 15th International Workshop on Enterprise and Organizational Modeling and Simulation, EOMAS 2019, held in Rome, Italy, in June 2019. The main focus of EOMAS is on the role, importance, and application of modeling and simulation within the extended organizational and enterprise context. The 12 full papers presented in this volume were carefully reviewed and selected from 25 submissions. They were organized in topical sections on conceptual modeling, enterprise engineering, and formal methods. 
    
 Product Identifiers  
 Publisher | Springer Nature Switzerland A&G 
  ISBN-13 | 9783030356453 
  eBay Product ID (ePID) | 19046650147 
    
 Product Key Features  
 Number of Pages | 175 Pages 
  Publication Name | Enterprise and Organizational Modeling and Simulation: 15th International Workshop, EOMAS 2019, Held at CAiSE 2019, Rome, Italy, June 3-4, 2019, Selected Papers 
  Language | English 
  Subject | Engineering & Technology, Government, Computer Science, Business 
  Publication Year | 2019 
  Type | Textbook 
  Author | Pavel Malyzhenkov, Robert Pergl, Eduard Babkin, Vojtech Merunka, Russell Lock 
  Series | Lecture Notes in Business Information Processing 
  Format | Paperback 
    
 Dimensions  
 Item Height | 235 mm 
  Item Weight | 296 g 
  Item Width | 155 mm 
  Volume | 366 
    
 Additional Product Features  
 Editor | Vojtech Merunka, Robert Pergl, Eduard Babkin, Pavel Malyzhenkov, Russell Lock 
  Country/Region of Manufacture | Switzerland 

 Show More    
 Show Less    

 Best Selling in Textbooks  
   
 Current slide {CURRENT_SLIDE} of {TOTAL_SLIDES}- Best Selling in Textbooks  
  Self Heal by Design : The Role of Micro-Organisms for Health by Barbara O'Neill (2018, Trade Paperback)    
 4.8 out of 5 stars based on 37 product ratings        (37)    279,487.18 VND New 
  256,410.26 VND Used 
  Diagnostic and Statistical Manual of Mental Disorders DSM-5-TR by American Psychiatric Association (2022, Trade Paperback)    
 4.7 out of 5 stars based on 23 product ratings        (23)    1,000,000.00 VND New 
  ---- Used 
  NFPA 70, National Electrical Code Handbook : 2023 Edition by National Fire Protection Association (NFPA) (2022, Hardcover)    
 4.6 out of 5 stars based on 7 product ratings        (7)    2,179,487.18 VND New 
  1,923,076.92 VND Used 
  NFPA 70, National Electrical Code : 2023 Edition by National Fire Protection Association (NFPA) (2022, Trade Paperback)    
 4.5 out of 5 stars based on 10 product ratings        (10)    1,022,820.51 VND New 
  758,717.95 VND Used 
  Diagnostic and Statistical Manual of Mental Disorders : DSM-5-TR by American Psychiatric Association (2022, Hardcover)    
 4.9 out of 5 stars based on 11 product ratings        (11)    1,538,461.54 VND New 
  1,001,025.64 VND Used 
  Real Anthony Fauci : Bill Gates, Big Pharma, and the Global War on Democracy and Public Health by Robert F. Kennedy Jr. (2021, Hardcover)    
 4.9 out of 5 stars based on 94 product ratings        (94)    578,717.95 VND New 
  306,923.08 VND Used 
  Mountain Is You : Transforming Self-Sabotage into Self-Mastery by Brianna Wiest (2020, Trade Paperback)    
 5.0 out of 5 stars based on 10 product ratings        (10)    133,076.92 VND New 
  ---- Used 

 You may also like  
   
 Current slide {CURRENT_SLIDE} of {TOTAL_SLIDES}- You may also like  
  Simulation Paperback Textbooks 
  Nora Roberts Paperbacks Books 
  Nora Roberts Paperbacks Books 
  Robert Ludlum Paperbacks Books 
  Robert Ludlum Paperbacks Books 
  Robert Jordan Fiction Paperbacks Books 

 Additional site navigation  
  
 About eBay 
  Announcements 
  Community 
  Security Center 
  Seller Information Center 
  Policies 
  Affiliates 
  Help & Contact 
  Site Map 
 Copyright © 1995-2024 eBay Inc. All Rights Reserved. Accessibility  , User Agreement  , Privacy  , Consumer Health Data  , Payments Terms of Use  , Cookies  , CA Privacy Notice  , Your Privacy Choices  and AdChoice |

30. EOMAS_2 conference:
Conferences    Journals    Workshops    Seminars     

  Conferences    Journals    Workshops    Seminars    Symposiums    Meetings     
  Conference Raking    Journal Ranking    Impact Score    Blog     
 LaTeX   5G Tutorial    

  IMPACT SCORE    JOURNAL RANKING    CONFERENCE RANKING    Conferences    Journals    Workshops    Seminars    SYMPOSIUMS    MEETINGS    BLOG   LaTeX   5G Tutorial   Free Tools     

  Home    Categories    About    Call for Papers     

 EOMAS 2020 : 16th International Workshop on Enterprise & Organizational Simulation and Modelling   
   
  Grenoble, France 

 Event Date: | June 08, 2020 - June 09, 2020 
 Submission Deadline: | March 13, 2020 
 Notification of Acceptance: | April 20, 2020 
 Camera Ready Version Due: | May 18, 2020 

  GO TO WEBSITE        

 Categories 
 ENTERPRISE ENGINEERING    

 CONCEPTUAL MODELING    

 ENTERPRISE INFORMATION SYSTEMS    

 SOFTWARE ENGINEERING 

 About 
 In modern enterprises, business process management, decision making, and modeling and simulation (M&S) play a significant role.  
 The main focus of EOMAS is on the role, importance, and application of modeling and simulation within the extended organizational and enterprise context. This context implies that systems and software applications are designed to enable certain business processes, and to facilitate interaction of the actors, the system, and the environment. Thus, application of modeling and simulation in this extended context involves a number of socio-technical aspects (Information Systems (IS), Enterprise Information Systems (EIS), Software Systems, and Business Systems, Business Processes, Human Interaction, etc.), as well as technical challenges as security, big data, etc.  
 EOMAS has been designed to prive a quality research outlet and networking opportunity for researchers, practitioners, and educators interested in the development and application of modeling and simulation within the enterprise and organizational context. 

 Call for Papers 
 Scope   
 The main focus of EOMAS is on the role, importance, and application of modeling and simulation within the extended organizational and enterprise context. This context implies that systems and software applications are designed to enable certain business processes, and to facilitate interaction of the actors, the system, and the environment. Thus, application of modeling and simulation in this extended context involves a number of socio-technical aspects (Information Systems (IS), Enterprise Information Systems (EIS), Software Systems, and Business Systems, Business Processes, Human Interaction, etc.), as well as technical challenges as security, big data, etc.  
 Suggested Topics   
 Enterprise Modeling and Simulation 
  Information Systems Modeling and Simulation 
  Business Processes Modeling and Simulation 
  Ontologies in Modeling and Simulation 
  Domain-specific Modeling and Languages 
  Business Process Innovation through Business Analytics 
  Business Rules Modeling 
  Organizational Modeling and Simulation 
  Animation Modeling and Simulation 
  Modeling and Simulation Methods, Techniques, and Tools and their Combinations 
  Modeling and Simulation Guidelines for Practitioners 
  Models Evolvability 
  Business Process Modeling & Simulation using Petri net, Arena, UML, EPC, BORM and similar approaches 
  Case Studies on Modeling and Simulation 
  Transformation of Conceptual Models into Implementation Models, Model-Driven Engineering, Implementations of Simulation Models 
  Requirements Modeling and Simulation 
  Secure Business Process Modeling and Simulation 
  Modeling and Simulation Tools Demonstration 
  Big data simulation 
  Simulations for Business Analytics 
  Human and Cognitive Aspects of Modeling 
  Educational Methods in Enterprise Modeling and Simulation 
  FAIR Principles | in the Context of Enterprise Modeling and Simulation 

 Summary 
 EOMAS 2020 : 16th International Workshop on Enterprise & Organizational Simulation and Modelling  will take place in Grenoble, France  . It’s a 2  days event starting on Jun 08, 2020 (Monday)  and will be winded up on Jun 09, 2020 (Tuesday)  .  
 EOMAS 2020  falls under the following areas: ENTERPRISE ENGINEERING, CONCEPTUAL MODELING, ENTERPRISE INFORMATION SYSTEMS, SOFTWARE ENGINEERING,  etc. Submissions for this Workshop  can be made by Mar 13, 2020  . Authors can expect the result of submission by Apr 20, 2020  . Upon acceptance, authors should submit the final version of the manuscript on or before May 18, 2020  to the official website of the Workshop  .  
 Please check the official event website for possible changes before you make any travelling arrangements. Generally, events are strict with their deadlines. It is advisable to check the official website for all the deadlines.  
 Other Details of the EOMAS 2020   
  
 Short Name: | EOMAS 2020 
  Full Name: | 16th International Workshop on Enterprise & Organizational Simulation and Modelling 
  Timing: | 09:00 AM-06:00 PM (expected) 
  Fees: | Check the official website of | EOMAS 2020 
  Event Type: | Workshop 
  Website Link: | https://eomas-workshop.org/ 
  Location/Address: | Grenoble, France 

 Credits and Sources 
 [1] EOMAS 2020 : 16th International Workshop on Enterprise & Organizational Simulation and Modelling 

  Check other Conferences, Workshops, Seminars, and Events  Search Here    

 OTHER ENTERPRISE ENGINEERING EVENTS 

 BMSD 2024:   14th Int. Symposium on Business Modeling and Software Design  
  Luxembourg, Grand Duchy of Luxembourg   
  Jul 1, 2024 
 BMSD 2022:   12th Int. Symposium on Business Modeling and Software Design  
  Fribourg, Switzerland   
  Jun 27, 2022 
 BMSD 2020:   10th Int. Symposium on Business Modeling and Software Design  
  Berlin, Germany   
  Jul 06, 2020 
 EOMAS 2019:   15th International Workshop on Enterprise & Organizational Modeling and Simulation Conference Series : Enterprise and Organizational Modeling and Simulation  
  Rome, Italy   
  Jun 3, 2019 
 BMSD 2019:   9th International Symposium on Business Modeling and Software Design Conference Series : Business Modeling and Software Design  
  Lisbon, Portugal   
  Jul 1, 2019 
 SHOW ALL 

 OTHER CONCEPTUAL MODELING EVENTS 

 ER 2024:   43rd International Conference on Conceptual Modeling  
  Carnegie Mellon University, Pittsburgh,   
  Oct 28, 2024 
 KG4SDSE@CAISE 2024:   KG4SDSE@CAiSE 2024 : 2nd Workshop on Knowledge Graphs for Semantics-driven Systems Engineering @ CAiSE 2024  
  Limassol   
  Jun 3, 2024 
 T&D Modellierung 2024:   Tools & Demo Track at the Modellierung 2024  
  Potsdam   
  Mar 12, 2024 
 OMiLAB-KNOW 2023:   1st Workshop on Domain-specific Modeling Methods and Tools - OMiLAB Nodes experience & knowledge exchange (OMiLAB-KNOW)  
  Ascoli Piceno, Italy   
  Sep 15, 2023 
 EmpER 2023:   6th International Workshop on Empirical Methods in Conceptual Modeling (*EXTENDED DEADLINE*)  
  Lisbon, Portugal   
  Nov 6, 2023 
 SHOW ALL 

 OTHER SOFTWARE ENGINEERING EVENTS 

 AREA 2024:   4th Workshop on Agents and Robots for reliable Engineered Autonomy  
  Santiago de Compostela   
  Oct 19, 2024 
 ADIP 2024:   2024 6th Asia Digital Image Processing Conference (ADIP 2024)  
  Tokyo, Japan   
  Dec 14, 2024 
 ICSESS 2024:   2024 15th International Conference on Software Engineering and Service Science  
  China   
  Aug 24, 2024 
 IT-Tage 2024:   IT-Tage - IT-Konferenz für Software-Entwicklung, -Architektur, KI, Datenbanken, DevOps, Agile und Management  
  Frankfurt am Main   
  Dec 9, 2024 
 ICVIP--EI 2024:   2024 The 8th International Conference on Video and Image Processing (ICVIP 2024)  
  Kuala Lumpur, Malaysia   
  Dec 13, 2024 
 SHOW ALL 

  About Us  | Contact Us  | Disclaimer  | Privacy Policy  | Terms and Conditions   
   
 © 2024 www.resurchify.com  All Rights Reserved.

31. MMSP_1 conference:
Skip to main content    
 IEEE.org 
  IEEE Xplore  Digital Library 
  IEEE Standards 
  IEEE Spectrum 
  More Sites 
  Subscribe to Newsletter 

 Create Account 
  Sign in 

   Our Story | Our Story 
  Mission 
  What is Signal Processing? 
  Boards & Committees | Board of Governors 
  Executive Committee 
  Awards Board 
  Conferences Board 
  Membership Board 
  Publications Board 
  Technical Directions Board 
  Standing Committees 
  Liaisons & Representatives 
  Education Board 
  Our Members 
  Society History 
  State of the Society 
  SPS Branding Materials 
  Contact Us 
  SPS Staff 
  Sitemap 
  Publications & Resources | Publications & Resources 
  Publications | IEEE Signal Processing Magazine 
  IEEE Journal of Selected Topics in Signal Processing 
  IEEE Signal Processing Letters 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  IEEE Transactions on Computational Imaging 
  IEEE Transactions on Image Processing 
  IEEE Transactions on Information Forensics and Security 
  IEEE Transactions on Multimedia 
  IEEE Transactions on Signal and Information Processing over Networks 
  IEEE Transactions on Signal Processing 
  IEEE TCI 
  IEEE TSIPN 
  Data & Challenges 
  Submit Manuscript 
  Guidelines 
  Information for Authors 
  Special Issue Deadlines 
  Overview Articles 
  Top Accessed Articles 
  SPS Newsletter 
  SigPort 
  SPS Resource Center 
  Publications FAQ 
  Blog 
  News 
  Dataset Papers 
  Conferences & Events | Conferences & Events 
  Conferences 
  Calendar 
  Attend an Event 
  Conference Call for Papers 
  Calls for Proposals 
  Conference Sponsorship Info 
  Conference Resources 
  SPS Travel Grants 
  Conferences FAQ 
  Community & Involvement | Getting Involved 
  Membership | Young Professionals 
  Technical Committees | Our Technical Committees 
  Contact Technical Committee 
  Technical Committees FAQ 
  SPS Initiatives | Data Science Initiative 
  Join Technical Committee 
  Chapters and Communities | Young Professionals Resources 
  Member-driven Initiatives 
  Chapter Locator 
  Awards & Submit Award Nomination | Award Recipients 
  IEEE Fellows Program 
  Volunteer Opportunities | Call for Nominations 
  Professional Development | Professional Development 
  Distinguished Lecturer Program | Distinguished Lecturers 
  Past Lecturers 
  Nominations 
  Distinguished Industry Speaker Program | DIS Nominations 
  Seasonal Schools 
  Industry Resources 
  Jobs in Signal Processing | Job Submission Form 
  IEEE Training Materials 
  For Volunteers | For Volunteers 
  Board Agenda/Minutes 
  Chapter Resources 
  Governance Documents 
  Membership Development | Membership Development Reports 
  TC Best Practices 
  State of the Society 
  SPS Directory 
  Society FAQ 
  Information for Authors-OJSP 

   Home | Today's Most Visited Pages | Conference Call for Papers 
    Tuomas_Virtanen.jpg   
        Lectures   Distinguished Lecture: Tuomas Virtanen (Tampere University, Finland)   
    29 January 2025      Xu_blog_header.jpg   
        Occlusion-Aware Human Mesh Model-Based Gait Recognition   
    22 November 2024        Webinar.jpg   
        Webinars   SPS-DSI (DEGAS) Webinar: SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups   
    4 December 2024        
  View the full SPS Feed »           Conferences   Events   IEEE JSTSP Article   IEEE Signal Processing Magazine   IEEE TIFS Article   IEEE TMM Article   IEEE TSP Article   Jobs in Signal Processing   Lectures   Machine Learning   Seasonal Schools   Signal Processing News   SPM Article   SPS Distinguished Lectures   SPS Newsletter Article   SPS Webinar   SPS Webinars   SPS Webinar Series   Webinar   webinars 
  Our Story | Our Story | Celebrating 75 Years of IEEE SPS 
  Mission 
  Our Members 
  Diversity, Equity, and Inclusion 
  State of the Society 
  Society History 
  SPS Branding Materials 
  SPS Staff 
  Contact Us 
  Sitemap 
  General SP Multimedia Content 
    What is Signal Processing?  
    
  The technology we use, and even rely on, in our everyday lives –computers, radios, video, cell phones – is enabled by signal processing. Learn More »   
       Boards & Committees | Board of Governors 
  Executive Committee 
  Awards Board 
  Conferences Board 
  Education Board 
  Membership Board 
  Publications Board 
  Technical Directions Board 
  Standing Committees 
  Liaisons & Representatives 
  Publications & Resources | Publications & Resources | Submit a Manuscript 
  Editorial Board Nominations 
  Challenges & Data Collections 
  Publication Guidelines 
  Information for Authors 
  Special Issue Deadlines 
  Overview Articles 
  Publications FAQ 
  Top Accessed Articles 
  Unified EDICS 
  Blog 
  News 
    SPS Resources | Signal Processing Magazine The premier publication of the society. 
  SPS Newsletter Monthly updates in Signal Processing 
  SPS Resource Center Online library of tutorials, lectures, and presentations. 
  SigPort Online repository for reports, papers, and more. 
  SPS Feed The latest news, events, and more from the world of Signal Processing. 
    Publications | IEEE SP Magazine 
  IEEE SPS Content Gazette 
  IEEE JSTSP 
  IEEE OJSP 
  IEEE SP Letters 
  IEEE/ACM TASLP 
  IEEE TCI 
  IEEE TIFS 
  IEEE TIP 
  IEEE TMM 
  IEEE TSIPN 
  IEEE TSP 
  All SPS Publications 
  Conferences & Events | Conference Information | Conferences & Events 
  SPS Entrepreneurship Forum 
  Call for Papers 
  Call for Proposals 
  Request Sponsorship 
  Conference Organizer Resources 
  SPS Travel Grants 
  Past Conferences & Events 
  Conferences FAQ 
  Event Calendar 
    Attend an Event | Conferences Meet colleagues and advance your career. 
  Webinars Register for upcoming webinars. 
  Distinguished Lectures Learn from experts in signal processing. 
  Seasonal Schools For graduate students and early stage researchers. 
  All Events Browse all upcoming events. 
  Community & Involvement | Membership | Join SPS The IEEE Signal Processing Magazine, Conference, Discounts, Awards, Collaborations, and more! 
  Chapter Locator Find your local chapter and connect with fellow industry professionals, academics and students 
  Women in Signal Processing Networking and engagement opportunities for women across signal processing disciplines 
  Students Scholarships, conference discounts, travel grants, SP Cup, VIP Cup, 5-MICC 
  Young Professionals Career development opportunities, networking 
  Get Involved | Chapters & Communities 
  Challenges & Data Collections 
  Member Advocate 
  Awards & Submit an Award Nomination 
  Award Recipients 
  Volunteer Opportunities 
  Call for Nominations 
  Organize Local Initiatives 
  Seasonal Schools 
  Autonomous Systems Initiative 
  Data Science Initiative 
    Technical Committees | Applied Signal Processing Systems 
  Audio and Acoustic Signal Processing 
  Bio Imaging and Signal Processing 
  Computational Imaging 
  Image Video and Multidimensional Signal Processing 
  Information Forensics and Security 
  Machine Learning for Signal Processing 
  Multimedia Signal Processing 
  Sensor Array and Multichannel 
  Signal Processing for Communication and Networking 
  Signal Processing Theory and Methods 
  Speech and Language Processing 
  Technical Working Groups | Synthetic Aperture Technical Working Group 
  Industry Technical Working Group 
  Integrated Sensing and Communication Technical Working Group 
  More TC Resources | TC Affiliate Membership 
  Co-Sponsorship of Non-Conference TC Events 
  Professional Development | Professional Development | Mentoring Experiences for Underrepresented Young Researchers (ME-UYR) 
  Micro Mentoring Experience Program (MiME) 
  Distinguished Lecturer Program 
  Distinguished Lecturers 
  Distinguished Lecturer Nominations 
  Past Lecturers 
  Distinguished Industry Speaker Program 
  Distinguished Industry Speakers 
  Distinguished Industry Speaker Nominations 
  Industry Resources 
  IEEE Training Materials 
  Jobs in Signal Processing: IEEE Job Site 
    Career Resources | SPS Education Program Educational content in signal processing and related fields. 
  Distinguished Lecturer Program Chapters have access to educators and authors in the fields of Signal Processing 
  PROGRESS Initiative Promoting diversity in the field of signal processing. 
  Job Opportunities Signal Processing and Technical Committee specific job opportunities 
  Job Submission Form Employers may submit opportunities in the area of Signal Processing. 
  For Volunteers | Volunteer Resources | Chapter Resources 
  Society FAQ 
  Technical Committee Best Practices 
  Conflict of Interest 
  Governance Documents | Policy and Procedures Manual 
  Bylaws 
  Constitution 
    For Board & Committee Members | Board Agenda/Minutes* Agendas, minutes and supporting documentation for Board and Committee Members 
  SPS Directory* Directory of volunteers, society and division directory for Board and Committee Members. 
  Membership Development Reports* Insight into the Society’s month-over-month and year-over-year growths and declines for Board and Committee Members 

 IEEE   

 Popular Pages  
 Today's:  
 Submit a Manuscript 
  Information for Authors 
  (ISBI 2025) 2025 IEEE International Symposium on Biomedical Imaging 
  IEEE Transactions on Multimedia 
  IEEE JSTSP Special Issue on Low-Bit-Resolution Signal Processing: Algorithms, Implementations, and Applications 
  IEEE Transactions on Signal Processing 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  IEEE JSTSP Special Series on AI in Signal & Data Science - Toward Large Language Model (LLM) Theory and Applications 
  IEEE Journal of Selected Topics in Signal Processing 
  IEEE Transactions on Image Processing 
  IEEE Transactions on Information Forensics and Security 
  Conferences & Events 
  Conference Call for Papers 
  IEEE Signal Processing Magazine 
  (ME-UYR) Mentoring Experiences for Underrepresented Young Researchers Program 

  All time:  
 Information for Authors 
  Submit a Manuscript 
  IEEE Transactions on Image Processing 
  404 Page 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  IEEE Transactions on Information Forensics and Security 
  IEEE Transactions on Multimedia 
  IEEE Signal Processing Letters 
  IEEE Transactions on Signal Processing 
  Conferences & Events 
  IEEE Journal of Selected Topics in Signal Processing 
  Information for Authors-SPL 
  Conference Call for Papers 
  Signal Processing 101 
  IEEE Signal Processing Magazine 

  Last viewed:  
 Submit a Manuscript 
  Information for Authors 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  (ICME 2024) 2024 IEEE International Conference on Multimedia and Expo 
  Probability-Guaranteed Distributed Filtering for Nonlinear Systems on Basis of Nonuniform Samplings Subject to Envelope Constraints 
  Signal Processing 101 
  Post Doctoral and PhD Positions in Signal and Image Processing for Cellular Microscopy – University of Texas at Dallas 
  (ICIP 2025) 2025 IEEE International Conference on Image Processing 
  Postdoctoral fellowship in Image processing for flood detection (in Sao Paulo, Brazil) 
  IEEE Transactions on Multimedia 
  Jobs in Signal Processing 
  IEEE Transactions on Computational Imaging 
  Leveraging Variational Autoencoders for Parameterized MMSE Estimation 
  IEEE Transactions on Image Processing 
  Call for Mentors: 2024 IEEE SPS ME-UYR Program - Mentoring Experiences for Underrepresented Young Researchers 

 (MMSP 2023) 2023 IEEE 25th International Workshop on Multimedia Signal Processing  

 Search form  
 Search     

 You are here  
 Home  » (MMSP 2023) 2023 IEEE 25th International Workshop on Multimedia Signal Processing   

 Conferences & Events  
    
 Conferences & Events 
  SPS Entrepreneurship Forum 
  Attend an Event | Conferences & Events 
  Distinguished Lectures 
  Seasonal Schools 
  Webinars 
  Call for Papers 
  Call for Proposals 
  Annual Call for Proposals 
  Request Sponsorship 
  Conference Organizer Resources 
  Conference Workshop Author Resources 
  SPS Travel Grants 
  Past Conferences & Events 
  Conferences FAQ 

 Top Reasons to Join SPS Today!  
 1. IEEE Signal Processing Magazine   
  2. Signal Processing Digital Library*   
  3. Inside Signal Processing Newsletter  
  4. SPS Resource Center  
  5. Career advancement & recognition  
  6. Discounts on conferences and publications  
  7. Professional networking  
  8. Communities for students, young professionals, and women  
  9. Volunteer opportunities  
  10. Coming soon!  PDH/CEU credits  
  Click here to learn more  .  

 MMSP_2023.jpg   

 Sep  
 27  
   
 Conferences & Workshops 
  (MMSP 2023) 2023 IEEE 25th International Workshop on Multimedia Signal Processing   
  
 Date:  27-29 September 2023  
  Location:  Poitiers, France  

 Conference Paper Submission Deadline:   
 14 June 2023    

 Website Link:   
 MMSP 2023 Website    

 Tags:   
 MMSP Workshop    
 MMSP 2023    

 Event Types  
    
 Chapter Initiatives 
  Conferences & Workshops 
  Conferences & Workshops Call for Proposals 
  Courses 
  Diversity Events 
  Forums 
  Humanitarian Activities Initiatives 
  Lectures 
  Member Driven Initiatives 
  Regional Meetings 
  Seasonal Schools 
  Special Issue Deadlines 
  Webinars 

 Events  

 Recent 
  Popular 
  Tuomas_Virtanen.jpg | Distinguished Lecture: Tuomas Virtanen (Tampere University, Finland) | 26 Nov 2024 
  Webinar.jpg | SPS-DSI (DEGAS) Webinar: SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups | 21 Nov 2024 
  BISP_TC_Webinar.jpg | SPS BSI Webinar: Mechanisms of fMRI (dys)connectivity | 21 Nov 2024 

 ISBI_2025.jpg | (ISBI 2025) 2025 IEEE International Symposium on Biomedical Imaging | 22 Apr 2024 
  ICASSP_2025_5.jpg | (ICASSP 2025) 2025 IEEE International Conference on Acoustics, Speech and Signal Processing | 04 Feb 2020 
  SLT2024.jpg | (SLT 2024) 2024 IEEE Spoken Language Technology Workshop | 18 Dec 2023 

 SPS Social Media  
 IEEE SPS Facebook Page | https://www.facebook.com/ieeeSPS 
  IEEE SPS X Page | https://x.com/IEEEsps 
  IEEE SPS Instagram Page | https://www.instagram.com/ieeesps/?hl=en 
  IEEE SPS LinkedIn Page | https://www.linkedin.com/company/ieeesps/ 
  IEEE SPS YouTube Channel | https://www.youtube.com/ieeeSPS 

 IEEE SPS Educational Resources  
  
 IEEE SPS Resource Center   
     
 IEEE SPS YouTube Channel   

 Home  | Sitemap  | Contact  | Accessibility  | Nondiscrimination Policy  | IEEE Ethics Reporting  | IEEE Privacy Policy  | Terms  | Feedback   
 © Copyright 2024 IEEE - All rights reserved. Use of this website signifies your agreement to the IEEE Terms and Conditions  .  
  A public charity, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

32. MABS_3 conference:
We use cookies. Read more about them in our Privacy Policy  .  
   
 Accept site cookies 
  Reject site cookies 

 Skip to content          
 Search for:        
 Menu        
   
 Home 
  About | About the Institute 
  Study with us 
  Work with us 
  Research areas | Biodiversity        Climate change adaptation and resilience        Climate change governance, legislation and litigation        Climate, health, and environment        Environmental behaviour        Environmental economic theory        Environmental policy evaluation        International climate politics        Science and impacts of climate change        Sustainable natural resources        Sustainable public and private finance        Transition to zero emissions growth        UK national and local climate policies          View all Research areas 
  Publications 
  Explainers | Adaptation to climate change        Biodiversity        Business and climate change        Climate and the economy        Climate change and the UK        Climate change policies        Climate change science        Energy and climate change        Impacts of climate change        International action on climate change        Oceans and the blue economy          View all Explainers 
  News & commentaries 
  Events 
  People 
  Contact 
  Close menu       

 Home    »  Publications   »  Book   »  Multi-Agent-Based Simulation XXIV     
  
 Multi-Agent-Based Simulation XXIV   
 Book  on 15 May, 2024   

 This book constitutes the refereed Proceedings of the 24th International Workshop on Multi-Agent-Based Simulation XXIV, MABS 2023, held in London, UK, during May 29–June 2, 2023.  
 The 11 regular papers presented were carefully reviewed and selected from 27 submissions. The papers are organized in subject areas as follows: MABS methodology and tools; MABS and social behavior; and MABS applications.  
 Multi-Agent-Based Simulation XXIV, Springer Cham 2024, Volume 14558, Editors  
  Luis G. Nardin, Sara Mehryar  
  ISBN : 978-3-031-61033-2.   
 External link to publisher     

 Authors  

 Sara Mehryar   
 Research Fellow  
   
 Read more about Sara Mehryar        

 Research areas  

 Climate change adaptation and resilience   Environmental behaviour     
  
 Keywords  

 agent based model   artificial intelligence   machine learning   modelling   multi-agent-based-simulation     
  
 Share this  

 Keep in touch with the Grantham Research Institute at LSE   
 Sign up to our newsletters and get the latest analysis, research, commentary and details of upcoming events.   
 Sign up to our newsletter   

 Legal   
 Accessibility 
  Privacy and data protection 
  Terms of use 
    
 Contact Grantham Research Institute   
 Email: Gri@lse.ac.uk   
  Tel: +44 (0)207 955 6425  
  Grantham Research Institute  
  LSE  
  Houghton Street  
  London  
  WC2A 2AE  
 Further contact details and map   
   
 Follow Grantham Research Institute   
 Visit our twitter page 
  Visit our linkedin page 
  Visit our youtube page 
  Visit our facebook page 
    
 Copyright © LSE 2024  
   
 Queen's Anniversary Prize Winner 2021   

 Opens in a new window    
   
  English   中文 (简体)

33. MISE_3 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
   
 Information  Conference Details  Technical Program  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Doctoral Consortium  Partners  Academic Partners  Industrial Partners  Exhibitors  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Abstracts  Awards    
  
 Sponsored by:    

 INSTICC is Member of:    

 Logistics:    

  Although the conference is back to the normal mode (i.e., in-person) speakers are allowed to present remotely if unable to travel to the venue (hybrid support).    

 The International Conference on Model-Based Software and Systems Engineering provides a platform for participants from all over the world to present research results and application experience in using model-based techniques for developing all sorts of systems. Model-based software engineering has emerged over many years as an approach for developing IT systems in which models take a central role, not only for the analysis of these systems but also for their construction. More recently, a similar approach has become increasingly widely used not only for software systems, but also for physical, cyber-physical and business systems. This model-based software and systems engineering approach relies on an increasingly rich ecosystem of techniques, languages and tools. MODELSWARD is an opportunity for researchers and practitioners alike to come together and discuss, moving this ecosystem forward.    
 Conference Areas  
 1  .  Methodologies, Processes and Platforms   
  
  2  .  Applications and System Development   
  
  3  .  Modeling Languages, Tools and Architectures   

  Conference Chair    
 Edwin Seidewitz  ,  Model Driven Solutions, United States   

 PROGRAM CO-CHAIRS    
 Francisco José Domínguez Mayo  ,  University of Seville, Spain   
  Luís Ferreira Pires  ,  Faculty of Electrical Engineering, University of Twente, Netherlands   

  Keynote Speakers   
 Sanford Friedenthal  ,  SAF Consulting, LLC, United States   
  Vadim Zaytsev  ,  University of Twente, Netherlands   
  Henderik A. Proper  ,  TU Wien, Vienna, Austria, Austria   

 Publications:     
  All papers presented at the conference venue  
  will be available at the SCITEPRESS Digital Library   
  ( consult SCITEPRESS  Ethics of Publication  )  

  A short list of best papers will be invited  
  for a post-conference special issue of the  
  Springer Nature Computer Science Journal   

  It is planned to publish a short list of revised and  
  extended versions of presented papers with  
  Springer in a CCIS Series book   

 In Collaboration with:   

 In Cooperation with:    

 Proceedings will be submitted for evaluation for indexing by:    

  Web of Science/Conference Proceedings Citation Index     

 © 2024  INSTICC

34. MMSP_2 conference:
Skip to content  IEEE.org 
  IEEE Xplore  Digital Library 
  IEEE Standards 
  IEEE Spectrum 
  More Sites 

  MENU     
     
 Email address   What would you like to search for?    

 SEARCH      

   About 
  For Authors | Instructions for Authors 
  Important Dates 
  Présentation Guidelines 
  CALLS | Call for papers 
  Call for Special Sessions 
  Call for Industry Demos 
  Call for Grand Challenges 
  Program | Technical Program 
  Special sessions 
  Keynote Speakers 
  Registration 
  Travel Grant 
  Organization | Organizing committee 
  Venue | Conference Venue 
  Accommodation 
  Travel connections 
  Social Program 
  Sponsors 

 The IEEE International Workshop on MultiMedia Signal Processing (MMSP 2023) will take place from September 27-29, 2023.  
 The workshop is 25 th  in the series, organized by the Multimedia Signal Processing Technical Committee of the IEEE Signal Processing Society (SPS), with the aim to bring together researchers and practitioners from academia and industry, passionate about multimedia signal processing, to share their knowledge, exchange ideas, explore future research directions and network.  

 Home 
  Sitemap 
  Accessibility 
  Nondiscrimination Policy 
  IEEE Ethics Reporting 
  IEEE Privacy Policy 
  Terms 

 © Copyright 2024 IEEE – All rights reserved. Use of this website signifies your agreement to the IEEE Terms and Conditions.  
  A not-for-profit organization, IEEE is the world’s largest technical professional organization dedicated to advancing technology for the benefit of humanity.  

  Back to Top

35. PQCrypto_0 conference:
Post-quantum cryptography 

 Motivation: 
 Introduction 
 Quantum computing | Motivation: | Introduction | Quantum computing 
 Motivation: 
 Introduction 
 Quantum computing 
 Cryptography: 
 Hash-based 
 Code-based 
 Lattice-based 
 MQ | Cryptography: | Hash-based | Code-based | Lattice-based | MQ 
 Cryptography: 
 Hash-based 
 Code-based 
 Lattice-based 
 MQ 
 Community: 
 Conferences 
 Workshops | Community: | Conferences | Workshops 
 Community: 
 Conferences 
 Workshops | Motivation: 
 Introduction 
 Quantum computing | Motivation: | Introduction | Quantum computing | Cryptography: 
 Hash-based 
 Code-based 
 Lattice-based 
 MQ | Cryptography: | Hash-based | Code-based | Lattice-based | MQ | Community: 
 Conferences 
 Workshops | Community: | Conferences | Workshops | Conferences  
 PQCrypto is the main conference series devoted to post-quantum cryptography:  
 #16: PQCrypto 2025. Taipei, Taiwan, 8–10 April 2025. | https://pqcrypto2025.iis.sinica.edu.tw/ | . 
  #15: PQCrypto 2024. Mathematical Institute, Oxford, UK, 12−14 June 2024. | https://www.maths.ox.ac.uk/events/conferences/pqcrypto-2024 | . 
  #14: PQCrypto 2023. College Park, Maryland, USA, 16−18 August 2023. | https://pqcrypto2023.umiacs.io/ | . 
  #13: PQCrypto 2022. Online, 28–30 September 2022. | https://2022.pqcrypto.org | . 
  #12: PQCrypto 2021. Daejeon, South Korea, 20–22 July 2021. | https://pqcrypto2021.kr/ | . 
  #11: PQCrypto 2020. Paris, France, 21–23 September 2020 (rescheduled from 15−17 April 2020). | https://pqcrypto2020.inria.fr/ | . 
  #10: PQCrypto 2019. Chongqing University, Chongqing, China, 8–10 May 2019. | https://pqcrypto2019.org/ | . 
  #9: PQCrypto 2018. Pier Sixty-Six, Fort Lauderdale, Florida, USA, 9–11 April 2018. | http://www.math.fau.edu/pqcrypto2018/ | . Followed immediately by NIST workshop 12–13 April 2018. | https://csrc.nist.gov/events/2018/first-pqc-standardization-conference | . 
  #8: PQCrypto 2017. Utrecht, the Netherlands, 26–28 June 2017, preceded by summer school 19–23 June 2017. | https://2017.pqcrypto.org/conference/ | . 
  #7: PQCrypto 2016. Fukuoka, Japan, February 2016. | https://pqcrypto2016.jp/ | . 
  #6: PQCrypto 2014. Institute for Quantum Computing, University of Waterloo Canada, October 1–3, 2014. | https://web.archive.org/web/20151201043013/http://pqcrypto2014.uwaterloo.ca:80/?page_id=10 | . 
  #5: PQCrypto 2013. Limoges, France, June 4–7, 2013. | https://web.archive.org/web/20140118225341/http://pqcrypto2013.xlim.fr/ | . 
  #4: PQCrypto 2011. Taipei, Taiwan, November 29–December 2, 2011. | https://troll.iis.sinica.edu.tw/pqc11/ | . 
  #3: PQCrypto 2010. Darmstadt (close to Frankfurt), Germany, May 25–28, 2010. | https://web.archive.org/web/20101008051348/http://pqc2010.cased.de/ | . 
  #2: PQCrypto 2008. University of Cincinnati, USA, October 17–19, 2008. | https://web.archive.org/web/20160818142141/https://math.uc.edu/~aac/pqcrypto2008/ | . 
  #1: PQCrypto 2006. Katholieke Universiteit Leuven, Belgium, May 23–26, 2006. | https://postquantum.cr.yp.to | . 
  PQCrypto 2006 did not have official proceedings but almost all of the accepted papers are available on the conference site. Subsequent PQCrypto events have had official proceedings published by Springer: https://link.springer.com/conference/pqcrypto   
 Steering committee  
 Please contact steeringcommittee at pqcrypto.org to propose subsequent PQCrypto events. Current members of the steering committee (alphabetical order):  
 Daniel J. Bernstein 
  Jung Hee Cheon 
  Claude Crépeau 
  Jintai Ding 
  Philippe Gaborit 
  Thomas Johansson 
  Tanja Lange (steering-committee chair) 
  Daniele Micciancio 
  Michele Mosca 
  Nicolas Sendrier 
  Rainer Steinwandt 
  Tsuyoshi Takagi 
  Jean-Pierre Tillich 
  Bo-Yin Yang 
  Former steering-committee members (alphabetical order):  
 Johannes Buchmann 
  Werner Schindler 
  Shigeo Tsujii 
  Other conferences  
 Post-quantum cryptography is also appearing more and more frequently at general cryptographic conferences.  
 Version  
 This is version 2024.07.22 of the conferences.html web page. 
 Motivation: 
 Introduction 
 Quantum computing | Motivation: | Introduction | Quantum computing 
 Motivation: 
 Introduction 
 Quantum computing 
 Cryptography: 
 Hash-based 
 Code-based 
 Lattice-based 
 MQ | Cryptography: | Hash-based | Code-based | Lattice-based | MQ 
 Cryptography: 
 Hash-based 
 Code-based 
 Lattice-based 
 MQ 
 Community: 
 Conferences 
 Workshops | Community: | Conferences | Workshops 
 Community: 
 Conferences 
 Workshops

36. MMSP_3 conference:
Skip to main content    
 IEEE.org 
  IEEE Xplore  Digital Library 
  IEEE Standards 
  IEEE Spectrum 
  More Sites 
  Subscribe to Newsletter 

 Create Account 
  Sign in 

   Our Story | Our Story 
  Mission 
  What is Signal Processing? 
  Boards & Committees | Board of Governors 
  Executive Committee 
  Awards Board 
  Conferences Board 
  Membership Board 
  Publications Board 
  Technical Directions Board 
  Standing Committees 
  Liaisons & Representatives 
  Education Board 
  Our Members 
  Society History 
  State of the Society 
  SPS Branding Materials 
  Contact Us 
  SPS Staff 
  Sitemap 
  Publications & Resources | Publications & Resources 
  Publications | IEEE Signal Processing Magazine 
  IEEE Journal of Selected Topics in Signal Processing 
  IEEE Signal Processing Letters 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  IEEE Transactions on Computational Imaging 
  IEEE Transactions on Image Processing 
  IEEE Transactions on Information Forensics and Security 
  IEEE Transactions on Multimedia 
  IEEE Transactions on Signal and Information Processing over Networks 
  IEEE Transactions on Signal Processing 
  IEEE TCI 
  IEEE TSIPN 
  Data & Challenges 
  Submit Manuscript 
  Guidelines 
  Information for Authors 
  Special Issue Deadlines 
  Overview Articles 
  Top Accessed Articles 
  SPS Newsletter 
  SigPort 
  SPS Resource Center 
  Publications FAQ 
  Blog 
  News 
  Dataset Papers 
  Conferences & Events | Conferences & Events 
  Conferences 
  Calendar 
  Attend an Event 
  Conference Call for Papers 
  Calls for Proposals 
  Conference Sponsorship Info 
  Conference Resources 
  SPS Travel Grants 
  Conferences FAQ 
  Community & Involvement | Getting Involved 
  Membership | Young Professionals 
  Technical Committees | Our Technical Committees 
  Contact Technical Committee 
  Technical Committees FAQ 
  SPS Initiatives | Data Science Initiative 
  Join Technical Committee 
  Chapters and Communities | Young Professionals Resources 
  Member-driven Initiatives 
  Chapter Locator 
  Awards & Submit Award Nomination | Award Recipients 
  IEEE Fellows Program 
  Volunteer Opportunities | Call for Nominations 
  Professional Development | Professional Development 
  Distinguished Lecturer Program | Distinguished Lecturers 
  Past Lecturers 
  Nominations 
  Distinguished Industry Speaker Program | DIS Nominations 
  Seasonal Schools 
  Industry Resources 
  Jobs in Signal Processing | Job Submission Form 
  IEEE Training Materials 
  For Volunteers | For Volunteers 
  Board Agenda/Minutes 
  Chapter Resources 
  Governance Documents 
  Membership Development | Membership Development Reports 
  TC Best Practices 
  State of the Society 
  SPS Directory 
  Society FAQ 
  Information for Authors-OJSP 

   Home | Today's Most Visited Pages | Conference Call for Papers 
    Tuomas_Virtanen.jpg   
        Lectures   Distinguished Lecture: Tuomas Virtanen (Tampere University, Finland)   
    29 January 2025      Xu_blog_header.jpg   
        Occlusion-Aware Human Mesh Model-Based Gait Recognition   
    22 November 2024        Webinar.jpg   
        Webinars   SPS-DSI (DEGAS) Webinar: SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups   
    4 December 2024        
  View the full SPS Feed »           Conferences   Events   IEEE JSTSP Article   IEEE Signal Processing Magazine   IEEE TIFS Article   IEEE TMM Article   IEEE TSP Article   Jobs in Signal Processing   Lectures   Machine Learning   Seasonal Schools   Signal Processing News   SPM Article   SPS Distinguished Lectures   SPS Newsletter Article   SPS Webinar   SPS Webinars   SPS Webinar Series   Webinar   webinars 
  Our Story | Our Story | Celebrating 75 Years of IEEE SPS 
  Mission 
  Our Members 
  Diversity, Equity, and Inclusion 
  State of the Society 
  Society History 
  SPS Branding Materials 
  SPS Staff 
  Contact Us 
  Sitemap 
  General SP Multimedia Content 
    What is Signal Processing?  
    
  The technology we use, and even rely on, in our everyday lives –computers, radios, video, cell phones – is enabled by signal processing. Learn More »   
       Boards & Committees | Board of Governors 
  Executive Committee 
  Awards Board 
  Conferences Board 
  Education Board 
  Membership Board 
  Publications Board 
  Technical Directions Board 
  Standing Committees 
  Liaisons & Representatives 
  Publications & Resources | Publications & Resources | Submit a Manuscript 
  Editorial Board Nominations 
  Challenges & Data Collections 
  Publication Guidelines 
  Information for Authors 
  Special Issue Deadlines 
  Overview Articles 
  Publications FAQ 
  Top Accessed Articles 
  Unified EDICS 
  Blog 
  News 
    SPS Resources | Signal Processing Magazine The premier publication of the society. 
  SPS Newsletter Monthly updates in Signal Processing 
  SPS Resource Center Online library of tutorials, lectures, and presentations. 
  SigPort Online repository for reports, papers, and more. 
  SPS Feed The latest news, events, and more from the world of Signal Processing. 
    Publications | IEEE SP Magazine 
  IEEE SPS Content Gazette 
  IEEE JSTSP 
  IEEE OJSP 
  IEEE SP Letters 
  IEEE/ACM TASLP 
  IEEE TCI 
  IEEE TIFS 
  IEEE TIP 
  IEEE TMM 
  IEEE TSIPN 
  IEEE TSP 
  All SPS Publications 
  Conferences & Events | Conference Information | Conferences & Events 
  SPS Entrepreneurship Forum 
  Call for Papers 
  Call for Proposals 
  Request Sponsorship 
  Conference Organizer Resources 
  SPS Travel Grants 
  Past Conferences & Events 
  Conferences FAQ 
  Event Calendar 
    Attend an Event | Conferences Meet colleagues and advance your career. 
  Webinars Register for upcoming webinars. 
  Distinguished Lectures Learn from experts in signal processing. 
  Seasonal Schools For graduate students and early stage researchers. 
  All Events Browse all upcoming events. 
  Community & Involvement | Membership | Join SPS The IEEE Signal Processing Magazine, Conference, Discounts, Awards, Collaborations, and more! 
  Chapter Locator Find your local chapter and connect with fellow industry professionals, academics and students 
  Women in Signal Processing Networking and engagement opportunities for women across signal processing disciplines 
  Students Scholarships, conference discounts, travel grants, SP Cup, VIP Cup, 5-MICC 
  Young Professionals Career development opportunities, networking 
  Get Involved | Chapters & Communities 
  Challenges & Data Collections 
  Member Advocate 
  Awards & Submit an Award Nomination 
  Award Recipients 
  Volunteer Opportunities 
  Call for Nominations 
  Organize Local Initiatives 
  Seasonal Schools 
  Autonomous Systems Initiative 
  Data Science Initiative 
    Technical Committees | Applied Signal Processing Systems 
  Audio and Acoustic Signal Processing 
  Bio Imaging and Signal Processing 
  Computational Imaging 
  Image Video and Multidimensional Signal Processing 
  Information Forensics and Security 
  Machine Learning for Signal Processing 
  Multimedia Signal Processing 
  Sensor Array and Multichannel 
  Signal Processing for Communication and Networking 
  Signal Processing Theory and Methods 
  Speech and Language Processing 
  Technical Working Groups | Synthetic Aperture Technical Working Group 
  Industry Technical Working Group 
  Integrated Sensing and Communication Technical Working Group 
  More TC Resources | TC Affiliate Membership 
  Co-Sponsorship of Non-Conference TC Events 
  Professional Development | Professional Development | Mentoring Experiences for Underrepresented Young Researchers (ME-UYR) 
  Micro Mentoring Experience Program (MiME) 
  Distinguished Lecturer Program 
  Distinguished Lecturers 
  Distinguished Lecturer Nominations 
  Past Lecturers 
  Distinguished Industry Speaker Program 
  Distinguished Industry Speakers 
  Distinguished Industry Speaker Nominations 
  Industry Resources 
  IEEE Training Materials 
  Jobs in Signal Processing: IEEE Job Site 
    Career Resources | SPS Education Program Educational content in signal processing and related fields. 
  Distinguished Lecturer Program Chapters have access to educators and authors in the fields of Signal Processing 
  PROGRESS Initiative Promoting diversity in the field of signal processing. 
  Job Opportunities Signal Processing and Technical Committee specific job opportunities 
  Job Submission Form Employers may submit opportunities in the area of Signal Processing. 
  For Volunteers | Volunteer Resources | Chapter Resources 
  Society FAQ 
  Technical Committee Best Practices 
  Conflict of Interest 
  Governance Documents | Policy and Procedures Manual 
  Bylaws 
  Constitution 
    For Board & Committee Members | Board Agenda/Minutes* Agendas, minutes and supporting documentation for Board and Committee Members 
  SPS Directory* Directory of volunteers, society and division directory for Board and Committee Members. 
  Membership Development Reports* Insight into the Society’s month-over-month and year-over-year growths and declines for Board and Committee Members 

 IEEE   

 Popular Pages  
 Today's:  
 Submit a Manuscript 
  Information for Authors 
  (ISBI 2025) 2025 IEEE International Symposium on Biomedical Imaging 
  IEEE Transactions on Multimedia 
  IEEE JSTSP Special Issue on Low-Bit-Resolution Signal Processing: Algorithms, Implementations, and Applications 
  IEEE Transactions on Signal Processing 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  IEEE JSTSP Special Series on AI in Signal & Data Science - Toward Large Language Model (LLM) Theory and Applications 
  IEEE Journal of Selected Topics in Signal Processing 
  IEEE Transactions on Image Processing 
  IEEE Transactions on Information Forensics and Security 
  Conferences & Events 
  Conference Call for Papers 
  IEEE Signal Processing Magazine 
  (ME-UYR) Mentoring Experiences for Underrepresented Young Researchers Program 

  All time:  
 Information for Authors 
  Submit a Manuscript 
  IEEE Transactions on Image Processing 
  404 Page 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  IEEE Transactions on Information Forensics and Security 
  IEEE Transactions on Multimedia 
  IEEE Signal Processing Letters 
  IEEE Transactions on Signal Processing 
  Conferences & Events 
  IEEE Journal of Selected Topics in Signal Processing 
  Information for Authors-SPL 
  Conference Call for Papers 
  Signal Processing 101 
  IEEE Signal Processing Magazine 

  Last viewed:  
 Submit a Manuscript 
  Information for Authors 
  IEEE/ACM Transactions on Audio Speech and Language Processing 
  (ICME 2024) 2024 IEEE International Conference on Multimedia and Expo 
  Probability-Guaranteed Distributed Filtering for Nonlinear Systems on Basis of Nonuniform Samplings Subject to Envelope Constraints 
  Signal Processing 101 
  Post Doctoral and PhD Positions in Signal and Image Processing for Cellular Microscopy – University of Texas at Dallas 
  (ICIP 2025) 2025 IEEE International Conference on Image Processing 
  Postdoctoral fellowship in Image processing for flood detection (in Sao Paulo, Brazil) 
  IEEE Transactions on Multimedia 
  Jobs in Signal Processing 
  IEEE Transactions on Computational Imaging 
  Leveraging Variational Autoencoders for Parameterized MMSE Estimation 
  IEEE Transactions on Image Processing 
  Call for Mentors: 2024 IEEE SPS ME-UYR Program - Mentoring Experiences for Underrepresented Young Researchers 

 MMSP  

 Search form  
 Search     

 You are here  
 Home  » MMSP   

 Top Reasons to Join SPS Today!  
 1. IEEE Signal Processing Magazine   
  2. Signal Processing Digital Library*   
  3. Inside Signal Processing Newsletter  
  4. SPS Resource Center  
  5. Career advancement & recognition  
  6. Discounts on conferences and publications  
  7. Professional networking  
  8. Communities for students, young professionals, and women  
  9. Volunteer opportunities  
  10. Coming soon!  PDH/CEU credits  
  Click here to learn more  .  

   27  Nov   
 newsletter_general.jpg   

 Conferences & Workshops Call for Proposals 
  Call for Proposals: (MMSP 2027) 2027 IEEE Multimedia Signal Processing Workshop   
  
 Submission Deadline:  27 November 2024  

   21  Sep  24  Sep   
 MMSP_2025.jpg   

 Conferences & Workshops 
  (MMSP 2025) 2025 IEEE 26th International Workshop on Multimedia Signal Processing   
  
 Date:  21-24 September 2025  
  Location:  Beijing, China  

 Conference Paper Submission Deadline:   
 19 May 2025    

 Website Link:   
 IEEE MMSP 2025 Website    

   02  Oct  04  Oct   
 MMSP_2024.jpg   

 Conferences & Workshops 
  (MMSP 2024) 2024 IEEE 26th International Workshop on Multimedia Signal Processing   
  
 Date:  2-4 October 2024  
  Location:  West Lafayette, IN, USA  

 Conference Paper Submission Deadline:   
 19 June 2024    

 Website Link:   
 MMSP 2024 Website    

   27  Sep  29  Sep   
 MMSP_2023.jpg   

 Conferences & Workshops 
  (MMSP 2023) 2023 IEEE 25th International Workshop on Multimedia Signal Processing   
  
 Date:  27-29 September 2023  
  Location:  Poitiers, France  

 Conference Paper Submission Deadline:   
 14 June 2023    

 Website Link:   
 MMSP 2023 Website    

   10  Jun   
 abstract_general_1.jpg   

 Conferences & Workshops Call for Proposals 
  Call For Proposals: IEEE MMSP 2024   
  
 Submission Deadline:  10 June 2023  
  Call for Proposals Document   

   26  Sep  28  Sep   
 MMSP_2022.jpg   

 Conferences & Workshops 
  (MMSP 2022) 2022 IEEE 24th International Workshop on Multimedia Signal Processing   
  
 Date: September 26-28, 2022  
  Location: Shanghai, China  

 Conference Paper Submission Deadline:   
 01 June 2022    

 Website Link:   
 MMSP 2022 Website    

   06  Oct  08  Oct   
 mmsp2020.jpg   

 Conferences & Workshops 
  (MMSP 2021) 2021 IEEE International Workshop on Multimedia Signal Processing   
  
 October 6-8, 2021  
  Location: Tampere, Finland  

 Conference Paper Submission Deadline:   
 28 May 2021    

 Website Link:   
 MMSP 2021 Website    

   21  Sep  23  Sep   
 mmsp2020.jpg   

 Conferences & Workshops 
  (MMSP 2020) 2020 IEEE 22nd International Workshop on Multimedia Signal Processing   
  
 September 21-23, 2020  
  NOTE: Location changed to--Virtual Conference    

 Conference Paper Submission Deadline:   
 31 May 2020    

 Archive Website Link:   
 MMSP 2020 Archived Site    

   27  Sep  29  Sep   
 MMSP_2019.jpg   

 Conferences & Workshops 
  (MMSP 2019) 2019 IEEE 21st International Workshop on Multimedia Signal Processing   
  
 September 27-29, 2019  
  Location: Kuala Lumpur, Malaysia  

 Conference Paper Submission Deadline:   
 18 May 2019    

 Website Link:   
 MMSP 2019 Website    

 SPS Social Media  
 IEEE SPS Facebook Page | https://www.facebook.com/ieeeSPS 
  IEEE SPS X Page | https://x.com/IEEEsps 
  IEEE SPS Instagram Page | https://www.instagram.com/ieeesps/?hl=en 
  IEEE SPS LinkedIn Page | https://www.linkedin.com/company/ieeesps/ 
  IEEE SPS YouTube Channel | https://www.youtube.com/ieeeSPS 

 IEEE SPS Educational Resources  
  
 IEEE SPS Resource Center   
     
 IEEE SPS YouTube Channel   

 Home  | Sitemap  | Contact  | Accessibility  | Nondiscrimination Policy  | IEEE Ethics Reporting  | IEEE Privacy Policy  | Terms  | Feedback   
 © Copyright 2024 IEEE - All rights reserved. Use of this website signifies your agreement to the IEEE Terms and Conditions  .  
  A public charity, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

37. DX_0 conference:
| Home | | CFP | | Dates | | Submission | | Registration | | Venue | | Program | | Awards | | Downloads | | People | | Contact | | Sponsors 

 The DX Workshop 2023 (DX'23) is an event co-organized by PARC, a Xerox company and the Silicon Austria Labs (SAL). |  
  
 News:  
 September 3rd:  Please note our updates regarding the program details  , like the exact times, some info about the social program, about the DX'23 Best Paper Award  , and also about the DX'23 keynote given by Kai Goebel  ! Looking forward to welcoming you in a week at Redwood Glen!  
   
  August 27th:  Info for our presenters  : Each paper will be allotted 30 minutes (including Q&A); please prepare presentations between 20 and 25 minutes so that there is enough time for discussions.  
   
  August 27th:  About two weeks to go - we're looking forwar to meeting you and discuss the newest research in diagnosis and related topics!  
   
  August 15th:  Don't miss submitting to our DX'23 fast track  - we're still accepting submissions  until late August!  
   
  August 15th:  Congratulations to the authors of all accepted papers. Please do not forget to submit your camera-ready versions  - we re-opened submissions via EasyChair so that you can conveniently upload your latest updates.  
   
  July 17th:  Please note our info about how to reach our venue Redwood Glen  that is available from here  . Don't miss the registration deadline and book your DX'23 package  today (registration, meals, and accommodation from Monday afternoon to Thursday lunch time). Please check out the links above for more details.  
   
  July 7th: Exciting news: This year we will have a fast track for breaking new research and results.  For your latest breaking research (work in progress or also finalized contributions), you will be able to submit a corresponding paper until August 25th. Please follow the preparation instructions as advertized for regular submissions (and mark it as regular submission on the cover page)!  
   
  July 7th:  With the remaining reviews coming in  , we will send out the notification soon. We are looking forward to having a diverse set of excellent papers being presented in September at DX'23!  
   
  June 23rd:  Please note our updates on how to reach the DX'23 venue  .  
   
  June 20th:  Registration is now open  !  
   
  June 9th:  The final DX'23 deadline is approaching quickly. Please note our updated information about registration packages and pricing available from our registration subpage   . Registration will open soon.  
   
  June 3rd:  Upon several requests, we extended the submission deadline a final time to June 14th  So there is less than two weeks left for preparing your paper and PhD panel submissions!  
   
  May 14:  The submission deadline  has been extended to June 1st!  
   
  May 14:  Please note our style files that are available from here  for preparing your submission  !  
   
  April 23:  Don't forget to plan your DX'23 submission. Our EasyChair submission site is open - check our submission information   to find out how to submit your paper / PhD panel entry!  
   
  April 3:  All PhD students working on DX-related topics are encouraged to submit a description of their research/thesis of up to five pages to be considered for presentation at a special DX'23 PhD panel  . The accepted entries will be included in the proceedings in a special section!  
   
  April 3:  The 34th International Workshop on Principles of Diagnosis (DX'23) will take place in the Santa Cruz Mountains community of Loma Mar, CA, USA, from September 11th to 14th, 2023  . At the top of this page you can already see some impressions of our venue. Stay tuned for further updates!  
   
  March 25:  We're proud to announce that Oliver Niggemann from the Helmut Schmidt University in Germany will chair the DX'23 PhD panel!  
   
  March 22:  We're happy to announce some first info about DX'23 - stay tuned for upcoming updates!

38. PQCrypto_1 conference:
Skip to main navigation 
  Skip to search 
  Skip to main content 
   
 Lund University Home    English 
  Svenska 

 Home 
  Profiles 
  Research output 
  Projects 
  Units 
  Infrastructure 
  Activities 
  Prizes and Distinction 
   
 Search by expertise, name or affiliation     

 Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings   
   
 Thomas Johansson   (Editor), Daniel Smith-Tone (Editor)  
   
 LTH Profile Area: AI and Digitalization 
  Networks and Security 
  ELLIIT: the Linköping-Lund initiative on IT and mobile communication 
    
 National Institute of Standards and Technology (NIST) 
    
 Research output :   Book/Report ›   Conference proceeding (editor)  ›  peer-review   

 Overview 
  Fingerprint 

 Abstract  
 This book constitutes the refereed proceedings of the 14th International Workshop on Post-Quantum Cryptography, PQCrypto 2022, held in College Park, MD, USA, in August 14–18, 2023.  
   
  The 25 full papers presented in this book were carefully reviewed and selected from 51 submissions. They are categorized in the following topical sections: code-based cryptography; group-action-based cryptography; isogenye-based cryptography; lattice-based cryptography; multivariate cryptography; quantum algorithms, cryptanalysis and models; post-quantum protocols; side channel cryptanalysis and countermeasures.  

 Original language | English 
 Place of Publication | Cham 
 Publisher | Springer 
 Number of pages | 714 
 Volume | LNCS 14154 
 ISBN (Electronic) | 9783031400032 
 ISBN (Print) | 9783031400025 
 DOIs | https://doi.org/10.1007/978-3-031-40003-2 
 Publication status | Published -  2023 
 Event | The 14th International Conference on Post-Quantum Cryptography    - Stamp Student Union, University of Maryland, College Park, United States  
  Duration: 2023 Aug 16  → 2023 Aug 18   
  https://pqcrypto2023.umiacs.io/ 

 Publication series  
  
 Name | Lecture Notes in Computer Science 
 Publisher | Springer 
 Volume | 14154 
 ISSN (Print) | 0302-9743 
 ISSN (Electronic) | 1611-3349 

 Subject classification (UKÄ)  
 Computer Science 

 Access to Document  
 10.1007/978-3-031-40003-2 

  Fingerprint   
 Dive into the research topics of 'Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings'. Together they form a unique fingerprint.    
 Parks  Engineering  100% 
  Cryptography  Engineering  100% 
  Quantum Cryptography  Engineering  100% 
  Cryptanalysis  Computer Science  50% 
  Countermeasures  Computer Science  25% 
  Lattice-Based Cryptography  Computer Science  25% 
  Models  Engineering  20% 
  Codes  Engineering  20% 
  View full fingerprint      

 Cite this  
 APA 
  Author 
  BIBTEX 
  Harvard 
  Standard 
  RIS 
  Vancouver 
  Johansson, T.   , & Smith-Tone, D. (Eds.) (2023). Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings    . (Lecture Notes in Computer Science; Vol. 14154). Springer. https://doi.org/10.1007/978-3-031-40003-2     
   
 Johansson, Thomas (Editor)   ; Smith-Tone, Daniel (Editor). / Post-Quantum Cryptography : 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings    . Cham : Springer, 2023. 714 p. (Lecture Notes in Computer Science).   
   
 @book{b620acff95f4484d8e1d431094b1e2d8,   
 title = "Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings",   
 abstract = "This book constitutes the refereed proceedings of the 14th International Workshop on Post-Quantum Cryptography, PQCrypto 2022, held in College Park, MD, USA, in August 14–18, 2023.The 25 full papers presented in this book were carefully reviewed and selected from 51 submissions. They are categorized in the following topical sections: code-based cryptography; group-action-based cryptography; isogenye-based cryptography; lattice-based cryptography; multivariate cryptography; quantum algorithms, cryptanalysis and models; post-quantum protocols; side channel cryptanalysis and countermeasures.",   
 editor = "Thomas Johansson and Daniel Smith-Tone",   
 year = "2023",   
 doi = "10.1007/978-3-031-40003-2",   
 language = "English",   
 isbn = "9783031400025",   
 volume = "LNCS 14154",   
 series = "Lecture Notes in Computer Science",   
 publisher = "Springer",   
 address = "Germany",   
 note = "The 14th International Conference on Post-Quantum Cryptography, PQCrypto 2023 ; Conference date: 16-08-2023 Through 18-08-2023",   
 url = "https://pqcrypto2023.umiacs.io/",   
 }  

 Johansson, T   & Smith-Tone, D (eds) 2023, Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings    . Lecture Notes in Computer Science, vol. 14154, vol. LNCS 14154, Springer, Cham. https://doi.org/10.1007/978-3-031-40003-2     
   
 Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings.    / Johansson, Thomas (Editor)   ; Smith-Tone, Daniel (Editor).  
  Cham: Springer, 2023. 714 p. (Lecture Notes in Computer Science; Vol. 14154). Research output :   Book/Report ›   Conference proceeding (editor)  ›  peer-review   

 TY - BOOK  
 T1 - Post-Quantum Cryptography  
 T2 - The 14th International Conference on Post-Quantum Cryptography  
 A2 - Johansson, Thomas  
 A2 - Smith-Tone, Daniel  
 PY - 2023  
 Y1 - 2023  
 N2 - This book constitutes the refereed proceedings of the 14th International Workshop on Post-Quantum Cryptography, PQCrypto 2022, held in College Park, MD, USA, in August 14–18, 2023.The 25 full papers presented in this book were carefully reviewed and selected from 51 submissions. They are categorized in the following topical sections: code-based cryptography; group-action-based cryptography; isogenye-based cryptography; lattice-based cryptography; multivariate cryptography; quantum algorithms, cryptanalysis and models; post-quantum protocols; side channel cryptanalysis and countermeasures.  
 AB - This book constitutes the refereed proceedings of the 14th International Workshop on Post-Quantum Cryptography, PQCrypto 2022, held in College Park, MD, USA, in August 14–18, 2023.The 25 full papers presented in this book were carefully reviewed and selected from 51 submissions. They are categorized in the following topical sections: code-based cryptography; group-action-based cryptography; isogenye-based cryptography; lattice-based cryptography; multivariate cryptography; quantum algorithms, cryptanalysis and models; post-quantum protocols; side channel cryptanalysis and countermeasures.  
 U2 - 10.1007/978-3-031-40003-2  
 DO - 10.1007/978-3-031-40003-2  
 M3 - Conference proceeding (editor)  
 SN - 9783031400025  
 VL - LNCS 14154  
 T3 - Lecture Notes in Computer Science  
 BT - Post-Quantum Cryptography  
 PB - Springer  
 CY - Cham  
 Y2 - 16 August 2023 through 18 August 2023  
 ER -  

 Johansson T, (ed.)   , Smith-Tone D, (ed.). Post-Quantum Cryptography: 14th International Workshop, PQCrypto 2023, College Park, MD, USA, August 16–18, 2023, Proceedings     . Cham: Springer, 2023. 714 p. (Lecture Notes in Computer Science). doi: 10.1007/978-3-031-40003-2   

 Powered by Pure  , Scopus  & Elsevier Fingerprint Engine™   
 All content on this site: Copyright © 2024 Elsevier B.V.  or its licensors and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply  
 We use cookies to help provide and enhance our service and tailor content. By continuing you agree to the use of cookies   
   
 Log in to Pure   

 Lund University data protection policy   
 About web accessibility   

 Report vulnerability

39. DX_1 conference:
| Home | | CFP | | Dates | | Submission | | Registration | | Venue | | Program | | Awards | | Downloads | | People | | Contact | | Sponsors 

 | California & Travels | | Accommodation 

   The 34th International Workshop on Principles of Diagnosis (DX'23) will be held at the Redwood Glen Camp and Convention Center  , 1430 Wurr Road, Loma Mar, CA, USA, where the Siden Conference Center will be available to us. When registering for DX'23, you will be able to book a full package that includes registration, lodging, and meals.  
 Redwood Glen is a retreat site in the Santa Cruz mountains. There is no public transportation to Redwood Glen or anyplace nearby. One of the chairs will be at Redwood Glen from 2PM onwards on Monday. When you arrive, do not go to the front desk, but drive to the Siden conference center right away and one of us will hand you your room key. Please be aware that there are two Redwood Glen's in the Santa Cruz mountains far apart and make sure to go to the one on Wurr road.  
 Your transportation options are as follows:  
 Renting a car is the simplest way to reach the venue. Please make sure you use a navigation device or app, since paper maps are out of date. A number of roads have been washed out recently, so please consider also the instructions available from the | Redwood Glen website | and bringing a copy of their | pdf instructions | . Redwood Glen suggests to use the address 1430 Wurr Road, Loma Mar, CA 94021 when using your navigation device. 
  Most Uber/Lyft drivers will take you there. Some will reject you because they are unlikely to get a return ride nearby, so you'll get rejected a couple of times. Costs vary by the time of the day and will be in the range of $75-$100 from San Francisco airport. The trip will take an hour+ (depeding on the route). 
  We will collect arrival times and try to coordinate rides for people who need them. So we will be asking people to let us know who can help and who needs help. 
  Worst case, we will arrange for someone to pick you up. Johan will distribute his cell phone number so that you will be able to contact him. Please note that cellphone coverage is spotty in the mountains, so please leave a voicemail and he will check periodically. 
  Very worst case, make your way to Johan's place (the address will be distributed). Someone will be there. Please note that Uber/Lyft will always work there. It is much closer to Redwood Glen. 
  Returning to the airport is even more difficult because Uber/Lyft typically won't come to the mountains for a pickup. Hopefully there will be enough people with cars.

40. REFSQ_0 conference:
REFSQ 2023   Barcelona, Catalunya, Spain, April 17-20, 2023   

 Toggle navigation        
 Programme | REFSQ Program 
  Your Program 
  Keynotes 
   Mon 17 Apr 
  Tue 18 Apr 
  Wed 19 Apr 
  Thu 20 Apr 
  Tracks | REFSQ 2023 
  Research Papers 
  Workshops 
  Posters and Tools 
  Doctoral Symposium 
  Industry Track 
  Journal Early Feedback 
  Registration 
  Steering Committee Meeting 
  Attending | Venue: Barcelona, Catalunya, Spain 
  Registration: REFSQ 2023 
  Accomodation 
  Event format 
  Info | Sponsor REFSQ 
  REFSQ Governance | Welcome 
  REFSQ 2023 - Steering Committee 
  REFSQ Charter 
  Organization | REFSQ 2023 Committees 
  Organizing Committee 
  Track Committees 
  Research Papers 
  Workshops 
  Posters and Tools | Posters and Tools Co-Chairs 
  Programme Committee 
  Doctoral Symposium | Doctoral Symposium Co-Chairs 
  Panel of Experts 
  Industry Track 
  Journal Early Feedback 
  Contributors 
  People Index 
  Search 
  Series | Series 
   REFSQ 2025 
  REFSQ 2024 
  REFSQ 2023 
  REFSQ 2022 
  REFSQ 2021 
  REFSQ 2020 
  REFSQ 2019 
  REFSQ 2018 
  REFSQ 2017 
  REFSQ 2016 
  REFSQ 2015 
  REFSQ 2014 
  REFSQ 2013 
  REFSQ 2012 
  REFSQ 2011 
  REFSQ 2010 
  REFSQ 2009 
  REFSQ 2008 
  REFSQ 2007 
  REFSQ 2006 
  REFSQ 2005 
  REFSQ 2004 
  REFSQ 2001 
  REFSQ 2000 
  REFSQ 1999 
  REFSQ 1998 
  REFSQ 1997 
  Sign in 
  Sign up 

 Proceedings available  
 Conference proceedings of Requirements Engineering: Foundation for Software Quality   
  29th International Working Conference, REFSQ 2023 Barcelona, Spain, April 17–20, 2023   
  You can find them here  . All registered participants can access them free of charge for a limited time period (from April 13 until May 14, 2023).  
 REFSQ 2023: Joint Proceedings of Workshops, Doctoral Symposium, Posters & Tools Track, and Journal Early Feedback Track   
  Co-located with the 29th International Working Conference, REFSQ 2023 Barcelona, Spain, April 17–20, 2023   
  Available here  .  

 REFSQ 2023  
   
 Welcome to the website of the REFSQ 2023  conference!  
 The 29th International Working Conference on Requirement Engineering: Foundation for Software Quality  will take place from the 17th to 20th of April 2023 in Barcelona, Catalunya, Spain ( live, in-person  ).  
 Special Theme: Human Values in Requirements Engineering   
 RE is at the boundary of humans and technology, and values play a crucial role in the interplay between developers, users and systems. When developing technology, we get to be cognizant of how our values inform our designs, because we unconsciously embed them into our systems. In addition, we need to carefully consider possible conflicts between human values and business value. The theme of this year thus aims to foster discussion around the following questions:  
 How do we take care of | human values | in RE? 
  How do we ensure that the systems we | design | incorporate the values we want them to stand for? 
  How do we validate and | measure | values? 
  How do we make sure that systems serve the human as opposed to having the human adapt to them? 
  How much do | developer | habits and characteristics influence their designs? 
  What is the | interplay | between developer and stakeholders’ values? 
  What is the interplay between human values and | business value | ? 
  We are working hard to fill the website with all related information. Please check back soon!  

 Call for Papers  
   
 The Call for Papers  document for REFSQ 2023 is available here  . Take a look at it to learn about the most relevant details of the conference, including submissions, important dates, organization, and more.  
 The best papers will be invited to submit an extended version of their contribution to a Special Issue of the Requirements Engineering Journal    

 About REFSQ  
   
 The REFSQ working conference is the leading European conference series on requirements engineering. It is the goal of REFSQ to foster the creation of a strong European RE community across industry and academia. Since 1994, Requirements Engineering continued to be a dominant factor influencing the quality of software, systems and services. REFSQ seeks contributions which report on novel ideas and techniques that enhance the quality of RE’s products and processes, as well as reflections on current research. REFSQ has a long tradition of being a highly structured and interactive forum. In addition to the high-quality scientific program, REFSQ also features a highly interactive Industry Track, Workshops, Posters and Tool demos, Doctoral Symposium and a Journal Early Feedback Track.  

  Featured News    

 Proceedings available Tue 11 Apr 2023 

   Posts   
   
 Twitter    
  REFSQ 2023 Tracks   
   
 Research Papers  | Workshops  | Posters and Tools  | Doctoral Symposium  | Industry Track  | Journal Early Feedback  | Registration  | Steering Committee Meeting    

 Supporters   

 Organized by 
 Organized by 
 Organized by 
 Sponsored by 
 Supported by 
 Supported by 

 x  Sat 30 Nov 07:39    

  REFSQ 2023   
  contact form    
  using conf.researchr.org  ( v1.67.1  )  
   Support page    
     
 Tracks  
 Research Papers   
  Workshops   
  Posters and Tools   
  Doctoral Symposium   
  Industry Track   
  Journal Early Feedback   
  Registration   
  Steering Committee Meeting    

 Attending  
 Venue: Barcelona, Catalunya, Spain   
  Registration: REFSQ 2023   
  Accomodation   
  Event format    
 Organized by

41. PQCrypto_2 conference:
You need to enable JavaScript to run this app.

42. IWSEC_0 conference:
Co-Organized by       

  IWSEC  
  2023  
 The 18th International Workshop on Security  
  August 29 (Tue) -- August 31 (Thu), 2023  
  Institute of Information Security, Yokohama, Japan and ONLINE  
   
 Overview  
 The 18th International Workshop on Security (IWSEC 2023) will be held hybrid at Institute of Information Security, Yokohama  , Japan and ONLINE  between August 29--31, 2023. IWSEC 2023 is co-organized by ISEC in ESS of IEICE (Technical Committee on Information Security in Engineering Sciences Society of the Institute of Electronics, Information and Communication Engineers) and CSEC of IPSJ (Special Interest Group on Computer Security of Information Processing Society of Japan).  
 Original papers on the research and development of various security topics, as well as case studies and implementation experiences, are solicited for submission to IWSEC 2023.  
 IWSEC is an annual international workshop in Japan, co-organized by ISEC in ESS of IEICE and CSEC of IPSJ.  
 Previous IWSEC have been held in Kyoto (2006), Nara (2007), Kagawa (2008), Toyama (2009), Kobe (2010), Tokyo (2011), Fukuoka (2012), Naha (2013), Hirosaki (2014), Nara (2015), Tokyo (2016), Hiroshima (2017), Sendai (2018), Tokyo (2019), Fukui/Online (2020), Tokyo/Online (2021), and Tokyo/Online (2022).  
 If you have any questions about the workshop, please ask us via e-mail  . Please do NOT contact the venue (Institute of Information Security) directly.  

 Proceedings  
 The accepted papers have been published by Springer, in the Lecture Notes in Computer Science (LNCS) Series  .  

 Awards  
 🎉 Best Paper Award 🎉  
 aPlonK : Aggregated PlonK from Multi-Polynomial Commitment Schemes | Miguel Ambrona, Marc Beunardeau, Anne-Laure Schmitt and Raphael Toledo 
  🎉 Best Student Paper Award 🎉  
 A New Security Analysis Against MAYO and QR-UOV Using Rectangular MinRank Attack | Hiroki Furue and Yasuhiko Ikematsu 
  🎉 Best Poster Award 🎉  
 KOTO Crypto: Educational cryptography with the Koto | Mine Arai, Naoto Yanai and Goichiro Hanaoka 
  Security Analysis on KpqC Round 1 Lattice-based Algorithms Using Lattice Estimator | Hyuna Noh, Eunmin Lee, Minju Lee, Suhri Kim and Joohee Lee 

 What's New  
 August 31st, 2023   
 Awards  
 Best Poster Award has been announced. Congratulations!!  

 August 30th, 2023   
 Timetable  
 Timetable  on Day 2 has been changed to make room for the presentation skipped in Track B Session. Please be careful that the lunch break has been shortened.  

 August 29th, 2023   
 Awards  
 Best Paper Award and Best Student Paper Award have been announced. Congratulations!!  

 August 29th, 2023   
 Program  
 Poster session and Track B session (System and Hardware Security) times have been changed. Please be careful not to make a mistake.  

 August 28th, 2023   
 Program  
 Session chairs have been added to the program  .  

 August 25th, 2023   
 Proceedings  
 IWSEC 2023 Proceedings are available here  (participants only). We have sent the password to the email you registered.  

 August 20th, 2023   
 Guidelines for Poster Presenters  
 Guidelines for Poster Presenters  have been updated.  

 August 17th, 2023   
 Accepted Posters  
 Accepted Posters  have been announced.  

 August 17th, 2023   
 Guidelines  
 The guidelines page  has been prepared.  

 August 12th, 2023   
 Keynote  
 Details of Keynote III  have been announced.  

 August 10th, 2023   
 Program  
 Speakers of SCIS/CSS Session  have been confirmed.  

 August 9th, 2023   
 Keynote  
 Details of Keynote II  have been announced.  

 August 7th, 2023   
 Welcome Reception  
 Information about Welcome Reception  has been announced.  

 August 1st, 2023   
 Banquet & Excursion  
 Information about Banquet  and Excursion  has been announced.  

 July 26th, 2023   
 Program  
 Time Table  has been published.  

 July 20th, 2023   
 Keynote  
 Details of Keynote I  have been announced.  

 July 17th, 2023   
 Call For Posters  
 Call for posters  for IWSEC2023 is now available.  

 June 12th, 2023   
 Accepted Papers  
 Accepted Papers  have been announced.  

 May 30th, 2023   
 Registration  
 The registration page  has been launched.  

 May 14th, 2023   
 iPWS Cup (Data Anonymization Competition)  
 iPWS Cup  website is launched.  

 May 11th, 2023   
 Keynote  
 Keynote III speaker  has been confirmed.  

 May 2nd, 2023   
 Keynote  
 Keynote II speaker  has been confirmed.  

 April 24th, 2023   
 Keynote  
 Keynote I speaker  has been confirmed.  

 March 25th, 2023   
 Extension of deadline for paper submission  
 Deadline for paper submission  has been extended.  

 March 18th, 2023   
 Program Committee  
 Program Committee  has been updated.  

 March 3rd, 2023   
 Submission  
 Paper submission  has been opened.  

 February 8th, 2023   
 Instructions for Authors and Important Dates  
 The instructions for authors are available in Call for Papers  .  
 Important Dates  has been fixed.  

 December 23th, 2022   
 IWSEC 2023 Website  
 IWSEC 2023 website is launched.

43. REFSQ_1 conference:
REFSQ 2025   Mon 7 - Thu 10 April 2025 Barcelona, Spain    

 Toggle navigation        
 Series 
  REFSQ 2025 
  All Editions | REFSQ 2025 
  REFSQ 2024 
  REFSQ 2023 
  REFSQ 2022 
  REFSQ 2021 
  REFSQ 2020 
  REFSQ 2019 
  REFSQ 2018 
  REFSQ 2017 
  REFSQ 2016 
  REFSQ 2015 
  REFSQ 2014 
  REFSQ 2013 
  REFSQ 2012 
  REFSQ 2011 
  REFSQ 2010 
  REFSQ 2009 
  REFSQ 2008 
  REFSQ 2007 
  REFSQ 2006 
  REFSQ 2005 
  REFSQ 2004 
  REFSQ 2001 
  REFSQ 2000 
  REFSQ 1999 
  REFSQ 1998 
  REFSQ 1997 
  Sign in 
  Sign up 

 Requirements Engineering: Foundation for Software Quality (REFSQ)  
   
 The REFSQ working conference is the leading European conference series on requirements engineering. It is the goal of REFSQ to foster the creation of a strong European RE community across industry and academia. Since 1994, Requirements Engineering continued to be a dominant factor influencing the quality of software, systems and services. REFSQ seeks contributions which report on novel ideas and techniques that enhance the quality of RE’s products and processes, as well as reflections on current research. REFSQ has a long tradition of being a highly structured and interactive forum.  
 All Editions   

 Mon 7 - Thu 10 April 2025 Barcelona, Spain  REFSQ 2025   
 Welcome to the website of the Requirements Engineering: Foundation for Software Quality (REFSQ) 2025 conference. The 31st International Working Conference on Requirement Engineering: Foundation for Software Quality will take place from the 7th to 10th of April 2025 in Barcelona, Catalunya, Spain (live, in-person). 
 Mon 8 - Thu 11 April 2024 Winterthur, Switzerland  REFSQ 2024   
 Proceedings available Conference proceedings of Requirements Engineering: Foundation for Software Quality 30th International Working Conference, REFSQ 2024, Winterthur, Switzerland, April 8–11, 2024, You can find them on SpringerLink. All registered participants can access them free of charge for a limited time period (until May 15, 2024). REFSQ Joint conference proceedings of all co-located events are ... 
 Mon 17 - Thu 20 April 2023 Barcelona, Spain  REFSQ 2023   
 Proceedings available Conference proceedings of Requirements Engineering: Foundation for Software Quality 29th International Working Conference, REFSQ 2023 Barcelona, Spain, April 17–20, 2023 You can find them here. All registered participants can access them free of charge for a limited time period (from April 13 until May 14, 2023). REFSQ 2023: Joint Proceedings of Workshops, Doctoral Symposium, Posters &amp ... 
 Mon 21 - Thu 24 March 2022 Aston, Birmingham, United Kingdom  REFSQ 2022   
 Welcome to the website of the REFSQ 2022 conference! The 28th International Working Conference on Requirement Engineering: Foundation for Software Quality will take place from the 21st to 24th of March 2022. It is our intention to hold a live, in-person event in Aston, Birmingham, UK. We are aware that health recommendations may change at any moment, and the REFSQ 2022 organization is committed to promptly adapt ... 
 Mon 12 - Thu 15 April 2021 Germany  REFSQ 2021   
 The 27th International Working Conference on Requirement Engineering: Foundation for Software Quality will take place from the 12th to 15th of April 2021 as a virtual event! 
 June 2020, Pisa, Italy  REFSQ 2020  
 More information on this edition at https://refsq.org/2020 
 March 2019, Essen, Germany  REFSQ 2019  
 More information on this edition at https://refsq.org/2019 
 March 2018, Utrecht, The Netherlands  REFSQ 2018  
 More information on this edition at https://refsq.org/2018 
 February 2017, Essen, Germany  REFSQ 2017  
 More information on this edition at https://refsq.org/2017 
 March 2016, Gothenburg, Sweden  REFSQ 2016  
 More information on this edition at https://refsq.org/2016 
 March 2015, Essen, Germany  REFSQ 2015  
 More information on this edition at https://refsq.org/2015 
 April 2014, Essen, Germany  REFSQ 2014  
 More information on this edition at https://refsq.org/2014 
 April 2013, Essen, Germany  REFSQ 2013  
 More information on this edition at https://refsq.org/2013 
 March 2012, Essen, Germany  REFSQ 2012  
 More information on this edition at https://refsq.org/2012 
 March 2011, Essen, Germany  REFSQ 2011  
 More information on this edition at https://refsq.org/2011 
 June 2010, Essen, Germany  REFSQ 2010  
 More information on this edition at https://refsq.org/2010 
 June 2009, Amsterdam, The Netherlands  REFSQ 2009  
 More information on this edition at https://refsq.org/2009 
 June 2008, Montpellier, France  REFSQ 2008  
 More information on this edition at https://refsq.org/2008 
 June 2007, Trondheim, Norway  REFSQ 2007  
 More information on this edition at https://refsq.org/2007 
 June 2006, Luxembourg, Grand-Duchy of Luxembourg  REFSQ 2006  
 More information on this edition at https://refsq.org/2006 
 June 2005, Porto, Portugal  REFSQ 2005  
 More information on this edition at https://refsq.org/2005 
 June 2004, Riga, Latvia  REFSQ 2004  
 More information on this edition at https://refsq.org/2004 
 June 2001, Interlaken, Switzerland  REFSQ 2001  
 More information on this edition at https://refsq.org/2001 
 June 2000, Stockholm, Sweden  REFSQ 2000  
 More information on this edition at https://refsq.org/2000 
 June 1999, Heidelberg, Germany  REFSQ 1999  
 More information on this edition at https://refsq.org/1999 
 June 1998, Pisa, Italy  REFSQ 1998  
 More information on this edition at https://refsq.org/1998 
 June 1997, Barcelona, Spain  REFSQ 1997  
 More information on this edition at https://refsq.org/1997 

 Organizing Committee (REFSQ 2025)   
   
 Anne Hess PC Chair    
 Technical University of Applied Sciences Würzburg-Schweinfurt   
 Germany 
  Angelo Susi PC Chair    
 Fondazione Bruno Kessler   
 Italy 
  Carles Farré Local Organization Co-Chair    
 Universitat Politècnica de Catalunya 
  Quim Motger Local Organization Co-Chair    
 Universitat Politècnica de Catalunya   
 Spain 
  Martina Beck Industry Track Co-Chair    
 MaibornWolff   
 Germany 
  Markus Borg Industry Track Co-Chair    
 CodeScene   
 Sweden 
  Eduard C. Groen Workshops Co-Chair    
 Fraunhofer IESE   
 Germany 
  Marcela Ruiz Workshops Co-Chair    
 Zurich University of Applied Sciences (ZHAW)   
 Switzerland 
  Elda Paja Doctoral Symposium Co-Chair    
 IT University of Copenhagen   
 Denmark 
  Andreas Vogelsang Doctoral Symposium Co-Chair    
 University of Cologne   
 Germany 
  Xavier Franch Background Organization Chair    
 Universitat Politècnica de Catalunya   
 Spain 
  Andrea Herrmann Proceedings Chair    
 Herrmann & Ehrlich   
 Germany 
  Carme Quer Finance Chair    
 Universitat Politècnica de Catalunya   
 Spain 
  Sallam Abualhaija Social Media Co-Chair    
 University of Luxembourg   
 Luxembourg 
  Fatma Başak Aydemir Education and Training co-chair, Education and Training co-chair    
 Utrecht University   
 Netherlands 
  Paola Spoletini Education and Training co-chair, Education and Training co-chair    
 Kennesaw State University   
 United States 
  Santiago del Rey Social Media Co-Chair    
 Universitat Politècnica de Catalunya (UPC)   
 Spain 
  Julian Frattini Open Science Co-Chair    
 Blekinge Institute of Technology   
 Sweden 
  Oliver Karras Open Science Co-Chair    
 TIB - Leibniz Information Centre for Science and Technology   
 Germany 
  Sylwia Kopczyńska Posters and Tools Co-Chair, Posters and Tools Co-Chair    
 Poznan University of Technology   
 Poland 
  Laura Semini Posters and Tools Co-Chair, Posters and Tools Co-Chair    
 Università di Pisa - Dipartimento di Informatica   
 Italy 
  Chetan Arora Journal Early Feedback Co-Chair    
 Monash University   
 Australia 
  Kelly Blincoe Journal Early Feedback Co-Chair    
 University of Auckland   
 New Zealand 
  Max Tiessler Web Chair    
 Universitat Politècnica de Catalunya   
 Austria 

 x  Sat 30 Nov 07:40    

 using conf.researchr.org  ( v1.67.1  )  
   Support page    

 Sign Up

44. DX_2 conference:
Skip to main menu 
  Skip to main content 
  Knowledge base: Warsaw University of Technology    
   
 Settings and your account  
 default font size  A | default font size 
  bigger font size  A | bigger font size 
  big font size  A | big font size 
  High contrast    tonality | High contrast  Normal contrast 
  Change language to: Polish  Zmień język na: Polski | Change language to polish 
  Log in 

   Main menu  
 menu  menu_open  Main menu   Start    Start 
  About arrow_drop_down | About the site 
  Openness in Science 
  FAQ 
  Units and people arrow_drop_down | Researchers 
  Research Units 
  Experts 
  Research teams 
  Scientific clubs 
  Research Outputs arrow_drop_down | Publications 
  Works 
  Research data 
  Multimedia 
  PhD Theses 
  Diplomas 
  Research potential arrow_drop_down | Patents 
  Projects 
  More arrow_drop_down | Cooperation 
  Achievements 
  Activities 
  Analysis / Reports 
  Media / Press 
  Journals 
  Conferences 

 You are here:  
 Start 
   Research Outputs 
   Publications 
  34rd International Workshop on Principle of Diagnosis – DX 2023 

 Back   34rd International Workshop on Principle of Diagnosis – DX 2023  
 Editors:  
  
 Abstract  
 HAL  is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.   
   
 Record ID   WUT5966c5f930814e6fa3301e79962570cf  Book type   Other  Corporate author   Graz University of Technology (TU GRAZ)    Publisher name (outside publisher list)   PARC and the Silicon Austria Labs (SAL)   
  Issue year   2023   
  Conference   34th International Workshop on Principles of Diagnosis (DX'2023), 2023, 11-09-2023 - 14-09-2024, Loma Mar , Stany Zjednoczone    Keywords in English   Benchmark Application, diagnosis, structural model, water network  Abstract in original language   L’archive ouverte pluridisciplinaire HAL  , est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.  URL   https://hal.science/ Opening in a new tab    Language   eng (en) English  Score (nominal)   0   Related publications (1)     

 Cite      Cite  

 Refresh page       
 Refresh page   

 RDF   

     Uniform Resource Identifier  https://repo.pw.edu.pl/info/book/WUT5966c5f930814e6fa3301e79962570cf/     
 URN  urn:pw-repo:WUT5966c5f930814e6fa3301e79962570cf      
   * presented citation count is obtained through Internet information analysis, and it is close to the number calculated by the Publish or Perish Opening in a new tab   system.   
  Back    

 Accessibility statement 
  API 
  5* Open Data 
  Site map 
  Help 
  Contact 

 © 1998-2024 Politechnika Warszawska,  
  Pl. Politechniki 1, 00-661 Warszawa  
   
 © 2024 Powered by Omega-PSIR     

     Confirmation    
  Are you sure?    
  Yes    Cancel     
   
 Report incorrect data on this page      

 clipboard   
     
 Refresh clipboard       
 Refresh clipboard

45. IWSEC_1 conference:
Program  
 August 28th (Day 0), 2023  
  
 Time (JST) | Session | Content 
 15:00–18:00 | iPWS Cup Open to all registered participants in IWSEC 2023 | Contest Summary  
 Hiroaki Kikuchi and Makoto Iguchi  
   
 Talks by excellent teams  
 Speakers will be anounced on | iPWS Cup 2023 website | . 
    
 Award ceremony 

 August 29th (Day 1), 2023  
  
 Time (JST) | Session | Content 
 09:00–09:30 | Registration |  
 09:30–09:40 | Opening Remarks |  
 09:40–10:40 | Keynote I  
 Session Chair: Junji Shikata (Yokohama National University, Japan) | Standardization of Ascon Family  
 Dr. Meltem Sonmez Turan (National Institute of Standards and Technology) 
 10:40–11:00 | Break |  
 11:00–12:00 | Track A Session:  
  Card Based Cryptography  
 Session Chair: Kazumasa Shinagawa (Ibaraki University, Japan) | Efficient Card-Based Millionaires' Protocols via Non-Binary Input Encoding  
 Koji Nuida  
   
 Check Alternating Patterns: A Physical Zero-Knowledge Proof for Moon-or-Sun  
 Samuel Hand, Alexander Koch, Pascal Lafourcade, Daiki Miyahara and Léo Robert 
 12:00–14:00 | Lunch Break |  
 14:00–15:00 | Keynote II  
 Session Chair: Hiroki Kuzuno (Kobe University, Japan) | Provable Security for the Real World  
 Prof. Gernot Heiser (University of New South Wales) (online) 
 15:00–15:20 | Break |  
 15:20–16:20 | Track B Session:  
  System and Hardware Security  
 Session Chair: Masaya Sato (Okayama Prefectural University) | Power analysis pushed too far: breaking Android-based isolation with fuel gauges  
 Vincent Giraud and David Naccache  
   
 Reliability of Ring Oscillator PUFs with Reduced Helper Data  
 Julien Béguinot, Jean-Luc Danger, Olivier Rioul, Sylvain Guilley, Wei Cheng and Ville-Oskari Yli-Mayry 
 16:20–18:00 | Poster Session |  
 19:00–21:00 | Welcome Reception |  

 August 30th (Day 2), 2023  
  
 Time (JST) | Session | Content 
 09:00–10:00 | Keynote III  
 Session Chair: Koji Chida (Gunma University, Japan) | PETs: Promise, expectation, hope, and reality  
 Dr. Kris Shrishak (Irish Council for Civil Liberties) 
 10:00–10:30 | Break / Poster Session |  
 10:30–12:00 | Track A Session:  
  Public Key Cryptography 1  
 Session Chair: Serge Vaudenay (EPFL, Switzerland) | A New Security Analysis Against MAYO and QR-UOV Using Rectangular MinRank Attack  
 Hiroki Furue and Yasuhiko Ikematsu  
   
 Improved Hybrid Attack via Error-Splitting Method for Finding Quinary Short Lattice Vectors  
 Haiming Zhu, Shoichi Kamada, Momonari Kudo and Tsuyoshi Takagi  
   
 Total Break of a Public Key Cryptosystem Based on a Group of Permutation Polynomials  
 Max Cartor, Ryann Cartor, Mark Lewis and Daniel Smith-Tone 
 12:00–13:20 | Lunch Break |  
 13:20–13:50 | Track B Session:  
  System and Hardware Security  
 Session Chair: Hiroki Kuzuno (Kobe University, Japan) | The Good, the Bad, and the Binary: An LSTM-Based Method for Section Boundary Detection in Firmware Analysis  
 Riccardo Remigio, Alessandro Bertani, Mario Polino, Michele Carminati and Stefano Zanero 
 13:50–14:00 | Break |  
 14:00–15:00 | Track A Session:  
  Public Key Cryptography 2  
 Session Chair: Atsushi Fujioka (Kanagawa University, Japan) | Extractable Witness Encryption for the Homogeneous Linear Equation problem  
 Bénédikt Tran and Serge Vaudenay  
   
 Making Classical (Threshold) Signatures Post-Quantum for Single Use on a Public Ledger  
 Laurane Marco, Abdullah Talayhan and Serge Vaudenay 
 15:00–15:20 | Break |  
 15:20–16:20 | Track A Session:  
  Zero Knowledge Proofs  
 Session Chair: Noboru Kunihiro (University of Tsukuba, Japan) | aPlonK : Aggregated PlonK from Multi-Polynomial Commitment Schemes  
 Miguel Ambrona, Marc Beunardeau, Anne-Laure Schmitt and Raphael Toledo  
   
 TENET : Sublogarithmic Proof, Sublinear Verifier Inner Product Argument without a Trusted Setup  
 Hyeonbum Lee and Jae Hong Seo 
 16:20–16:40 | Break |  
 16:40–18:25 | SCIS/CSS Session  
 Session Chair: Kazuhiko Minematsu (NEC, Japan and Yokohama National University, Japan) | Embedding Input Data Information in AI's Inference Results  
 Kazuki Iwahana (NTT Social Informatics Laboratories)  
   
 Limitations of Ring Oscillator-based Detection Methods against Laser Fault Injection Attacks on FPGA  
 Shungo Hayashi (Yokohama National University / National Institute of Advanced Industrial Science and Technology)  
   
 A Security Analysis and Efficient Generic Construction of CCA-Secure Updatable Public Key Encryption  
 Kyoichi Asano (The University of Electro-Communications)  
   
 Homomorphic Evaluation for Indeterminate Equations Public-key Cryptosystem (Giophantus+)  
 Akira Nakashima (Secure System Platform Research Laboratories, NEC Corporation)  
   
 Browser Permission Mechanisms Demystified  
 Kazuki Nomoto (Waseda University / Deloitte Tohmatsu Cyber LLC)  
   
 Qubit-less Shor's Algorithm for Binary ECDLP  
 Ren Taguchi (The University of Tokyo)  
   
 Study on Linkable Ring Signature with Unconditional Anonymity  
 Kazuki Yamamura (NTT Social Informatics Laboratories) 
 19:00–21:00 | Banquet |  

 August 31st (Day 3), 2023  
  
 Time (JST) | Session | Content 
 09:00–10:00 | Track A Session:  
  Symmetric Key Cryptography  
 Session Chair: Shoichi Hirose (University of Fukui, Japan) | Improved Boomerang Attacks on Deoxys-BC  
 Jiahao Zhao, Ling Song, Qianqian Yang, Nana Zhang and Lei Hu  
   
 PMACrx: a vector-input MAC for high-dimensional vectors with BBB security  
 Isamu Furuya, Hayato Kasahara, Akiko Inoue, Kazuhiko Minematsu and Tetsu Iwata 
 10:00–10:20 | Break |  
 10:20–11:35 | SCIS/CSS Session  
 Session Chair: Naoto Yanai (Osaka University, Japan) | xltrace: Cross-architecture Tracing for Library Functions  
 Shu Akabane (Kanagawa Institute of Technology)  
   
 Card-based Cryptographic Protocols for Private Set Intersection and Union  
 Tomoki Ono (The University of Electro-Communications)  
   
 Post-Quantum Authenticated Key Exchange Resilient to Ephemeral Key Leakage in the Bounded-Retrieval Model for IoT  
 Kazuki Yoneyama (Ibaraki University)  
   
 The Catcher in the Eye: Recognizing Users by their Blinks  
 Ryo Iijima (National Institute of Advanced Industrial Science and Technology (AIST) / Waseda University)  
   
 On Rényi Differential Privacy in Statistics-Based Synthetic Data Generation  
 Takayuki Miura (NTT Social Informatics Laboratories) 
 11:35–11:45 | Closing Remarks |  
 12:30–18:00 | Excursion |  

 Accepted Papers  
 Track A  
 Efficient Card-Based Millionaires' Protocols via Non-Binary Input Encoding | Koji Nuida 
  Extractable Witness Encryption for the Homogeneous Linear Equation problem | Bénédikt Tran and Serge Vaudenay 
  Making Classical (Threshold) Signatures Post-Quantum for Single Use on a Public Ledger | Laurane Marco, Abdullah Talayhan and Serge Vaudenay 
  Improved Boomerang Attacks on Deoxys-BC | Jiahao Zhao, Ling Song, Qianqian Yang, Nana Zhang and Lei Hu 
  TENET : Sublogarithmic Proof, Sublinear Verifier Inner Product Argument without a Trusted Setup | Hyeonbum Lee and Jae Hong Seo 
  PMACrx: a vector-input MAC for high-dimensional vectors with BBB security | Isamu Furuya, Hayato Kasahara, Akiko Inoue, Kazuhiko Minematsu and Tetsu Iwata 
  Improved Hybrid Attack via Error-Splitting Method for Finding Quinary Short Lattice Vectors | Haiming Zhu, Shoichi Kamada, Momonari Kudo and Tsuyoshi Takagi 
  A New Security Analysis Against MAYO and QR-UOV Using Rectangular MinRank Attack | Hiroki Furue and Yasuhiko Ikematsu 
  aPlonK : Aggregated PlonK from Multi-Polynomial Commitment Schemes | Miguel Ambrona, Marc Beunardeau, Anne-Laure Schmitt and Raphael Toledo 
  Check Alternating Patterns: A Physical Zero-Knowledge Proof for Moon-or-Sun | Samuel Hand, Alexander Koch, Pascal Lafourcade, Daiki Miyahara and Léo Robert 
  Total Break of a Public Key Cryptosystem Based on a Group of Permutation Polynomials | Max Cartor, Ryann Cartor, Mark Lewis and Daniel Smith-Tone 
  Track B  
 Power analysis pushed too far: breaking Android-based isolation with fuel gauges | Vincent Giraud and David Naccache 
  Reliability of Ring Oscillator PUFs with Reduced Helper Data | Julien Béguinot, Jean-Luc Danger, Olivier Rioul, Sylvain Guilley, Wei Cheng and Ville-Oskari Yli-Mayry 
  The Good, the Bad, and the Binary: An LSTM-Based Method for Section Boundary Detection in Firmware Analysis | Riccardo Remigio, Alessandro Bertani, Mario Polino, Michele Carminati and Stefano Zanero 

 Accepted Posters  
 Proposal of the IoT malware-disabling method | Kazuki Takada, Kazuki Iwamoto and Yuki Ishida 
  A Combinatorial Approach to IoT Data Security | Bimal Kumar Roy, Kouichi Sakurai, Anandarup Roy and Suprita Talnikar 
  KOTO Crypto: Educational cryptography with the Koto | Mine Arai, Naoto Yanai and Goichiro Hanaoka 
  Credit-Aware Voting for Decentralized Federated Learning | Zhuotao Lian, Kouichi Sakurai and Chunhua Su 
  Security Analysis on KpqC Round 1 Lattice-based Algorithms Using Lattice Estimator | Hyuna Noh, Eunmin Lee, Minju Lee, Suhri Kim and Joohee Lee 
  Research on False Base Station Attack Detection Technique Based on Behavior Rule Specification | Hoonyong Park, Yeongshin Park, Jiyoon Kim and Ilsun You 
  Multivariate Gaussian Mechanism for Approximate Differential Privacy | Debolina Ghatak and Kouichi Sakurai 
  Confidentiality Assurance Using Proof-Carrying Code for TEE-Based Secure Computation: Experiment on TEE | Misato Nakabayashi and Tetsuya Okuda 
  Member Removal Mechanism for TreeKEM to Deal with Offline Members | Zihao Zheng and Kazue Sako 
  Leveraging Trusted Execution Environment and Distributed Ledger Technology for Data Usage Control: Towards Confidential Data Integration | Shota Tokuda, Shohei Kakei, Yoshiaki Shiraishi and Shoichi Saito

46. SCOPES_0 conference:
Conference Partner   Home 
  Conferences 
  Journals 
  Proofreading 
  Login 

  中文  |  English  |  Español  |  日本語     

 Conference Partner  » Conferences  » SCOPES    
  Conference Information   
   
 SCOPES 2017: International Workshop on Software and Compilers for Embedded Systems  
 http://www.scopesconf.org/scopes-17/   
   
 Submission Date: | 2017-03-03 Extended 
 Notification Date: | 2017-04-07 
 Conference Date: | 2017-06-12 
 Location: | Sankt Goar, Germany 
 Years: | 20 
  
 CORE: a  Viewed: 7378  Tracked: 1  Attend: 0    

  Call For Papers   
   
 A next edition of the workshop on Software and Compilers for Embedded Systems (SCOPES) will be organized in 2017. The workshop will feature a combination of research papers and research presentations (details see below). The papers and presentation abstracts will also be published in the ACM digital library. The workshop is held in cooperation with ACM SIGBED and EDAA. AIM AND SCOPE The influence of embedded systems is constantly growing. Increasingly powerful and versatile devices are developed and put on the market at a fast pace. Their functionality and number of features is increasing, and so are the constraints on the systems concerning size, performance, energy dissipation and timing predictability. To meet all these constraints, multi-processor systems on a chip (MPSoCs) are becoming popular in embedded systems. In order to meet the performance and energy constraints of embedded applications, heterogeneous architectures incorporating functional units optimized for specific functions are commonly employed. This technological trend has dramatic consequences on the parallelization, mapping, compiler and design technology used to develop these systems. The SCOPES workshop focuses on the software generation process for modern embedded systems. Topics of interest include all aspects of the compilation and mapping process of embedded single and multi-processor systems. This includes (but is not limited to): models of computation and programming languages; performance analysis techniques for models of computation; automatic code parallelization techniques; mapping and scheduling techniques for embedded multi-processor systems; code generation techniques for embedded single- and multi-processor architectures; design-space exploration techniques for use in the HW/SW codesign process; techniques to exploit the dynamic behavior in embedded applications; interactions between operating systems and compilers; techniques for compiler aided profiling, measurement, debugging and validation of embedded software. WORKSHOP STRUCTURE The workshop structure (presentations followed by intensive discussions) allows for an interactive atmosphere in which industrial and academic representatives can exchange new ideas and trends in the area MPSoC mapping and code generation.  Last updated by Dou Sun  in 2017-02-26   

  Related Conferences   

 CCF | CORE | QUALIS | Short | Full Name | Submission | Notification | Conference 
 ICAR'' | International Conference on Availability and Reliability | 2017-11-20 | 2017-12-12 | 2017-12-17 
 ICCMIT | International Conference on Communication, Management and Information Technology | 2018-12-31 | 2019-01-15 | 2019-03-26 
 b3 | AISC | International Conference on Artificial Intelligence and Symbolic Computation | 2010-04-19 | 2010-07-05 
 CCNET | International Conference on Computer Networks & Communications | 2023-02-11 | 2023-02-18 | 2023-02-25 
 ArIT | International Conference on Advances in Artificial Intelligence Techniques | 2023-06-03 | 2023-06-12 | 2023-06-17 
 CIS' | International Conference on Cybernetics and Intelligent Systems | 2015-01-31 | 2015-03-31 | 2015-07-15 
 IEEE CVMI | IEEE International Conference on Computer Vision and Machine Intelligence | 2024-06-15 | 2024-07-30 | 2024-10-19 
 Mobility IoT | EAI International Conference on Mobility, IoT and Smart Cities | 2021-07-01 | 2021-09-01 | 2021-11-24 
 CoSIT' | International Conference on Computer Science and Information Technology | 2023-05-06 | 2023-05-17 | 2023-05-20 
 GridCom | International Conference on Grid Computing | 2022-11-12 | 2022-11-15 | 2022-11-26 
  
 2040  2489  590  3885  3836  1585  4808  3760  2991  1434    

 Short | Full Name | Submission | Conference 
 ICAR'' | International Conference on Availability and Reliability | 2017-11-20 | 2017-12-17 
 ICCMIT | International Conference on Communication, Management and Information Technology | 2018-12-31 | 2019-03-26 
 AISC | International Conference on Artificial Intelligence and Symbolic Computation | 2010-07-05 
 CCNET | International Conference on Computer Networks & Communications | 2023-02-11 | 2023-02-25 
 ArIT | International Conference on Advances in Artificial Intelligence Techniques | 2023-06-03 | 2023-06-17 
 CIS' | International Conference on Cybernetics and Intelligent Systems | 2015-01-31 | 2015-07-15 
 IEEE CVMI | IEEE International Conference on Computer Vision and Machine Intelligence | 2024-06-15 | 2024-10-19 
 Mobility IoT | EAI International Conference on Mobility, IoT and Smart Cities | 2021-07-01 | 2021-11-24 
 CoSIT' | International Conference on Computer Science and Information Technology | 2023-05-06 | 2023-05-20 
 GridCom | International Conference on Grid Computing | 2022-11-12 | 2022-11-26 
  
 2040  2489  590  3885  3836  1585  4808  3760  2991  1434    

  Related Journals   

 CCF | Full Name | Impact Factor | Publisher | ISSN 
 b | ACM Transactions on Internet Technology | 3.900 | ACM | 1533-5399 
 IEEE Transactions on Education | 2.100 | IEEE | 0018-9359 
 IEEE Transactions on Power Systems | 6.500 | IEEE | 0885-8950 
 Discover Applied Sciences | 2.800 | Springer | 3004-9261 
 b | Frontiers of Computer Science | 3.400 | Springer | 2095-2228 
 b | Computers & Security | 4.800 | Elsevier | 0167-4048 
 c | Pervasive and Mobile Computing | 3.000 | Elsevier | 1574-1192 
 Simulation & Gaming | SAGE | 1046-8781 
 Brain Informatics | Springer | 2198-4018 
 The International Journal of Advanced Manufacturing Technology | 2.900 | Springer | 0268-3768 
  
 22  1159  1129  1109  480  29  341  926  790  487    

 Full Name | Impact Factor | Publisher 
 ACM Transactions on Internet Technology | 3.900 | ACM 
 IEEE Transactions on Education | 2.100 | IEEE 
 IEEE Transactions on Power Systems | 6.500 | IEEE 
 Discover Applied Sciences | 2.800 | Springer 
 Frontiers of Computer Science | 3.400 | Springer 
 Computers & Security | 4.800 | Elsevier 
 Pervasive and Mobile Computing | 3.000 | Elsevier 
 Simulation & Gaming | SAGE 
 Brain Informatics | Springer 
 The International Journal of Advanced Manufacturing Technology | 2.900 | Springer 
  
 22  1159  1129  1109  480  29  341  926  790  487    

  Recommendation   

 Track It 1 
  Attend It 0 
  Edit CFP 

 Tracker 
 Naveed Akram (761) 
  
 11398    
   
  Advertisment   

  4,945  Conferences | 1,179  Journals | 69,529  Researchers | 383,507,699 PV  
  Copyright © 2011-2024 myhuiban.com. All Rights Reserved. About Us  | Facebook  | X  | Post CFP or Contact Us  | Promotion

47. IWSEC_2 conference:
Skip to content      
     Digital Watch Observatory   
 Digital Governance in 50+ issues, 500+ actors, 5+ processes  
   
 Subscribe                       
 Search for:        

  Menu  Topics | Infrastructure | Telecommunications infrastructure 
  Critical internet resources 
  Digital standards 
  Net neutrality and zero-rating 
  Cybersecurity | Cybercrime 
  Critical infrastructure 
  Cyberconflict and warfare 
  Violent extremism 
  Child safety online 
  Encryption 
  Network security 
  Human rights | Privacy and data protection 
  Right to be forgotten 
  Freedom of expression 
  Rights of persons with disabilities 
  Gender rights online 
  Freedom of the press 
  Children rights 
  Human rights principles 
  Legal and regulatory | Jurisdiction 
  Alternative dispute resolution 
  Data governance 
  Intellectual property rights 
  Liability of intermediaries 
  Convergence and OTT 
  Digital legacies 
  Economic | E-commerce and Digital Trade 
  Digital business models 
  Taxation 
  Consumer protection 
  Future of work 
  Cryptocurrencies 
  Development | Sustainable development 
  Digital access 
  Inclusive finance 
  Capacity development 
  E-waste 
  Sociocultural | Content policy 
  Cultural diversity 
  Multilingualism 
  Digital identities 
  Online education 
  Interdisciplinary approaches 
  Technologies 
  Processes | UN Open-ended Working Group (OEWG) 
  Explore Global Digital Compact 
  WSIS+20 process 
  EU’s DSA & DMA 
  Geneva Dialogue on Responsible Behaviour in Cyberspace 
  Convention on AI and human rights (CoE) 
  GGE on LAWS 
  Internet Governance Forum 
  Ad Hoc Committee on Cybercrime 
  The WTO Joint Initiative on e-commerce 
  Trends 
  Events | Event calendar 
  Briefings 
  Reporting 
  Analysis 
  Policy players | Geneva Digital Atlas 
  Organisations 
  Countries 
  Resources 
  Newsletters | The Digital Watch Weekly 
  The Digital Watch Monthly 
  Lettre d’information mensuelle 
  The Digital Watch Shorts 
  Team 

 Home  |  Events  |  The 18th International Workshop on Security (IWSEC 2023)   

 The 18th International Workshop on Security (IWSEC 2023)  
   
 29 Aug 2023 - 31 Aug 2023  
 Yokohama, Japan and Online   

 The 18th annual International Workshop on Security (IWSEC 2023) will be held hybrid at the Institute of Information Security, Yokohama, Japan and Online on 29-31 August 2023.  
 IWSEC 2023 is organized by ISEC in ESS of IEICE (Technical Committee on Information Security in Engineering Sciences Society of the Institute of Electronics, Information and Communication Engineers).  
 Original papers on the research and development of various security topics, case studies, and implementation experiences are solicited for submission to IWSEC 2023.  
 Please visit the dedicated page  for more information about the event.  

 Related topics  
 Cybersecurity    

 Find us here   

 Subscribe to the Digital Watch newsletters  Choose whether you would like to receive our weekly and/or monthly newsletters.  By clicking on the Subscribe button, you are agreeing to our Privacy Policy  .     Digital Watch weekly   Digital Watch monthly   Lettre d'information mensuelle Digital Watch   GIP News - Latest updates and events from the Geneva Internet Platform     
    Subscribe       

 The Digital Watch  is an initiative of the Geneva Internet Platform, supported by the Swiss Confederation and the Republic and Canton of Geneva. The GIP is operated by DiploFoundation.  

 Contribute to the observatory  
 The GIP Digital Watch observatory reflects on a wide variety of themes and actors involved in global digital policy, curated by a dedicated team of experts from around the world. To submit updates about your organisation, or to join our team of curators, or to enquire about partnerships, write to us at [email protected]   . We look forward to hearing from you.  

 Except where otherwise noted, the content on this website is licensed by DiploFoundation  under CC BY-NC-ND 4.0 International  . External content is licensed by the respective authors. Please inform us  when making use of the content.   
 About 
  Privacy Policy 
  Terms 
  Log in 

  About WordPress | WordPress.org 
  Documentation 
  Learn WordPress 
  Support 
  Feedback 
  Search

48. DX_3 conference:
Login    Lost password ?  Create account 

 Home 
  Call for papers 
  Committees 
  For authors 
  Program 
  Contacts 
  Participants 
  Registration 
  DX Series 
  New submission 
  Sponsors 

 33rd International Workshop on Principles of Diagnosis – DX 2022 Workshop in Toulouse, France, September 14th – 16th, 2022  
 DX 2022  
 Diagnosis is a classical part of Artificial Intelligence research. The aim of the annual DX Workshop is to unite researchers and practitioners with diverse backgrounds in order to leverage research in diagnosis, that is, identifying the root causes for encountered malfunctions. Since 1989, the DX Workshop series has been offering a forum to present current research and experience reports, exchange and discuss emerging ideas, as well as debate current issues and envisioned future challenges. Relevant topics are related to fault diagnosis, monitoring, testing, debugging, reconfiguration, fault-adaptive control, fault recovery, and repair.  
 Workshop Location  
 The 33rd International Workshop on Principles of Diagnosis (DX-2022) will be held September 14-16, 2022, in Toulouse, France. Toulouse is in the French department of Haute-Garonne and of the larger region of Occitanie. The city is on the banks of the River Garonne, 150 kilometres (93 miles) from the Mediterranean Sea. It is the fourth-largest commune in France.  
   
 photo: Claude Attard  , license (CC BY 2.0)    

 DX 2022 IS SUPPORTED BY   
 ANITI | - | Artificial and Natural Intelligence Toulouse Institute 
  SAGIP | - | Society of Automation, Industrial Engineering and Productics 
  INSA | - Institute of Higher Education and Research 
  LAAS-CNRS | - | Laboratory for Analysis and Architecture of Systems 
  AFIA | - Association Française pour l'Intelligence Artificielle 
  DX 2022 papers   
 are available on : https://hal.science/DX2022/search/index?q=DX+2022   
 DX 2022 social event  
    
 slides | Important Dates  
 Paper submission deadline  : 23:59 (anywhere) on May 15th, 2022  May 31, 2022    
 Notification of acceptance:  July 15, 2022  
 Camera ready copies due:  July 31, 2022  
 Registration deadline:  August 30, 2022  
 Workshop  : September 14th-16th, 2022 

 Online user: 1 | Privacy |  

   Loading...

49. REFSQ_2 conference:
JavaScript must be enabled to use the system

50. SCOPES_1 conference:
Navigation überspringen 
  Zur Navigation 
  Zum Seitenende 
   
 Organisationsmenü öffnen      Organisationsmenü schließen      
    Friedrich-Alexander-Universität  Hardware/Software Co-Design       

 FAU  Zur zentralen FAU Website 
  Friedrich-Alexander-Universität 
  Faculty of Engineering 
  Department Electrical Engineering 
   
  Geben Sie hier den Suchbegriff ein, um in diesem Webauftritt zu suchen:      Suche öffnen    

 English 
  Deutsch 

 Department Informatik 
  UnivIS 
  Lageplan 

 Friedrich-Alexander-Universität 
  Faculty of Engineering 
  Department Electrical Engineering 
   
    Friedrich-Alexander-Universität  Hardware/Software Co-Design       
 Menu   Menu schließen   Chair | News 
  Staff 
  Jobs 
  Contact 
  Portal Chair 
  Research | Groups 
  Projects 
  Publications 
  Industrial partners 
  Portal Research 
  Teaching | Theses 
  Teaching events 
  Study program 
  General stuff 
  Study Abroad 
  Portal Teaching 

 Startseite 
  23.05.2016 – 25.05.2016 19th International Workshop on Software and Compilers for Embedded Systems (SCOPES) 

 23.05.2016 – 25.05.2016 19th International Workshop on Software and Compilers for Embedded Systems (SCOPES)  

 23.05.2016 – 25.05.2016 19th International Workshop on Software and Compilers for Embedded Systems (SCOPES)  
   Bild der Teilnehmer des Workshop on Software and Compilers for Embedded Systems (SCOPES) 2016 in Sankt Goar     
 29. November 2016    
 Prof. Dr.-Ing. Jürgen Teich and Andreas Weichslgartner took part in the 19th International Workshop on Software and Compilers for Embedded Systems (SCOPES) in Sankt Goar, Germany. There, Andreas Weichslgartner presented the work entitled “Design-Time/Run-Time Mapping of Security-Critical Applications in Heterogeneous MPSoCs”.  
 Kategorie: News    
 Post navigation  
 25.04.2016 – 29.04.2016: Exhibition Invasive Computing at Hannover Fair    
 06.06.2016: Talk “Predictable MPSoC Stream Processing Using Invasive Computing” von Prof. Teich an der University of Austin/Texas    

 Recent Posts  
 MEMSYS Best Paper Award for Nils Wilbert 
  Master Award for Abrarul Karim 
  Best Paper in the Swarm Intelligence Track of GECCO 2024 
  Embedded Talk “Next-Generation IoT – Communication and AI” 
  06.06.2024: ALPACA Chip at “Chipdesign Germany” Kick-Off Event 
    
 Archives  
 October 2024 
  September 2024 
  July 2024 
  June 2024 
  May 2024 
  April 2024 
  March 2024 
  January 2024 
  December 2023 
  November 2023 
  September 2023 
  August 2023 
  June 2023 
  May 2023 
  April 2023 
  November 2022 
  October 2022 
  August 2022 
  July 2022 
  June 2022 
  May 2022 
  January 2022 
  December 2021 
  November 2021 
  October 2021 
  September 2021 
  May 2021 
  February 2021 
  January 2021 
  October 2020 
  July 2020 
  June 2020 
  May 2020 
  April 2020 
  December 2019 
  October 2019 
  September 2019 
  August 2019 
  June 2019 
  April 2019 
  March 2019 
  November 2018 
  August 2018 
  July 2018 
  June 2018 
  April 2018 
  January 2018 
  November 2017 
  October 2017 
  September 2017 
  May 2017 
  March 2017 
  February 2017 
  January 2017 
  December 2016 
  November 2016 
  September 2016 
  May 2016 
  April 2016 
  March 2016 
  February 2016 
  January 2016 
  December 2015 
  November 2015 
  October 2015 
  September 2015 
  August 2015 
  July 2015 
  June 2015 
  May 2015 
  April 2015 
  March 2015 
  February 2015 
  January 2015 

  Lehrstuhl für Informatik 12  
  Hardware-Software-Co-Design   
  Cauerstraße 11   
  91058  Erlangen   
      
 Interna 
  Legal Notice 
  Privacy 
  Accessibility 
   
   Facebook 
  RSS Feed 
  Twitter 
  Xing 

  Nach oben

51. IWSEC_3 conference:
Skip to main content       Infoscience   

    English 
  French 
    
   Log In   Log in with EPFL account     Email address   Password    Log in with local account      Have you forgotten your password? 

   Infoscience   
 Research      
   Education      
   Innovation     
  Explore      
   Statistics      

    English 
  French 
    
   Log In   Log in with EPFL account     Email address   Password    Log in with local account      Have you forgotten your password? 

   Home 
  Person, units and other authorities 
  Events 
  18th International Workshop on Security (IWSEC 2023) 

 18th International Workshop on Security (IWSEC 2023)  

   Event date  August 29-31,2023  

  Event location  Yokohama, Japan  

  Contributions 
   Metrics 

   results   Back to results    
  Filters  
  Reset filters    
   Settings  
 Sort By   

 Results per page   
 1  5  10  20  40  60  80  100     

  Show as list    Show as grid     
   Search Tools    
 Search Results  
   
 Loading search results...              

 Contact 
  infoscience@epfl.ch 
  Follow us on Facebook 
  Follow us on Instagram 
  Follow us on LinkedIn 
  Follow us on X 
  Follow us on Youtube 
    
 Accessibility  Legal notice  Privacy policy  Cookie settings  End User Agreement  Get help  Feedback    
 Infoscience is a service managed and provided by the Library and IT Services of EPFL. © EPFL, tous droits réservés

52. REFSQ_3 conference:
Vol-3378   
  urn:nbn:de:0074-3378-8  Copyright © 2023 for the individual papers by the papers' authors. Copyright © 2023  for the volume as a collection by its editors. This volume and its papers are published under the Creative Commons License Attribution 4.0 International ( CC BY 4.0  )  . 

  REFSQ-JP 2023    
  REFSQ Co-Located Events 2023   
  
  Joint Proceedings of REFSQ-2023 Workshops, Doctoral Symposium, Posters & Tools Track and Journal Early Feedback   
  co-located with the 28th International Conference on Requirements Engineering: Foundation for Software Quality ( REFSQ 2023  )  
   
 Barcelona, Catalunya, Spain, April 17-20, 2023  .  
  
  Edited by   
 Alessio Ferrari  , National Research Council, Italy  
  Birgit Penzenstadler  , Chalmers University, Sweden and Lappeenranta University of Technology, Finland  
  Irit Hadar  , University of Haifa, Israel  
  Shola Oyedeji  , LUT University, Finland  
  Sallam Abualhaija  , University of Luxembourg, Luxembourg  
  Andreas Vogelsang  , University of Cologne, Germany  
  Gouri Deshpande  , University of Calgary, Canada  
  Alexander Rachmann  , Cologne Business School, Germany  
  Jens Gulden  , Utrecht University, Nederland  
  Andrea Wohlgemuth  , Swisslog GmbH, Germany  
  Anne Hess  , Fraunhofer IESE, Germany  
  Samuel Fricker  , University of Applied Sciences and Arts Northwestern Switzerland, Switzerland  
  Renata Guizzardi  , University of Twente, Netherlands  
  Jennifer Horkoff  , University of Gothenburg, Sweden  
  Anna Perini  , Fondazione Bruno Kessler, Italy  
  Angelo Susi  , Fondazione Bruno Kessler, Italy  
  Oliver Karras  , Leibniz Information Centre for Science and Technology, Germany  
  Fabiano Dalpiaz  , Utrecht University, Netherlands  
  Ana Moreira  , NOVA University Lisbon, Portugal  
  Daniel Amyot  , University of Ottawa, Canada  
  Paola Spoletini  , Kennesaw State University, United States  

  Table of Contents  
 Preface | Giorgio O. Spagnolo | , | Alessio Ferrari | , | Birgit Penzenstadler 
   
  NLP4RE: 6th Workshop on Natural Language Processing for Requirements Engineering   
 Preface: 6th Workshop on Natural Language Processing for Requirements Engineering (NLP4RE'23) | Sallam Abualhaija | , | Andreas Vogelsang | , | Gouri Deshpande 
  Rule-based NLP vs ChatGPT in ambiguity detection, a preliminary study | Alessandro Fantechi | , | Stefania Gnesi | , | Laura Semini 
  From US to Domain Models: Recommending Relationships between Entities | Maxim Bragilovski | , | Fabiano Dalpiaz | , | Arnon Sturm 
  Let’s Stop Building at the Feet of Giants: Recovering unavailable Requirements Quality Artifacts | Julian Frattini | , | Lloyd Montgomery | , | Davide Fucci | , | Jannik Fischbach | , | Michael Unterkalmsteiner | , | Daniel Mendez 
  Comparing general purpose pre-trained Word and Sentence embeddings for Requirements Classification | Federico Cruciani | , | Samuel Moore | , | Chris Nugent 
  Understanding Developers Privacy Concerns Through Reddit Thread Analysis | Jonathan Parsons | , | Michael Schrider | , | Oyebanjo Ogunlela | , | Sepideh Ghanavati 
  Chatbots4Mobile: Feature-oriented Knowledge Base Generation Using Natural Language | (short paper) | Quim Motger | , | Xavier Franch | , | Jordi Marco 
  REFrame: 1st Workshop on Requirements Engineering Frameworks   
 Preface: First Workshop on Requirements Engineering Frameworks (REFrame'23) – "Human Values in RE" | Andrea Wohlgemuth | , | Anne Hess | , | Samuel A. Fricker 
  Towards an Agent-Oriented Approach for Requirements Engineering in the Digital Era | (short paper) | Eric Yu 
  Modeling and Analysis of Emotion-Oriented Goal Models: Virtual Clinics Case Study | Mashail N. Alkhomsan | , | Malak Baslyman | , | Mohammad Alshayeb 
  Trustworthy "blackbox" Self-Adaptive Systems | (short paper) | Beatriz Cabrero-Daniel | , | Yasamin Fazelidehkordi | , | Olga Ratushniak 
  Vision Paper: The Sustainability Awareness Framework (SusAF) as a De-Facto Standard? | (short paper) | Birgit Penzenstadler | , | Norbert Seyff | , | Stefanie Betz | , | Leticia Duboc | , | Jari Porras | , | Ruzanna Chitchyan | , | Ian Brooks | , | Shola Oyedeji | , | Karina B. Villela | , | Colin C. Venters 
  RE4AI: 4th Workshop on Requirements Engineering for Artificial Intelligence   
 Preface: The 4th International Workshop on Requirements Engineering for Artificial Intelligence (RE4AI’23) | Renata Guizzardi | , | Jennifer Horkoff | , | Anna Perini | , | Angelo Susi 
  ViVA RE!: 1st Workshop on Virtues and Values in Requirements Engineering   
 Preface: ViVaRE! '23 – Workshop on Virtues and Values in Requirements Engineering | Alexander Rachmann | , | Jens Gulden 
  Doctoral Symposium   
 Preface: REFSQ 2023 Doctoral Symposium | Fabiano Dalpiaz | , | Ana Moreira 
  Exploring Challenges and Solutions for Non-Functional Requirements for Machine Learning Systems | Khan Mohammad Habibullah 
  Evaluation of Quality Requirements for Explanations in AI-based Healthcare Systems | Zubaria Inayat 
  Relating User Feedback and Existing Requirements | Michael Anders 
  Improving the Completeness of Acceptance Criteria | Astrid Rohmann 
  Conversational Requirements Engineering: Pinpointing Requirements-Relevant Information in Conversations | Tjerk Spijkman 
  Ensuring Software Quality through Videos in Requirements Engineering | Jianwei Shi 
  Posters and Tools Track   
 Preface: REFSQ 2023 Posters and Tools Track | Sallam Abualhaija | , | Oliver Karras 
  TransFeatEx: a NLP pipeline for feature extraction | Agustí Gállego Marfà | , | Quim Motger | , | Xavier Franch | , | Jordi Marco 
  Digitalisation of Agriculture: Development and Evaluation of a Model-based Requirements Engineering Process | Chiara Mannari | , | Giorgio Oronzo Spagnolo | , | Manlio Bacco | , | Alessio Malizia 
  Automated Traceability between Requirements and Model-Based Design | Maria Bonner | , | Marc Zeller | , | Gabor Schulz | , | Dagmar Beyer | , | Mihaela Olteanu 
  Taxonomic Trace Links Recommender: Context Aware Hierarchical Classification | Waleed Abdeen 
  REIT-Builder: Customizable Training for Requirements Elicitation Interviews | Roger Ian Konlog | , | Paola Spoletini 
  Journal Early Feedback Track   
 Preface: Journal Early Feedback Track | Paola Spoletini | , | Daniel Amyot 
    
  2023-03-27: submitted by Alessio Ferrari, metadata incl. bibliographic data published under Creative Commons CC0   
  2023-04-20  : published on CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073) | valid HTML5  |

53. SCOPES_2 conference:
PLDI 2023   Sat 17 - Wed 21 June 2023 Orlando, Florida, United States    

 Toggle navigation        
 Attending | Venue: Orlando World Center Marriott 
  PLDI'23 Attendee's Guide 
  Registration 
  Code of Conduct 
  Sponsorship 
  Visa 
  Volunteers 
  Childcare 
  Program | PLDI Program 
  Your Program 
   Sat 17 Jun 
  Sun 18 Jun 
  Mon 19 Jun 
  Tue 20 Jun 
  Wed 21 Jun 
  Tracks | PLDI 2023 
  FCRC 
  PLDI Research Papers 
  Social 
  Research Artifacts 
  Tutorials 
  Workshops and Tutorials 
  SRC 
  Volunteering 
  W@PLDI 
  LGBTQ+ Lunch 
  SIGPLAN 
   Co-hosted Conferences 
  ISMM 
  LCTES 
  Workshops 
  ARRAY 
  ASA 
  CSC 
  CTSTA 
  DRAGSTERS 
  EGRAPHS 
  Infer 
  PLARCH 
  PLMW@PLDI 
  SOAP 
  Organization | PLDI 2023 Committees 
  Organizing Committee 
  Track Committees 
  PLDI Research Papers 
  Research Artifacts 
  Workshops and Tutorials 
  SRC 
  Volunteering 
  Contributors 
  People Index 
   Co-hosted Conferences 
  ISMM | Organizing Committee 
  Program Committee 
  LCTES | Organizing Committee 
  Program Committee 
  Steering Committee 
  Workshops 
  ARRAY | Organizing Committee 
  Program Committee 
  ASA | Organizing Committee 
  CSC | Organizing Committee 
  Program Committee 
  CTSTA | Organizing Committee 
  Program Committee 
  DRAGSTERS | Organizing Committee 
  Program Committee 
  EGRAPHS | Organizing Committee 
  Program Committee 
  Infer | Organizing Committee 
  Program Committee 
  PLARCH | Organizing Committee 
  Program Committee 
  PLMW@PLDI | Organizing Committee 
  SOAP | Organizing Committee 
  Program Committee 
  Search 
  Series | Series 
   PLDI 2025 
  PLDI 2024 
  PLDI 2023 
  PLDI 2022 
  PLDI 2021 
  PLDI 2020 
  PLDI 2019 
  PLDI 2018 
  PLDI 2017 
  PLDI 2016 
  PLDI 2015 
  Sign in 
  Sign up 

  PLDI 2023  ( series  ) /  LCTES 2023 ( series  ) /  Languages, Compilers, Tools and Theory of Embedded Systems LCTES 2023   
   
 About 
  Program 
  Accepted Papers 
  Speaker's Guide 
  Attendee's Guide 
  Call for Artifacts 
  Call for Papers 
  Submission 
  LCTES 2023   
  Welcome to the 24th ACM SIGPLAN/SIGBED International Conference on L  anguages, C  ompilers, and T  ools for E  mbedded S  ystems ( LCTES  2023)!  
 LCTES provides a link between the programming languages and embedded systems engineering communities. Researchers and developers in these areas are addressing many similar problems but with different backgrounds and approaches. LCTES is intended to expose researchers and developers from either area to relevant work and interesting problems in the other area and provide a forum where they can interact.  
 LCTES’23 is co-located with PLDI  and FCRC 2023  , sharing the venue and activities with ten top computer science conferences.  
 Proceedings   
 The proceedings for LCTES 2023 are now available in the ACM Digital Library  .  
 Attendance policy   
 Accepted papers must be presented at LCTES 2023 to appear in the proceedings. Since FCRC is an in-person conference, presentations must be in person and the presenter must register for LCTES. Under special circumstances, pre-recorded presentations are allowed but require three virtual PLDI registrations to help us cover the cost of the proceedings. If you are unable to present your paper in person, contact the general chair  as soon as possible.  
 Registration is open   
 Registrations are now open  . Access to LCTES is possible with a 2-day or single-day (Sunday) pass as part of PLDI. Make sure to tick the “LCTES” box when signing up!  
 Accommodation   
 Visit the FCRC 2023 hotel information  page to book your rooms at a discounted rate.  
 Conference attendance grants for student authors   
 If you are a student and a presenter or co-author of a paper but need money to attend the conference, you can apply for a SIGPLAN grant. Visit https://pac.sigplan.org/  to learn more.  
   
 Plenary    

   Program Display Configuration  
   
   Time Zone   
   
 The program is currently displayed in (GMT-04:00) Eastern Time (US & Canada)  .   Use conference time zone: (GMT-04:00) Eastern Time (US & Canada)    Select other time zone   (GMT-12:00) AoE (Anywhere On Earth)  (GMT-11:00) Midway Island, Samoa  (GMT-09:00) Hawaii-Aleutian  (GMT-10:00) Hawaii  (GMT-09:30) Marquesas Islands  (GMT-09:00) Gambier Islands  (GMT-08:00) Alaska  (GMT-07:00) Tijuana, Baja California  (GMT-08:00) Pitcairn Islands  (GMT-07:00) Pacific Time (US & Canada)  (GMT-06:00) Mountain Time (US & Canada)  (GMT-06:00) Chihuahua, La Paz, Mazatlan  (GMT-07:00) Arizona  (GMT-06:00) Saskatchewan, Central America  (GMT-05:00) Guadalajara, Mexico City, Monterrey  (GMT-06:00) Easter Island  (GMT-05:00) Central Time (US & Canada)  (GMT-04:00) Eastern Time (US & Canada)  (GMT-04:00) Cuba  (GMT-05:00) Bogota, Lima, Quito, Rio Branco  (GMT-04:00) Caracas  (GMT-04:00) Santiago  (GMT-04:00) La Paz  (GMT-03:00) Faukland Islands  (GMT-04:00) Manaus, Amazonas, Brazil  (GMT-03:00) Atlantic Time (Goose Bay)  (GMT-03:00) Atlantic Time (Canada)  (GMT-02:30) Newfoundland  (GMT-03:00) UTC-3  (GMT-03:00) Montevideo  (GMT-02:00) Miquelon, St. Pierre  (GMT-02:00) Greenland  (GMT-03:00) Buenos Aires  (GMT-03:00) Brasilia, Distrito Federal, Brazil  (GMT-02:00) Mid-Atlantic  (GMT-01:00) Cape Verde Is.  (GMT) Azores  (UTC) Coordinated Universal Time  (GMT+01:00) Belfast  (GMT+01:00) Dublin  (GMT+01:00) Lisbon  (GMT+01:00) London  (GMT) Monrovia, Reykjavik  (GMT+02:00) Amsterdam, Berlin, Bern, Rome, Stockholm, Vienna  (GMT+02:00) Belgrade, Bratislava, Budapest, Ljubljana, Prague  (GMT+02:00) Brussels, Copenhagen, Madrid, Paris  (GMT+01:00) West Central Africa  (GMT+02:00) Windhoek  (GMT+03:00) Athens  (GMT+03:00) Beirut  (GMT+02:00) Cairo  (GMT+03:00) Gaza  (GMT+02:00) Harare, Pretoria  (GMT+03:00) Jerusalem  (GMT+03:00) Minsk  (GMT+03:00) Syria  (GMT+03:00) Moscow, St. Petersburg, Volgograd  (GMT+03:00) Nairobi  (GMT+03:30) Tehran  (GMT+04:00) Abu Dhabi, Muscat  (GMT+04:00) Yerevan  (GMT+04:30) Kabul  (GMT+05:00) Ekaterinburg  (GMT+05:00) Tashkent  (GMT+05:30) Chennai, Kolkata, Mumbai, New Delhi  (GMT+05:45) Kathmandu  (GMT+06:00) Astana, Dhaka  (GMT+07:00) Novosibirsk  (GMT+06:30) Yangon (Rangoon)  (GMT+07:00) Bangkok, Hanoi, Jakarta  (GMT+07:00) Krasnoyarsk  (GMT+08:00) Beijing, Chongqing, Hong Kong, Urumqi  (GMT+08:00) Irkutsk, Ulaan Bataar  (GMT+08:00) Perth  (GMT+08:45) Eucla  (GMT+09:00) Osaka, Sapporo, Tokyo  (GMT+09:00) Seoul  (GMT+09:00) Yakutsk  (GMT+09:30) Adelaide  (GMT+09:30) Darwin  (GMT+10:00) Brisbane  (GMT+10:00) Hobart  (GMT+10:00) Vladivostok  (GMT+10:30) Lord Howe Island  (GMT+11:00) Solomon Is., New Caledonia  (GMT+11:00) Magadan  (GMT+11:00) Norfolk Island  (GMT+12:00) Anadyr, Kamchatka  (GMT+12:00) Auckland, Wellington  (GMT+12:00) Fiji, Kamchatka, Marshall Is.  (GMT+12:45) Chatham Islands  (GMT+13:00) Nuku'alofa  (GMT+14:00) Kiritimati     

   The GMT offsets shown reflect the offsets at the moment of the conference  .     
   
 Time Band   
   
 By setting a time band, the program will dim events that are outside this time window. This is useful for (virtual) conferences with a continuous program (with repeated sessions).  
  The time band will also limit the events that are included in the personal iCalendar subscription service.   Display full program    Specify a time band   -      

  Save    

  Close    

 ×    You're viewing the program in a time zone which is different from your device's time zone change time zone     

 Sun 18 Jun   
  
 Displayed time zone: Eastern Time (US & Canada)  change      

 07:30 - 09:00 | Breakfast Catering   at Royal 
 07:30   
 90m    
 Other | Breakfast   Catering 

 09:00 - 10:00 | LCTES: Keynote LCTES   at Magnolia 7-8   
  Chair(s): Bernhard Egger  Seoul National University   #lctes-0900-keynote-magnolia78 
 09:00   
 60m    
 Keynote | Decreasing the Miss Rate and Eliminating the Performance Penalty of a Data Filter Cache   LCTES   
 David B. Whalley  Florida State University    
  DOI 

 10:00 - 11:00 | LCTES: Code Gen LCTES   at Magnolia 7-8   
  Chair(s): Bernhard Egger  Seoul National University   #lctes-1000-codegen-magnolia78 
 10:00   
 20m    
 Talk | Facilitating the Bootstrapping of a New ISA   LCTES   
 Abigail Mortensen  Florida State University  , Scott Pomerville  Michigan Technological University  , David B. Whalley  Florida State University  , Soner Onder  Michigan Technological University  , Gang-Ryung Uh  Florida State University    
  DOI 
 10:20   
 20m    
 Talk | Synchronization-aware NAS for an Efficient Collaborative Inference on Mobile Platforms   LCTES   
 Beom Woo Kang  Hanyang University  , Junho Wohn  Hanyang University  , Seongju Lee  Hanyang University  , Sunghyun Park  University of Michigan  , Yung-Kyun Noh  Hanyang University  , Yongjun Park  Yonsei University    
  DOI 
 10:40   
 20m    
 Talk | MinUn: Accurate ML Inference on Microcontrollers Virtual      LCTES   
 Shikhar Jaiswal  Microsoft Research  , Rahul Kranti Kiran Goli  Microsoft Research  , Aayan Kumar  Microsoft Research  , Vivek Seshadri  Microsoft Research  , Rahul Sharma  Microsoft Research    
  DOI   Pre-print   Media Attached 

 11:00 - 11:20 | Break Catering 
 11:00   
 20m    
 Coffee break | Break   Catering 

 11:20 - 12:30 | LCTES: Code/Image Size LCTES   at Magnolia 7-8   
  Chair(s): Jongouk Choi  University of Central Florida   #lctes-1120-codeimgsize-magnolia78 
 11:30   
 20m    
 Talk | reUpNix: Reconfigurable and Updateable Embedded Systems   LCTES   
 Niklas Golenstede  Hamburg University of Technology  , Ulf Kulau  Hamburg University of Technology  , Christian Dietrich  Hamburg University of Technology    
  DOI 
 11:50   
 20m    
 Talk | Optimizing Function Layout for Mobile Applications   LCTES   
 Ellis Hoag  Meta  , Kyungwoo Lee  Meta  , Julián Mestre  University of Sydney  , Sergey Pupyrev  Meta Platforms Inc., Facebook    
  DOI 
 12:10   
 20m    
 Talk | Thread-Level Attack-Surface Reduction   LCTES   
 Florian Rommel  Leibniz Universität Hannover  , Christian Dietrich  Hamburg University of Technology  , Andreas Ziegler  Friedrich-Alexander University Erlangen-Nürnberg (FAU)  , Illia Ostapyshyn  Leibniz Universität Hannover  , Daniel Lohmann  Leibniz Universität Hannover    
  DOI 

 12:30 - 14:00 | Lunch Catering   at Royal 
 12:30   
 90m    
 Lunch | Lunch   Catering 

 14:00 - 15:30 | LCTES: Scheduling & WIP LCTES   at Magnolia 7-8   
  Chair(s): Dongyoon Lee  Stony Brook University   #lctes-1400-scheduling-magnolia78 
 14:00   
 20m    
 Talk | Sequential Scheduling of Dataflow Graphs for Memory Peak Minimization   LCTES   
 Pascal Fradet  Inria Grenoble, France  , Alain Girault  INRIA  , Alexandre Honorat  Inria    
  DOI 
 14:20   
 20m    
 Talk | PinIt: Influencing OS Scheduling via Compiler-Induced Affinities in Embedded Media Servers Virtual      LCTES   
 Girish Mururu  Georgia Institute of Technology  , vincentni   , Ada Gavrilovska   , Santosh Pande  Georgia Institute of Technology    
  DOI 
 14:40   
 10m    
 Talk | (WIP) Towards Secure MicroPython on Morello   LCTES   
 Jeremy Singer  University of Glasgow    
  DOI   Pre-print 
 14:50   
 10m    
 Talk | (WIP) Towards Automated Identification of Layering Violations in Embedded Applications   LCTES   
 Mingjie Shen  Purdue University  , James C. Davis  Purdue University  , Aravind Machiry  Purdue University    
  DOI   Pre-print 
 15:00   
 10m    
 Talk | (WIP) Tiling for DMA-Based Hardware Accelerators Virtual      LCTES   
 Alexandre Singer  Huawei Canada Research Centre  , Kai-Ting Amy Wang  Huawei Canada Research Centre    
  DOI 

 15:30 - 16:00 | Break Catering 
 15:30   
 30m    
 Coffee break | Break   Catering 

 16:00 - 17:00 | LCTES: Storage LCTES   at Magnolia 7-8   
  Chair(s): Dongyoon Lee  Stony Brook University   #lctes-1600-storage-magnolia78 
 16:00   
 20m    
 Talk | Rep-RAID: An Integrated Approach to Optimizing Data Replication and Garbage Collection in RAID-enabled SSDs   LCTES   
 Jun Li  Southwest University  , Balazs Gerofi  Intel Corporation  , Francois Trahay  Telecom SudParis  , Zhigang Cai  Southwest University  , Jianwei Liao  Southwest University    
  DOI 
 16:20   
 20m    
 Talk | ISVABI: In-Storage Video Analytics Engine with Block Interface   LCTES   
 Yi Zheng  The Pennsylvania State University  , Joshua Fixelle  University of Virginia  , Pingyi Huo  The Pennsylvania State University  , Mircea R. Stan  University of Virginia  , Mike Mesnier  Intel Labs  , Vijaykrishnan Narayanan  The Pennsylvania State University    
  DOI 
 16:40   
 20m    
 Talk | LUNAR: A Native Table Engine for Embedded Devices Virtual      LCTES   
 Xiaopeng Fan  East China Normal University  , Song Yan  East China Normal University  , Yuchen Huang  East China Normal University  , Chuliang Weng  East China Normal University    
  DOI 

 Accepted Papers  
  
 Title 
 Decreasing the Miss Rate and Eliminating the Performance Penalty of a Data Filter Cache  LCTES   
 David B. Whalley    
  DOI 
 Facilitating the Bootstrapping of a New ISA  LCTES   
 Abigail Mortensen  , Scott Pomerville  , David B. Whalley  , Soner Onder  , Gang-Ryung Uh    
  DOI 
 ISVABI: In-Storage Video Analytics Engine with Block Interface  LCTES   
 Yi Zheng  , Joshua Fixelle  , Pingyi Huo  , Mircea R. Stan  , Mike Mesnier  , Vijaykrishnan Narayanan    
  DOI 
 LUNAR: A Native Table Engine for Embedded Devices Virtual     LCTES   
 Xiaopeng Fan  , Song Yan  , Yuchen Huang  , Chuliang Weng    
  DOI 
 MinUn: Accurate ML Inference on Microcontrollers Virtual     LCTES   
 Shikhar Jaiswal  , Rahul Kranti Kiran Goli  , Aayan Kumar  , Vivek Seshadri  , Rahul Sharma    
  DOI   Pre-print   Media Attached 
 Optimizing Function Layout for Mobile Applications  LCTES   
 Ellis Hoag  , Kyungwoo Lee  , Julián Mestre  , Sergey Pupyrev    
  DOI 
 PinIt: Influencing OS Scheduling via Compiler-Induced Affinities in Embedded Media Servers Virtual     LCTES   
 Girish Mururu  , vincentni  , Ada Gavrilovska  , Santosh Pande    
  DOI 
 Rep-RAID: An Integrated Approach to Optimizing Data Replication and Garbage Collection in RAID-enabled SSDs  LCTES   
 Jun Li  , Balazs Gerofi  , Francois Trahay  , Zhigang Cai  , Jianwei Liao    
  DOI 
 reUpNix: Reconfigurable and Updateable Embedded Systems  LCTES   
 Niklas Golenstede  , Ulf Kulau  , Christian Dietrich    
  DOI 
 Sequential Scheduling of Dataflow Graphs for Memory Peak Minimization  LCTES   
 Pascal Fradet  , Alain Girault  , Alexandre Honorat    
  DOI 
 Synchronization-aware NAS for an Efficient Collaborative Inference on Mobile Platforms  LCTES   
 Beom Woo Kang  , Junho Wohn  , Seongju Lee  , Sunghyun Park  , Yung-Kyun Noh  , Yongjun Park    
  DOI 
 Thread-Level Attack-Surface Reduction  LCTES   
 Florian Rommel  , Christian Dietrich  , Andreas Ziegler  , Illia Ostapyshyn  , Daniel Lohmann    
  DOI 
 (WIP) Tiling for DMA-Based Hardware Accelerators Virtual     LCTES   
 Alexandre Singer  , Kai-Ting Amy Wang    
  DOI 
 (WIP) Towards Automated Identification of Layering Violations in Embedded Applications  LCTES   
 Mingjie Shen  , James C. Davis  , Aravind Machiry    
  DOI   Pre-print 
 (WIP) Towards Secure MicroPython on Morello  LCTES   
 Jeremy Singer    
  DOI   Pre-print 

 Call for Papers  
  
  Programming languages, compilers, and tools are important interfaces between embedded systems and emerging applications in the real world. Embedded systems are aggressively adapted for deep neural network applications, autonomous vehicles, robots, healthcare applications, etc. However, these emerging applications impose challenges that conflict with conventional design requirements and increase the complexity of embedded system designs. Furthermore, they exploit new hardware paradigms to scale up multicores (including GPUs and FPGAs) and distributed systems built from many cores. Therefore, programming languages, compilers, and tools are becoming more important to address these issues, such as productivity, validation, verification, maintainability, safety, and reliability for meeting both performance goals and resource constraints.  
 The 24th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, Tools and Theory of Embedded Systems (LCTES 2023) solicits papers presenting original work on programming languages, compilers, tools, theory, and architectures that help in overcoming these challenges. Research papers on innovative techniques are welcome, as well as experience papers on insights obtained by experimenting with real-world systems and applications. Papers can be submitted to https://lctes23.hotcrp.com/  .  

 Important Dates   
 Paper submission deadline: March 24, 2023 (extended firm deadline) 
  Paper notification: April 24, 2023 
  AE submission deadline: May 1, 2023 
  AE notification: May 12, 2023 
  Camera-ready deadline: May 15, 2023 
  Conference: June 18, 2023 

 Paper Categories   
 Full paper: 10 pages presenting original work. 
  Work-in-progress paper: 4 pages papers presenting original ideas that are likely to trigger interesting discussions. 
  Accepted papers in both categories will appear in the proceedings published by ACM. In addition, this year’s LCTES introduces two journal modes.  
 All accepted full papers will be invited to be published in a special issue of the ACM Transactions on Embedded Computing Systems (TECS). A TECS publication will require substantial additional material over the conference publication and will undergo a separate review process. 
  Rejected full papers may have the option to submit their work to the IEEE Embedded Systems Letters (ESL; 4 pages). IEEE ESL submissions will undergo a separate review process. 

 Original contributions are solicited on the topics of interest including, but not limited to:   
 Programming language challenges   
 Domain-specific languages 
  Features to exploit multicore, reconfigurable, and other emerging architectures 
  Features for distributed, adaptive, and real-time control embedded systems 
  Capabilities for specification, composition, and construction of embedded systems 
  Language features and techniques to enhance reliability, verifiability, and security 
  Virtual machines, concurrency, inter-processor synchronization, and memory management 
  Compiler challenges   
 Interaction between embedded architectures, operating systems, and compilers 
  Interpreters, binary translation, just-in-time compilation, and split compilation 
  Support for enhanced programmer productivity 
  Support for enhanced debugging, profiling, and exception/interrupt handling 
  Optimization for low power/energy, code/data size, and real-time performance 
  Parameterized and structural compiler design space exploration and auto-tuning 
  Tools for analysis, specification, design, and implementation, including: 
  Hardware, system software, application software, and their interfaces 
  Distributed real-time control, media players, and reconfigurable architectures 
  System integration and testing 
  Performance estimation, monitoring, and tuning 
  Run-time system support for embedded systems 
  Design space exploration tools 
  Support for system security and system-level reliability 
  Approaches for cross-layer system optimization 
  Theory and foundations of embedded systems   
 Predictability of resource behavior: energy, space, time 
  Validation and verification, in particular of concurrent and distributed systems 
  Formal foundations of model-based design as the basis for code generation, analysis, and verification 
  Mathematical foundations for embedded systems 
  Models of computations for embedded applications 
  Novel embedded architectures   
 Design and implementation of novel architectures 
  Workload analysis and performance evaluation 
  Architecture support for new language features, virtualization, compiler techniques, debugging tools 
  Architectural features to improve power/energy, code/data size, and predictability 
  Mobile systems and IoT   
 Operating systems for mobile and IoT devices 
  Compiler and software tools for mobile and IoT systems 
  Energy management for mobile and IoT devices 
  Memory and IO techniques for mobile and IoT devices 

 ACM Publications Policies   
 By submitting your article to an ACM Publication, you are hereby acknowledging that you and your co-authors are subject to all ACM Publications Policies  , including ACM’s new Publications Policy on Research Involving Human Participants and Subjects  . Alleged violations of this policy or any ACM Publications Policy will be investigated by ACM and may result in a full retraction of your paper, in addition to other potential penalties, as per ACM Publications Policy.  
  Please ensure that you and your co-authors obtain an ORCID ID  , so you can complete the publishing process for your accepted paper. ACM has been involved in ORCID from the start, and we have recently made a commitment to collect ORCID IDs from all of our published authors  . The collection process has started and will roll out as a requirement throughout 2022. We are committed to improve author discoverability, ensure proper attribution and contribute to ongoing community efforts around name normalization; your ORCID ID will help in these efforts.  

 Authors take note   
 The official publication date is the date the proceedings are made available in the ACM Digital Library. This date may be up to two weeks prior to the first day of your conference. The official publication date affects the deadline for any patent filings related to published work.  

 Submission  
  
  Submissions must be in ACM SIGPLAN subformat of the acmart format (available at and explained in more detail at https://www.sigplan.org/Resources/Author/  ). Each paper should be in 10pt font and have no more than 10 pages for full papers  or 4 pages for work-in-progress papers  , excluding the bibliography. There is no limit on the page count for references. Each reference must list all authors of the paper (do not use et al.  ). The citations should be in numeric style, e.g., [52]. Submissions must be in PDF format and printable on US Letter and A4-sized paper. For papers in the work-in-progress category, add the suffix “(WIP)” to your title, such as “Title (WIP)”  .  
 To enable double-blind reviewing, submissions must adhere to two rules:  
 author names and their affiliations must be omitted; and, 
  references to related work by the authors should be in the third person (e.g., not “We build on our previous work …” but rather “We build on the work of …”). 
  However, nothing should be done in the name of anonymity that weakens the submission or makes the job of reviewing the paper more difficult (e.g., important background references should not be omitted or anonymized). Papers must describe unpublished work that is not currently submitted for publication elsewhere as discussed here  . Authors of accepted papers will be required to sign an ACM copyright release.  

 ACM Publications Policies   
 By submitting your article to an ACM Publication, you are hereby acknowledging that you and your co-authors are subject to all ACM Publications Policies  , including ACM’s new Publications Policy on Research Involving Human Participants and Subjects  . Alleged violations of this policy or any ACM Publications Policy will be investigated by ACM and may result in a full retraction of your paper, in addition to other potential penalties, as per ACM Publications Policy. Please ensure that you and your co-authors obtain an ORCID ID  , so you can complete the publishing process for your accepted paper. ACM has been involved in ORCID from the start, and we have recently made a commitment to collect ORCID IDs from all of our published authors  . The collection process has started and will roll out as a requirement throughout 2022. We are committed to improve author discoverability, ensure proper attribution and contribute to ongoing community efforts around name normalization; your ORCID ID will help in these efforts.  

 AUTHORS TAKE NOTE   
 The official publication date is the date the proceedings are made available in the ACM Digital Library. This date may be up to two weeks prior to the first day of your conference. The official publication date affects the deadline for any patent filings related to published work.  

 Submission site   
 https://lctes23.hotcrp.com   

 Call for Artifacts  
  
 Submission Site   
 Submit your artifacts through https://lctes23ae.hotcrp.com  .  
 Artifact Submission Deadline  : 11:59 pm May 1, 2023 (AoE)  
  Artifact Decision Notification  : May 12, 2023  
 General Info   
 The authors of all accepted LCTES papers (including WIP papers) are invited to submit supporting materials to the Artifact Evaluation process. Artifact Evaluation is run by a separate Artifact Evaluation Committee (AEC) whose task is to assess how well the artifacts support the work described in the papers. This submission is voluntary but strongly encouraged and will not influence the final decision regarding the papers.  
 At LCTES, we follow ACM’s artifact review and badging policy, version 1.1  . ACM describes a research artifact as follows:  
 By “artifact” we mean a digital object created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.  
  
 Submission of an artifact does not imply automatic permission to make its content public. AEC members will be instructed that they may not publicize any part of the submitted artifacts during or after completing the evaluation, and they will not retain any part of any artifact after evaluation. Thus, you can include models, data files, proprietary binaries, and similar items in your artifact.  
 We expect each artifact to receive three reviews. Papers that pass the Artifact Evaluation process will receive up to three ACM artifact evaluation badge(s) directly printed on the paper and available as meta information in the ACM Digital Library.  
 Artifact evaluation is single-blind. Please take precautions (e.g., turning off analytics and logging) to help prevent accidentally learning the reviewers’ identities.  
 Badging   
 The papers with accepted artifacts will be assigned official ACM artifact evaluation badges based on the criteria defined by ACM  . The AEC awards up to three types of badges that reflect the evaluation (red), availability (green), and validation (blue) of the artifact and the results of the paper. Refer to the ACM website  for detailed badge information. Note that artifacts will be evaluated with respect to the claims and presentation in the submitted version of the paper, not the camera-ready version   .  
 The awarded badges will appear on the first page of the camera-ready version of the paper. Artifact authors will be allowed to revise their camera-ready paper after being notified of their artifact’s publication to include a link to the artifact’s DOI.  
 Note that we only award the “Reproducibility” (dark blue) badge but not the “Replication” (light blue) one.  
 Guidelines   
 Carefully think which badge(s) you seek to receive | . | If your only goal is to publish your code, seek the “Availability” (green) badge. The reviewers will not exercise the artifact for its functionality or validate the paper’s claims. 
  If you wish you have your results reproduced without making your artifact documented, consistent, complete, and exercisable, seek the “Reproducibility” (blue) badge rather than the “Functionality/Reusability” (red) badge. 
  If you do not want to make the artifact available publicly, do not seek the “Availability” (green) badge. 
  Minimize the artifact setup overhead | . | A well-packaged artifact is easily usable by the reviewers and conveys the value of your work. A great way to package an artifact is as a VM image or Docker container that runs “out of the box.” We encourage authors to include pre-built binaries along with the source and build scripts that allow the AEC to regenerate the binaries to guarantee maximum transparency. Pre-built VM or docker images are preferred over scripts that build the image since this alleviates reliance on external dependencies. Make sure to test your artifact on at least one machine different than the one you used to prepare the artifact to identify missing dependencies. 
  Giving AE reviewers remote access to your machines with preinstalled (proprietary) software is possible. 
  Preparing the Artifact   
 You should make your artifact available as a single archive file and use the naming convention <paper #>.<suffix>  , where the appropriate suffix is used for the given archive format. Use a commonly-available compressed archive format such as .tgz, .tbz2, or .zip and open document formats. The link to download your artifact must protect the anonymity of the reviewers (e.g., a Google Drive URL).  
 The compressed archive should consist of three pieces:  
 The submission version of your accepted paper. 
  A | README | file (PDF or plaintext format) that explains your artifact (details below). 
  A folder containing the artifact. 
  The README.txt  should consist of two parts:  
 a | Getting Started Guide | and 
  Step-by-Step Instructions | that detail how your artifact can be evaluated. Include appropriate references to the relevant sections of your paper. 
  The Getting Started Guide  should contain instructions on how to set up (including, for example, a pointer to the VM player software, its version, and passwords if needed) and test your artifact. Anyone following this guide should be able to handle the rest of your artifact easily.  
 The Step by Step Instructions  explain how to reproduce experiments or other activities supporting your paper’s conclusions. Write this for readers who are deeply interested in your work and are studying to improve or compare against it. If your artifact runs for more than a few minutes, point this out and explain how to run it on smaller inputs.  
 Where appropriate, include descriptions of and links to files (included in the archive) that represent the expected outputs   such as log files generated by your tool for a given set of inputs. If there are warnings that can be ignored, explain what they are.  
 Further, include the following:  
 A list of claims from the paper supported by the artifact. Explain how and why the artifact supports those claims. 
  A list of claims from the paper | not | supported by the artifact. Explain why the artifact does not support those claims. Examples: performance claims cannot be reproduced in a VM, authors cannot redistribute specific benchmarks, etc. 
  When preparing your artifact, please keep in mind:  
 How accessible you are making your artifact to other researchers. 
  The AEC members will have a limited time to assess your artifact. 
  Artifact Evaluation Committee   
 Other than the chair, the AEC members are senior graduate students, postdocs, or recent Ph.D. graduates, identified with the help of the LCTES PC and recent artifact evaluation committees. Please check SIGPLAN’s Empirical Evaluation Guidelines  for some methodologies to consider during evaluation.  
 Throughout the review period, reviews will be submitted to HotCRP and continuously visible to authors. AEC reviewers will be able to continuously interact (anonymously) with authors for clarifications, system-specific patches, and other logistics to help ensure that the artifact can be evaluated. Continuous interaction prevents rejecting artifacts for “wrong library version” types of problems.  
 Our goal is that all submitted artifacts successfully pass the artifact evaluation.   

 Speaker's Guide  
   
 LCTES’23 Speaker’s Guide   
 This document is for those presenting a paper at LCTES’23. If you’re presenting at ISMM, PLDI, or a PLDI tutorial or workshop, please see the Speaker’s Guide on the page for the corresponding conference/track.  
 Congratulations on having your paper accepted at LCTES’23! This document will help ensure your presentation runs smoothly and has the best possible audience impact. Please read it in its entirety.  
 Checklist   
 Before LCTES:  
 Prepare and | practice | . 
  Ensure your talk runs for | no more than 15 minutes | . 
  Sign up for Discord | ( | details here | ). 
  Upload a backup copy of your talk slides | (details below). 
  Check the program | to establish when and where your talk will be. 
  Ensure you have a | HDMI adaptor | for your device. 
  Before your talk:  
 Familiarize yourself with the room | you will be speaking in. 
  Find and | introduce yourself to your session chair | . 
  Find the Discord channel | for your session, so you can monitor questions. 
  Attend the mandatory video check | before your session (details below). 
  Ensure that you are in the room no later than 5 minutes before your session. 
  After your talk:  
 Expect | questions from the floor | and session chair. 
  Once you’ve completed your talk and Q&A, | monitor Discord for follow-up questions | . 
  Preparing Your Talk   
 Your work will have a greater impact if you’re well prepared.  
 It is very important that you run to schedule. The LCTES’23 schedule is extremely tight, with hard stops imposed by FCRC scheduling. Session chairs have been asked to stick rigidly to the schedule.  
 Guidelines   
 Your talk should run for no more than 16 minutes | , uninterrupted. This gives you about three minutes for questions and one minute for speaker change-over. 
  Your talk should be prepared for the standard | 16:9 widescreen ratio | . If your talk is in a different ratio, at best it will be | pillarboxed | , wasting screen real estate and diminishing impact, and at worst, it won’t display correctly. 
  You will present your talk from a lectern, using a fixed lectern mic. 
  You will need to provide your talk ahead of time in either pdf or powerpoint. 
  If you have an embedded video in your presentation, please inform the video team via Discord and be sure to test it with them before your session. 
  Uploading Your Presentation   
 As an insurance against technical failures, we ask all speakers to make a backup copy of their presentation available to the video team by uploading it the day before the session. You’re welcome to upload fresh copies at any time.  
 Format: your presentation must be saved as a powerpoint or pdf file (sorry!) 
  Naming: you must use your paper ID as your file name (<LCTES paper ID>.[ppt,pptx,pdf], e.g., LCTES-paper43.pptx) 
  Location: please use this link: | https://bit.ly/pldi23upload 
  This requirement gives you assurance that if some major technical problem were to arise (such as a failure of your laptop), you will still be able to give your talk. If you do not make your presentation available in advance, and significant technical problems arise, we may have to shorten your presentation to keep to our tight schedule.  
 The requirement for you to use pdf or powerpoint for your backup copy is a pragmatic tradeoff. These slides will only be used in case of a technical emergency. We want to have the highest possible assurance that they will work without fuss on a third party device should such an emergency occur. If you use Google slides, Keynote, or some other software, please use the export feature to create either powerpoint or pdf backups.  
 If you elect not to upload a backup copy, please understand that this limits our volunteers’ capacity to assist you if a technical problem arises when you give your presentation.  
 Advice   
 There are many excellent sources of advice on giving good talks, including from Simon Peyton Jones  , Michael Hicks  , Michael Ernst  , and Derek Dreyer  . Make good use of these!  
 Mandatory Video Check   
 All speakers are required to be in the room and check in with their session chair and the video team no later than 5 minutes before their session starts. Please note that due to our very tight schedule, speakers who fail to upload their talk in advance and/or fail to attend the video check 5 minutes before their session may have their talk cut short if technical issues arise  .  
 Q&A   
 If you stick to the above schedule you will have about 3 minutes for questions. The in-room audience will be able to ask questions via a queue at a single microphone on a mic stand in the center of the room. In-room attendees and remote attendees will also be able to ask questions via Discord. Your session chair will monitor questions on Discord and might ask questions as they see them appearing there.  
 It is good practice, as the speaker, to repeat your understanding of the question before providing your answer. This is particularly important when time is tight because it reduces opportunities for time being wasted on account of a misunderstanding.  
 Once your talk is finished, please go on to Discord and respond to any questions or follow-up questions that appear there.  
 Remote Audience   
 Your talk will be streamed to Discord and YouTube with live captioning. Your remote audience will be able to write questions in the Discord channel created for your session (they won’t be able to ask questions via audio or video). They should see your slides, a video feed of you speaking, and live captions. As mentioned above, your session chair may relay questions from Discord.  
 Remote Presenters   
 LCTES’23 authors are expected to be present at the conference. If it is impossible for any of the authors to attend LCTES in person, please let the chairs know immediately.  
 Discord   
 We will use Discord as the virtual platform for this conference. You should familiarize yourself with Discord as an audience member well before your talk. We will use it for: a) the remote audience, b) delivery of remote talks, and c) communicating logistics.  
 Remote speakers will receive instructions on how to present your talk over Discord.  
 Connectivity   
 Please do the best you can to ensure that you have good connectivity at the time of your presentation. To protect against the possibility of a major technical failure, we encourage you to record a 15 minute video version of your talk, and share it with the video team well before your talk (see notes above for sharing slides). The video team can then use your video as a back-up in case of a major problem.  
 Limitations   
 Remote speakers should take care before including embedded audio in their talk. While this is possible, it can be difficult. If you need to do this, please consult directly with the video team and have it tested well in advance.  
 Q&A   
 Your session chair will introduce you and field questions just as for in-person sessions. You do not need to monitor Discord during your talk. Your Q&A will involve answering questions verbally, just as for in-person talks, only you’ll do so via video. As with in-person sessions, timing is very tight, so please be sure to stick to your 15 minute speaking time, or risk losing your Q&A time. You can respond to follow-up questions online after your talk has finished.  
   
 Attendee's Guide  
   
 Accessing Papers   
 PLDI papers are all open-access and available from the ACM Digital Library. Just follow the small “DOI” link in the PLDI schedule below the paper.  
 In-Person Attendees   
 The venue for PLDI’23 is Orlando World Center Marriott  .  
 The map here  indicates where you’ll find key events.  
 Registration   
 Please make your way to the FCRC registration desk, which will be in “Palms Registration” until Saturday noon, at which point it will move to " Cypress 2 Alcove" for the remainder of FCRC. Both are marked on the map above.  
 Welcome Reception   
 We are holding a joint welcome reception with ISCA in Royal from 6:00 pm to 8:00 pm on Sunday. Everyone registered for PLDI (or ISCA) is invited.  
 Catering   
 Lunch and breakfast will be in Cypress 3, with the exception of Monday’s PLDI Awards Lunch, which will be held in Cypress 1.  
 Wi-Fi   
 FCRC provides wifi for the whole event. You should receive information on how to access it at the registration desk.  
 Discord   
 In-person attendees are warmly encouraged to use Discord (details below). You can use it to coordinate with colleagues, get updates on activities, and engage with authors via Q&A. You are very welcome to ask questions via each session’s text channel on Discord, before, during, and after the talk. If the session chair has time, they will ask the question on your behalf. Authors have been asked to check their channel for questions, so you can engage with the authors that way if not in person.  
 Virtual Attendees   
 Watching Talks (YouTube)   
 LCTES will be streamed to YouTube, and accessible to everyone (Tutorials are not streamed by our video team, but organizers may have made their own arrangements).  
 Each session will be streamed separately. The link to LCTES live stream will be visible at the top of the LCTES Discord channel and in the schedule.  
 We have paid for live subtitling of the main PLDI tracks.  
 Engaging in Q&A (Discord)   
 We encourage remote attendees to engage with talks via Discord (details below in the “Using Discord”  section).  
 Using Discord   
 As an affiliated conference, LCTES will use PLDI’s Discord server for virtual engagement. Discord is a social media platform that supports text, voice, and video chats. This section of the guide gives a brief overview of how we’ll use it and pointers to more general tips on how to use Discord.  
 Key information:  
 Our conference is completely | open and free for virtual attendees | , so our Discord server is open to everyone. 
  Please use this link  to join the PLDI’23 Discord server. 
  You can use Discord entirely | from the browser, or via an app | you install. 
  We have | one channel for each technical session | . Use that channel to engage virtually with the authors and other attendees. 
  We ask everyone to | edit their profile | to set their “server nickname” to their preferred name, and their pronouns (if they wish). Updating your avatar with a profile photo can also help improve the virtual experience. 
  Editing Your Profile   
 You first need to open user settings. At the lower left corner, you should see a small icon with your avatar and your Discord username, with a gear symbol to the right of it. If you click on the gear icon, it will open user settings.  
 On the left-hand side, you’ll see menu options. Select “Profiles”.  
 First, update your nickname. Under “Profiles”, select the “Server Profiles” tab, and ensure that the server is “PLDI 2023” under “Choose a server” like this  , and then edit your “Server Nickname” to be your preferred name, including your pronouns if you wish.  
 Next, update your avatar if you wish. Under “Profiles”, select “User Profile” and under “Avatar” select “Change Avatar”.  
 Click on the “X” at the top right to exit the user settings dialogue.  
 PLDI’s Discord Setup   
 The most important thing to know is that we’ll have a text channel for each session, and that’s the place to go to engage with the speakers at that session. You’ll find them listed under “PLDI MAIN TRACK”, “ISMM & LCTES”, and “WORKSHOPS AND TUTORIALS” respectively, as shown here  . You’ll see one channel per session for PLDI, ISMM, and LCTES and one per event for the workshops and tutorials.  
 You’ll also find links to these channels on the program, such as the link to #pldi-mon-0900-opening-royal in this example screenshot  .  
 We also have a number of other channels, including ones for general questions (#general) and for social events.  
 Discord Etiquette   
 To help make the platform friendly and engaging, we encourage you to clearly identify yourself (see notes above on editing your profile).  
 PLDI is committed to creating a safe and respectful environment for everyone. This includes conversations on Discord. Please be respectful and kind in your interactions on Discord.  
 Discord for talk Q&A   
 You can use Discord to ask questions of the authors, before the talk, during the talk, and after the talk, whether you are on-site or remote. When time permits, during each live post-talk Q&A, session chairs will be able to consult Discord and read questions on behalf of the person who posted it. Authors are encouraged to go to Discord after their talk and respond to follow-up questions.  
 How to Use Discord   
 The basic use of the text channels is quite straightforward.  
 Rather than attempting to provide a first-principles guide on how to use Discord in this document, we encourage you to check out the following references:  
 “How to Use Discord: A Beginner’s Guide” | by Boone Ashworth of Wired. 
  “Beginner’s Guide to Discord” | from Discord. 
  “How to Use Discord: A Beginner’s Guide” | by Marshall Gunnell of PCWorld. 

 Questions? Use the LCTES contact form  .    
    
 Important Dates   AoE (UTC-12h)     

 Mon 15 May 2023  
  Camera-ready deadline 
 Fri 12 May 2023  
  Artifact decision 
 Mon 1 May 2023  
  Artifact submission deadline 
 Mon 24 Apr 2023  
  Paper notification 
 Fri 24 Mar 2023  
  Paper submission deadline 

 Submission Link   
   
   https://lctes23.hotcrp.com/     
   
 Organizing Committee    
   
 Bernhard Egger General Chair    
 Seoul National University   
 South Korea 
  Dongyoon Lee Program Chair    
 Stony Brook University   
 United States 
  Younghyun Cho Publicity Chair, Artifact Evaluation Co-Chair    
 University of California, Berkeley 
  Xinwei Fu Artifact Evaluation Co-Chair    
 Amazon Web Services   
 United States 
  Daon Park Web Chair    
 Seoul National University   
 South Korea 
    
 Program Committee    
   
 Dongyoon Lee PC Chair    
 Stony Brook University   
 United States 
  Guillaume Baudart Programme Committee    
 Inria   
 France 
  Younghyun Cho Programme Committee    
 University of California, Berkeley 
  Jongouk Choi Programme Committee    
 University of Central Florida   
 United States 
  James C. Davis Programme Committee    
 Purdue University   
 United States 
  Xinwei Fu Programme Committee    
 Amazon Web Services   
 United States 
  Adrien Guatto Programme Committee    
 IRIF, Université Paris Diderot   
 France 
  Dongjie He Programme Committee    
 UNSW   
 Australia 
  Seonyeong Heo Programme Committee    
 ETH Zurich   
 Switzerland 
  Hyeran Jeon Programme Committee    
 University of California, Merced   
 United States 
  Teresa Johnson Programme Committee    
 Google 
  Hanjun Kim Programme Committee    
 Yonsei University 
  Hyoseung Kim Programme Committee    
 University of California, Riverside   
 United States 
  Fengyun Liu Programme Committee    
 Oracle Labs   
 Switzerland 
  Tongping Liu Programme Committee    
 University of Massachusetts at Amherst 
  David Monniaux Programme Committee    
 CNRS/VERIMAG   
 France 
  Yongjun Park Programme Committee    
 Yonsei University 
  Rodrigo C. O. Rocha Programme Committee    
 Huawei   
 United Kingdom 
  Klaus Schneider Programme Committee    
 University of Kaiserslautern 
  Zili Shao Programme Committee    
 The Chinese University of Hong Kong 
  Zhaoyan Shen Programme Committee    
 Shandong University 
  Hyojin Sung Programme Committee    
 POSTECH 
  Munehiro Takimoto Programme Committee    
 Tokyo University of Science 
  Zheng Wang Programme Committee    
 University of Leeds, UK 
  Elliott Wen Programme Committee    
 The University of Auckland 
  Peter Wägemann Programme Committee    
 Friedrich-Alexander University Erlangen-Nürnberg (FAU) 
  Lei Zhang Programme Committee    
 ICT CAS 
  Tong Zhang Programme Committee    
 Samsung   
 United States 
    
 Steering Committee    
   
 No members yet 

 x  Sat 30 Nov 07:40    

  PLDI 2023   
  using conf.researchr.org  ( v1.67.1  )  
   Support page    
     
 Tracks  
 FCRC   
  PLDI Research Papers   
  Social   
  Research Artifacts   
  Tutorials   
  Workshops and Tutorials   
  SRC   
  Volunteering   
  W@PLDI   
  LGBTQ+ Lunch   
  SIGPLAN    
 Co-hosted Conferences  
 ISMM 2023   
  LCTES 2023   
  Workshops  
 ARRAY 2023   
  ASA 2023   
  CSC 2023   
  CTSTA 2023   
  DRAGSTERS 2023   
  EGRAPHS 2023   
  Infer 2023   
  PLARCH 2023   
  PLMW@PLDI 2023   
  SOAP 2023    

 Attending  
 Venue: Orlando World Center Marriott   
  PLDI'23 Attendee's Guide   
  Registration   
  Code of Conduct   
  Sponsorship   
  Visa   
  Volunteers   
  Childcare    
 Sign Up

54. WAFR_0 conference:
Menu   Updates 
  Program 
  Travel & Local Information 
  Keynotes 
  Committee 

 WAFR 2022  

 The 15th International Workshop on the Algorithmic Foundations of Robotics will be held on June 22-24, 2022 at the University of Maryland, College Park  
   
  Registration   Live Streaming    
  
 News & Updates  

 Springer Book Copyright Form is available online | here | for authors. 
  To attend the talks virtually, you can either register for the Zoom meeting using this | link | or follow along on the | WAFR YouTube channel | . 
  The conference banquet will take place at the Smithsonian Castle in Washington DC! Details are posted on the | program page | . 
  The program and the list of accepted papers has been posted! 
  We are pleased to offer, via generous support from NSF, a limited number of travel grants for students at US institutions. More information on | this page | . 
  Registration page | is now active. Early registration ends on May 15. 
  Paper decisions are out! 
  Due to multiple requests, the organizing committee has decided to make an | extension of the final submission deadline to Monday, February 14 | , at the end of day (AOE time). This is intended to allow authors who have submitted abstracts more time to improve the quality of their manuscripts. Nevertheless, | new abstract submissions will also be allowed through EasyChair until Friday, February 4 | , at the end of day (AOE time). 
  The | submission site | is now active. 
  Because of the continuing uncertainty about the feasibility of international travel in the future, the location for WAFR 2022 has been changed to the | University of Maryland, College Park | . This change of venue is intended to maximize the probability of a successful in-person meeting in Summer 2022. Remote participation options will be made available for authors of accepted papers that cannot participate in person. The submission timeline remains unchanged. 
  The call for papers is out! 

 Contribute  

 The Workshop on the Algorithmic Foundations of Robotics (WAFR) is a biannual multi-disciplinary single-track meeting of international researchers presenting the latest advances on algorithmic problems in robotics. Since its inception in 1994, WAFR has established a reputation as a premier venue for presenting algorithmic work related to robotics.  
 The focus of WAFR is on the design and analysis of robot algorithms from both theoretical and practical angles. The design and analysis of algorithms and foundations of robotics raise unique questions in a variety of traditional and new fields including but not limited to:  

 Control theory and optimization, 
  Computational geometry and topology, 
  Motion planning, 
  Planning and reasoning under uncertainty, 
  Randomized and sampling-based algorithms, 
  Decision theory and game theory, 
  Sensing and perception, 

 Machine learning, including supervised and unsupervised learning, reinforcement learning, representation learning, imitation learning, 
  Algorithmic approaches to human-robot interaction, and 
  Theoretical computer science. 

 In addition to these topics, we also encourage papers on applications of robot algorithms to important or new domains, such as:  

 Manufacturing, 
  Assistive and service robots, 
  Legged locomotion, 
  Surgical robots, 
  Intelligent prosthetics, 
  Multi-agent and transportation networks, 
    
 Computational biology, 
  Graphics and animation, 
  Sensor networks, 
  Brain-controlled robots, and 
  others. 

 The workshop proceedings will be published in the Springer Proceedings in Advanced Robotics (SPAR)  series.  

 Manuscript preparation  
 Manuscripts should be prepared using the Springer conference proceedings format. You can download the LaTeX and MS Word style/template files here  .  
 Submissions should not exceed 16 pages (including references) in this format.  
 Supplementary material  
 In the submissions before the conference, supplementary material (appendices with supportive proofs or multimedia files) can be submitted together with the main manuscript. While encouraged, the program committee members are not required to check the supplementary material in order to prepare their comments. No supplementary material will be included in the book.  
   
  Paper Submission    

 Important Dates  

 Abstract submission deadline | February 1  February 4, 2022 (AoE) 
 Paper submission deadline | February 7  February 14, 2022 (AoE) 
 Notification of acceptance | March 31, 2022 
 Conference | June 22-24, 2022 

 Venue  

 The D.C.-Maryland-Virginia (DMV) metropolitan area is without question one of the must-see visitor destinations in the United States. The University of Maryland -- only nine miles from the U.S. Capitol in D.C. -- is centrally located to all the region has to offer. The town of College Park has interesting historical sights, restaurants, businesses, and recreation areas. It is home to a wide variety of businesses from major retail outlets to locally owned unique businesses, recreation options, swimming pools, bowling alley, tennis complex, golf courses, nature trails, lake Artemisia, and a Smithsonian-affiliated aviation museum with the oldest continuously operated airport where the Wright brothers established their first commercial flight training operations. College Park also has many options for enjoying the arts with the UMD Clarice Smith Performing Arts Center with a variety of world-class performances on campus.  
 Washington, D.C., with its many monuments, museums, famous federal buildings, cultural attractions and world-class dining, is nearly inexhaustible. Everything is easily accessible from the University of Maryland’s Green Line station on the regional Metro subway system. Possibilities include the Lincoln Memorial, Jefferson Memorial, Vietnam Veterans Memorial, the Washington Monument, the White House, the U.S. Capitol, the Supreme Court and more. Here too are the many free museums associated with the Smithsonian Institution, including the National Gallery of Art, the Air and Space Museum, the National Museum of African American History and Culture, the National Museum of the American Indian, the National Zoo and its pandas.  
   
  Brendan Iribe Center for Computer Science and Engineering  

  McKeldin Mall on campus  

  DMV metropolitan area  

 Keynote Speakers  

  Oliver Brock   

  Hadas Kress-Gazit   

  Russ Tedrake   

 Co-Chairs  

 Steve LaValle | , University of Oulu, steven.lavalle@oulu.fi 
  Jason M. O'Kane | , University of South Carolina, jokane@cse.sc.edu 
  Michael Otte | , University of Maryland, otte@umd.edu 
  Dorsa Sadigh | , Stanford University, dorsa@cs.stanford.edu 
  Pratap Tokekar | , University of Maryland, tokekar@umd.edu 

 Program Committee  

 Ian Abraham, Yale University 
  Pulkit Agrawal, MIT 
  Nancy M. Amato, University of Illinois at Urbana-Champaign 
  Israel Becerra, Centro de Investigación en Matemáticas (CIMAT) 
  Kostas Bekris, Rutgers University 
  Dmitry Berenson, University of Michigan 
  Leonardo Bobadilla, Florida International University 
  Stefano Carpin, University of California 
  Nilanjan Chakraborty, Stony Brook University 
  Suman Chakravorty, Texas A&M University 
  Sonia Chernova, Georgia Institute of Technology 
  Greg Chirikjian, National University of Singapore 
  Howie Choset, Carnegie Mellon University 
  Philip Dames, Temple University 
  Neil Dantam, Colorado School of Mines 
  Jory Denny, University of Richmond 
  Mehmet R Dogar, University of Leeds 
  Katherine Driggs-Campbell, University of Illinois at Urbana-Champaign 
  Chinwe Ekenna, University at Albany 
  Brendan Englot, Stevens Institute of Technology 
  Esra Erdem, Sabanci University 
  Claudia Esteves, Departamento de Matemáticas. Universidad de Guanajuato 
  Jie Gao, Rutgers University 
  Abhishek Gupta, University of Washington 
  Dylan Hadfield-Menell, Massachusetts Institute of Technology 
  Dan Halperin, Tel Aviv University 
  David Held, Carnegie Mellon University 
  Seth Hutchinson, Georgia Institute of Technology 
  Volkan Isler, University of Minnesota 
  Marcelo Kallmann, University of California, Merced 
  Lydia Kavraki, Rice University 
  Sven Koenig, University of Southern California 
  Hadas Kress-Gazit, Cornell University 
  Alan Kuntz, University of Utah 
  Hanna Kurniawati, The Australian National University 
  John Leonard, Massachusetts Institute of Technology 
  Maxim Likhachev, Carnegie Mellon University 
  Dylan Losey, Virginia Tech 
  Tomas Lozano-Perez, Massachusetts Institute of Technology 
  Zach Manchester, Carnegie Mellon University 
  Negar Mehr, University of Illinois Urbana Champaign 
  Bhubaneswar Mishra, New York University 
  Marco Morales, University of Illinois Urbana-Champaign, Instituto Tecnológico Autónomo de México 
  Todd Murphey, Northwestern University 
  Rafael Murrieta, Centro de Investigación en Matemáticas (CIMAT) 
  Songhwai Oh, Seoul National University 
  Jia Pan, The University of Hong Kong 
  Marco Pavone, Stanford University and NVIDIA 
  Alyssa Pierson, Boston University 
  Valentin Polishchuk, Linkoping University 
  Elon Rimon, Technion 
  Nicholas Roy, Massachusetts Institute of Technology 
  Basak Sakcak, University of Oulu 
  Oren Salzman, Technion 
  Dylan Shell, Texas A&M University 
  Thierry Simeon, LAAS 
  Stephen L. Smith, University of Waterloo 
  Kiril Solovey, Technion--Israel Institute of Technology 
  Dezhen Song, Texas A&M University 
  Nicholas Stiffler, University of Dayton 
  Cynthia Sung, University of Pennsylvania 
  Subhash Suri, University of California, Santa Barbara 
  Frank van der Stappen, Utrecht University 
  Chee Yap, New York University 
  Sung-Eui Yoon, Korea Advanced Institute of Science and Technology 
  Jingjin Yu, Rutgers University at New Brunswick 
  Liangjun Zhang, Baidu Research 

 Previous WAFRs  

  WAFR 2020, Virtual   
  WAFR 2018, Merida, Mexico   
  WAFR 2016, San Francisco, USA   
  WAFR 2014, Istanbul, Turkey   
  WAFR 2012, Cambridge, MA, USA   
  WAFR 2010, Singapore   
  WAFR 2008, Guanajuato, México   
  WAFR 2006, New York City, USA   
  WAFR 2004, Zeist, The Netherlands   
  WAFR 2002, Nice, France   
  WAFR 2000, Hanover, NH, USA   
  WAFR 1998, Houston, TX, USA   
  WAFR 1996, Toulouse, France   
  WAFR 1994, Stanford, CA, USA    

 Follow us

55. SCOPES_3 conference:
Promotor of PhD students:     
 Svetlana Minakova (LIACS -- Leiden University, November 24, 2022) 
  Sobhan Niknam (LIACS -- Leiden University, August 25, 2020) 
  Hongchang Shan (LIACS -- Leiden University, February 25, 2020) 
  Peng Wang (LIACS -- Leiden University, February 12, 2020) 
  Jelena Spasic (LIACS -- Leiden University, November 14, 2017) 
  Di Liu (LIACS -- Leiden University, September 06, 2017) 
  Emanuele Cannella (LIACS -- Leiden University, October 11, 2016) 
  Teddy Zhai (LIACS -- Leiden University, May 13, 2015) 
  Mohamed Bamakhrama (LIACS -- Leiden University, March 12, 2014) 
  Dmitry Nadezhkin (LIACS -- Leiden University, December 20, 2012) 
  Sjoerd Meijer (LIACS -- Leiden University, December 08, 2010) 
  Hristo Nikolov (LIACS -- Leiden University, April 16, 2009) | Professional Memberships, Boards, Committees: 
  Member of the Editorial Board (Associate Editor) of EURASIP Journal on Embedded Systems 
  Member of the Editorial Board (Associate Editor) of International Journal of Reconfigurable Computing 
  Member of the IEEE Benelux Embedded Systems Chapter 
  Member of the European Network of Excellence on High-Performance Embedded Architecture and Compilation (HiPEAC) 
  Member of the IEEE since 2000 
  Associate Member of the European Network of Excellence on Embedded Systems Design (ARTIST Design) 
  Chairman of the DAEDALUS Foundation for research and education in Embedded Systems and Software 
  Member of PhD defense committees: | Vladimir Zivkovic (Leidn University, 23.09.2008), Jae Young Hur (TU Delft, 28.02.2011), Mojtaba Sabeghi (TU Delft, 04.04.2011), Tjerk Bijlsma (University of Twente, 01.07.2011), Toktam Taghavi (University of Amsterdam, 18.01.2012), Mark Thompson (University of Amsterdam, 18.01.2012), Roberta Piscitelli (University of Amsterdam, 18.09.2014), Ramin Etemadi (Leiden University, 11.12.2014), Stefan Geuns (University of Twente, 28.05.2015), Imran Ashraf (TU Delft, 28.04.2016), Nikolaus Bezirgiannis (Leiden University, 17.04.2018), Mark Westmijze (University of Twente, 29.06.2018), Gabriela Breaban (TU Eindhoven, 27.11.2018). | Involvement in conferences, workshops, and journal special issues: | 2025 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) | 2024 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) | RECIPIENT of the DATE Outstanding Reviewer Award 2024!!! 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , XXIV International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XXIV) | 2023 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , XXIII International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XXIII) | 2022 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , XXII International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XXII) | 2021 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , The 31th International Conference on Field Programmable Logic and Applications (FPL) 
  PC member | , XXI International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XXI) 
  PC member | , 24th International Workshop on Software and Compilers for Embedded Systems (SCOPES) 
  PC member | , Workshop on System-Level Design Methods for Deep Learning on Heterogeneous Architectures (SLOHA) | 2020 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , The 30th International Conference on Field Programmable Logic and Applications (FPL) 
  PC member | , XX International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XX) 
  PC member | , 23rd International Workshop on Software and Compilers for Embedded Systems (SCOPES) 
  PC member | , XXIX International Scientific Conference "Electronics - ET2020" | 2019 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , International Conference on Languages, Compilers, Tools and Theory of Embedded Systems (LCTES) 
  PC member | , XIX International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XIX) 
  PC member | , 22nd International Workshop on Software and Compilers for Embedded Systems (SCOPES) | 2018 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , XVIII International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XVIII) 
  PC member | , 21st International Workshop on Software and Compilers for Embedded Systems (SCOPES) | 2017 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , IEEE International Real-Time Systems Symposium (RTSS) 
  PC member | , IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) 
  PC member | , 20th International Workshop on Software and Compilers for Embedded Systems (SCOPES) | 2016 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) 
  PC member | , International Conference on Embedded and Ubiquitous Computing (EUC) 
  PC member | , 19th International Workshop on Software and Compilers for Embedded Systems (SCOPES) | 2015 
  General Chair | , IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) 
  Local Arrangements Co-Chair | , Embedded Systems Week (ESWeek) 
  Local Arrangements Committee Member | , HiPEAC 2015 Conference (HiPEAC) 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , XV International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS XV) 
  PC member | , 18th International Workshop on Software and Compilers for Embedded Systems (SCOPES) | 2014 
  Program Co-Chair | , IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) 
  Track Chair | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , 17th International Workshop on Software and Compilers for Embedded Systems (SCOPES) 
  PC member | , International Conference on Embedded and Ubiquitous Computing (EUC) 
  PC member | , XXIII International Scientific Conference "Electronics - ET2014" | 2013 
  Program Co-Chair | , IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) 
  Track Chair | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  Special Issue Co-Editor | , ACM Transactions on Embedded Computing Systems (TECS) 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , 16th International Workshop on Software and Compilers for Embedded Systems (M-SCOPES) 
  PC member | , International Workshop on Mapping of Applications to MPSoCs (M-SCOPES) 
  PC member | , XXII International Scientific Conference "Electronics - ET2013" | 2012 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) 
  PC member | , 15th International Workshop on Software and Compilers for Embedded Systems (SCOPES) 
  PC member | , International Workshop on Mapping of Applications to MPSoCs (Map2MPSoC) 
  PC member | , Forum on specification and Design Languages (FDL) 
  PC member | , XXI International Scientific Conference "Electronics - ET2012" | 2011 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , IEEE International Real-Time Systems Symposium (RTSS) 
  PC member | , 14th International Workshop on Software and Compilers for Embedded Systems (SCOPES) | 2010 
  Program Chair | , 13th International Workshop on Software and Compilers for Embedded Systems (SCOPES) 
  PC member | , International Conference on Design, Automation and Test in Europe (DATE) 
  PC member | , IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) 
  PC member | , IEEE/IFIP International Conference on Very Large Scale Integration and System-on-Chip (VLSI-SoC) 
  PC member | , IEEE International Conference on Computer Design (ICCD) | 2009, 2008, 2007 
  PC member | , IEEE/ACM/IFPI International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) | Reviewer for Internationa and National Funding Agencies: 
  Swiss National Science Foundation (SNF) | Reviewer for Journals: 
  IEEE Transactions on Computer Aided Design of Integrated Circuits and Systems 
  IEEE Transactions on VLSI Systems 
  IEEE Transactions on Signal Processing 
  IEEE Transactions on Industrial Informatics 
  IEEE Transactions on Emerging Topics in Computing 
  IEEE Embedded Systems Letters 
  ACM Transactions on Design Automation of Electronic Systems 
  ACM Transactions on Embedded Computing Systems 
  Transactions on HiPEAC 
  Design Automation for Embedded Systems (Springer) 
  Journal of Signal Processing Systems (Springer) 
  Journal of Systems Architecture (Elsevier) 
  Simulation Modelling Practice and Theory (Elsevier) 
  EURASIP Journal on Embedded Systems 
  EURASIP Journal on Applied Signal Processing 
  Natural Computing (Springer) | Reviewer for Conferences: 
  Design Automation Conference (DAC) -- 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014 
  Design, Automation and Test in Europe (DATE) -- 2003, 2006, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2018, 2022, 2023, 2024, 2025 
  IEEE/ACM/IFIP International Conference on Hardware/Software Co-design and System Synthesis (CODES+ISSS) -- 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024 
  IEEE/ACM Internationa Conference on Computer-Aided Design (ICCAD) -- 2012 
  IEEE/ACM International Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia) -- 2012. 2013, 2014, 2015, 2016, 2017 
  IEEE/IFIP International Conference on Very Large Scale Integration and System-on-Chip (VLSI-SoC) -- 2010 
  IEEE International Conference on Application-Specific Systems, Architectures and Processors (ASAP) -- 2002, 2003, 2007 
  IEEE International Conference on Computer Design (ICCD) -- 2008, 2010 
  International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS) -- 2015, 2018, 2019, 2020, 2021, 2022, 2023, 2024 
  Int. Conference on Compilers, Architecture, and Synthesis for Embedded Systems (CASES) -- 2005 
  Int. Conference on Field Programmable Logic and Applications (FPL) -- 2008, 2020, 2021 
  IEEE Computer Society Annual Symposium on VLSI -- 2008 
  ACM International Conference on Supercomputing (ICS) -- 2006 
  ACM International Conference on Computing Frontiers (CF) -- 2010 
  International Workshop on System Architecture, Modeling, and Simulation (SAMOS) -- 2002 
  International Workshop on Software and Compilers for Embedded Systems (SCOPES) -- 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021 
  International Workshop on Mapping of Applications to MPSoCs (Map2MPSoC) -- 2012, 2013 
  Forum on specification and Design Languages (FDL) -- 2012

56. WAFR_1 conference:
An official website of the United States government  
 Here's how you know  
   
 Here's how you know     
   
  Official websites use .gov   
  A .gov  website belongs to an official government organization in the United States.  

  Secure .gov websites use HTTPS.   
  A lock  ( Lock  Locked padlock     ) or https://  means you've safely connected to the .gov website. Share sensitive information only on official, secure websites.  

 Search   Menu    

   Search    search      

 Find Funding & Apply | Where to Start   For All Researchers & Educators 
  For Early-Career Researchers 
  For Postdoctoral Fellows 
  For Graduate Students 
  For Undergraduates 
  For Entrepreneurs 
  For Industry 
   Explore Funding   Search All Opportunities 
  By Directorate 
  By Upcoming Due Date 
  NSF-wide Initiatives 
  Search Funded Projects (Awards) 
   How to Apply   Preparing Your Proposal 
  Submitting Your Proposal 
  How We Make Funding Decisions 
  Proposal & Award Policies & Procedures Guide (PAPPG) 
   Additional Resources   Research.gov 
  Grants.gov 
  Baam.nsf.gov 
  Our Directorates & Offices 
  Manage Your Award | Guidance For Awardees   Getting Started 
  Request a Change to Your Award 
  Report Your Outcomes 
  Proposal & Award Policies & Procedures Guide (PAPPG) 
   Additional Resources   Research.gov 
  NSF Public Access Repository (PAR) 
  Our Directorates & Offices 
  Search Funded Projects (Awards) 
  Focus Areas | Areas We Fund   Arctic & Antarctic 
  Astronomy & Space 
  Biology 
  Chemistry 
  Computing 
  Diversity in STEM 
  Earth & Environment 
  Education & Training 
  Engineering 
  Facilities & Infrastructure 
  Materials Research 
  Mathematics 
  People & Society 
  Physics 
  Research Partnerships 
  Technology 
  Explore all Focus Areas 
   Additional Resources   Explore Our Impacts 
  Search Funded Projects (Awards) 
  NSF by the Numbers 
  Our Directorates & Offices 
  News & Events | News   News & Announcements 
  Science Matters Blog 
  Multimedia Gallery 
  For the Press 
   Events   Upcoming Events 
  NSF 75th Anniversary 
  NSF Grants Conference 
  About | Learn About NSF   Overview 
  Our Directorates & Offices 
  NSF & Congress 
  Honorary Awards 
  Visit NSF 
  Contact Us 
   Work With NSF   Careers at NSF 
  Contracting With NSF 
  Partnering With NSF 
   Additional Resources   National Science Board 
  National Center for Science & Engineering Statistics (NCSES) 
  Documents & Reports 
  Budget, Performance & Financial Reporting 
  Staff Directory 

 Awards 
   Search Awards 
   Recent Awards 
   Presidential and Honorary Awards 
   About Awards 
  How to Manage Your Award 
  Grant General Conditions 
   Cooperative Agreement Conditions 
   Special Conditions 
   Federal Demonstration Partnership 
   Policy Office Website 

 Award Abstract # 2011778    
 The 14th International Workshop on the Algorithmic Foundations of Robotics (WAFR'20) Student Travel Awards    

 NSF Org: | IIS   
  Div Of Information & Intelligent Systems 
 Recipient: | TEXAS A&M ENGINEERING EXPERIMENT STATION 
 Initial Amendment Date: | January 23, 2020 
 Latest Amendment Date: | January 23, 2020 
 Award Number: | 2011778 
 Award Instrument: | Standard Grant 
 Program Manager: | Juan Wachs   
  IIS  Div Of Information & Intelligent Systems   
  CSE  Direct For Computer & Info Scie & Enginr 
 Start Date: | May 1, 2020 
 End Date: | April 30, 2023 (Estimated) 
 Total Intended Award Amount: | $15,000.00 
 Total Awarded Amount to Date: | $15,000.00 
 Funds Obligated to Date: | FY 2020 = $15,000.00 
 History of Investigator: | Dylan | Shell | (Principal Investigator)  
   dshell@cs.tamu.edu 
 Recipient Sponsored Research Office: | Texas A&M Engineering Experiment Station  
   3124 TAMU  
   COLLEGE STATION  
   TX  US  77843-3124   
  (979)862-6777 
 Sponsor Congressional District: | 10 
 Primary Place of Performance: | Texas A&M Engineering Experiment Station  
   HRBB 330C  
   College Station  
   TX  US  77845-3112 
 Primary Place of Performance  
  Congressional District: | 10 
 Unique Entity Identifier (UEI): | QD1MX6N5YTN4 
 Parent UEI: | QD1MX6N5YTN4 
 NSF Program(s): | Robust Intelligence 
 Primary Program Source: | 01002021DB NSF RESEARCH & RELATED ACTIVIT 
 Program Reference Code(s): | 7495,   7556 
 Program Element Code(s): | 749500 
 Award Agency Code: | 4900 
 Fund Agency Code: | 4900 
 Assistance Listing Number(s): | 47.070 

  ABSTRACT   
    
  The award will support student attendance of the three-day single-track Fourteenth International Workshop on the Algorithmic Foundations of Robotics (WAFR), along with mentoring activities associated with the event. The conference will be held in Oulu, Finland, between June 15th and June 17th in 2020 at the Sokos Hotel Eden, two miles from the city center of Oulu and the supporting local institution the University of Oulu. The funds will be used to provide financial support to help approximately 15 U.S. students to attend the WAFR 2020, helping to ensure good representation of students from U.S. institutions at this leading research venue. Participation in the conference will boost motivation and excitement for graduate students and advanced undergraduate students as they continue their research journey into the foundational aspects of robotics.  
   
  The purpose of this grant for student travel to enable U.S. students, who otherwise might be unable to attend the WAFR conference, to present their work and forge connections with colleagues from around the world. In addition to paper presentations, the event will feature 3?5 keynote presentations by influential and internationally renowned roboticists. These presentations will provide succinct, first-hand information on historical perspectives, current state-of-the-art, and future trends of robotic planning, optimization and control, computational geometry and algorithms. With a long-established history, in the past WAFR has been among the best venues to focus on fundamental computer science topics and technical questions which underpin robotics, being known as the place where several foundational breakthroughs were presented in the past. In particular, themes covered by WAFR submissions and presentations previously have addressed fundamental algorithmic issues, such as complexity, completeness, machine learning, and probabilistic reasoning. Work has also been presented with applicability to manufacturing, legged locomotion, distributed robotics, human-robot interaction, surgical robots, and intelligent prosthetics, and even domains beyond the traditional scope of robotics, e.g., computational biology, computer animation, sensor networks. These match NSF's robotics priorities very closely.  
   
  This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.   
 PROJECT OUTCOMES REPORT   

 Disclaimer   
 This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.  

 This award provided funds in order to support travel of US-based graduate students to the 1 4th International Workshop on the Algorithmic Foundations of Robotics (WAFR'20), originally planned to be held in Oulu, Finland.  Already having had a long andillustrious existence, WAFR has historically been among the best venues to focus on fundamental computer sciencetopics and technical questions which underpin robotics, being known as the place where several foundationalbreakthroughs were presented in the past. In particular, themes covered by WAFR submissions and presentationspreviously have addressed fundamental algorithmic issues, such as complexity, completeness, machine learning, andprobabilistic reasoning. Work has also been presented with applicability to manufacturing, legged locomotion, distributedrobotics, human-robot interaction, surgical robots, and intelligent prosthetics, and even domains beyond the traditionalscope of robotics, e.g., computational biology, computer animation, sensor networks.  
 Unfortunately, the COVID-19 pandemic led to the cancellation of international travel and hence the event was postponed. Later, a small virtual was held on the 29 March 2021 instead. As no funds had been expended up this point, we were able to put that money toward support of travel the following event, WAFR'22 held at the University of Maryland June 22-24, 2022. As students were, thus, travelling to College Park, Maryland, the funds were able to support a greater number of participants than originally planned. A public announcment was made (via a community e-mail list) and, i   n the end, a total of 23 students were identified as meeting the constraints. These students had some portion of the costs (a median of amound of $651) of their travel offset by the NSF.  

  Last Modified: 12/21/2023  
  Modified by: Dylan A Shell  

  Please report errors in award information by writing to: awardsearch@nsf.gov  .  

  Top    

 2415 Eisenhower Ave Alexandria, VA 22314   
 (703) 292-5111    
     Sign up for email updates   

 Footer  
 About Us | About NSF 
  Careers 
  Our Directorates & Offices 
  National Science Board 
  Contact Us 
  What's New | News & Announcements 
  Events 
  Science Matters Blog 
  Multimedia Gallery 
  Information For | Funding Seekers 
  NSF Awardees 
  Congress 
  Media 
  Educators 
  Panelists 
  Resources | Documents & Reports 
  Budget, Performance & Financial Reporting 
  Public Access 
  Stopping Harassment 
  Research Security 
  Scientific Integrity 
  Research.gov 

 Required Policy Links  
 Vulnerability disclosure 
  Inspector General 
  Privacy 
  FOIA 
  No FEAR Act 
  USA.gov 
  Accessibility 
  Plain language

57. WETICE_0 conference:
The requested URL was rejected. Please consult with your administrator.  
   
  Your support ID is: < 8203162004128806770>  
   
  [Go Back]

58. WAFR_2 conference:
Socialist Republic of Vietnam    Choose your Country/Region  

 Asia   
   
  China    
  India    
  Japan    
  South Korea    
   
  Malaysia    
  Taiwan, China    
  United Arab Emirates    
  Indonesia    
   
  Hong Kong, China    
  Singapore    
  Thailand    
  Turkey    

 America   
   
  United States    
  Canada    
  Brazil    
  Argentina    
   
  Mexico    
  Colombia    
  Chile    
  Peru    
   
  Guatemala    

 Europe   
   
  Italy    
  United Kingdom    
  France    
  Germany    
   
  Spain    
  Portugal    
  Austria    
  Poland    
   
  Greece    
  Russian Federation    
  Czech Republic    
  Switzerland    
   
  Netherlands    
  Sweden    
  Romania    
  Hungary    
   
  Belgium    
  Ukraine    
  Ireland    
  Croatia    
   
  Finland    
  Denmark    
  Cyprus    
  Serbia    
   
  Slovakia    
  Norway    
  Bulgaria    
  Iceland    

 Oceania   
   
  Australia    
  New Zealand    
  Fiji    

 Africa   
   
  South Africa    
  Tunisia    
  Morocco    
  Egypt    

 Product | Software 
  Webinar 
  Video conference 
  Virtual conference 
  Institution Edition 
  Discover | Subject category 
  Conference in Socialist Republic of Vietnam 
  Contribution library 
  Browse by venue 
  Services 
         
 Create an event  Lecture    
   
 Meeting/Workshop/Tutorials    
   
 Conference    

 Log in  Sign up    

 The 13th International Workshop on the Algorithmic Foundations of Robotics (WAFR 2018)  
   
 Dec. 09 - 11, 2018  
 · Mexico  
   
 Conference  Offline Conference    
  0  Views   
  0  Comments   
 Favorite    
  Share    

  Call for paper 〔OPEN〕  
 My Submissions   

 〔CLOSED〕    
   
  Registration 〔OPEN〕  
 My tickets   

 〔CLOSED〕    

 Introduction  
   
 The Workshop on the Algorithmic Foundations of Robotics (WAFR) is a biannual multi-disciplinary single-track meeting of international researchers presenting the latest advances on algorithmic problems in robotics. WAFR was created in 1994 as a small-size workshop. It has been held since 1994 and while it spans a relatively small community, it has an established reputation as one, if not the most, important venue for presenting algorithmic work related to robotics.  
 The focus of WAFR is on the design and analysis of robot algorithms from both theoretical and practical angles. These algorithms process inputs from noisy sensors, build geometric and physical models of the world, plan high-and low-level actions at different time horizons, and execute these actions on actuators with limited precision. The design and analysis of robot algorithms raise a unique combination of questions from many fields, including control theory, computational geometry and topology, geometrical and physical modeling, reasoning under uncertainty, probabilistic algorithms, game theory, machine learning, and theoretical computer science. In addition to these topics, we also encourage papers on applications of robot algorithms to important or new domains, such as manufacturing, legged locomotion, distributed robotics, human-robot interaction, surgical robots, intelligent prosthetics, and brain-controlled robots. Furthermore, we welcome papers on applications of robot algorithms in domains beyond the traditional scope of robotics, e.g., computational biology, computer animation, sensor networks. The workshop proceedings will be published in the Springer Proceedings in Advanced Robotics (SPAR) series and selected papers will be invited for publication in a special issue of the International Journal of Robotics Research.  

 Committee  
   
 Co-Chairs   
 Marco Morales, Instituto Tecnológico Autónomo de México (ITAM) 
  Gildardo Sánchez-Ante, Universidad Politécnica de Yucatán 
  Lydia Tapia, University of New Mexico 
  Seth Hutchinson, Georgia Tech 
  Program Committee   
 Srinivas Akella, University of North Carolina at Charlotte 
  Ron Alterovitz, University of North Carolina at Chapel Hill 
  Nancy Amato, Texas A&M University 
  Devin Balkcom, Dartmouth College 
  Kostas Bekris, Rutgers University 
  Dmitry Berenson, University of Michigan, Ann Arbor 
  Timothy Bretl, University of Illinois Urbana-Champaign 
  Nilanjan Chakraborty, Stony Brook University 
  Suman Chakravorty, Texas A&M University 
  Juan Cortes, LAAS-CNRS 
  Jory Denny, University of Richmond 
  Mehmet Dogar, University of Leeds 
  Chinwe Ekenna, University at Albany 
  Brendan Englot, Stevens Institute of Technology 
  Esra Erdem, Sabanci University 
  Claudia Esteves, Universidad de Guanajuato 
  Aleksandra Faust, Google Brain Robotics 
  Anthony Francis, Google Brain Robotics 
  Emilio Frazzoli, Massachusetts Institute of Technology 
  Stephen Guy, University of Minnesota 
  Kris Hauser, Duke University 
  Jean-Bernard Hayet, CIMAT 
  Geoffrey Hollinger, Oregon State University 
  David Hsu, National University of Singapore 
  Volkan Isler, University of Minnesota 
  Leslie Kaelbling, Massachusetts Institute of Technology 
  Marcelo Kallmann, UC Merced 
  Sertac Karaman, Massachusetts Institute of Technology 
  Lydia Kavraki, Rice University 
  Ross Knepper, Cornell University 
  Sven Koenig, University of Southern California 
  Torsten Kroeger, Karlsruhe Institute of Technology (KIT) 
  Hanna Kurniawati, University of Queensland 
  Jyh-Ming Lien, George Mason University 
  Ming Lin, University of Maryland College Park 
  Tomas Lozano-Perez, Massachusetts Institute of Technology 
  Dinesh Manocha, University of Maryland at College Park 
  Nicolas Mansard , LAAS-CNRS 
  Jose Martínez-Carranza, INAOE 
  Troy McMahon, University of Queensland 
  Mark Moll, Rice University 
  Rafael Murrieta-Cid, CIMAT 
  Jason O'Kane, University of South Carolina 
  Songhwai Oh, Seoul National University 
  Jia Pan, City University of Hong Kong 
  Marco Pavone, Stanford University 
  Florian Pokorny, KTH Royal Institute of Technology 
  Elon Rimon, Technion-Israel Institute of Technology 
  Sam Rodriguez, Texas Wesleyan University 
  Alberto Rodriguez, Massachusetts Institute of Technology 
  José Guadalupe Romero, Instituto Tecnológico Autónomo de México (ITAM) 
  Nicholas Roy, Massachusetts Institute of Technology 
  Dylan Shell, Texas A&M University 
  Stephen L. Smith, University of Waterloo 
  Stephen F. Smith, Carnegie Mellon University 
  Dezhen Song, Texas A&M University 
  Enrique Sucar, INAOE 
  Subhash Suri, UC Santa Barbara 
  Shawna Thomas, Texas A&M University 
  Pratap Tokekar, Virginia Tech 
  Chee Yap, New York University 
  Jingjin Yu, Rutgers University 
  WAFR Steering Committee   
 Nancy Amato, Texas A&M University 
  Oliver Brock, Technische Universität Berlin 
  Ken Goldberg, UC Berkeley 
  Dan Halperin, Tel Aviv University 
  Seth Hutchinson, Georgia Institute of Technology 
  David Hsu, National University of Singapore 
  Lydia Kavraki, Rice University 
  Daniela Rus, Massachusetts Institute of Technology 
  Frank van der Stappen, Utrecht University 
  WAFR Advisory Committee   
 Jean-Claude Latombe, Stanford University 
  Jean-Paul Laumond, LAAS-CNRS 
  Matt Mason, Carnegie Mellon University 

 Call for paper  
   
 Important date  
 2018-07-23   
 Draft paper submission deadline   

 Submission Topics  
 When submitting your manuscript, please use at least one of the following suggested keywords:  
 Automation, Manufacturing & Logistics 
  Collision Avoidance 
  Completeness and Complexity 
  Computational Biology 
  Computational Geometry 
  Computer Animation & Simulation 
  Dynamics 
  Field Robotics Algorithms 
  Human-Robot Interaction 
  Humanoids and Legged Systems 
  Intelligent Prosthetics 
  Kinematics 
  Localization and Mapping 
  Logic and Verification 
  Machine Learning 
  Manipulation & Grasping 
  Medical & Surgical Robots 
  Micro/Nano Robots 
  Mobile Robots 
  Motion and Path Planning 
  Multiple and Distributed Robots 
  Novel Robot Mechanisms and Design 
  Optimization and Optimal Control 
  Perception 
  Probabilistic Reasoning 
  Sensor Networks 
  Service Robots 
  Task Planning and AI Reasoning 
  Vision and Sensor-based Control 

 Submit Comment  

 Verify Code     Change Another   Submit    

 All Comments  

 Submission Template  
 ×    
  Paper Template  
  Paper Template  

 Home 
  Program 
  Timetable 
  Abstract List 
  Photo 
  Review 
  Management 
    
 Important Date  
   
 Conference Date | Dec 09  
 2018  
  to  Dec 11  
 2018 
  Jul 23  2018 | Draft paper submission deadline 
  Dec 11  2018 | Registration deadline 

 Contact Information  
   
 wa******@cse.tamu.edu 
  Login to view full info    

 Contact Information  
 ×    

 OK    

 About Us  |  News  |  Blog  |  Feedback  |  Disclaimer  |  Privacy Policy  |  Terms of Service  |  Cancellation Policy  |  Contact   
 Copyright © 2013-2021 Aconf.org  - One-stop solutions for academic events   
 鄂ICP备09016152号-4

59. WETICE_1 conference:


60. WAFR_3 conference:
Skip to main content    
 Shape Copy 6        Shape Copy 18            Shape          Shape      next month copy         next month                      
 A. James Clark School of Engineering  Contact | Clark School Administration 
  Give | Ways to Give 
  Contact Us About Giving 
  Visit 
  Apply 
  Recruit 

 Search      
  Search this site     Mobile Navigation Trigger     Home     Main Menu     Mobile Navigation Trigger Reverse     
 About Us | Facts & Figures | Who Was A. James Clark? 
  2030 Strategic Plan 
  Meet the Dean 
  Board of Visitors 
  Diversity 
  Recognition | Clark School and UMD-Wide Honors | Professional Track Faculty Excellence Award 
  Invention of the Year Award 
  Distinguished University Professor 
  Distinguished Scholar-Teacher Award 
  E. Robert Kent Outstanding Teaching Award for Junior Faculty 
  Dr. Marilyn Berman Pollans Outstanding Service Award for Staff 
  Newcomer of the Year Staff Award 
  The Poole and Kent Teaching Award for Senior Faculty 
  Faculty Service Award 
  Humble Hero Staff Award 
  Esprit de Corps Staff Award 
  Junior Faculty Outstanding Research Award 
  Senior Faculty Outstanding Research Award 
  Dean’s Outstanding Performance Award for Professional Track Faculty 
  Student Competition Advisor Award 
  Terry Island Outstanding Advisor Award 
  Maureen Meyer Staff Excellence Award 
  National Awards and Honors | National Science Foundation CAREER Awards 
  PECASE Awards 
  Academics & Students | Degree Programs | Majors & Minors 
  Online Learning 
  Professional Programs 
  Current Students | Commencement Ceremony 
  Counseling Support 
  Dean's List 
  Engineering Academic Services 
  Engineering Career Services 
  Engineering Honors Program 
  Financial Aid 
  Future Faculty Program 
  Graduate Students | Recruitment, Fellowships & Scholarships 
  Fellowship & Scholarship Acceptance 
  Development Resources 
  Societies & Clubs 
  Student Competitions | Alumni Cup Competition 
  Clark School Three-Minute Thesis (3MT) Competition 
  Study Abroad 
  Undergraduate Awards Ceremony | 2023 Clark School's Honors & Awards 
  2024 Clark School's Honors & Awards 
  2020 Clark School's Honors & Awards 
  2021 Clark School's Honors & Awards 
  2022 Clark School's Honors & Awards 
  Undergraduate Students 
  Prospective Students | Freshmen Applicants | Prepare | Top 25 Engineering Source Schools | 2019-2020 
  2020-2021 
  2018-2019 
  2017-2018 
  2016-2017 
  2015-2016 
  2014-2015 
  2013-2014 
  2012-2013 
  2011-2012 
  2010-2011 
  2009-2010 
  2024-2025 
  2021-22 
  2022-2023 
  2023-2024 
  Decide | Degree Programs 
  Information Sessions 
  Clark School Ambassadors | Nadeen Alomar 
  Tanya Budhiraja 
  Nathan Chandran 
  Alessandra Contreras 
  Kyla Erman 
  Jillian Jacob 
  Michael Ogunsemowo 
  Jennifer Tartaglia 
  Ankit Verghese 
  Noah Wigglesworth 
  Scholarships 
  Societies & Clubs 
  Research & Innovation 
  Diversity 
  Engineering Academic Services 
  The First Year 
  Graduate Applicants 
  Transfer and Current UMD Student Applicants 
  Contact Us 
  Scholarships | Scholarships Application 
  Searchable Scholarships Database 
  Irving & Ida Rabin Scholarship 
  Engie Chuck Edwards Memorial Scholarships 
  Clark Foundation Scholarships & Fellowships | A. James Clark Scholars Program 
  Clark Opportunity Transfer Scholars Program 
  Clark Doctoral Fellows Program 
  Clark Legacy Scholarships 
  Clark Scholarship Programs Team 
  Acceptance Instructions | Thanking Your Scholarship Donor 
  Emergency Funding & Basic Needs Resources 
  Additional Funding Information 
  K-12 Pre-College Programs | School Year Programs 
  Summer Programs | Experience Engineering Virtual Summer Program 
  Discovering Engineering 
  Student Affairs & Academic Success Programs | Center for Minorities in Science and Engineering | About Us | Visitor Information 
  CMSE Advisory Board 
  Admitted Students 
  Student Programs & Support | University of Maryland LSAMP | Bridge Program for Scientists and Engineers 
  Undergraduate Research Program 
  STEM Program 
  Bridge to the Doctorate Fellowship Program 
  Alumni Registration 
  Post-Baccalaureate Research Experiences for LSAMP Students (PRELS) 
  Summer Internship with CMSE 
  Engineering Student Societies 
  Funding Request for Student Programs 
  Winter Student Leadership Retreat 
  AmazonNext Scholars Program 
  Pre-College Programs | ESTEEM/SER-Quest Summer Program 
  STEM Expo 
  Diversity in Engineering at UMD 
  Events & Photos | Annual Student Recognition Ceremony 
  LSAMP Fall Research Symposium | USM LSAMP Student Presentations 
  Support CMSE | The Path Forward 
  Alumni Stories 
  Remembering CMSE Alumni 
  Alumni Events 
  40th Anniversary | CMSE Alumni Testimonials 
  Engineering Career Services | About Us | Peer Assistants 
  For Students | Career Exploration 
  Career Fairs 
  Career Resources & Handouts 
  Diversity & Inclusion at Work 
  Events 
  Job Boards 
  Information Sessions 
  International Students 
  Internships & Co-ops | Internship/Co-op FAQs 
  Job Update Form 
  On-Campus Jobs 
  Policies 
  Recruiters in Residence 
  Salaries & Employer Lists 
  Summer Advice 
  Workshops 
  For Employers | Building a Pipeline for Employment 
  Career Fairs 
  Co-op & Internship Programs 
  Enrollment Data 
  Hiring an International Student 
  Information Sessions 
  Job Postings & Campus Interviews 
  Policies 
  Recruiters in Residence 
  Salary Information 
  Student Societies 
  Visitor Information 
  Employment Outcomes | Cybersecurity Employers 
  Employers for Product Management 
  Energy Systems Employers 
  Project Management Employers 
  Reliability Engineering Employers 
  Robotics Employers 
  Software Engineering Employers 
  Systems Engineering Employers 
  Technology Entrepreneurship and Corporate Innovation Employers 
  Telecommunications Employers 
  Employers for Aerospace Engineering 
  Employers for Bioengineering 
  Employers for Chemical Engineering 
  Employers for Civil Engineering 
  Employers for Computer Engineering 
  Employers for Electrical Engineering 
  Employers for Fire Protection Engineering 
  Employers for Materials Engineering 
  Employers for Mechanical Engineering 
  Employers for Professional Master's of Energy Systems 
  Global Engineering Leadership | Study Abroad | Clark Abroad Fall 2021 
  Course Database 
  Research and Internships Abroad 
  Short-Term Study Abroad 
  Study Abroad Contacts 
  Study Abroad FAQ 
  Clark in Madrid Program Guide 
  Aerospace Engineering 
  Bioengineering 
  Chemical and Biomolecular Engineering 
  Civil Engineering 
  Computer Engineering 
  Electrical Engineering 
  Fire Protection Engineering 
  Materials Science and Engineering 
  Mechanical Engineering 
  Computer Science 
  Clark in Madrid for the iSchool 
  Clark in Prague 
  ClarkABROAD Photo Contest Winners 
  Maryland Engineering Study Abroad Scholarships 
  ClarkABROAD Program of the Week | ClarkABROAD Program of the Week: Technical University of Denmark (DTU) 
  Minor in Global Engineering Leadership 
  Global Leadership Courses | ENES317 Introduction to Engineering Leadership 
  ENES424 Engineering Leadership Capstone 
  ENES472 International Business Cultures 
  ENES475/675 Leadership in Times of Crisis 
  Visiting International Students | Degree-Seeking Students 
  Engineering Exchanges 
  Resources 
  Global Visitor Research Training Program 
  Leadership & Service 
  Contact Us 
  Engineering Academic Services | Academic Peer Coaching Program 
  Academic Policies 
  Academic Probation 
  Contact an Advisor 
  Course Permission for Non-Majors 
  Engineering Honors 
  Engineering Student Athletes 
  Exception to Policy | Submission Checklist 
  FAQs 
  Sample Documents 
  Types of Exceptions 
  Forms & Requests 
  Graduation Plans 
  Graduation 
  Minors 
  Parent & Family Resources 
  Registration 
  Summer Engineering Orientation 
  Transfer | Current Transfer Students 
  Internal Transfer Students 
  External Transfer Students 
  Contact Us 
  Tutoring Resources 
  Undecided Engineering | Preparing for an Advising Appointment 
  Exploring Engineering Majors 
  Declaring an Engineering Major 
  Women In Engineering | About Us | Facts & Figures 
  Meet the WIE Staff 
  Visitor Information 
  Student Advisory Board 
  Current Students | Living & Learning Communities | Flexus 
  Virtus 
  Flexus & Virtus Programming Board 
  Peer Mentoring Program 
  Summer Internship Positions 
  Get Involved 
  WIE Tutoring Services 
  Technical Workshops 
  Resources 
  Partnerships & Giving 
  Future Students | 6-12 Programs | emPower Summer Program 
  WIE Change the World: An Introduction to Engineering 
  Admitted Students 
  Prospective Students 
  Superstar Spotlights 
  Keystone Program | About 
  Faculty 
  Courses 
  Student Resources | Teaching Fellows Program 
  Keystone Center 
  Alternative Proctoring 
  Engineering Education Research | Engineering Education Speakers Series 
  Research & Innovation | Innovation & Entrepreneurship 
  Featured Research Areas 
  Featured Institutes and Centers 
  Research Facilities 
  Research Opportunities 
  Alumni | Update Your Information 
  Alumni Network | Board of Directors 
  Alumni Photo Galleries 
  Golden Terps 
  Alumni Cup | Become an Alumni Cup Sponsor 
  Alumni Awards | Glenn L. Martin Medal 
  Greenaugh Award 
  Innovation Hall of Fame | All Inductees 
  125th Anniversay Medal 
  Signature Events 
  Early Career Distinguished Alumni Society | ECDA Society - Class of 2022 
  Announcing the Early Career Distinguished Alumni Society - Class of 2024 
  Early Career Award 2014-2018 
  Industry | Partner with the Clark School | Corporate Partners 
  Hire a Student 
  News | E@M Magazine 
  E-Newsletter 
  News Center 
  Press Releases 
  Events | Mpact Lecture Series 
  Commencement Ceremony | Commencement Livestream 
  Digital Commencement Program 
  Graduate Celebration 
  Events Calendar 

 Contact | Clark School Administration 
  Give | Ways to Give 
  Contact Us About Giving 
  Visit 
  Apply 
  Recruit 
    
 Directories 
  Corporate Partners 
  Careers 
  Media 
  Facilities 
  ClarkNet 
    
 Facebook 
  Twitter 
  Youtube 
  Linkedin 

 Home 
  Events 
  15th International Workshop on Algorithmic... 
    
 Event  

 15th International Workshop on Algorithmic Foundations of Robotics (WAFR)  
 Wednesday, June 22, 2022  
  10:00 a.m.  
  Edward St. John Learning and Teaching Center   
  Pratap Tokekar  
  tokekar@umd.edu   
  https://wafr2022.github.io/    

 The 15th International Workshop on Algorithmic Foundations of Robotics (WAFR) will be held June 22-24 at the University of Maryland. WAFR is a biannual multi-disciplinary single-track meeting of international researchers presenting the latest advances on algorithmic problems in robotics. Since its inception in 1994, WAFR has established a reputation as a premier venue for presenting algorithmic work related to robotics. The focus of WAFR is on the design and analysis of robot algorithms from both theoretical and practical angles. The design and analysis of algorithms and foundations of robotics raise unique questions in a variety of traditional and new fields.  
 Highlights   
 Invited Talks by Oliver Brock, Hadas Kress-Gazit, Russ Tedrake  
 33 papers, open problems session, lab tours of the University of Maryland  
 Jean-Paul Laumond memorial lecture by Steve LaValle  
 More information is available at https://wafr2022.github.io/   
 UMD hosts   
  Michael Otte and Pratap Tokekar  

 Browse All Events    
      
 November 2024  

 SU | MO | TU | WE | TH | FR | SA 
 27 | 28 | 29 | 30 | 31 | 1 | 2 
 3 | 4 | 5 | 6 | 7 | 8 | 9 
 10 | 11 | 12 | 13 | 14 | 15 | 16 
 17 | 18 | 19 | 20 | 21 | 22 | 23 
 24 | 25 | 26 | 27 | 28 | 29 | 30 
 1 | 2 | 3 | 4 | 5 | 6 | 7 
  
 Submit an Event   

 Directories 
  Corporate Partners 
  Careers 
  Media 
  Facilities 
  ClarkNet 

 Facebook 
  Twitter 
  Youtube 
  Instagram 
  Linkedin 

 Privacy Policy 
  Accessibility 
  umd.edu 

 © 2024 University of Maryland

61. WETICE_2 conference:
31th IEEE International Conference on Enabling Technologies:  
  Infrastructure for Collaborative Enterprises (WETICE-2023)  
  Paris, December 14-16, 2023 
 Home 
  Tracks | Main Track 
  invited Tracks | Security and Privacy in Metaverse 
  Soft Computing Methods for Modern Applications 
  Calls | For Papers 
  Program | Detailed Program 
  Keynote Speakers 
  Conference | Camera ready and registration 
  Venue 
  Committees 
  Previous | WETICE 2022 
  WETICE 2021 
  WETICE 2020 
  WETICE 2019 
  WETICE 2018 
  WETICE 2017 
  WETICE 2016 
  WETICE 2015 
  WETICE 2014 
  WETICE 2013 
  WETICE 2012 | About the Conference  
 WETICE is an annual international conference on state-of-the-art research in enabling technologies for collaboration, consisting of a main track and number of special tracks. Established thirty-one years ago, WETICE aims at promoting fruitful discussions on the latest technology developments, directions, problems, and requirements. The main and invited tracks include paper presentations and group discussions while the keynote sessions and summary of discussions take place in joint sessions.  
 WETICE welcomes papers on "work-in-progress" from the Ph.D. students. All papers are refereed by the Scientific Review of Committee. Proceedings will be submitted to be included into IEEE Xplore and INSPEC, Compendex, and Thomson Reuters, DBLP, Google Scholar, and EI Index.  

  Topics include but are not limited to:  
 Distributed and Collaborative Business Processes 
  Process Mining 
  Workflow Automation 
  Cloud Computing for Collaboration 
  Collaboration in Digital Strategies 
  Collaboration in Legal service 
  Collaborative and Distributed Learning Platforms 
  Collaborative Supply Chain Management 
  Distributed Database Technologies 
  Data Analytics and Fraud Detection 
  Data for Collaborative Processes 
  Data Privacy, Anonymity and Confidentiality 
  Data Visualization and Analytics 
  Consensus and Fault Tolerance Algorithms 
  Decentralized Autonomous Organization (DAO) 
  Distributed Cryptographic Algorithms and Protocols 
  Distributed Trust Management 
  Formal Verification and Model checking 
  Blockchain-based Applications and Services 
  Decentralized Application Development 
  Performance and Scalability Issues 
  Internet of Things enabled Collaboration 
  Distributed Platforms for IoT 
  Architecture, Scalability, Governance and Interoperability 
  Monitoring and Analysis 
  Service-Orientation in systems engineering 
  Healthcare Systems 
  Intelligent Manufacturing Platforms 
  Logistics and Intralogistics 
  Regulatory and standards frameworks 
  Smart Grid and Energy Systems 
  Submission guidelines  
 WETICE will be accepting the submissions starting from 14 th May. The submissions must be original that have not been published or submitted for publication elsewhere. All papers (including Selected invited tracks) should be submitted via CMT: https://cmt3.research.microsoft.com/WETICE2023   
 Papers must be submitted in English and formatted according to the following instructions. Please check and carefully follow the instructions provided in the templates which contains detailed instructions on formatting your document. IEEE template files: https://www.ieee.org/conferences/publishing/templates.html   
 Authors should submit a paper maximum of 6 pages including figures and references, carefully checked for correct grammar and spelling, using the on-line submission procedure indicated below. Authors are required to include their names and affiliations in their papers.  

 Important Dates  
  
 Submission Deadline | October 20th, 2023 (hard) 
 Notification of acceptance | on the fly, not later than October 31th, 2023 
 Camera Ready Deadline | 3 days after the notification of acceptance and later than November 3th, 2023 
 Registration Deadline | 4 days after the notification of acceptance and not later than November 4th, 2023 
 Conference | December 14th—16th, 2023 | Submission Deadline | October 20th, 2023 (hard) | Notification of acceptance | on the fly, not later than October 31th, 2023 | Camera Ready Deadline | 3 days after the notification of acceptance and later than November 3th, 2023 | Registration Deadline | 4 days after the notification of acceptance and not later than November 4th, 2023 | Conference | December 14th—16th, 2023 
 Submission Deadline | October 20th, 2023 (hard) 
 Notification of acceptance | on the fly, not later than October 31th, 2023 
 Camera Ready Deadline | 3 days after the notification of acceptance and later than November 3th, 2023 
 Registration Deadline | 4 days after the notification of acceptance and not later than November 4th, 2023 
 Conference | December 14th—16th, 2023

62. WETICE_3 conference:
Socialist Republic of Vietnam    Choose your Country/Region  

 Asia   
   
  China    
  India    
  Japan    
  South Korea    
   
  Malaysia    
  Taiwan, China    
  United Arab Emirates    
  Indonesia    
   
  Hong Kong, China    
  Singapore    
  Thailand    
  Turkey    

 America   
   
  United States    
  Canada    
  Brazil    
  Argentina    
   
  Mexico    
  Colombia    
  Chile    
  Peru    
   
  Guatemala    

 Europe   
   
  Italy    
  United Kingdom    
  France    
  Germany    
   
  Spain    
  Portugal    
  Austria    
  Poland    
   
  Greece    
  Russian Federation    
  Czech Republic    
  Switzerland    
   
  Netherlands    
  Sweden    
  Romania    
  Hungary    
   
  Belgium    
  Ukraine    
  Ireland    
  Croatia    
   
  Finland    
  Denmark    
  Cyprus    
  Serbia    
   
  Slovakia    
  Norway    
  Bulgaria    
  Iceland    

 Oceania   
   
  Australia    
  New Zealand    
  Fiji    

 Africa   
   
  South Africa    
  Tunisia    
  Morocco    
  Egypt    

 Product | Software 
  Webinar 
  Video conference 
  Virtual conference 
  Institution Edition 
  Discover | Subject category 
  Conference in Socialist Republic of Vietnam 
  Contribution library 
  Browse by venue 
  Services 
         
 Create an event  Lecture    
   
 Meeting/Workshop/Tutorials    
   
 Conference    

 Log in  Sign up    

 2023 IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)  
   
 Jun. 28 - Jul 01, 2023  
 CNAM , Paris , , , , , , France, , ;, Paris  
   
 Conference  Virtual Conference    
  0  Views   
  0  Comments   
 Favorite    
  Share    

 Introduction  
   
 AboutCommunication, Networking and Broadcast Technologies; Computing and Processing; Robotics and Control Systems  
  Keywords:Collaboration, Business Process, Cloud Computing, artificial intelligence, formal verification ,Business Process, Cloud Computing, artificial intelligence, formal verification,  
  Scope:Enabling Technologies: Infrastructure for Collaborative Enterprises and businesses  
  Sponsor Type:1; 9  

 Call for paper  

 Submit Comment  

 Verify Code     Change Another   Submit    

 All Comments  

 Submission Template  
 ×    
  Paper Template  
  Paper Template  

 Home 
  Program 
  Timetable 
  Abstract List 
  Photo 
  Review 
  Management 
    
 Important Date  
   
 Conference Date | Jun 28  
 2023  
  to  Jul 01  
 2023 
  Jul 01  2023 | Registration deadline 

 Sponsored By  
   
 IEEE Computer Society Olab-Dynamics   
   
 Contact Information  
   
 la******@gmail.com 
  Login to view full info    

 Contact Information  
 ×    

 OK    

 About Us  |  News  |  Blog  |  Feedback  |  Disclaimer  |  Privacy Policy  |  Terms of Service  |  Cancellation Policy  |  Contact   
 Copyright © 2013-2021 Aconf.org  - One-stop solutions for academic events   
 鄂ICP备09016152号-4

63. WWW_0 conference:
Calls | Research Tracks | Crowdsourcing and Human Computation 
  Economics, Monetization, and Online Markets 
  Fairness, Accountability, Transparency, and Ethics on the Web 
  Search 
  Security, Privacy, and Trust 
  Semantics and Knowledge 
  Social Network Analysis and Graph Algorithms 
  Systems and Infrastructure for Web, Mobile Web, and Web of Things 
  User Modeling and Personalization 
  Web and Society 
  Web Mining and Content Analysis 
  Special Track | Web4Good 
  History of the Web 
  The Creative Web 
  Workshops 
  Tutorials 
  PhD Symposium 
  Posters and Demos 
  Industry 
  Web Developer and W3C 
  Test of Time Award 
  Proceedings 
  Special Days | Health Day 
  Knowledge Graph Day 
  Entrepreneur Day 
  Satellite Events | W4A | W4A 2023 
  W4A 2023 Call for Paper 
  Web Sci2023 
  Provenance Week 
  Program | Live Stream of TheWebConf 
  Program Overview 
  Keynote 
  Workshops 
  Tutorials 
  Accepted Papers 
  Accepted Posters 
  Detailed Program 
  ACM A.M. Turing Award Lecture 
  BBQ and Live Music 
  Attendees | Visa Letter 
  Registration 
  Venue and Hotel 
  Travel Information 
  Student Travel Award 
  Virtual Participation Award 
  Important Dates 
  About | The Web Conference 
  Organizing Committee 
  Sponsors 
  Code of Conduct 

 Welcome to The  
  Web Conference 2023 in  
  Austin, Texas, USA  
 April 30 - May 4, 2023  
 AT&T Hotel and Conference Center at The University of Texas at Austin  
 Live Stream of TheWebConf    

  INTRODUCTION  
 Since the invention of the World Wide Web in 1989, The Web Conference (formerly known as International World Wide Web Conference, abbreviated as WWW) is a yearly international academic conference on the topic of the future direction of the World Wide Web. This conference has been the premier venue to present and discuss progress in research, development, standards, and applications of the topics related to the Web. Over the past three decades, The Web Conference has been the forum where some of the most fundamental Web technologies have been introduced, such as the Anatomy of a Large Scale Web Search Engine in 1998 prefiguring Google, the EigenTrust algorithm in 2003 and the YAGO knowledge base in 2007 (see also the Test of Time Award past recipients). The conference assembles scholars, researchers, policymakers, practitioners, and end-users with one unifying goal: to envision and create the future of the Web.  
 The 2023 ACM Web Conference will offer a high quality program made of research sessions, posters and demonstrations, a PhD symposium for the junior scholars, workshops, tutorials, a developers track for the practitioners as well as thought provoking keynote speakers, panels, special track on web for good, History of the Web, and colocated special days.  
 The 2023 ACM Web Conference is an in-person conference with virtual components  including live streaming of ceremonies and keynotes, access to pre-recorded videos of talks, and the Whova platform for interaction with all conference attendees.  
 All speakers, presenters, organizers participating in any way at The Web Conference are expected to attend the conference in person. For exceptional reasons, if you are not able to attend in person to present, you may assign a proxy who must be in person. For special cases, we may make an exception for a live remote presentation.  
   
  (UPDATE: As of April 26, due to logistical situations, we are not able to accommodate anymore remote presentations)   
 Austin is honored and thrilled to host The Web Conference and is preparing an exceptional conference for sharing the latest insights of academic and industrial research. We thank you for your participation and look forward to seeing you in Austin!  
  
  ACM A.M. Turing Award Lecture  
   Bob Metcalfe  MIT    
    
 KEYNOTES  
   Barbara Poblete  University of Chile Amazon Visiting Academic    
    David Rand  MIT    
    Payal Arora  Erasmus University Rotterdam    

    Zachary Elkins  UT Austin    
    Michalis Vazirgiannis  LIX, Ecole Polytechnique    

  SILVER SPONSOR  
     (Student Travel Award Sponsor)  

  BRONZE SPONSOR  

  ORGANIZED BY  

  The Web Conference @ Austin  
 The Web Conference 2023 is organized by the School of Information at The University of Texas at Austin

64. WWW_1 conference:
Calls | Research Tracks | Crowdsourcing and Human Computation 
  Economics, Monetization, and Online Markets 
  Fairness, Accountability, Transparency, and Ethics on the Web 
  Search 
  Security, Privacy, and Trust 
  Semantics and Knowledge 
  Social Network Analysis and Graph Algorithms 
  Systems and Infrastructure for Web, Mobile Web, and Web of Things 
  User Modeling and Personalization 
  Web and Society 
  Web Mining and Content Analysis 
  Special Track | Web4Good 
  History of the Web 
  The Creative Web 
  Workshops 
  Tutorials 
  PhD Symposium 
  Posters and Demos 
  Industry 
  Web Developer and W3C 
  Test of Time Award 
  Proceedings 
  Special Days | Health Day 
  Knowledge Graph Day 
  Entrepreneur Day 
  Satellite Events | W4A | W4A 2023 
  W4A 2023 Call for Paper 
  Web Sci2023 
  Provenance Week 
  Program | Live Stream of TheWebConf 
  Program Overview 
  Keynote 
  Workshops 
  Tutorials 
  Accepted Papers 
  Accepted Posters 
  Detailed Program 
  ACM A.M. Turing Award Lecture 
  BBQ and Live Music 
  Attendees | Visa Letter 
  Registration 
  Venue and Hotel 
  Travel Information 
  Student Travel Award 
  Virtual Participation Award 
  Important Dates 
  About | The Web Conference 
  Organizing Committee 
  Sponsors 
  Code of Conduct 

 The Web Conference  
 The Web Conference (formerly WWW conference) is a yearly international conference on the topic of the future directions of the World Wide Web.  
 The Conference aims to provide the world with a premier forum for discussion and debate about the evolution of the Web, the standardization of its associated technologies, and the impact of those technologies on society and culture. The conference brings together researchers, developers, users and commercial ventures — indeed all those who are passionate about the Web and what it has to offer.  
 The World Wide Web was first conceived in 1989 by Tim Berners-Lee at CERN in Geneva, Switzerland. The first conference of the series, WWW1, was held at CERN in 1994 and organized by Robert Cailliau. The International World Wide Web Conferences Committee (IW3C2) was founded by Joseph Hardin and Robert Cailliau later in 1994 and has been responsible for the conference series ever since. Except for 1994 and 1995 when two conferences were held each year, WWW became an annual event held in late April or early May. The location of the conference rotates among America, Europe, and Asia. In 2001 the conference designator changed from a number (1 through 10) to the year it is held; i.e., WWW11 became known as WWW2002, and so on.  
 Starting with the 2018 edition, the conference series previously known as World Wide Web Conference (WWW) has been renamed as The Web Conference or TheWebConf with a new brand and a new communication model. Until 2021, the conferences have been organized by the IW3C2 in collaboration with Local Organizing Committees and Technical Program Committees. The 2022 edition marks another important change in the history of this conference series  : it is now integrated into ACM SIGWEB and will be managed by a new steering committee within ACM.  

  The Web Conference @ Austin  
 The Web Conference 2023 is organized by the School of Information at The University of Texas at Austin

65. WWW_2 conference:
Calls | Research Tracks | Crowdsourcing and Human Computation 
  Economics, Monetization, and Online Markets 
  Fairness, Accountability, Transparency, and Ethics on the Web 
  Search 
  Security, Privacy, and Trust 
  Semantics and Knowledge 
  Social Network Analysis and Graph Algorithms 
  Systems and Infrastructure for Web, Mobile Web, and Web of Things 
  User Modeling and Personalization 
  Web and Society 
  Web Mining and Content Analysis 
  Special Track | Web4Good 
  History of the Web 
  The Creative Web 
  Workshops 
  Tutorials 
  PhD Symposium 
  Posters and Demos 
  Industry 
  Web Developer and W3C 
  Test of Time Award 
  Proceedings 
  Special Days | Health Day 
  Knowledge Graph Day 
  Entrepreneur Day 
  Satellite Events | W4A | W4A 2023 
  W4A 2023 Call for Paper 
  Web Sci2023 
  Provenance Week 
  Program | Live Stream of TheWebConf 
  Program Overview 
  Keynote 
  Workshops 
  Tutorials 
  Accepted Papers 
  Accepted Posters 
  Detailed Program 
  ACM A.M. Turing Award Lecture 
  BBQ and Live Music 
  Attendees | Visa Letter 
  Registration 
  Venue and Hotel 
  Travel Information 
  Student Travel Award 
  Virtual Participation Award 
  Important Dates 
  About | The Web Conference 
  Organizing Committee 
  Sponsors 
  Code of Conduct 

 Accepted Papers  

  Research Tracks   
 All Papers  
      
 Special Track: Web4Good   
 Special Track: History of the Web   
 Special Track: The Creative Web   
 Industry Track   
 All Papers  
      
 Posters and Demo   

  The Web Conference @ Austin  
 The Web Conference 2023 is organized by the School of Information at The University of Texas at Austin

66. IMC_0 conference:
Toggle navigation      IMC 2023    
 Home 
  Calls | Call for Papers 
  Call for Posters 
  Call for Travel Grant Applicants 
  Submission Instructions 
  Presentation Guidelines 
  Paper Anonymity 
  Committees 
  Local Info | Conference Venue 
  Conference Dinners 
  Accommodation 
  Visa 
  Policy Against Discrimination and Harassment 
  Program | Schedule 
  Accepted Posters 
  Registration 
  Sponsorship 
  Side Events | Hackathon 

 ACM Internet Measurement Conference 2023  
 The 2023 Internet Measurement Conference (IMC) is a three-day event focusing on Internet measurement and analysis. The conference is sponsored by ACM SIGCOMM. IMC 2023 is the 23rd in a series  of highly successful Internet Measurement Workshops and Conferences.  
 The ACM IMC 2023 conference will be held in Montréal, Canada on October 24 - 26, 2023.  
 News  
 2023-10-11 : The | presentation guidelines | are now available. 
  2023-10-03 : The | schedule | and the list of | accepted posters | are available. 
  2023-09-12 : The | registration | is now open. 
  2023-08-15 : The | Call for Travel Grants | is available. 
  2023-06-02: The | Call for Posters | is available. 
  2023-02-16: The IMC 2023 website is up. 
  Conference Venue  
 École de technologie supérieure  
  1100 Notre-Dame Street West, Montréal, Québec H3C 1K3  
  Canada  
 Important Dates  
  
 Expression of Interest (papers for the Replicability Track) | Tuesday March 21, 2023 at 11:59 PM AoE (UTC-12) 
 Notification for the Replicability Track Expression of Interest | Thursday March 30, 2023 at 11:59PM AoE (UTC-12) 
 Paper registration (with abstract, including invited papers to Replicability Track) | Saturday May 20, 2023 at 11:59PM AoE (UTC-12) 
 Paper submission (including invited papers to Replicability Track) | Friday May 26, 2023 at 11:59PM AoE (UTC-12) 
 Early reject notification | Approximately Friday July 14, 2023 
 Notification | Friday August 18, 2023 
 Presentation submission | Friday October, 20 at 2:00PM CEST 
 Camera-ready due | TBA 
 CCR submission (papers on the replicability track only) | TBA 
 Conference | October 24 - 26, 2023 
  
 Paper Submission  
 Please submit your paper at https://imc2023.hotcrp.com  .  
 Please submit your Expression of Interest for the Replicability Track at https://imc2023-repro.hotcrp.com/  .  
 Anti-Harassment policy  
 ACM IMC 2023 strongly endorses the ACM Anti-Harassment policy  .  
 Sponsored by  
         
 Supporters  
 Silver Supporters  
           
 Travel Grant Supporters  
       
 Other Supporters  

 Last modified: Oct 26 2023   
 Automne au Parc du Mont-Royal  © Hussein Abdallah  , CC BY 2.0

67. WWW_3 conference:
Calls | Research Tracks | Crowdsourcing and Human Computation 
  Economics, Monetization, and Online Markets 
  Fairness, Accountability, Transparency, and Ethics on the Web 
  Search 
  Security, Privacy, and Trust 
  Semantics and Knowledge 
  Social Network Analysis and Graph Algorithms 
  Systems and Infrastructure for Web, Mobile Web, and Web of Things 
  User Modeling and Personalization 
  Web and Society 
  Web Mining and Content Analysis 
  Special Track | Web4Good 
  History of the Web 
  The Creative Web 
  Workshops 
  Tutorials 
  PhD Symposium 
  Posters and Demos 
  Industry 
  Web Developer and W3C 
  Test of Time Award 
  Proceedings 
  Special Days | Health Day 
  Knowledge Graph Day 
  Entrepreneur Day 
  Satellite Events | W4A | W4A 2023 
  W4A 2023 Call for Paper 
  Web Sci2023 
  Provenance Week 
  Program | Live Stream of TheWebConf 
  Program Overview 
  Keynote 
  Workshops 
  Tutorials 
  Accepted Papers 
  Accepted Posters 
  Detailed Program 
  ACM A.M. Turing Award Lecture 
  BBQ and Live Music 
  Attendees | Visa Letter 
  Registration 
  Venue and Hotel 
  Travel Information 
  Student Travel Award 
  Virtual Participation Award 
  Important Dates 
  About | The Web Conference 
  Organizing Committee 
  Sponsors 
  Code of Conduct 

 CFP: The Creative Web  
 Track chairs:  
 Marieke van Erp | ( KNAW Humanities Cluster, NL) 
  Albert Meroño-Peñuela | (King’s College London, UK) 
  Contact  
 creative2023@thewebconf.org   A large variety of content has been published on the World Wide Web since its invention at CERN by Sir Tim Berners-Lee in 1989. Initially conceived as a platform to publish and consume hypertext in a distributed fashion, the Web has never stopped growing in content in various forms and media —text, images, sound, music, video, data, and a large et cetera. Traditionally these forms and media have been the natural spaces of human creativity, imagination and invention.  
 Many of these have transcended the limitations of their original ecosystems in isolation, becoming creative works in ways that would have been impossible without the Web as a collaboration, cooperation, and publication platform. More and more often, we see the Web as a facilitator of creative work not just between collaborating humans, but also between humans and machines.  
 Much attention has been given to the information content of the Web. This track investigates the creative aspects of this content on the Web. The track consists of two strands: a research strand and (focusing on the scientific study of Web creativity systems) an art works strand (focusing on the creative output of such systems). Through the submissions and session at the conference we hope to gain more insights into the following:  
 What is creativity on the Web? 
  In what forms does creativity on the Web manifest? 
  How do we value creativity on the Web? 
  How do we facilitate creativity on the Web? 
  How do communities gather and leverage the Web for creativity? 
  What properties do Web-based creative sociotechnical systems have? 
  Topics include but are not limited to:  
 Human-machine creativity on the Web 
  Artificial Intelligence, creativity and the Web 
  Multimodality 
  Ownership, provenance, and IPR (NFTs?) 
  Sustainability of creative works 
  Value 
  Web enabled co-creation 
  Contributions  
 We invite two types of contributions:  
 For the research strand, we invite original papers that describe original and unpublished research work. Research strand contributions should be between 4 and 8 content pages, including all figures and tables, but exclude supplementary material and references; 
  For the art strand, we invite 2-page extended abstracts that describe art projects that use the Web either as a source, publication method or reflect on the Web. We encourage contributors to reflect on the connection between their art work and the central questions and/or topics of the track. Submissions to the art strand must include a presentation format section with a description of how their contribution can be accessed or experienced (e.g. a live concert, a video recording, a printed painting, etc.), and the author’s plans for showing their work in the track exhibition space. 
  Accepted research papers and art work descriptions will be published by ACM in the conference proceedings. Research papers will be published in the research volume, while art work descriptions will be published in the companion volume.  
 Submissions will be handled via Easychair, at https://easychair.org/conferences/?conf=thewebconf2023  for research strand papers (make sure to select “Creativity on the Web”); and at https://easychair.org/conferences/?conf=thewebconf2023iwpd  for art work description papers (make sure to select “Creativity on the Web - Art”).  
 Review procedure  
 All submissions will be peer-reviewed and evaluated on the basis of originality, relevance, quality, and technical, sociological, or creative contribution. Research papers will be reviewed according to their relevance, method, and scientific rigour. Art abstracts will be reviewed according to their significance, originality and use of Web creativity.  
 Please carefully verify that when submitting your contribution, that it is related to creativity and the Web. Ensure the connection to a topic or topics within the track is evident on the first page of your paper. This is important because your paper will be assessed early on regarding its relevance to the conference and its relevance to the track-specific CFP of the track that you submitted to, and will be rejected with minimal feedback (“desk rejected”) if it does not fit that track.  
 Formatting the submissions  
 Submissions must adhere to the ACM template and format published in the ACM guidelines at https://www.acm.org/publications/proceedings-template. Please remember to add Concepts and Keywords. Please use the template in traditional double-column format to prepare your submissions. For example, word users may use Word Interim Template, and LaTeX users may use sample-sigconf template.  
 For overleaf users, you may want to use  
 https://www.overleaf.com/latex/templates/association-for-computing-machinery-acm-sig-proceedings-template/bmvfhcdnxfty.  Submissions for review must be in PDF format. They must be self-contained and written in English. Authors are requested to use spell checking and grammar checking tools. Submissions will be subjected to plagiarism-checking software.  
 Publication policy  
 Accepted papers will require a further revision in order to meet the requirements and page limits of the camera-ready format required by ACM. Instructions for the preparation of the camera-ready versions of the papers will be provided after acceptance.  
  All accepted papers will be published by ACM in the conference proceedings together with the research track papers and will be available via the ACM Digital Library. To be included in the proceedings, at least one author of each accepted paper must register for the conference and present the paper there.  
 Important dates  
 Full paper: 6 February 2023 
  Acceptance notification: 6 March 2023 
  Camera-ready version: 20 March 2023 
  All submission deadlines are end-of-day in the Anywhere on Earth (AoE) time zone  

  The Web Conference @ Austin  
 The Web Conference 2023 is organized by the School of Information at The University of Texas at Austin

68. IMC_1 conference:
Toggle navigation      IMC 2023    
 Home 
  Calls | Call for Papers 
  Call for Posters 
  Call for Travel Grant Applicants 
  Submission Instructions 
  Presentation Guidelines 
  Paper Anonymity 
  Committees 
  Local Info | Conference Venue 
  Conference Dinners 
  Accommodation 
  Visa 
  Policy Against Discrimination and Harassment 
  Program | Schedule 
  Accepted Posters 
  Registration 
  Sponsorship 
  Side Events | Hackathon 

 Call For Papers, ACM IMC 2023  
 The Internet Measurement Conference (IMC) is a highly selective venue for the presentation of measurement-based research in data communications. As we are in the era of data-driven research, IMC 2023 will focus on improving the standard in the collection, usage, and sharing of network measurements for the research community. Despite the efforts in stimulating reproducibility of research as well as sharing of data, little progress has been made in our community to make research data open. Therefore, without fundamentally changing the topics in scope compared to previous years, our attention when assessing contributions will be particularly on the willingness of the authors to share their data and make their work reproducible.  
 New this year:  To encourage data sharing and reproducibility, authors will be required to make a declaration on artifact availability (full, partial, or no availability) for the submitted work. Since legitimate reasons (such as proprietary and privacy reasons) may prevent authors from sharing artifacts, papers will be assessed based on whether the contributions warrant acceptance despite the lack of artifact availability. In the case of no availability of artifacts, the authors are expected to explain why this is the case in a specific section. Artifact submission is not required at the paper submission time. All papers accepted to the program will be shepherded to ensure that the artifacts promised have been made available.  
 IMC takes a broad view of contributions that are considered in scope for improving the practice of network measurement, including, but not limited to:  
 collection and analysis of data that yield new insights about network structure and network performance (e.g., traffic, topology, routing, energy utilization, performance) 
  collection and analysis of data that yield new insights about application and end-user behavior (e.g., economics, privacy, security, application interaction with protocols) 
  measurement-based modeling (e.g., workloads, scaling behavior, assessment of performance bottlenecks, causality) 
  methods and tools to monitor and visualize network-based phenomena 
  systems and algorithms that build on measurement-based findings 
  Theoretical analysis and modeling of networked-systems and measurement techniques 
  Novel methods for data collection, analysis, and storage (e.g., anonymization, querying, sharing) 
  reappraisal of previous empirical network measurements and measurement-based conclusions 
  descriptions of challenges and future directions the measurement community should pursue 
  Networks of interest include:  
 Internet transit networks 
  edge networks, including home networks, broadband access networks (e.g., cable, fiber), and cellular networks 
  data center networks and cloud computing infrastructure 
  peer-to-peer, overlay, and content distribution networks 
  software-defined networks 
  online social networks 
  online services, platforms, and content providers 
  experimental networks, prototype networks, and future internetworks 
  Replicability Track:   
 IMC 2023 will trial a new Replicability Track for submissions that aim to reproduce or replicate results that have been previously published at IMC. Papers accepted to this track will be published in ACM SIGCOMM Computer Communication Review (CCR). Priority will be given to replicability studies, although reproducibility studies are also in scope. For the definitions, please see ACM’s site  . The authors of outstanding replicability papers may receive an invitation to present at the main conference. In that case, the paper would also be included in IMC’s proceedings (rather than CCR).  
 Submissions to this track are two-phase. Prospective authors are invited to submit an Expression of Interest (EoI) via the submission system in the form of an abstract which must explain:  
 Which paper the authors aim to replicate 
  Whether the paper will be replicated or reproduced 
  What the IMC community stands to learn from the replication or reproduction 
  Chosen approach, and why it will lead to new insights 
  A small committee will evaluate the EoIs and their potential to be of interest to the IMC community. The authors of strong abstracts will receive an invitation to a full submission.  
 The EoI serves to avoid misunderstandings and disappointment for authors as we acknowledge that replicating or reproducing a paper is a very significant effort to which potential authors would commit much time.  
 Full submissions will then be assessed by a sub-committee of the TPC. Full submissions must otherwise conform to the same criteria and rules as full submissions on the main track (see below).  
 See the Important Dates section  for the EoI deadline. Full submissions have the same deadlines (abstract registration and full submission) as the IMC main deadline.  
 Review process and criteria  
 IMC 2023 invites two forms of submissions:  
 Full papers | (up to 13 pages for text and figures + unlimited pages for references and appendix) that describe original research, with succinctness appropriate to the topics and themes they discuss. 
  Short papers | (up to 6 pages for text and figures + unlimited pages for references and appendix) that convey work that is less mature but shows exciting promise, OR offer results that do not merit a full submission. Short papers could articulate a high-level vision and describe challenging future directions that the authors believe the community should tackle; validate, verify, or update important results; or present new ideas that challenge existing assumptions. 
  Any submission exceeding the short paper page-length limit will be evaluated as a full paper.  
 Authors should submit only original work that has not been published before and is not under submission to any other venue. We will consider full paper submissions that extend previously published short, preliminary papers (including IMC short papers), in accordance with the SIGCOMM policy  and the ACM Plagiarism Policy  . The ACM policy on simultaneous submissions  does not consider technical reports (including arXiv) to be concurrent publication or submission.  
 IMC 2023 will bestow two awards on paper submissions, (1) a Best Paper award; and (2) a Community Contribution award. The best paper award will recognize the outstanding paper at the conference, and all accepted papers are eligible for it. The community contribution award will recognize a paper with an outstanding contribution to the community in the form of a new dataset, source code distribution, open platform, or other noteworthy service to the community. To be eligible for the community award, the authors must make data or source code publicly available or have a software artifact that is accessible and usable by the public at the time of the camera-ready deadline. The authors indicate their eligibility on the submission form and are also encouraged to include a link to the contribution in the submitted paper.  
 A few accepted papers may be forwarded for fast-track submission to IEEE/ACM Transactions on Networking.  
 Early Rejection  
 The review process will have several reviewing rounds. To allow authors time to improve their work and submit to other venues, authors of submissions for which there is a consensus on rejection will be notified early.  
 Anonymity Guidelines  
 Authors are expected to make a good-faith effort to anonymize papers. As an author, you should not identify yourself in the paper either explicitly or by implication (e.g., through the references).  
 Anonymization of important details are not required, if they are critical for the evaluation of the paper. For example, system names may be left de-anonymized, if the system name is important for a reviewer to be able to evaluate the work, and the paper may point to existing public datasets and artifacts that highlight or underscore the contributions outlined in the paper.  
 Please follow the following guidelines:  
 Author names and affiliations must not appear on any submission. 
  Identifying information such as grant numbers must not be included on submissions. 
  The text of the submission must refer to the authors’ own previous work in the third person, unless the previous work provides important context for evaluation of the existing contribution (e.g., the paper describes an implementation or deployment of a method previously developed by the same authors). 
  Authors are explicitly allowed to post their submissions on Arxiv or other public websites, and reviewers will be discouraged from searching for such listings while a submission is under review. However, authors are strongly discouraged from engaging in publicity for their papers while they are under review, unless doing so is in the public interest (e.g., responsible disclosure). Authors who are unsure of whether they are allowed to publicize their double-blind submissions should contact the program committee co-chairs at imc2023pcchairs@acm.org  .  
 Additional details on paper anonymity are available here  and in the statement released by the IMC Steering Committee here  .  
 Ethics  
 The program committee may raise concerns around the ethics of the work, even if it does not involve human subjects. All papers must include, in a clearly marked appendix section with the heading “Ethics”, a statement about ethical issues; papers that do not include such a statement may be rejected.  This could be, if appropriate for the paper, simply the sentence “This work does not raise any ethical issues.”. If the work involves human subjects or potentially sensitive data (e.g., user traffic or social network information, evaluation of censorship, etc.), the paper should clearly discuss these issues, perhaps in a separate subsection.  
 Research that entails experiments involving human subjects or user data (e.g., network traffic, passwords, social network information) should adhere to community norms. Any work that raises potential ethics considerations should indicate this on the submission form. The basic principles of ethical research are outlined in the Belmont Report  : (1) respect for persons (which may involve obtaining consent); (2) beneficence (a careful consideration of risks and benefits); and (3) justice (ensuring that parts of the population that bear the risks of the research also are poised to obtain some benefit from it). Authors should further consult the ACM policy on research involving human subjects  for further information on ethical principles that apply to this conference.  
 Research involving human subjects must be approved by the researchers’ respective Institutional Review Boards before the research takes place. Authors should indicate on the submission form whether the work involves human subjects. If so, the authors must indicate whether an IRB protocol has been approved for the research, or if the research has been determined exempt (either self-determination or IRB determination). We expect that any research follows the practices and procedures of the institution(s) where the work is being carried out; for example, some universities require separate approval for the use of campus data. We expect researchers to abide by these protocols.  
 We recognize that different IRBs follow different procedures for determining the status of human subject research, and approval or exempt status from a single institution may not align with community norms. To help the Ethics Committee review cases of concern, there is a need for more information about the research protocol. To this end, if the work involves human subjects, the authors must include with their submission a copy of the form that was used to determine IRB status (approved or exempt), sufficiently anonymized to preserve double-blind review.  
 If the submission describes research involving human subjects and none of the authors are at an institution with an IRB (or equivalent), the authors are nonetheless expected to follow a research protocol that adheres to ethical principles, as stated in the ACM policy on research involving human subjects  . In such cases, the authors must use the Ethics section of their appendix to explain how their research protocol satisfies the principles of ethical research.  
 Some research does not involve human subjects yet nonetheless raises questions of ethics, which may be wide-ranging and not necessarily limited to direct effects. We encourage authors to be mindful of the ethics of the research that they undertake; these considerations are often not clear-cut, but often warrant thoughtful consideration. Discussions of these issues should be placed in the “Ethics” appendix section mentioned above, or in the main body of the paper where appropriate.  
 Additionally, the program committee reserves the right to conduct additional evaluations and reviews of research ethics and reserves the right to independent judgment concerning the ethics of the conducted research.  
 Contact the program committee co-chairs at imc2023pcchairs@acm.org  if you have any questions.  
 Authorship policy  
 The ACM Publications Board has recently updated the ACM Policy on Authorship  in several ways:  
 Addressing the use of generative AI systems in the publications process 
  Clarifying criteria for authorship and the responsibilities of authors 
  Defining prohibited behavior, such as gift, ghost, or purchased authorship 
  Providing a linked FAQ explaining the rationale for the policy and providing additional details 
  Please refer to the updated ACM Policy on Authorship  .  
 Submission guidelines  
 Below is a summary of key submission guidelines, but it is not exhaustive. For a complete list of requirements, visit the detailed submission instructions page  . Authors are expected to comply with all the requirements listed on that page.   
 All submissions must satisfy the following requirements:  
 Full papers: | up to 13 pages for technical content + unlimited pages for references and appendix 
  Short papers: | up to 6 pages for technical content + unlimited pages for references and appendix 
  Note that, apart from the mandatory Ethics appendix, reviewers have no obligation to read anything in the appendices. Please use this space judiciously, and as a rule of thumb, keep it to no longer than three pages.  
 Formatting should be as follows:  
 10-point font for main text. Fonts used in other elements of the paper (e.g., figures) should be no smaller than 9 point. 
  Papers must be formatted for printing on Letter-sized (8.5” by 11”) paper. Paper text blocks must follow ACM guidelines: double-column, with each column 9.25” by 3.33”, 0.33” space between columns. Each column must use 10-point font or larger, and contain no more than 55 lines of text. 
  It is your responsibility to ensure that your submission satisfies the above requirements. If you are using LaTeX, you may make use of this template for ACM conference proceedings. With the older versions of this template you must add “10pt” to the documentclass command to meet the submission requirements. The current template sets 10pt by default. (Unlike the official template, it only includes an example for conference proceedings.) 
  Submissions that do not comply with these requirements will be rejected without review. The ACM template  style file satisfies the formatting requirements, provided you compile your source with options that produce letter page size and  10-point  fonts. The following settings in your LaTeX source should achieve that (but please verify the output):  
 \documentclass[10pt,sigconf,letterpaper,anonymous]{acmart}   As an example, we also provide a sample template for ACM conference proceedings  , which you can make use of.  
 Important Dates  
  
 Expression of Interest (papers for the Replicability Track) | Tuesday March 21, 2023 at 11:59 PM AoE (UTC-12) 
 Notification for the Replicability Track Expression of Interest | Thursday March 30, 2023 at 11:59PM AoE (UTC-12) 
 Paper registration (with abstract, including invited papers to Replicability Track) | Saturday May 20, 2023 at 11:59PM AoE (UTC-12) 
 Paper submission (including invited papers to Replicability Track) | Friday May 26, 2023 at 11:59PM AoE (UTC-12) 
 Early reject notification | Approximately Friday July 14, 2023 
 Notification | Friday August 18, 2023 
 Presentation submission | Friday October, 20 at 2:00PM CEST 
 Camera-ready due | TBA 
 CCR submission (papers on the replicability track only) | TBA 
 Conference | October 24 - 26, 2023 
  
 Submission Site  
 Please submit your paper at https://imc2023.hotcrp.com/  .  
 Please submit your Expression of Interest for the Replicability Track at https://imc2023-repro.hotcrp.com/  .  
  
 Last modified: Oct 26 2023   
 Automne au Parc du Mont-Royal  © Hussein Abdallah  , CC BY 2.0

69. IMC_2 conference:
IMC '23: Proceedings of the 2023 ACM on Internet Measurement Conference  
  Full Citation in the ACM Digital Library    
 SESSION: Replication  
 Replication: Towards a Publicly Available Internet Scale IP Geolocation Dataset   
 Omar Darwich 
  Hugo Rimlinger 
  Milo Dreyfus 
  Matthieu Gouel 
  Kevin Vermeulen 
  IP geolocation is one of the most widely used forms of metadata for IP addresses, and despite almost twenty years of effort from the research community, the reality is that there is no accurate, complete, up-to-date, and explainable publicly available dataset for IP geolocation. We argue that a central reason for this state of affairs is the impressive results from prior publications, both in terms of accuracy and coverage: up to street level accuracy and locating millions of IP addresses with a few hundred vantage points in months. We believe the community would substantially benefit from a public baseline dataset and code. To encourage future research in IP geolocation, we replicate two geolocation techniques and evaluate their accuracy and coverage. We show that we can neither use the first technique to obtain the previously claimed street level accuracy, nor the second to geolocate millions of IP addresses on today's Internet and with publicly available measurement infrastructure. In addition to this reappraisal, we re-evaluate the fundamental insights that led to these prior results, as well as provide new insights and recommendations to help the design of future geolocation techniques. All of our code and data are publicly available to support reproducibility.  

 Replication: 20 Years of Inferring Interdomain Routing Policies   
 Savvas Kastanakis 
  Vasileios Giotsas 
  Ioana Livadariu 
  Neeraj Suri 
  In 2003, Wang and Gao [67] presented an algorithm to infer and characterize routing policies as this knowledge could be valuable in predicting and debugging routing paths. They used their algorithm to measure the phenomenon of selectively announced prefixes, in which, ASes would announce their prefixes to specific providers to manipulate incoming traffic. Since 2003, the Internet has evolved from a hierarchical graph, to a flat and dense structure. Despite 20 years of extensive research since that seminal work, the impact of these topological changes on routing policies is still blurred.  
 In this paper we conduct a replicability study of the Wang and Gao paper [67], to shed light on the evolution and the current state of selectively announced prefixes. We show that selective announcements are persistent, not only across time, but also across networks. Moreover, we observe that neighbors of different AS relationships may be assigned with the same local preference values, and path selection is not as heavily dependent on AS relationships as it used to be. Our results highlight the need for BGP policy inference to be conducted as a high-periodicity process to account for the dynamic nature of AS connectivity and the derived policies.  

 Replication: "When to Use and When Not to Use BBR"   
 Soumyadeep Datta 
  Fraida Fund 
  We replicate the paper, "When to Use and When Not to Use BBR: An Empirical Analysis and Evaluation Study" by Cao et al, published in IMC 2019 [2], with a focus on the relative goodput of TCP BBR and TCP CUBIC for a range of bottleneck buffer sizes, bandwidths, and delays. We replicate the experiments performed by the original authors on two large-scale open-access testbeds, to validate the conclusions of the paper. We further extend the experiments to BBRv2. We package the experiment artifacts and make them publicly available so that others can repeat and build on this work.  

 Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation   
 Alessandro Finamore 
  Chao Wang 
  Jonatan Krolikowski 
  Jose M. Navarro 
  Fuxing Chen 
  Dario Rossi 
  Over the last years we witnessed a renewed interest toward Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) methods. Among those works, a recent study from IMC'22 [16] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-supervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [16] on the UCDAVIS, ISCXVPN and ISCXTOR datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called "flowpic'' (i.e., a per-flow 2d histograms of the packets size evolution over time).  
 In this paper (i) we reproduce[16] on the same datasets and (ii) we replicate its most salient aspect (the importance of data augmentation) on three additional public datasets (MIRAGEA, MIRAGEB and UTMOBILENET). While we confirm most of the original results, we also found a ≈ 20% accuracy drop on some of the investigated scenarios due to a data shift in the original dataset that we uncovered. Additionally, our study validates that the data augmentation strategies studied in[16] perform well on other datasets too. In the spirit of reproducibility and replicability we make all artifacts (code and data) available to the research community at https://tcbenchstack.github.io/tcbench/  .  

 SESSION: Routing  
 On the Importance of Being an AS: An Approach to Country-Level AS Rankings   
 Bradley Huffaker 
  Romain Fontugne 
  Alexander Marder 
  kc claffy 
  Recent geopolitical events demonstrate that control of Internet infrastructure in a region is critical to economic activity and defense against armed conflict. This geopolitical importance necessitates novel empirical techniques to assess which countries remain susceptible to degraded or severed Internet connectivity because they rely heavily on networks based in other nation states. Currently, two preeminent BGP-based methods exist to identify influential or market-dominant networks on a global scale-network-level customer cone size and path hegemony-but these metrics fail to capture regional or national differences.  
 We adapt the two global metrics to capture country-specific differences by restricting the input data for a country-specific metric to destination prefixes in that country. Although conceptually simple, our study required tackling methodological challenges common to most Internet measurement research today, such as geolocation, incomplete data, vantage point access, and lack of ground truth. Restricting public routing data to individual countries requires substantial downsampling compared to global analysis, and we analyze the impact of downsampling on the robustness and stability of our country-specific metrics. As a measure of validation, we apply our country-specific metrics to case studies of Australia, Japan, Russia, Taiwan, and the United States, illuminating aspects of concentration and interdependence in telecommunications markets. To support reproducibility, we will share our code, inferences, and data sets with other researchers.  

 Coarse-grained Inference of BGP Community Intent   
 Thomas Krenc 
  Matthew Luckie 
  Alexander Marder 
  kc claffy 
  BGP communities allow operators to influence routing decisions made by other networks (action communities) and to annotate their network's routing information with metadata such as where each route was learned or the relationship the network has with their neighbor (information communities). BGP communities also help researchers understand complex Internet routing behaviors. However, there is no standard convention for how operators assign community values, and significant efforts to scalably infer community meanings have ignored this high-level classification. We discovered that doing so comes at significant cost in accuracy, of both inference and validation. To advance this narrow but powerful direction in Internet infrastructure research, we design and validate an algorithm to execute this first fundamental step: inferring whether a BGP community is action or information. We applied our method to 78,480 community values observed in public BGP data for May 2023. Validating our inferences (24,376 action and 54,104 informational communities) against available ground truth (6,259 communities) we find that our method classified 96.5% correctly. We found that the precision of a state-of-the-art location community inference method increased from 68.2% to 94.8% with our classifications. We publicly share our code, dictionaries, inferences, and datasets to enable the community to benefit from them.  

 RoVista: Measuring and Analyzing the Route Origin Validation (ROV) in RPKI   
 Weitong Li 
  Zhexiao Lin 
  Md. Ishtiaq Ashiq 
  Emile Aben 
  Romain Fontugne 
  Amreesh Phokeer 
  Taejoong Chung 
  The Resource Public Key Infrastructure (RPKI) is a system to add security to the Internet routing. In recent years, the publication of Route Origin Authorization (ROA) objects, which bind IP prefixes to their legitimate origin ASN, has been rapidly increasing. However, ROAs are effective only if the routers use them to verify and filter invalid BGP announcements, a process called Route Origin Validation (ROV).  
 There are many proposed approaches to measure the status of ROV in the wild, but they are limited in scalability or accuracy. In this paper, we present RoVista, an ROV measurement framework that leverages IP-ID side channel and in-the-wild RPKI-invalid prefix. With over 20 months of longitudinal measurement, RoVista successfully covers more than 28K ASes where 63.8% of ASes have derived benefits from ROV, although the percentage of fully protected ASes remains relatively low at 12.3%. In order to validate our findings, we have also sought input from network operators.  
 We then evaluate the security impact of current ROV deployment and reveal misconfigurations that will weaken the protection of ROV. Lastly, we compare RoVista with other approaches and conclude with a discussion of our findings and limitations.  

 Illuminating Router Vendor Diversity Within Providers and Along Network Paths   
 Taha Albakour 
  Oliver Gasser 
  Robert Beverly 
  Georgios Smaragdakis 
  The Internet architecture has facilitated a multi-party, distributed, and heterogeneous physical infrastructure where routers from different vendors connect and inter-operate via IP. Such vendor heterogeneity can have important security and policy implications. For example, a security vulnerability may be specific to a particular vendor and implementation, and thus will have a disproportionate impact on particular networks and paths if exploited. From a policy perspective, governments are now explicitly banning particular vendors-or have threatened to do so.  
 Despite these critical issues, the composition of router vendors across the Internet remains largely opaque. Remotely identifying router vendors is challenging due to their strict security posture, indistinguishability due to code sharing across vendors, and noise due to vendor mergers. We make progress in overcoming these challenges by developing LFP, a tool that improves the coverage, accuracy, and efficiency of router fingerprinting as compared to the current state-of-the-art. We leverage LFP to characterize the degree of router vendor homogeneity within networks and the regional distribution of vendors. We then take a path-centric view and apply LFP to better understand the potential for correlated failures and fate-sharing. Finally, we perform a case study on inter and intra-United States data paths to explore the feasibility to make vendor-based routing policy decisions, i.e., whether it is possible to avoid a particular vendor given the current infrastructure.  

 IRRegularities in the Internet Routing Registry   
 Ben Du 
  Katherine Izhikevich 
  Sumanth Rao 
  Guatam Akiwate 
  Cecilia Testart 
  Alex C. Snoeren 
  kc claffy 
  The Internet Routing Registry (IRR) is a set of distributed databases used by networks to register routing policy information and to validate messages received in the Border Gateway Protocol (BGP). First deployed in the 1990s, the IRR remains the most widely used database for routing security purposes, despite the existence of more recent and more secure alternatives. Yet, the IRR lacks a strict validation standard and the limited coordination across different database providers can lead to inaccuracies. Moreover, it has been reported that attackers have begun to register false records in the IRR to bypass operators' defenses when launching attacks on the Internet routing system, such as BGP hijacks. In this paper, we provide a longitudinal analysis of the IRR over the span of 1.5 years. We develop a workflow to identify irregular IRR records that contain conflicting information compared to different routing data sources. We identify 34,199 irregular route objects out of 1,542,724 route objects from November 2021 to May 2023 in the largest IRR database and find 6,373 to be potentially suspicious.  

 SESSION: Web 1  
 Flocking to Mastodon: Tracking the Great Twitter Migration   
 Jiahui He 
  Haris Bin Zia 
  Ignacio Castro 
  Aravindh Raman 
  Nishanth Sastry 
  Gareth Tyson 
  The acquisition of Twitter by Elon Musk has spurred controversy and uncertainty among Twitter users. The move raised both praise and concerns, particularly regarding Musk's views on free speech. As a result, a large number of Twitter users have looked for alternatives to Twitter. Mastodon, a decentralized micro-blogging social network, has attracted the attention of many users and the general media. In this paper, we analyze the migration of 136,009 users from Twitter to Mastodon. We inspect the impact that this has on the wider Mastodon ecosystem, particularly in terms of user-driven pressure towards centralization. We further explore factors that influence users to migrate, highlighting the effect of users' social networks. Finally, we inspect the behavior of individual users, showing how they utilize both Twitter and Mastodon in parallel. We find a clear difference in the topics discussed on the two platforms. This leads us to build classifiers to explore if migration is predictable. Through feature analysis, we find that the content of tweets as well as the number of URLs, the number of likes, and the length of tweets are effective metrics for the prediction of user migration.  

 The Prevalence of Single Sign-On on the Web: Towards the Next Generation of Web Content Measurement   
 Calvin Ardi 
  Matt Calder 
  Much of the content and structure of the Web remains inaccessible to evaluate at scale because it is gated by user authentication. This limitation restricts researchers to examining only a superficial layer of a website: the landing page or public, search-indexable pages. Since it is infeasible to create individual accounts across thousands of webpages, we examine the prevalence of Single Sign-On (SSO) on the web to explore the feasibility of using a few accounts to authenticate to many sites. We find that 58% of the top 10K websites with logins are accessible with popular 3rd-party SSO providers, such as Google, Facebook, and Apple, indicating that leveraging SSO offers a scalable solution to access a large volume of user-gated content.  

 Reviving Dead Links on the Web with Fable   
 Jingyuan Zhu 
  Anish Nyayachavadi 
  Jiangchen Zhu 
  Vaspol Ruamviboonsuk 
  Harsha V. Madhyastha 
  The web is littered with millions of links which previously worked but no longer do. When users encounter any such broken link, they resort to looking up an archived copy of the linked page. But, for a sizeable fraction of these broken links, no archived copies exist. Even if a copy exists, it often poorly approximates the original page, e.g., any functionality on the page which requires the client browser to communicate with the page's backend servers will not work, and even the latest copy will be missing updates made to the page's content after that copy was captured.  
 To address this situation, we observe that broken links are often merely a result of website reorganizations; the linked page still exists on the same site, albeit at a different URL. Therefore, given a broken link, our system FABLE attempts to find the linked page's new URL by learning and exploiting the pattern in how the old URLs for other pages on the same site have transformed to their new URLs. We show that our approach is significantly more accurate and efficient than prior approaches which rely on stability in page content over time. FABLE increases the fraction of dead links for which the corresponding new URLs can be found by 50%, while reducing the median delay incurred in identifying the new URL for a broken link from over 40 seconds to less than 10 seconds.  

 Demystifying Web-based Mobile Extended Reality Accelerated by WebAssembly   
 Kaiyan Liu 
  Nan Wu 
  Bo Han 
  By combining various emerging technologies, mobile extended reality (XR) blends the real world with virtual content to create a spectrum of immersive experiences. Although Web-based XR can offer attractive features such as better accessibility, cross-platform compatibility, and instant updates, its performance may not be on par with its standalone counterpart. As a low-level bytecode, WebAssembly has the potential to drastically accelerate Web-based XR by enabling near-native execution speed. However, little has been known about how well Web-based XR performs with WebAssembly acceleration. To bridge this crucial gap, we conduct a first-of-its-kind systematic and empirical study to analyze the performance of Web-based XR expedited by WebAssembly on four diverse platforms with five different browsers. Our measurement results reveal that although WebAssemlby can accelerate different XR tasks in various contexts, there remains a substantial performance disparity between Web-based and standalone XR. We hope our findings can foster the realization of an immersive Web that is accessible to a wider audience with various emerging technologies.  

 SESSION: Web 2  
 Thou Shalt Not Reject: Analyzing Accept-Or-Pay Cookie Banners on the Web   
 Ali Rasaii 
  Devashish Gosain 
  Oliver Gasser 
  Privacy regulations have led to many websites showing cookie banners to their users. Usually, cookie banners present the user with the option to "accept" or "reject" cookies. Recently, a new form of paywall-like cookie banner has taken hold on the Web, giving users the option to either accept cookies (and consequently user tracking) or buy a paid subscription for a tracking-free website experience.  
 In this paper, we perform the first completely automated analysis of cookiewalls, i.e., cookie banners acting as a paywall. We find cookiewalls on 0.6% of all queried 45k websites. Moreover, cookiewalls are deployed to a large degree on European websites, e.g., for Germany we see cookiewalls on 8.5% of top 1k websites. Additionally, websites using cookiewalls send 6.4 times more third-party cookies and 42 times more tracking cookies to visitors, compared to regular cookie banner websites. We also uncover two large subscription Management Platforms used on hundreds of websites, which provide website operators with easy-to-setup cookiewall solutions. Finally, we publish tools, data, and code to foster reproducibility and further studies.  

 A Longitudinal Study of Vulnerable Client-side Resources and Web Developers' Updating Behaviors   
 Kyungchan Lim 
  Yonghwi Kwon 
  Doowon Kim 
  Modern Websites rely on various client-side web resources, such as JavaScript libraries, to provide end-users with rich and interactive web experiences. Unfortunately, anecdotal evidence shows that improperly managed client-side resources could open up attack surfaces that adversaries can exploit. However, there is still a lack of a comprehensive understanding of the updating practices among web developers and the potential impact of inaccuracies in Common Vulnerabilities and Exposures (CVE) information on the security of the web ecosystem. In this paper, we conduct a longitudinal (four-year) measurement study of the security practices and implications on client-side resources (e.g., JavaScript libraries and Adobe Flash) across the Web. Specifically, we first collect a large-scale dataset of 157.2M webpages of Alexa Top 1M websites for four years in the wild. Analyzing the dataset, we find an average of 41.2% of websites (in each year of the four years) carry at least one vulnerable client-side resource (e.g., JavaScript or Adobe Flash). We also reveal that vulnerable JavaScript library versions are frequently observed in the wild, suggesting a concerning level of lagging update practice in the wild. On average, we observe 531.2 days with 25,337 websites of the window of vulnerability due to the unpatched client-side resources from the release of security patches. Furthermore, we manually investigate the fidelity of CVE (Common Vulnerabilities and Exposures) reports on client-side resources, leveraging PoC (Proof of Concept) code. We find that 13 CVE reports (out of 27) have incorrect vulnerable version information, which may impact security-related tasks such as security updates.  

 Not only E.T. Phones Home: Analysing the Native User Tracking of Mobile Browsers   
 John Pegioudis 
  Emmanouil Papadogiannakis 
  Nicolas Kourtellis 
  Evangelos P. Markatos 
  Panagiotis Papadopoulos 
  Contemporary browsers constitute a critical component of our everyday interactions with the Web. Similar to a small, but powerful operating system, a browser is responsible to fetch and run web apps locally, on the user's (mobile) device. Even though in the last few years, there has been an increased interest for tools and mechanisms to block potentially malicious behaviours of web domains against the users' privacy (e.g., ad blockers, incognito browsing mode, etc.), it is still unclear if the user can browse the Web in private.  
 In this paper, we analyse the natively generated network traffic of 15 mobile browser apps under different configurations to investigate if the users are capable of browsing the Web privately, without sharing their browsing history with remote servers. We develop a novel framework (Panoptes) to instrument and monitor separately the mobile browser traffic generated by (a) the web engine and (b) natively by the mobile app. By crawling a set of websites via Panoptes, and analyzing the native traffic of browsers, we find that there are browsers (i) who persistently track their users, and (ii) browsers that report to remote servers (geolocated outside EU), the exact page and content the user is browsing at that moment. Finally, we see browsers communicating with third-party ad servers while leaking personal and device identifiers.  

 SESSION: Security 1  
 Wolf in Sheep's Clothing: Evaluating Security Risks of the Undelegated Record on DNS Hosting Services   
 Fenglu Zhang 
  Yunyi Zhang 
  Baojun Liu 
  Eihal Alowaisheq 
  Lingyun Ying 
  Xiang Li 
  Zaifeng Zhang 
  Ying Liu 
  Haixin Duan 
  Min Zhang 
  Leveraging DNS for covert communications is appealing since most networks allow DNS traffic, especially the ones directed toward renowned DNS hosting services. Unfortunately, most DNS hosting services overlook domain ownership verification, enabling miscreants to host undelegated DNS records of a domain they do not own. Consequently, miscreants can conduct covert communication through such undelegated records for whitelisted domains on reputable hosting providers. In this paper, we shed light on the emerging threat posed by undelegated records and demonstrate their exploitation in the wild. To the best of our knowledge, this security risk has not been studied before.  
 We conducted a comprehensive measurement to reveal the prevalence of the risk. In total, we observed 1,580,925 unique undelegated records that are potentially abused. We further observed that a considerable portion of these records are associated with malicious behaviors. By utilizing threat intelligence and malicious traffic collected by malware sandbox, we extracted malicious IP addresses from 25.41% of these records, spanning 1,369 Tranco top 2K domains and 248 DNS hosting providers, including Cloudflare and Amazon. Furthermore, we discovered that the majority of the identified malicious activities are Trojan-related. Moreover, we conducted case studies on two malware families (Dark.IOT and Specter) that exploit undelegated records to obtain C2 servers, in addition to the masquerading SPF records to conceal SMTP-based covert communication. Also, we provided mitigation options for different entities. As a result of our disclosure, several popular hosting providers have taken action to address this issue.  

 Dial "N" for NXDomain: The Scale, Origin, and Security Implications of DNS Queries to Non-Existent Domains   
 Guannan Liu 
  Lin Jin 
  Shuai Hao 
  Yubao Zhang 
  Daiping Liu 
  Angelos Stavrou 
  Haining Wang 
  Non-Existent Domain (NXDomain) is one type of the Domain Name System (DNS) error responses, indicating that the queried domain name does not exist and cannot be resolved. Unfortunately, little research has focused on understanding why and how NXDomain responses are generated, utilized, and exploited. In this paper, we conduct the first comprehensive and systematic study on NXDomain by investigating its scale, origin, and security implications. Utilizing a large-scale passive DNS database, we identify 146,363,745,785 NXDomains queried by DNS users between 2014 and 2022. Within these 146 billion NXDomains, 91 million of them hold historic WHOIS records, of which 5.3 million are identified as malicious domains including about 2.4 million blocklisted domains, 2.8 million DGA (Domain Generation Algorithms) based domains, and 90 thousand squatting domains targeting popular domains. To gain more insights into the usage patterns and security risks of NXDomains, we register 19 carefully selected NXDomains in the DNS database, each of which received more than ten thousand DNS queries per month. We then deploy a honeypot for our registered domains and collect 5,925,311 incoming queries for 6 months, from which we discover that 5,186,858 and 505,238 queries are generated from automated processes and web crawlers, respectively. Finally, we perform extensive traffic analysis on our collected data and reveal that NXDomains can be misused for various purposes, including botnet takeover, malicious file injection, and residue trust exploitation.  

 Extended DNS Errors: Unlocking the Full Potential of DNS Troubleshooting   
 Yevheniya Nosyk 
  Maciej Korczyński 
  Andrzej Duda 
  The Domain Name System (DNS) relies on response codes to confirm successful transactions or indicate anomalies. Yet, the codes are not sufficiently fine-grained to pinpoint the root causes of resolution failures. RFC~8914 (Extended DNS Errors or EDE) addresses the problem by defining a new extensible registry of error codes to be served inside the OPT resource record. In this paper, we show that four major DNS resolver vendors and three large public DNS resolvers support this standard and correctly narrow down the cause of underlying problems. Yet, they do not agree in 94% of our test cases in terms of the returned EDE codes. We reveal that Cloudflare DNS is the most precise in indicating various DNS misconfigurations via the EDE mechanism, so we use it to perform a large-scale analysis of more than 303M registered domain names. We show that 17.7M of them trigger EDE codes. Lame delegations and DNSSEC validation failures are the most common problems encountered.  

 Stale TLS Certificates: Investigating Precarious Third-Party Access to Valid TLS Keys   
 Zane Ma 
  Aaron Faulkenberry 
  Thomas Papastergiou 
  Zakir Durumeric 
  Michael D. Bailey 
  Angelos D. Keromytis 
  Fabian Monrose 
  Manos Antonakakis 
  Certificate authorities enable TLS server authentication by generating certificates that attest to the mapping between a domain name and a cryptographic keypair, for up to 398 days. This static, name-to-key caching mechanism belies a complex reality: a tangle of dynamic infrastructure involving domains, servers, cryptographic keys, etc. When any of these operations changes, the authentication information in a certificate becomes stale and no longer accurately reflects reality. In this work, we examine the broader phenomenon of certificate invalidation events and discover three classes of security-relevant events that enable a third-party to impersonate a domain outside of their control. Longitudinal measurement of these precarious scenarios reveals that they affect over 15K new domains per day, on average. Unfortunately, modern certificate revocation provides little recourse, so we examine the potential impact of reducing certificate lifetimes (cache duration): shortening the current 398-day limit to 90 days yields a 75% decrease in precarious access to valid TLS keys.  

 The CVE Wayback Machine: Measuring Coordinated Disclosure from Exploits against Two Years of Zero-Days   
 Eric Pauley 
  Paul Barford 
  Patrick McDaniel 
  Software security depends on coordinated vulnerability disclosure (CVD) from researchers, a process that the community has continually sought to measure and improve. Yet, CVD practices are only as effective as the data that informs them. In this paper, we use DScope, a cloud-based interactive Internet telescope, to build statistical models of vulnerability lifecycles, bridging the data gap in over 20 years of CVD research. By analyzing application-layer Internet scanning traffic over two years, we identify real-world exploitation timelines for 63 threats. We bring this data together with six additional datasets to build a complete birth-to-death model of these vulnerabilities, the most complete analysis of vulnerability lifecycles to date. Our analysis reaches three key recommendations: (1) CVD across diverse vendors shows lower effectiveness than previously thought, (2) intrusion detection systems are underutilized to provide protection for critical vulnerabilities, and (3) existing data sources of CVD can be augmented by novel approaches to Internet measurement. In this way, our vantage point offers new opportunities to improve the CVD process, achieving a safer software ecosystem in practice.  

 SESSION: Security 2  
 Re-measuring the Label Dynamics of Online Anti-Malware Engines from Millions of Samples   
 Jingjing Wang 
  Liu Wang 
  Feng Dong 
  Haoyu Wang 
  VirusTotal is the most widely used online scanning service in both academia and industry. However, it is known that the results returned by antivirus engines are often inconsistent and changing over time. The intrinsic dynamics of VirusTotal labeling have prompted researchers to investigate the characteristics of label dynamics for more effective use. However, they are generally limited in terms of the size and diversity of the datasets used in the measurements. This poses threats to many of their conclusions. In this paper, we perform an extraordinary large-scale study to re-measure the label dynamics of VirusTotal. Our dataset involves all the scan data in VirusTotal over a 14-month period, including over 571 million samples and 847 million reports in total. With this large dataset, we are able to revisit many issues related to the label dynamics of VirusTotal, including the prevalence of label dynamics/silence, the characteristics across file types, the impact of label dynamics on common label aggregation methods, the stabilization patterns of labels, etc. Our measurement reveals some observations that are unknown to the research community and even inconsistent with previous research. We believe that our findings could help researchers advance the understanding of the VirusTotal ecosystem.  

 Phishing in the Free Waters: A Study of Phishing Attacks Created using Free Website Building Services   
 Sayak Saha Roy 
  Unique Karanjit 
  Shirin Nilizadeh 
  Free Website Building services (FWBs) provide individuals with a cost-effective and convenient way to create a website without requiring advanced technical knowledge or coding skills. However, malicious actors often abuse these services to host phishing websites. In this work, we propose FreePhish, a scalable framework to continuously identify phishing websites that are created using FWBs. Using FreePhish, we were able to detect and characterize more than 31.4K phishing URLs that were created using 17 unique free website builder services and shared on Twitter and Facebook over a period of six months. We find that FWBs provide attackers with several features that make it easier to create and maintain phishing websites at scale while simultaneously evading anti-phishing countermeasures. Our study indicates that anti-phishing blocklists and browser protection tools have significantly lower coverage and high detection time against FWB phishing attacks when compared to regular (self-hosted) phishing websites. While our prompt disclosure of these attacks helped some FWBs to remove these attacks, we found several others who were slow at removal or did not remove them outright, with the same also being true for Twitter and Facebook. Finally, we also provide FreePhish as a free Chromium web extension that can be utilized to prevent end-users from accessing potential FWB-based phishing attacks.  

 Fifteen Months in the Life of a Honeyfarm   
 Cristian Munteanu 
  Said Jawad Saidi 
  Oliver Gasser 
  Georgios Smaragdakis 
  Anja Feldmann 
  Honeypots have been used for decades to detect, monitor, and understand attempts of unauthorized use of information systems. Previous studies focused on characterizing the spread of malware, e.g., Mirai and other attacks, or proposed stealthy and interactive architectures to improve honeypot efficiency.  
 In this paper, we present insights and benefits gained from collaborating with an operational honeyfarm, i.e., a set of honeypots distributed around the globe with centralized data collection. We analyze data of about 400 million sessions over a 15-month period, gathered from a globally distributed honeyfarm consisting of 221 honeypots deployed in 55 countries. Our analysis unveils stark differences among the activity seen by the honeypots-some are contacted millions of times while others only observe a few thousand sessions. We also analyze the behavior of scouters and intruders of these honeypots. Again, some honeypots report orders of magnitude more interactions with command execution than others. Still, diversity is needed since even if we focus on the honeypots with the highest visibility, they see only a small fraction of the intrusions, including only 5% of the files. Thus, although around 2% of intrusions are visible by most of the honeypots in our honeyfarm, the rest are only visible to a few. We conclude with a discussion of the findings of work.  

 Evolving Bots: The New Generation of Comment Bots and their Underlying Scam Campaigns in YouTube   
 Seung Ho Na 
  Sumin Cho 
  Seungwon Shin 
  This paper presents a pioneering investigation into a novel form of scam advertising method on YouTube, termed "social scam bots'' (SSBs). These bots have evolved to emulate benign user behavior by posting comments and engaging with other users, oftentimes appearing prominently among the top rated comments. We analyzed the YouTube video comments and proposed a method to identify SSBs and extract the underlying scam domains. Our study revealed 1,134 SSBs promoting 72 scam campaigns responsible for infecting 31.73% of crawled videos. Further investigation revealed that SSBs exhibit advances that surpass traditional bots. Notably, they targeted specific audience by aligning scam campaigns with related video content, effectively leveraging the YouTube recommendation algorithm. We monitored these SSBs over a period of six months, enabling us to evaluate the effectiveness of YouTube's mitigation efforts. We also uncovered various strategies they use to evade mitigation attempts, including a novel strategy called "self-engagement," aimed at boosting their comment ranking. By shedding light on the phenomenon of SSBs and their evolving tactics, our study aims to raise awareness and contribute to the prevention of these malicious actors, ultimately fostering a safer online platform.  

 Cloud Watching: Understanding Attacks Against Cloud-Hosted Services   
 Liz Izhikevich 
  Manda Tran 
  Michalis Kallitsis 
  Aurore Fass 
  Zakir Durumeric 
  Cloud computing has dramatically changed service deployment patterns. In this work, we analyze how attackers identify and target cloud services in contrast to traditional enterprise networks and network telescopes. Using a diverse set of cloud honeypots in 5 providers and 23 countries as well as 2 educational networks and 1 network telescope, we analyze how IP address assignment, geography, network, and service-port selection, influence what services are targeted in the cloud. We find that scanners that target cloud compute are selective: they avoid scanning networks without legitimate services and they discriminate between geographic regions. Further, attackers mine Internet-service search engines to find exploitable services and, in some cases, they avoid targeting IANA-assigned protocols, causing researchers to misclassify at least 15% of traffic on select ports. Based on our results, we derive recommendations for researchers and operators.  

 SESSION: Security 3  
 How to Operate a Meta-Telescope in your Spare Time   
 Daniel Wagner 
  Sahil Ashish Ranadive 
  Harm Griffioen 
  Michalis Kallitsis 
  Alberto Dainotti 
  Georgios Smaragdakis 
  Anja Feldmann 
  Unsolicited traffic sent to advertised network space that does not host active services provides insights about misconfigurations as well as potentially malicious activities, including the spread of Botnets, DDoS campaigns, and exploitation of vulnerabilities. Network telescopes have been used for many years to monitor such unsolicited traffic. Unfortunately, they are limi the available address space for such tasks and, thus, limited to specific geographic and/or network regions.  
 In this paper, we introduce a novel concept to broadly capture unsolicited Internet traffic, which we call a "meta-telescope". A meta-telescope is based on the intuition that, with the availability of appropriate vantage points, one can (i) infer which address blocks on the Internet are unused and (ii) capture traffic towards them-both without having control of such address blocks. From this intuition, we develop and evaluate a methodology for identifying unlikely to be used Internet address space and build a meta-telescope that has very desirable properties, such as broad coverage of dark space both in terms of size and topological placement. Such meta-telescope identifies and captures unsolicited traffic to more than 350k /24 blocks in more than 7k ASes. Through the analysis of background radiation towards these networks, we also highlight that unsolicited traffic differs by destination network/geographic region as well as by network type. Finally, we discuss our experience and challenges when operating a meta-telescope in the wild.  

 Lazy Gatekeepers: A Large-Scale Study on SPF Configuration in the Wild   
 Stefan Czybik 
  Micha Horlboge 
  Konrad Rieck 
  The Sender Policy Framework (SPF) is a basic mechanism for authorizing the use of domains in email. In combination with other mechanisms, it serves as a cornerstone for protecting users from forged senders. In this paper, we investigate the configuration of SPF across the Internet. To this end, we analyze SPF records from 12 million domains in the wild. Our analysis shows a growing adoption, with 56.5 % of the domains providing SPF records. However, we also uncover notable security issues: First, 2.9 % of the SPF records have errors, undefined content or ineffective rules, undermining the intended protection. Second, we observe a large number of very lax configurations. For example, 34.7 % of the domains allow emails to be sent from over 100 000 IP addresses. We explore the reasons for these loose policies and demonstrate that they facilitate email forgery. As a remedy, we derive recommendations for an adequate configuration and notify all operators of domains with misconfigured SPF records.  

 On the Similarity of Web Measurements Under Different Experimental Setups   
 Nurullah Demir 
  Jan Hörnemann 
  Matteo Große-Kampmann 
  Tobias Urban 
  Norbert Pohlmann 
  Thorsten Holz 
  Christian Wressnegger 
  Measurement studies are essential for research and industry alike better understand the Web's inner workings and help quantify specific phenomena. Performing such studies is demanding due to the dynamic nature and size of the Web. Designing and setting up an experiment is a complex task, and many factors might affect the results. However, while several works have independently observed differences in the outcome of an experiment (e.g., the number of observed trackers) based on the measurement setup, it is unclear what causes such deviations. This work investigates the reasons for these differences by visiting 1.7M webpages with five different measurement setups. Based on this investigation, we build 'dependency trees' for each page and cross-compare the nodes in the trees. The results show that the measured trees differ considerably, that the cause of differences can be attributed to specific nodes, and that even identical measurement setups can produce different results.  

 Understanding the Privacy Risks of Popular Search Engine Advertising Systems   
 Salim Chouaki 
  Oana Goga 
  Hamed Haddadi 
  Peter Snyder 
  We present the first extensive measurement of the privacy properties of the advertising systems used by privacy-focused search engines. We propose an automated methodology to study the impact of clicking on search ads on three popularprivate search engines which have advertising-based business models: StartPage, Qwant, and DuckDuckGo, and we compare them to two dominant data-harvesting ones: Google and Bing. We investigate the possibility of third parties tracking users when clicking on ads by analyzing first-party storage, redirection domain paths, and requests sent before, when, and after the clicks.  
 Our results show that privacy-focused search engines fail to protect users' privacy when clicking ads. Users' requests are sent through redirectors on 4% of ad clicks on Bing, 86% of ad clicks on Qwant, and 100% of ad clicks on Google, DuckDuckGo, and StartPage. Even worse, advertising systems collude with advertisers across all search engines by passing unique IDs to advertisers in most ad clicks. These IDs allow redirectors to aggregate users' activity on ads' destination websites in addition to the activity they record when users are redirected through them. Overall, we observe that both privacy-focused and traditional search engines engage in privacy-harming behaviors allowing cross-site tracking, even in privacy-enhanced browsers.  

 A First Look at the Privacy Harms of the Public Suffix List   
 Stephen McQuistin 
  Peter Snyder 
  Colin Perkins 
  Hamed Haddadi 
  Gareth Tyson 
  The public suffix list is a community-maintained list of rules that can be applied to domain names to determine how they should be grouped into logical organizations or companies. We present the first large-scale measurement study of how the public suffix list is used by open-source software on the Web and the privacy harm resulting from projects using outdated versions of the list. We measure how often developers include out-of-date versions of the public suffix list in their projects, how old included lists are, and estimate the real-world privacy harm with a model based on a large-scale crawl of the Web. We find that incorrect use of the public suffix list is common in open-source software, and that at least 43 open-source projects use hard-coded, outdated versions of the public suffix list. These include popular, security-focused projects, such as password managers and digital forensics tools. We also estimate that, because of these out-of-date lists, these projects make incorrect privacy decisions for 1313 effective top-level domains (eTLDs), affecting 50,750 domains, by extrapolating from data gathered by the HTTP Archive project.  

 SESSION: Distributed protocols  
 The Cloud Strikes Back: Investigating the Decentralization of IPFS   
 Leonhard Balduf 
  Maciej Korczyński 
  Onur Ascigil 
  Navin V. Keizer 
  George Pavlou 
  Björn Scheuermann 
  Michał Król 
  Interplanetary Filesystem (IPFS) is one of the largest peer-to-peer filesystems in operation. The network is the default storage layer for Web3 and is being presented as a solution to the centralization of the web. In this paper, we present a large-scale, multi-modal measurement study of the IPFS network. We analyze the topology, the traffic, the content providers and the entry points from the classical Internet. Our measurements show significant centralization in the IPFS network and a high share of nodes hosted in the cloud. We also shed light on the main stakeholders in the ecosystem. We discuss key challenges that might disrupt continuing efforts to decentralize the Web and highlight multiple properties that are creating pressures toward centralization.  

 Ethereum's Proposer-Builder Separation: Promises and Realities   
 Lioba Heimbach 
  Lucianna Kiffer 
  Christof Ferreira Torres 
  Roger Wattenhofer 
  With Ethereum's transition from Proof-of-Work to Proof-of-Stake in September 2022 came another paradigm shift, the Proposer-Builder Separation (PBS) scheme. PBS was introduced to decouple the roles of selecting and ordering transactions in a block (i.e., the builder), from those validating its contents and proposing the block to the network as the new head of the blockchain (i.e., the proposer). In this landscape, proposers are the validators in the Proof-of-Stake consensus protocol, while now relying on specialized block builders for creating blocks with the highest value for the proposer. Additionally, relays act as mediators between builders and proposers. We study PBS adoption and show that the current landscape exhibits significant centralization amongst the builders and relays. Further, we explore whether PBS effectively achieves its intended objectives of enabling hobbyist validators to maximize block profitability and preventing censorship. Our findings reveal that although PBS grants validators the opportunity to access optimized and competitive blocks, it tends to stimulate censorship rather than reduce it. Additionally, we demonstrate that relays do not consistently uphold their commitments and may prove unreliable. Specifically, proposers do not always receive the complete promised value, and the censorship or filtering capabilities pledged by relays exhibit significant gaps.  

 SESSION: IoT  
 BehavIoT: Measuring Smart Home IoT Behavior Using Network-Inferred Behavior Models   
 Tianrui Hu 
  Daniel J. Dubois 
  David Choffnes 
  Smart home IoT platforms are typically closed systems, meaning that there is poor visibility into device behavior. Understanding device behavior is important not only for determining whether devices are functioning as expected, but also can reveal implications for privacy (e.g., surreptitious audio/video recording), security (e.g., device compromise), and safety (e.g., denial of service on a baby monitor). While there has been some work on identifying devices and a handful of activities, an open question is what is the extent to which we can automatically model the entire behavior of an IoT deployment, and how it changes over time, without any privileged access to IoT devices or platform messages.  
 In this work, we demonstrate that the vast majority of IoT behavior can indeed be modeled, using a novel multi-dimensional approach that relies only on the (often encrypted) network traffic exchanged by IoT devices. Our key insight is that IoT behavior (including cross-device interactions) can often be captured using relatively simple models such as timers (for periodic behavior) and probabilistic state-machines (for user-initiated behavior and devices interactions) during a limited observation phase. We then propose deviation metrics that can identify when the behavior of an IoT device or an IoT system changes over time. Our models and metrics successfully identify several notable changes in our IoT deployment, including a camera that changed locations, network outages that impact connectivity, and device malfunctions.  

 In the Room Where It Happens: Characterizing Local Communication and Threats in Smart Homes   
 Aniketh Girish 
  Tianrui Hu 
  Vijay Prakash 
  Daniel J. Dubois 
  Srdjan Matic 
  Danny Yuxing Huang 
  Serge Egelman 
  Joel Reardon 
  Juan Tapiador 
  David Choffnes 
  Narseo Vallina-Rodriguez 
  The network communication between Internet of Things (IoT) devices on the same local network has significant implications for platform and device interoperability, security, privacy, and correctness. Yet, the analysis of local home Wi-Fi network traffic and its associated security and privacy threats have been largely ignored by prior literature, which typically focuses on studying the communication between IoT devices and cloud end-points, or detecting vulnerable IoT devices exposed to the Internet. In this paper, we present a comprehensive and empirical measurement study to shed light on the local communication within a smart home deployment and its threats. We use a unique combination of passive network traffic captures, protocol honeypots, dynamic mobile app analysis, and crowdsourced IoT data from participants to identify and analyze a wide range of device activities on the local network. We then analyze these datasets to characterize local network protocols, security and privacy threats associated with them. Our analysis reveals vulnerable devices, insecure use of network protocols, and sensitive data exposure by IoT devices. We provide evidence of how this information is exfiltrated to remote servers by mobile apps and third-party SDKs, potentially for household fingerprinting, surveillance and cross-device tracking. We make our datasets and analysis publicly available to support further research in this area.  

 Behind the Scenes: Uncovering TLS and Server Certificate Practice of IoT Device Vendors in the Wild   
 Hongying Dong 
  Hao Shu 
  Vijay Prakash 
  Yizhe Zhang 
  Muhammad Talha Paracha 
  David Choffnes 
  Santiago Torres-Arias 
  Danny Yuxing Huang 
  Yixin Sun 
  IoT devices are increasingly used in consumer homes. Despite recent works in characterizing IoT TLS usage for a limited number of in-lab devices, there exists a gap in quantitatively understanding TLS behaviors from devices in the wild and server-side certificate management.  
 To bridge this knowledge gap, we conduct a new measurement study by focusing on the practice of device vendors, through a crowdsourced dataset of network traffic from 2,014 real-world IoT devices across 721 global users. By quantifying the sharing of TLS fingerprints across vendors and across devices, we uncover the prevalent use of customized TLS libraries (i.e., not matched to any known TLS libraries) and potential security concerns resulting from co-located TLS stacks of different services. Furthermore, we present the first known study on server-side certificate management for servers contacted by IoT devices. Our study highlights potential concerns in the TLS/PKI practice by IoT device vendors. We aim to raise visibility for these issues and motivate vendors to improve security practice.  

 An LLM-based Framework for Fingerprinting Internet-connected Devices   
 Armin Sarabi 
  Tongxin Yin 
  Mingyan Liu 
  In this paper we propose the use of large language models (LLMs) for characterizing, clustering, and fingerprinting raw text obtained from network measurements. To this end, We first train a transformer-based masked language model, namely RoBERTa, on a dataset containing hundreds of millions of banners obtained from Internet-wide scans. We further fine-tune this model using a contrastive loss function (driven by domain knowledge) to produce temporally stable numerical representations (embeddings) that can be used out-of-the-box for downstream learning tasks. Our embeddings are robust, resilient to small random changes in the content of a banner, and maintain proximity between embeddings of similar hardware/software products. We further cluster HTTP banners using a density-based approach (HDBSCAN), and examine the obtained clusters to generate text-based fingerprints for the purpose of labeling raw scan data. We compare our fingerprints to Recog, an existing database of manually curated fingerprints, and show that we can identify new IoT devices and server products that were not previously captured by Recog. Our proposed methodology poses an important direction for future research by utilizing state-of-the-art language models to automatically analyze, interpret, and label the large amounts of data generated by Internet scans.  

 SESSION: Transport  
 Estimating WebRTC Video QoE Metrics Without Using Application Headers   
 Taveesh Sharma 
  Tarun Mangla 
  Arpit Gupta 
  Junchen Jiang 
  Nick Feamster 
  The increased use of video conferencing applications (VCAs) has made it critical to understand and support end-user quality of experience (QoE) by all stakeholders in the VCA ecosystem, especially network operators, who typically do not have direct access to client software. Existing VCA QoE estimation methods use passive measurements of application-level Real-time Transport Protocol (RTP) headers. However, a network operator does not always have access to RTP headers in all cases, particularly when VCAs use custom RTP protocols (e.g., Zoom) or due to system constraints (e.g., legacy measurement systems). Given this challenge, this paper considers the use of more standard features in the network traffic, namely, IP and UDP headers, to provide per-second estimates of key VCA QoE metrics such as frames rate and video resolution. We develop a method that uses machine learning with a combination of flow statistics (e.g., throughput) and features derived based on the mechanisms used by the VCAs to fragment video frames into packets. We evaluate our method for three prevalent VCAs running over WebRTC: Google Meet, Microsoft Teams, and Cisco Webex. Our evaluation consists of 54,696 seconds of VCA data collected from both (1), controlled in-lab network conditions, and (2) real-world networks from 15 households. We show that the ML-based approach yields similar accuracy compared to the RTP-based methods, despite using only IP/UDP data. For instance, we can estimate FPS within 2 FPS for up to 83.05% of one-second intervals in the real-world data, which is only 1.76% lower than using the application-level RTP headers.  

 PTPerf: On the Performance Evaluation of Tor Pluggable Transports   
 Zeya Umayya 
  Dhruv Malik 
  Devashish Gosain 
  Piyush Kumar Sharma 
  Tor, one of the most popular censorship circumvention systems, faces regular blocking attempts by censors. Thus, to facilitate access, it relies on "pluggable transports" (PTs) that disguise Tor's traffic and make it hard for the adversary to block Tor. However, these are not yet well studied and compared for the performance they provide to the users. Thus, we conduct a first comparative performance evaluation of a total of 12 PTs-the ones currently supported by the Tor project and those that can be integrated in the future.  
 Our results reveal multiple facets of the PT ecosystem. (1) PTs' download time significantly varies even under similar network conditions. (2) All PTs are not equally reliable. Thus, clients who regularly suffer censorship may falsely believe that such PTs are blocked. (3) PT performance depends on the underlying communication primitive. (4) PTs performance significantly depends on the website access method (browser or command-line). Surprisingly, for some PTs, website access time was even less than vanilla Tor.  
 Based on our findings from more than 1.25M measurements, we provide recommendations about selecting PTs and believe that our study can facilitate access for users who face censorship.  

 Containing the Cambrian Explosion in QUIC Congestion Control   
 Ayush Mishra 
  Ben Leong 
  Since its introduction in 2015, QUIC has seen rapid adoption and is set to be the default transport stack for HTTP3. Given that developers can now easily implement and deploy their own congestion control algorithms in the user space, there is an imminent risk of the proliferation of QUIC implementations of congestion control algorithms that no longer resemble their corresponding standard kernel implementations.  
 In this paper, we present the results of a comprehensive measurement study of the congestion control algorithm (CCA) implementations for 11 popular open-source QUIC stacks. We propose a new metric called Conformance-T that can help us identify the implementations with large deviations more accurately and also provide hints on how they can be modified to be more conformant to reference kernel implementations. Our results show that while most QUIC CCA implementations are conformant in shallow buffers, they become less conformant in deep buffers. In the process, we also identified five new QUIC implementations that had low conformance and demonstrated how low-conformance implementations can cause unfairness and subvert our expectations of how we expect different CCAs to interact. With the hints obtained from our new metric, we were able to identify implementation-level differences that led to the low conformance and derive the modifications required to improve conformance for three of them.  

 ECN with QUIC: Challenges in the Wild   
 Constantin Sander 
  Ike Kunze 
  Leo Blöcher 
  Mike Kosek 
  Klaus Wehrle 
  TCP and QUIC can both leverage ECN to avoid congestion loss and its retransmission overhead. However, both protocols require support of their remote endpoints and it took two decades since the initial standardization of ECN for TCP to reach 80% ECN support and more in the wild. In contrast, the QUIC standard mandates ECN support, but there are notable ambiguities that make it unclear if and how ECN can actually be used with QUIC on the Internet. Hence, in this paper, we analyze ECN support with QUIC in the wild: We conduct repeated measurements on more than 180 M domains to identify HTTP/3 websites and analyze the underlying QUIC connections w.r.t. ECN support. We only find 20% of QUIC hosts, providing 6% of HTTP/3 websites, to mirror client ECN codepoints. Yet, mirroring ECN is only half of what is required for ECN with QUIC, as QUIC validates mirrored ECN codepoints to detect network impairments: We observe that less than 2% of QUIC hosts, providing less than 0.3% of HTTP/3 websites, pass this validation. We identify possible root causes in content providers not supporting ECN via QUIC and network impairments hindering ECN. We thus also characterize ECN with QUIC distributedly to traverse other paths and discuss our results w.r.t. QUIC and ECN innovations beyond QUIC.  

 Does It Spin? On the Adoption and Use of QUIC's Spin Bit   
 Ike Kunze 
  Constantin Sander 
  Klaus Wehrle 
  Encrypted QUIC traffic complicates network management as traditional transport layer semantics can no longer be used for RTT or packet loss measurements. Addressing this challenge, QUIC includes an optional, carefully designed mechanism: the spin bit. While its capabilities have already been studied in test settings, its real-world usefulness and adoption are unknown. In this paper, we thus investigate the spin bit's deployment and utility on the web.  
 Analyzing our long-term measurements of more than 200 M domains, we find that the spin bit is enabled on ~10% of those with QUIC support and for ~50% / 60% of the underlying IPv4 / IPv6 hosts. The support is mainly driven by medium-sized cloud providers while most hyperscalers do not implement it. Assessing the utility of spin bit RTT measurements, the theoretical issue of reordering does not significantly manifest in our study and the spin bit provides accurate estimates for around 30.5% of connections using the mechanism, but drastically overestimates the RTT for another 51.7%. Overall, we conclude that the spin bit, even though an optional feature, indeed sees use in the wild and is able to provide reasonable RTT estimates for a solid share of QUIC connections, but requires solutions for making its measurements more robust.  

 SESSION: Tagging  
 I Tag, You Tag, Everybody Tags!   
 Hazem Ibrahim 
  Rohail Asim 
  Matteo Varvello 
  Yasir Zaki 
  Location tags are designed to track personal belongings. Nevertheless, there has been anecdotal evidence that location tags are also misused to stalk people. Tracking is achieved locally, e.g., via Bluetooth with a paired phone, and remotely, by piggybacking on location-reporting devices which come into proximity of a tag. This paper studies the performance of the two most popular location tags (Apple's AirTag and Samsung's SmartTag) through controlled experiments - with a known large distribution of location-reporting devices - as well as in-the-wild experiments - with no control on the number and kind of reporting devices encountered, thus emulating real-life use-cases. We find that both tags achieve similar performance, e.g., they are located 55% of the times in about 10 minutes within a 100~m radius. It follows that real time stalking to a precise location via location tags is impractical, even when both tags are concurrently deployed which achieves comparable accuracy in half the time. Nevertheless, half of a victim's exact movements can be backtracked accurately (10m error) with just a one-hour delay, which is still perilous information in the possession of a stalker.  

 Tracking, Profiling, and Ad Targeting in the Alexa Echo Smart Speaker Ecosystem   
 Umar Iqbal 
  Pouneh Nikkhah Bahrami 
  Rahmadi Trimananda 
  Hao Cui 
  Alexander Gamero-Garrido 
  Daniel J. Dubois 
  David Choffnes 
  Athina Markopoulou 
  Franziska Roesner 
  Zubair Shafiq 
  Smart speakers collect voice commands, which can be used to infer sensitive information about users. Given the potential for privacy harms, there is a need for greater transparency and control over the data collected, used, and shared by smart speaker platforms as well as third party skills supported on them. To bridge this gap, we build a framework to measure data collection, usage, and sharing by the smart speaker platforms. We apply our framework to the Amazon smart speaker ecosystem. Our results show that Amazon and third parties, including advertising and tracking services that are unique to the smart speaker ecosystem, collect smart speaker interaction data. We also find that Amazon processes smart speaker interaction data to infer user interests and uses those inferences to serve targeted ads to users. Smart speaker interaction also leads to ad targeting and as much as 30X higher bids in ad auctions, from third party advertisers. Finally, we find that Amazon's and third party skills' data practices are often not clearly disclosed in their policy documents.  

 Pushing Alias Resolution to the Limit   
 Taha Albakour 
  Oliver Gasser 
  Georgios Smaragdakis 
  In this paper, we show that utilizing multiple protocols offers a unique opportunity to improve IP alias resolution and dual-stack inference substantially. Our key observation is that prevalent protocols, e.g., SSH and BGP, reply to unsolicited requests with a set of values that can be combined to form a unique device identifier. More importantly, this is possible by just completing the TCP hand-shake. Our empirical study shows that utilizing readily available scans and our active measurements can double the discovered IPv4 alias sets and more than 30× the dual-stack sets compared to the state-of-the-art techniques. We provide insights into our method's accuracy and performance compared to popular techniques.  

 SESSION: Latency  
 Localizing Traffic Differentiation   
 Zeinab Shmeis 
  Muhammad Abdullah 
  Pavlos Nikolopoulos 
  Katerina Argyraki 
  David Choffnes 
  Phillipa Gill 
  Network neutrality is important for users, content providers, policymakers, and regulators interested in understanding how network providers differentiate performance. When determining whether a network differentiates against certain traffic, it is important to have strong evidence, especially given that traffic differentiation is illegal in certain countries. In prior work, WeHe detects differentiation via end-to-end throughput measurements between a client and server but does not isolate the network responsible for it. Differentiation can occur anywhere on the network path between endpoints; thus, further evidence is needed to attribute differentiation to a specific network. We present a system, WeHeY, built atop WeHe, that can localize traffic differentiation, i.e., obtain concrete evidence that the differentiation happened within the client's ISP. Our system builds on ideas from network performance tomography; the challenge we solve is that TCP congestion control creates an adversarial environment for performance tomography (because it can significantly reduce the performance correlation on which tomography fundamentally relies). We evaluate our system via measurements "in the wild,'' as well as in emulated scenarios with a wide-area testbed; we further explore its limits via simulations and show that it accurately localizes traffic differentiation across a wide range of network conditions. WeHeY's source code is publicly available athttps://nal-epfl.github.io/WeHeY.  

 Using Gaming Footage as a Source of Internet Latency Information   
 Catalina Alvarez 
  Katerina Argyraki 
  Keeping track of Internet latency is a classic measurement problem. Open measurement platforms like RIPE Atlas are a great solution, but they also face challenges: preventing network overload that may result from uncontrolled active measurements, and maintaining the involved devices, which are typically contributed by volunteers and non-profit organizations, and tend to lag behind the state of the art in terms of features and performance. We explore gaming footage as a new source of real-time, publicly available, passive latency measurements, which have the potential to complement open measurement platforms. We show that it is feasible to mine this source of information by presenting Tero, a system that continuously downloads gaming footage from the Twitch streaming platform, extracts latency measurements from it, and converts them to latency distributions per geographical location. Our data-sets and source code are publicly available at https://nal-epfl.github.io/tero-project  .  

 Inferring Changes in Daily Human Activity from Internet Response   
 Xiao Song 
  Guillermo Baltra 
  John Heidemann 
  Network traffic is often diurnal, with some networks peaking during the workday and many homes during evening streaming hours. Monitoring systems consider diurnal trends for capacity planning and anomaly detection. In this paper, we reverse this inference and use diurnal network trends and their absence to infer human activity. We draw on existing and new ICMP echo-request scans of more than 5.2M /24 IPv4 networks to identify diurnal trends in IP address responsiveness. Some of these networks are change-sensitive, with diurnal patterns correlating with human activity. We develop algorithms to clean this data, extract underlying trends from diurnal and weekly fluctuation, and detect changes in that activity. Although firewalls hide many networks, and Network Address Translation often hides human trends, we show about 168k to 330k (3.3-6.4% of the 5.2M) /24 IPv4 networks are change-sensitive. These blocks are spread globally, representing some of the most active 60% of 2 × 2° geographic gridcells, regions that include 98.5% of ping-responsive blocks. Finally, we detect interesting changes in human activity. Reusing existing data allows our new algorithm to identify changes, such as Work-from-Home due to the global reaction to the emergence of Covid-19 in 2020. We also see other changes in human activity, such as national holidays and government-mandated curfews. This ability to detect trends in human activity from the Internet data provides a new ability to understand our world, complementing other sources of public information such as news reports and wastewater virus observation.  

 SESSION: Cellular and mobile networks  
 Characterizing Mobile Service Demands at Indoor Cellular Networks   
 Stefanos Bakirtzis 
  André Felipe Zanella 
  Stefania Rubrichi 
  Cezary Ziemlicki 
  Zbigniew Smoreda 
  Ian Wassell 
  Jie Zhang 
  Marco Fiore 
  Indoor cellular networks (ICNs) are anticipated to become a principal component of 5G and beyond systems. ICNs aim at extending network coverage and enhancing users' quality of service and experience, consequently producing a substantial volume of traffic in the coming years. Despite the increasing importance that ICNs will have in cellular deployments, there is nowadays little understanding of the type of traffic demands that they serve. Our work contributes to closing that gap, by providing a first characterization of the usage of mobile services across more than 4, 500 cellular antennas deployed at over 1,000 indoor locations in a whole country. Our analysis reveals that ICNs inherently manifest a limited set of mobile application utilization profiles, which are not present in conventional outdoor macro base stations (BSs). We interpret the indoor traffic profiles via explainable machine learning techniques, and show how they are correlated to the indoor environment. Our findings show how indoor cellular demands are strongly dependent on the nature of the deployment location, which allows anticipating the type of demands that indoor 5G networks will have to serve and paves the way for their efficient planning and dimensioning.  

 Modeling and Generating Control-Plane Traffic for Cellular Networks   
 Jiayi Meng 
  Jingqi Huang 
  Y. Charlie Hu 
  Yaron Koral 
  Xiaojun Lin 
  Muhammad Shahbaz 
  Abhigyan Sharma 
  With 5G deployment gaining momentum, the control-plane traffic volume of cellular networks is escalating. Such rapid traffic growth motivates the need to study the mobile core network (MCN) control-plane design and performance optimization. Doing so requires realistic, large control-plane traffic traces in order to profile and debug the mobile network performance under real workload. However, large-scale control-plane traffic traces are not made available to the public by mobile operators due to business and privacy concerns. As such, it is critically important to develop accurate, scalable, versatile, and open-to-innovation control traffic generators, which in turn critically rely on an accurate traffic model for the control plane. Developing an accurate model of control-plane traffic faces several challenges: (1) how to capture the dependence among the control events generated by each User Equipment (UE), (2) how to model the inter-arrival time and sojourn time of control events of individual UEs, and (3) how to capture the diversity of control-plane traffic across UEs. We present a novel two-level hierarchical state-machine-based control-plane traffic model. We further show how our model can be easily adjusted from LTE to NextG networks (e.g., 5G) to support modeling future control-plane traffic. We experimentally validate that the proposed model can generate large realistic control-plane traffic traces. We have open-sourced our traffic generator to the public to foster MCN research.  

 Performance of Cellular Networks on the Wheels   
 Moinak Ghoshal 
  Imran Khan 
  Z. Jonny Kong 
  Phuc Dinh 
  Jiayi Meng 
  Y. Charlie Hu 
  Dimitrios Koutsonikolas 
  After 4 years of rapid deployment in the US, 5G is expected to have significantly improved the performance and overall user experience of mobile networks. However, recent measurement studies have focused either on static performance or a single aspect (e.g., handovers) under driving conditions of 5G, and do not provide a complete picture of cellular network performance today under driving conditions - a major use case of mobile networks. Through a cross-continental US driving trip (from LA to Boston, 5700km+), we conduct an in-depth measurement study of user-perceived experience (network coverage/performance and QoE of a set of major latency-critical 5G "killer'' apps) To understand the root cause of the observed network performance, while collecting low-level 5G statistics and signaling messages. Our study shows disappointingly low coverage of 5G networks today under driving and highly fragmented coverage by cellular technologies. More importantly, network and application performance are often poor under driving even in areas with full 5G coverage. We also examine the correlation of technology-wise coverage and performance with geo-location and the vehicle's speed and analyze the impact of a number of lower layer KPIs on network performance.  

 Characterizing and Modeling Session-Level Mobile Traffic Demands from Large-Scale Measurements   
 André Felipe Zanella 
  Antonio Bazco-Nogueras 
  Cezary Ziemlicki 
  Marco Fiore 
  We analyze 4G and 5G transport-layer sessions generated by a wide range of mobile services at over 282,000 base stations (BSs) of an operational mobile network, and carry out a statistical characterization of their demand rates, associated traffic volume and temporal duration. Based on the gained insights, we model the arrival process of sessions at heterogeneously loaded BSs, the distribution of the session-level load and its relationship with the session duration, using simple yet effective mathematical approaches. Our models are fine-tuned to a variety of services, and complement existing tools that mimic packet-level statistics or aggregated spatiotemporal traffic demands at mobile network BSs. They thus offer an original angle to mobile traffic data generation, and support a more credible performance evaluation of solutions for network planning and management. We assess the utility of the models in practical application use cases, demonstrating how they enable a more trustworthy evaluation of solutions for the orchestration of sliced and virtualized networks.  

 SESSION: Poster Session  
 Poster: SmartX BGP BVT: A First Real-Time BGP Blackholing Visibility Tool   
 Talaya Farasat 
  Muhammad Ahmad Rathore 
  Zeeshan Asim 
  Akmal Khan 
  JongWon Kim 
  Joachim Posegga 
  BGP Blackholing is an effective mitigation solution for networks to counter the frequent Distributed Denial of Service (DDoS) attacks. It enables to drop all network traffic that is directed towards a particular victim prefix under DDoS attack, ideally, as close to the source as possible. Despite its huge importance in the Internet, there is no tool available for the real-time visualization of BGP Blackholing activity. Visualization is one of the most powerful techniques for network operators to monitor network activity. From discovering successful network topology to expose anomalous behaviors in networks, easy-to-use visualizations are powerful weapons to capture important patterns on the Internet traffic[1, 5]. In this work, we propose a first real-time BGP Blackholing Visibility Tool (named as SmartX BGP-BVT) to detect and visualize community based BGP Blackholing on live BGP data. This tool will be helpful for network operators and researchers interested in BGP Blackholing service and DDoS mitigation in the Internet.  
 Analysis of BGP Blackholing on BGP datasets and its significant usage rate is highlighted in the literature [2, 3]. Community based BGP Blackholing enables with the help of BGP community attribute. So, for the monitoring of BGP Blackholing activity, first, we need to collect all BGP Blackhole communities from the Internet. Today, many networks including ISPs and IXPs offer BGP Blackholing service to their customers, and they publish their corresponding BGP Blackhole communities either on their official Web pages or in their Internet Routing Registry (IRR) records. By following Giotsas et al. [3] methodology, we make BGP Blackhole communities dictionary that is described in detail in our previous work [2].  
 To the best of our knowledge, SmartX BGP BVT3 is the first real-time BGP Blackholing Visibility Tool which visualizes community-based BGP Blackholing with the help of Blackhole Communities Dictionary. However, real-time BGP Blackholing visualizations are helpful for measuring the current adoption rate of this mitigation solution and also can serve as a proxy for identifying DDoS attacks.  

 Poster: The Impact of the Client Environment on Residential IP Proxies Detection   
 Elisa Chiapponi 
  Marc Dacier 
  Olivier Thonnard 
  Residential IP Proxies (RESIPs) enable proxying out requests from a vast network of residential devices without inserting any information revealing it. While RESIPs can be used for legitimate purposes, previous studies also associate them with malicious activities. In our last work, we proposed a server-side detection method for RESIP connections based on the difference in the Round Trip Time at the TCP and TLS layers. In this new work, thanks to real-world connections, we investigate if and how specific factors in the client environment influence the technique. We show that genuine users utilizing web browsers or performing hotspots do not result in false positives for our technique. Moreover, our early results suggest that false positives caused by Mobile TCP Terminating Proxies used by mobile Internet Service Providers have a Round Trip Time difference higher than the detection threshold but much smaller than the average RESIP one. This suggests that we can reduce these false positives by highering the detection threshold for mobile connections.  

 Poster: Through the ccTLD Looking Glass: Mining CT Logs for Fun, Profit and Domain Names   
 Raffaele Sommese 
  Mattijs Jonker 

 Poster: E3PO - An Open Platform for 360° Video Streaming Simulation and Evaluation   
 Yongqiang Gui 
  Yanyan Suo 
  Tian Zhang 
  Zhenhua Yu 
  Shu Shi 
  The simulation and evaluation of diverse 360° video streaming systems present inherent challenges due to the varied design objectives, streaming strategies, and evaluation metrics. This poster introduces E3PO, a versatile and extensible simulation and evaluation platform for 360° video streaming systems. E3PO excels in simulating all proposed variations of 360° video streaming methods, while also generating the precise view that users experience. We have implemented E3PO and developed a variety of examples that simulate different streaming approaches. The promising results from our evaluation of these simulated scenarios demonstrate E3PO's substantial potential to assist researchers in testing new designs, fine-tuning parameters, and comparing performance with their counterparts.  

 Poster: Modified Dynamic Beta RED -- A New AQM Algorithm for Internet Congestion Control   
 Ángel Giménez 
  Óscar M. Bonastre 
  José Valero 
  José M. Amigó 
  In this work we present a performance study of modified Dynamic Beta RED (mDBetaRED), a new Active Queue Management (AQM) RED-type algorithm based on dynamical variants with the ability to adapt to the characteristics of mixed networks. As well, we rely on real campus traffic traces to build a traffic model on which we evaluate our proposal under the NS3 simulator. The parameters of our AQM algorithm are dynamically adjusted so that the queue length remains stable around a predetermined reference value and according to changing network traffic conditions. We present a performance comparison of mDBetaRED with other AQM algorithms and diverse flows of Internet. Of all the AQM algorithms compared, the mDBetaRED algorithm offers the best throughput while effectively controlling delay and stability.  

 Poster: Novel Client-Side Watermarking Technique for Tor User De-Anonymization   
 Natalija Vlajic 
  Daniel Brown 
  Most traditional techniques for Tor user de-anonymization rely on the use of server-side originating traffic watermarks. In this poster, we outline the key ideas behind our novel client-side originating watermark scheme and describe some possible ways of how this scheme could be implemented in practice. We also demonstrate the superior real-world performance of this approach vs. those previously discussed in the literature.  

 Poster: COPA -- Parsing Outputs of CLI Commands for Failure Diagnosis of Network Devices   
 Keitaro Kaida 
  Tatsuaki Kimura 
  Hiroshi Yamauchi 
  Tetsuya Takine 
  Network operators in telecom carriers perform failure diagnosis by executing various commands (e.g., show interfaces) on network devices through a command line interface (CLI). The outputs of these CLI commands, which we call command outputs, contain more detailed information than typical one-line system logs but have complex characteristics; thus, existing parsers for system logs are not applicable. In this study, we propose COPA, a parsing method for command outputs for automating time-consuming and labor-intensive failure diagnosis.  

 Poster: Empirically Testing the PacketLab Model   
 Tzu-Bin Yan 
  Zesen Zhang 
  Bradley Huffaker 
  Ricky Mok 
  kc claffy 
  Kirill Levchenko 
  PacketLab is a recently proposed model for accessing remote vantage points. The core design is for the vantage points to export low-level network operations that measurement researchers could rely on to construct more complex measurements. Motivating the model is the assumption that such an approach can overcome persistent challenges such as the operational cost and security concerns of vantage point sharing that researchers face in launching distributed active Internet measurement experiments. However, the limitations imposed by the core design merit a deeper analysis of the applicability of such model to real-world measurements of interest. We undertook this analysis based on a survey of recent Internet measurement studies, followed by an empirical comparison of PacketLab-based versus native implementations of common measurement methods. We showed that for several canonical measurement types common in past studies, PacketLab yielded similar results to native versions of the same measurements. Our results suggest that PacketLab could help reproduce or extend around 16.4% (28 out of 171) of all surveyed studies and accommodate a variety of measurements from latency, throughput, network path, to non-timing data.  

 Poster: Analysis of User Uniqueness on LinkedIn Based on Publicly Available Non-PII   
 Ángel Merino 
  José González-Cabañas 
  Ángel Cuevas 
  Rubén Cuevas 
  The literature has shown combining a few non-Personal Identifiable Information (non-PII) is enough to make a user unique in a dataset including millions of users. In this work, we demonstrate that the combination of the location and 6 rare (14 random) skills in a LinkedIn profile is enough to become unique in a user base of ~800M users with a probability of 75%. The novelty is these attributes are publicly accessible to anyone registered on LinkedIn and could be activated through advertising campaigns.  

 Poster: Towards a Publicly Available Framework to Process Traceroutes with MetaTrace   
 Matthieu Gouel 
  Omar Darwich 
  Maxime Mouchet 
  Kevin Vermeulen 
  The objective of this research is to contribute towards the development of an open-source framework for processing large-scale traceroute datasets. By providing such a framework, we aim to benefit the community by saving time in everyday traceroute analysis and enabling the design of new scalable reactive measurements [1], where prior traceroute measurements are leveraged to make informed decisions for future ones[8, 12].  
 It is important to clarify that our goal is not to surpass proprietary solutions like BigQuery, which are utilized by CDNs for processing billions of traceroutes [6, 10]. These proprietary solutions are not freely accessible to the public, whereas our focus is on creating an open and freely available framework for the wider community.  
 Our contributions include (1) sharing the ideas and thinking process behind building MetaTrace, which efficiently utilizes ClickHouse features for traceroute processing; and (2) providing an open-source implementation of MetaTrace.  
 We evaluated MetaTrace using two types of queries: predicate queries for filtering traceroutes based on conditions, and aggregate queries for computing metrics on traceroutes. Our results show that MetaTrace is significantly faster compared to alternative solutions. For predicate queries, it outperforms a multiprocessed Rust solution by a factor of 552 and is 3.4 times faster than ClickHouse without MetaTrace optimizations. For aggregate queries, MetaTrace processes 202 million traceroutes in 11 seconds, with its performance scaling linearly with traceroute volume. Notably, on a single server, MetaTrace can perform a predicate query on a 6-year dataset of 6 billion traceroutes in just 240 seconds.  
 Furthermore, MetaTrace is resource-efficient, making it accessible for research groups with limited resources to conduct Internet-scale traceroute studies.  

 Poster: QUIC is not Quick Enough over Fast Internet   
 Xumiao Zhang 
  Shuowei Jin 
  Yi He 
  Ahmad Hassan 
  Z. Morley Mao 
  Feng Qian 
  Zhi-Li Zhang 
  QUIC is a multiplexed transport-layer protocol over UDP and comes with enforced encryption. It is expected to be a game-changer in improving web application performance. Together with the network layer and layers below, UDP, QUIC, and HTTP/3 form a new protocol stack for future network communication, whose current counterpart is TCP, TLS, and HTTP/2. In this study, to understand QUIC's performance over high-speed networks and its potential to replace the TCP stack, we carry out a series of experiments to compare the UDP+QUIC+HTTP/3 (QUIC) stack and the TCP+TLS+HTTP/2 (HTTP/2) stack. Preliminary measurements on file download reveal that QUIC suffers from a data rate reduction compared to HTTP/2 across different hosts.

70. Interspeech_0 conference:
ISCA - International Speech  
  Communication Association | Log in    

 ISCA Archive 

 Home 
  About   Newsroom   Latest News 
  ISCApad 
  Job Offers 
  Logo Design Competition 
  Objectives 
  Structure   Overview 
  Board 
  Portfolio of Board Members 
  Committees 
  Advisory Council 
  General Assembly 
  Staff 
  Legal Documents   Statutes   Statutes 2023 
  Statutes 2010 
  Statutes 1999 
  Statutes 1998 
  By-laws   By-laws 2023-present 
  By-laws 2022-2023 
  By-laws 2020-2022 
  By-laws 2018-2020 
  By-laws 2017-2018 
  By-laws 2008-2017 
  History   Early Steps 
  Previous Boards 
  Events   Overview of Events 
  Interspeech   About Interspeech 
  Upcoming Interspeech 
  Past Interspeech Conferences 
  Organizing Interspeech Conferences 
  Interspeech Policy 
  Workshops   Upcoming ISCA Workshops 
  Upcoming Other Events 
  Organizing Workshops 
  Organizational Ethics   Code of Ethics for Authors 
  Code of Conduct for Conference and Workshop Attendees 
  Activities   Overview of Activities 
  Special Interest Groups   Speech Synthesis - SynSig 
  Audio Visual Speech - AVISA 
  Under-resourced Languages - SIGUL 
  Speaker and Language Characterization - SpLC 
  Speech Prosody - SProSIG 
  Discourse and Dialogue - SIGdial 
  Speech and Language Technology in Education - SLaTE 
  Machine Learning - SIGML 
  Speech and Language in Multimedia - SLIM 
  Speech and Language Processing for Assistive Technologies - SLPAT 
  The History of Speech Communication Sciences - SIG-HIST 
  Child Computer Interaction - SIG-CHILD 
  Robust Speech Processing - SIG-RoSP 
  Security and Privacy in Speech Communication - SIG-SPSC 
  Spoken Language Translation - SIG-SLT 
  Chinese Spoken Language Processing - SIG-CSLP 
  Association Francophone de la Communication Parlée - AFCP 
  Associazione Italiana di Scienze della Voce - AISV 
  Iberian Languages - SIG-IL 
  Russian Speech Analysis - SIGRU 
  Speech And Language Technology for Minority Languages - SALTMIL 
  Indian Language Speech Processing - SIG-ILSP 
  By-Laws on SIGs 
  Model Constitution for SIGs 
  Postdoc & Early Career Researcher Committee 
  Diversity   WomenNSpeech 
  LGBTQI* 
  Reviewing 
  Students   Student Advisory Committee 
  Services   Grants   How to Claim an ISCA Grant 
  Honours   ISCA Medal for Scientific Achievement 
  ISCA Service Medal 
  ISCA Awards 
  ISCA Fellow Program 
  ISCA Honorary Members 
  Training   Training Program 
  SCOOT   Sound 
  Signal Processing 
  Linguistics   Psycholinguistics 
  Phonetics   Articulatory Phonetics 
  Acoustic Phonetics 
  Auditory Phonetics 
  Phonology 
  Speech Technology   Speech Coding 
  Speech Synthesis   Classic Speech Synthesis 
  Concatenative Synthesis 
  HMM-based Synthesis 
  DNN Synthesis 
  Speech Recognition   DTW 
  HMMs 
  ANN-ASR 
  Toolkits   ASR Toolkits 
  Synthesis Toolkits 
  Databases 
  SCOOT Overview 
  Prosodics 
  International Virtual Seminars 
  Outreach   Distinguished Lecturers 
  Geographical Outreach 
  Liaisons   Speech Laboratories 
  Professional Organisations 
  Corpora Associations 
  Resources   Conference and workshop archive 
  Video Archive 
  Journals 
  List of Books 
  List of Databases 
  List of Software Tools 
  Join   Membership Benefits 
  Membership Schemes and Fees 
  Become a new member - Renew your membership 
  Member directory 
  Help   Contact Us 
  FAQ 
  ISCA Logos 

 Home 
  Events 
  Interspeech 
  About Interspeech 

 About Interspeech  
 ISCA manages the organization of an annual conference, Interspeech, which integrates two previous series of biennial international conferences: EUROSPEECH, the European Conference on Speech Communication and Technology, and ICSLP, the International Conference on Spoken Language Processing. Since 2000, both conferences have been held under the common label INTERSPEECH. The first Interspeech event was ICSLP 2000, which took place in 2000, in Beijing, China.      

  Previous EUROSPEECH conferences were held in 1989 in Paris (France); 1991 in Genoa (Italy); 1993 in Berlin (Germany); 1995 in Madrid (Spain); 1997 in Rhodes (Greece); and 1999 in Budapest (Hungary); Previous ICSLP conferences, coordinated by the Permanent Council for the Organization of International Conferences on Spoken Language Processing (PC-ICSLP), were held in 1990 in Kobe (Japan); 1992 in Banff (Canada); 1994 in Yokohama (Japan); 1996 in Philadelphia (USA); and 1998 in Sydney (Australia). All INTERSPEECH conferences are recorded chronologically here  .  
   
  INTERSPEECH conferences include papers on all the scientific and technological aspects of Speech. More than 1,000 participants from all over the world attend the conference annually to present their work in oral and poster sessions. Several satellite workshops and a Scientific and Industrial Exhibition highly enrich the conference content. INTERSPEECH conferences may be held in any country, although they generally should not occur in the same continent in two consecutive years. The next Interspeech conferences will be held in Greece (2024)  , Rotterdam, the Netherlands (2025)  , and Sydney, Australia (2026).   
 INTERSPEECH papers are indexed in ISI, Engineering Index, Scopus, and Google Scholar. It is rated as Rank A conference by the Computing and Research Education Association of Australasia ( CORE   ).   

 Organisation | Events | Membership | Help 
 > Board | > Interspeech | > Join - renew | > Sitemap 
 > Legal documents | > Workshops | > Membership directory | > Contact 
 > Logos | > FAQ 
 > Privacy policy | Organisation | Events | Membership | Help | > Board | > Interspeech | > Join - renew | > Sitemap | > Legal documents | > Workshops | > Membership directory | > Contact | > Logos | > FAQ | > Privacy policy |  
 Organisation | Events | Membership | Help 
 > Board | > Interspeech | > Join - renew | > Sitemap 
 > Legal documents | > Workshops | > Membership directory | > Contact 
 > Logos | > FAQ 
 > Privacy policy 

 © Copyright 2024 - ISCA International Speech Communication Association - All right reserved.   

 Powered by Wild Apricot  Membership Software

71. IMC_3 conference:
IMC 2023   

 This site requires JavaScript.  Your browser does not support JavaScript.  
  Report bad compatibility problems    
  Conference information  
 Deadlines 
  Program committee 
  52 of 208 submissions accepted 

 Welcome to the 2023 Internet Measurement Conference (IMC) (IMC 2023) submissions site.  
   
  HotCRP.com signin  
 Sign in using your HotCRP.com username and password.  
 Email     
 Forgot your password?    
 Password     
 Sign in    
 New to the site? Create an account   

 Sign in    
  Submissions  
 The deadline  for registering submissions has passed.  

 HotCRP

72. Interspeech_1 conference:
Home 
  Calls | Call for Papers 
  Call for Show & Tell 
  Call for Tutorials 
  Call for Satellite Events 
  Important Dates & Deadlines 
  For Authors | Oral Presentation Guidelines 
  Special Session Presentation Guidelines 
  Poster Presentation Guidelines 
  Author Resources 
  INTERSPEECH 2023 Code of Ethics for Authors 
  Submission Policy 
  Paper Submission 
  Show and Tell Paper Submission 
  Programme | Conference Theme 
  Programme at a Glance/Overview 
  Keynote Speakers 
  ISCA Meetings 
  Survey Talks 
  Special Sessions/Challenges 
  Satellite Events 
  Tutorials 
  Registration | Registration & Payment 
  Accommodation 
  Social Events 
  Travel Grants 
  Code of Conduct 
  Accommodation | Accommodation 
  Students | Students 
  ISCA Best Student Paper Award 2023 
  Sponsorship & Exhibition | Opportunities to Support 
  Sponsors 
  Exhibitors 
  Venue & Travel | City of Dublin 
  Conference Venue 
  FAQ 
  Travel 
  Visas 
  General Info | About Us 
  Contact Us 
  Important Dates & Deadlines 
  Organising Committee 
  Area Chairs 
  Outstanding Reviewers 
  Food and Drink Locations 
  INTERSPEECH Kids! 
  Conference Bag Reuse Competition 
  Volunteers 
  Gallery 
  Menu    Menu 

 Welcome to Interspeech 2023  
 20th – 24th August 2023  
  Dublin, Ireland 

 Welcome to INTERSPEECH 2023  

 We are delighted to invite you to the 24th INTERSPEECH Conference from August 20th to 24 th  , to be held in the Convention Centre Dublin, Ireland. INTERSPEECH, as the world’s largest and most comprehensive conference on the science and technology of spoken language processing, is a hugely important event for our community. We are particularly keen to offer our community a chance to reconnect in a friendly and relaxed atmosphere, where everyone is welcome.  
 Despite being a major European city, Dublin remains as intimate as a village. Framed by mountains, centred on a river and edged by a beautiful bay, the city’s streets are filled with vibrant art and historic buildings, modern cafés and traditional pubs. Walk the streets and you’ll feel the energy of over 1,000 years of history, as echoes of the Vikings mix with buzzing boutiques, cobbled streets reverberate with the sounds of buskers, and 18th century parks play host to festivals, film and food markets. The city of Dublin is known for its friendliness and ability to make visitors feel at home – we aim for a similar feeling at INTERSPEECH, providing networking opportunities that will be inclusive and value the diversity of our delegates.  
 With days filled with top-quality research and vibrant discussion, and evenings packed with culture, creativity and craic (the Irish term for fun!), INTERSPEECH 2023 is the perfect opportunity to reconnect with colleagues from across the world.  

 Latest News  

 Conference Photos Available Now   
 Click here to view photos from conference opening and closing days 
  Full programme and abstract book now available   
 Click here to access it. 
  One Day Registration Now Available   
 Click here for details 
  Poster Presentation Guidelines   
 Click here to learn more 
  Conference Bag Reuse Competition   
 Dust off those old INTERSPEECH bags and win!  
  Click here for more information  . 
  Programme Highlights   
 Satellite Events   
  Special Sessions, Challenges and Panel Session   
  Tutorials 
  Sponsorship & Exhibition Opportunities   
 Click here to learn more 

   Sign up  to receive the INTERSPEECH 2023  newsletter.  

  Email     
  
  First Name     
  
  Last Name     

 With thanks to our main sponsors  

 INTERSPEECH 2023  
 Conference Secretariat Keynote PCO  
  Suite 26, Anglesea House, 63 Carysfort Ave.,  
  Blackrock, Co Dublin, A94 FF63, Ireland  
  Tel: +353 1 4003611 Email: registration@interspeech2023.org   
 Privacy Policy   

 Scroll to top

73. Interspeech_2 conference:


74. ISC_0 conference:
Skip to main content    

 About MaX | Goals 
  Organisation 
  MaX in a nutshell 
  People at MaX 
  Codes at MaX 
  Project Repository 
  Publications 
  Job openings 
  Newsletter 
  Communication 
  News & Events 
  MaX 2018-2021 
  Software | Codes 
  Features and algorithms 
  Libraries 
  Workflows 
  Exascale | Deployments 
  Programming models 
  Co-design 
  Performances 
  Separation of concerns 
  Data | Fact & Figures 
  Services | MaX Container technology for HPC system 
  MaX Help Desk 
  MaX High level consultancy 
  Simulations on premises and in the cloud 
  Turn-key materials solutions 
  Services to the Industry 
  Facts & Figures 
  FAQ 
  Training | Training materials | Open Online courses and videolectures 
  Presentations 
  Training material related to the MaX flagship codes 
  List of workshops & schools 
  Training through research in the MaX labs 
  Fact & Figures 
  Contact us 

   Search     

 Sort by  Most recent  Most relevant     
   
 Order  Asc  Desc     

   About MaX | About M A  X  
 M A  X (M A  terials design at the eXascale) is a European Centre of Excellence which enables materials modelling, simulations, discovery and design at the frontiers of the current and future High Performance Computing (HPC), High Throughput Computing (HTC) and data analytics technologies.  
        Goals 
  Organisation 
  MaX in a nutshell 
  People at MaX 
  Codes at MaX 
  Project Repository 
  Publications 
  Job openings 
  Newsletter 
    Communication 
  News & Events 
  MaX 2018-2021 
  Software | SOFTWARE  
 The software developed by M A  X is made available to the whole community in open-source form. In this section you can find our main software output and how to obtain it.  
   
           Codes  
          Software libraries  
           Features and algorithms  
          Workflows  
           Impact of M A  X flagship codes  
                  Codes 
  Features and algorithms 
  Libraries 
  Workflows 
  Exascale | EXASCALE  
 M A  X addresses the challenges of porting, scaling, and optimising material science application codes for the peta- and exascale platforms in order to deliver best code performance and improve users productivity on the upcoming architectures.  
           Programming models  
     
       Performances  
     
        Co-design  
     
       Separation of concerns  
     
             Programming models 
  Co-design 
  Deployments 
  Performances 
  Separation of concerns 
  Data | DATA  
 M A  X is committed in supporting data stewardship by adhering to the FAIR-sharing principles. High-quality data is provided both in the format of curated scientific results and raw data, focusing on the tracking of provenance to ensure the full reproducibility of results.  
           Data at MaX  
          Complete archived data  
           Curated data  
          Data on demand  
           FAIR data  
          Facts and Figures  
                Fact & Figures 
  Services | SERVICES  
 M A  X develops and offers services and technical support dedicated to the general public and the expert users from both industry and academia.  
           Help Desk  
          High level consultancy  
           Turn-key materials solutions  
          Container technology for HPC system  
           Simulations on premises and in the cloud  
          Services to the Industry  
           Facts and Figures  
          FAQs  
                MaX Container technology for HPC system 
  MaX Help Desk 
  MaX High level consultancy 
  Simulations on premises and in the cloud 
  Turn-key materials solutions 
  Services to the Industry 
  Facts & Figures 
  FAQ 
  Training | TRAINING  
 M A  X offers integrated training and education in the field of HPC  developments and in the computational materials science domain, including workshops and schools, contributions to University courses and training through research in the CoE labs.  
           List of workshops & schools  
          Training through research in the MaX labs  
           Training materials  
          Facts and Figures  
                Training materials | Open Online courses and videolectures 
  Training material related to the MaX flagship codes 
  Presentations 
  List of workshops & schools | Quantum Espresso Targeting Accelerators 
  Training through research in the MaX labs 
  Fact & Figures 
  Contact us 

   About MaX | Goals 
  Organisation 
  MaX in a nutshell 
  People at MaX 
  Codes at MaX 
  Project Repository 
  Publications 
  Job openings 
  Newsletter 
  Communication 
  News & Events 
  MaX 2018-2021 
  Software | Codes 
  Features and algorithms 
  Libraries 
  Workflows 
  Exascale | Deployments 
  Programming models 
  Co-design 
  Performances 
  Separation of concerns 
  Data | Fact & Figures 
  Services | MaX Container technology for HPC system 
  MaX Help Desk 
  MaX High level consultancy 
  Simulations on premises and in the cloud 
  Turn-key materials solutions 
  Services to the Industry 
  Facts & Figures 
  FAQ 
  Training | Training materials | Open Online courses and videolectures 
  Presentations 
  Training material related to the MaX flagship codes 
  List of workshops & schools 
  Training through research in the MaX labs 
  Fact & Figures 
  Contact us 

 ISC High Performance 2023  
 Home   /  Events  /  ISC High Performance 2023    

 Date:  21 May 2023  to 25 May 2023     
 Location:  Hamburg (Germany)   
   
   THE EVENT FOR HIGH PERFORMANCE COMPUTING, MACHINE  LEARNING, DATA ANALYTICS & QUANTUM COMPUTING   
 ISC 2023, the leading High Performance Computing (HPC) conference and exhibition in Europe, will take place in Hamburg (Germany) from 21 to 25 May. This year’s topics will illuminate the challenges that HPC, machine learning, data analytics, and quantum computing face.  
 The programme agenda  is now online. All programs are divided into invited, contributed, vendor sessions, and special events. The agenda will be continuously updated with talk abstracts and speakers' biodata.  
 Application is now open! Check the programme and register here  .  
   
 MaX will be represented at the conference jointly with 30 R&I projects funded by the EuroHPC Joint Undertaking  in a unique booth: B201  . The presentation of the MaX project by Nicola Spallanzani (CNR Nano) is scheduled on Tuesday, May 23 at 17.45: half an hour of MaX goals, challanges and plans for the futrue + a Question & Answer session open to the audience!   
 Don't miss the opportunity to discuss cutting-edge aspects of supercomputing with all the representatives of the HPC Centres of Excellence and among the HPC community. Come see us at booth B201  on the exhibitor floor, attend the EuroHPC JU conference session and join our workshop!  
 On Tuesday, May 23 at 16:15  , EuroHPC JU Executive Director, Anders Dam Jensen, will be speaking at the conference session  "Leading the Way in European Supercomputing: User's opportunities and latest updates from the EuroHPC JU". (HALL Z- 3rd Floor).  
 On Thursday 25 May  , EuroHPC JU will host a workshop  from 09h00 to 13h00  : "What’s Next in European Supercomputing and How to Get Access to Europe's Biggest Supercomputers for Free?". (Hall Y7 - 2nd Floor)  
 Detailed info are available here  .  
 Furthermore you may meet MaX representative @ E4 Computer Engeneering  booth ( C323) on Wednesday 24 from 11.00 to 11.25.  Several  speeches and interventions will be given at this booth on EU projects and initiatives.  
   
  Euro HPC Summit...     
   Efficient many-...     

  M | A | X Centre of Excellence 
  c/o CNR NANO 
  via Campi 213A 
  I-41125 Modena 
  ph +39 059 2055629 
  email: info@max-centre.eu 
  communication@max-centre.eu 

 SITEMAP  About MAX 
  SOFTWARE 
  EXASCALE 
   DATA 
  SERVICES 
  TRAINING 
  CONTACT US 

 INFORMATION  Privacy Policy 
  Terms and Conditions 

 Connect with us        

 ©2023-MAX.All rights reserved. Privacy Policy  Terms of Service   

 MaX - Materials design at the Exascale has received funding from the European High Performance Computing Joint Undertaking and Participating Countries in Project (Czechia, France, Germany, Italy, Slovenia and Spain) under grant agreement no. 101093374.  
 Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European High Performance Computing Joint Undertaking. Neither the European Union nor the granting authority can be held responsible for them.  
 © Copyright 2023

75. ISC_1 conference:
Skip to content      
   
  Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 
   
     Menu    
 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 

 Newsletter   

  Contact   

  Our Story   

  Submissions   

 CONNECTING THE DOTS  

 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 
   
     Menu    
 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 

 ISC HIGH PERFORMANCE  

 2025  

 JUNE 10-13, 2025  
  HAMBURG  
  GERMANY  

        Calls are now open      

 CONFERENCE & EXHIBITION  

 THE EVENT FOR HIGH PERFORMANCE COMPUTING, AI, DATA ANALYTICS & QUANTUM COMPUTING  

 ISC 2025 connects users from academia, government, and the private sector with technology developers and providers, fostering a global exchange of knowledge, innovation, and collaboration. Each participant plays a crucial role in the event.  

 All information  
 TO SUBMIT  

 All information  
 TO ATTEND  

 All information  
 TO EXHIBIT  

 join as a contributor now!  

 UPCOMING SUBMISSION DEADLINES  

 December 10, 2024  

      Research Paper      

 December 11, 2024  

      Regular Workshops      

 December 12, 2024  

      Tutorials      

 COMPLIMENTARY 2024 CONTENT  

 If you were unable to attend ISC 2024 and would like to watch the main talks, we are happy to register you for complimentary on-demand access.  

      REGISTER NOW      

 THEME:  

 CONNECTING THE DOTS  

 ISC is commemorating its 40th anniversary as the leading forum for fostering discussions and promoting innovation in high performance computing. Whether you are just starting out, in a mid-level position, in senior management, or a student, we welcome you to join our efforts to connect people and technologies.  

      READ MORE      

 REGISTRATION  

 EARLY BIRD REGISTRATION  

  Days    
  Hours    
  Minutes    

 WELCOME TO ISC 2025 – REGISTRATION IS NOW OPEN!  

 Hier steht ein kurzer Introtext zum Thema Registration. Die Registrierung ist nun offen  
  und es gibt bis zu einem bestimmten Zeitpunkt noch early bird rates.  

 PASS TYPES  

 REGISTER NOW  

 2025 PROGRAM CHAIR & DEPUTY CHAIR  

 “We look forward to seeing everyone interested in HPC and AI at the 40th edition of ISC.  
  If you have a passion for big iron, whether you’re a student, a researcher, a decision-maker, or a politician, we welcome your participation.“   
   
 Torsten Hoefler   

 TORSTEN HOEFLER  
 Professor of Computer Science, ETH Zurich, Switzerland  

 rosa badia  
 Manager of the Workflows & Distributed Computing Research Group; Barcelona Supercomputing Center (BSC), Spain  

 TORSTEN HOEFLER  

 Torsten Hoefler, ISC 2025 Program Chair, is a Professor of Computer Science at ETH Zurich, a member of Academia Europaea, and a Fellow of the ACM and IEEE. His research interests revolve around the central topic of “Performance-centric System Design” and include scalable networks, parallel programming techniques, and performance modeling.   
   
      send request      

 ROSA BADIA  

 Rosa M. Badia, ISC 2025 Program Deputy Chair, manages the Workflows and Distributed Computing research group at the Barcelona Supercomputing Center (BSC), and she is involved in several notable European projects – AI-Sprint, CALESTIS, ICOS, CEEC CoE, PerMedCoE, and DT-GEO. She is the PI of the EuroHPC eFlows4HPC project. Her current research interest is programming models for complex platforms (from multicore GPUs to Grid/Cloud).   
   
      send request      

        BIOGRAPHIES      

 LAST YEAR'S KEYNOTE SPEAKERS  

 Conference Keynote  
   
  KATHY YELICK    
  Vice Chancellor for Research, Cal, UC Berkeley, USA  

 Tuesday Keynote  
   
  ISABELL GRADERT    
  Vice President Central Research & Technology, Airbus  

 Wednesday Keynote  
   
  JOHN SHALF    
  Department Head for Computer Science LBNL, USA  

 Wednesday Keynote  
   
  ROSA BADIA   
    Manager Workflows & Distributed Computing Group, BSC, Spain  

 ISC Fellow  
   
 KATHY YELICK   

 ISC 2024 Topic Area Chair  
   
 ISABELL GRADERT   

 ISC Fellow  
   
 JOHN SHALF   

 Sollten hier nur die drei Keynotesprecher gezeigt werden, können in dieser vierten Kachel Anmerkungen auf weitere Speaker gemacht werden.  

      Chairs | Committees      

 All informations  
 SCHEDULE  

 ISC 2025 SPONSORS  

 SUBMISSIONS AND ALL INFORMATIONS ABOUT  
 FOR EXHIBITS  

 A UNIQUE EVENT  

 01   

 Pays close attention to topics likely to shape the future    

 02   

 Close to 50% of ISC attendees attend only ISC  

 03   

 OVER 60% OF ATTENDEES ARE DECISION-MAKERS  

 04   

 The exhibition showcases all major European and global vendors  

 05   

 Highlights community achievements through special programs  

 EXHIBITION  

 If you want to connect with a distinctive attendee crowd, you should present your organization at ISC 2025. To participate as an exhibitor or sponsor, please express your interest  , and we will get in touch with you.  

 EXHIBITION SCHEDULE   
 June 10, 3:00 pm – 8:30 pm  
  June 11, 10:00 pm – 6:00 pm  
  June 12, 9:00 am – 4:00 pm  

 All information  
 To exhibit  

 All information  
 TO VISIT  

 ISC 2025 SPONSORS  

        RESERVE YOUR BOOTH SPACE NOW!      

 News  

 FOR EXHIBITS  

 Scientists, researchers, and engineers of all career levels are invited to contribute to the program. Their submissions will have an impact on the conference program.  

      read more      

 FOR EXHIBITS  

 Scientists, researchers, and engineers of all career levels are invited to contribute to the program. Their submissions will have an impact on the conference program.  

      read more      

 FOR EXHIBITS  

 Scientists, researchers, and engineers of all career levels are invited to contribute to the program. Their submissions will have an impact on the conference program.  

      read more      

 Calling for Jack Dongarra Early Career Award Nominations  

 We are pleased to announce that the 2025 ISC Jack Dongarra Early Career Award is now open for nomination. We encourage community members to nominate early-career researchers they consider to have significantly contributed to scientific progress in their research fields.   
   
      read more      

 ISC 2025 is Now Open for Submissions  

 The ISC 2025 Contributed Program is now open for submission, and we invite scientists and engineers from academia, industry, and government to actively submit their proposals. The contributed program comprises research papers, posters, birds of a feather, tutorials, and workshop sessions.   
   
      read more      

 ISC 2024 Wrap-Up and a Peek at 2025  

 ISC 2024 ended a few weeks ago as the most successful event in the conference series, post-COVID. The high performance computing–focused community forum brought together 3,416 end users, as well as 160 technology developers and providers from 59 countries for an intense exchange of insights and ideas under the theme “Reinventing HPC.”   
   
      read more      

 PLATINUM SPONSORS  

 SITEMAP 
  CONTACT 
  IMPRINT 
  PRIVACY 
   
 SITEMAP 
  CONTACT 
  IMPRINT 
  PRIVACY 

 Linkedin      Flickr      Youtube

76. ISC_2 conference:
Skip to content      
   
  Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 
   
     Menu    
 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 

 Newsletter   

  Contact   

  Our Story   

  Submissions   

 CONNECTING THE DOTS  

 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 
   
     Menu    
 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 

 ISC HIGH PERFORMANCE   
 2025   
 JUNE 10-13, 2025  
  HAMBURG  
  GERMANY   

        Submit your proposal      

 News  

 ISC 2025 is Now Open for Submissions  

  2024-09-10 

 HAMBURG, Germany, September 10, 2024  – The ISC 2025 Contributed Program is now open for submission, and we invite scientists and engineers from academia, industry, and government to actively submit their proposals. The contributed program comprises research papers, posters, birds of a feather, tutorials, and workshop sessions.  
 ISC is an international conference and exhibition that attracts over 3,500 public and private users, developers, and providers of high performance computing, artificial intelligence, high-performance data analytics, and quantum computing from all over the world. The ISC 2025 contributed sessions will be held in Hamburg, Germany, from June 10 to 13, 2025.  
 Proposals submitted to specific programs, such as research papers, will undergo peer review by the overseeing committees to ensure their academic credibility and quality. Here are the submission deadlines and acceptance notifications.  

 Programs | Submission Deadline | Notification of Acceptance 
 Workshops with proceedings | October 21, 2024 | December 6, 2024 
 Regular Workshops | December 11, 2024 | January 28, 2025 
 Research Papers | December 10, 2024 | February 28, 2025 
 Tutorials | December 12, 2024 | February 14, 2025 
 Research Posters | January 16, 2025 | February 26, 2025 
 Project Posters | January 16, 2025 | February 26, 2025 
 Women in HPC Posters* | January 16, 2025 | February 26, 2025 
 Birds of a Feather | January 22, 2025 | February 24, 2025 

  Submission Guidelines   
 Please refer to the submission page  for complete program details and submission requirements. We advise you to follow the specified guidelines to ensure a successful submission and acceptance process. *Please note the Women in HPC Poster submission page will be published next week.  
  
  Contributed Program Areas   
 The ISC Contributed Program is based on the topic areas  covered at ISC 2025, which address the development and trends in high performance computing.  
  
  Connecting the Dots   
 Next year, ISC commemorates its 40th anniversary as the leading forum for fostering discussions and promoting innovation in high performance computing. Whether you are just beginning your professional career, in a mid-level position, in senior management, or still a student, we welcome you to join our effort to connect people and HPC. Read more about the theme here  .  
  
  About ISC 2025   
 Join ISC High Performance 2025 in #ConnectingTheDots  
 ISC 2025 returns to the Congress Center Hamburg from June 10 – 13 next year for its 40th edition. Since its inception in 1986, it has been recognized as the world’s oldest and Europe’s must-attend event for HPC, high-performance data analytics, AI, and Quantum Computing professionals. The exhibition will showcase the latest developments in HPC, covering all significant advancements in system design, programming models, applications, artificial intelligence, quantum computing, and emerging technologies.  
   
 https://www.isc-hpc.com   

      Go Back      

 Contact  

  MS. NAGES SIESLACK​   
 Communications and Digital Content Manager  

      send request      

 PLATINUM SPONSORS  

 GOLD SPONSORS  

 SITEMAP 
  CONTACT 
  IMPRINT 
  PRIVACY 
   
 SITEMAP 
  CONTACT 
  IMPRINT 
  PRIVACY 

 Linkedin      Flickr      Youtube

77. Interspeech_3 conference:
ISCA  Archive  Interspeech 2023  Sessions   Search  Website  Booklet    
   
   ISCA  Archive  Sessions   Search  Website  Booklet    
   
 ×  Click on column names to sort.  
 Searching uses the 'and' of terms e.g. Smith Interspeech  matches all papers by Smith in any Interspeech. The order of terms is not significant.  
 Use double quotes for exact phrasal matches e.g. "acoustic features"  .  
 Case is ignored.  
 Diacritics are optional e.g. lefevre  also matches lefèvre  (but not vice versa).  
 It can be useful to turn off spell-checking for the search box in your browser preferences.  
 If you prefer to scroll rather than page, increase the number in the show entries dropdown.  

 top    
   
 INTERSPEECH 2023   
 Dublin, Ireland  
  20-24 August 2023  
  
  Chairs: Naomi Harte, Julie Carson-Berndsen, Gareth Jones  
 doi: 10.21437/Interspeech.2023  ISSN: 2958-1796    

 Keynote 1 ISCA Medallist  
  Bridging Speech Science and Technology — Now and Into the Future  
  Shrikanth Narayanan   

 Speech Synthesis: Prosody and Emotion  
  Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks  
  Jianrong Wang, Yaxin Zhao, Li Liu, Tianyi Xu, Qi Li, Sen Li   
  Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations  
  Zhao-Ci Liu, Zhen-Hua Ling, Ya-Jun Hu, Jia Pan, Jin-Wei Wang, Yun-Di Wu   
  EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis  
  Haobin Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao   
  Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus  
  Detai Xin, Shinnosuke Takamichi, Ai Morimatsu, Hiroshi Saruwatari   
  Explicit Intensity Control for Accented Text-to-speech  
  Rui Liu, Haolin Zuo, De Hu, Guanglai Gao, Haizhou Li   
  Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech  
  Guangyan Zhang, Thomas Merritt, Sam Ribeiro, Biel Tura-Vecino, Kayoko Yanagisawa, Kamil Pokora, Abdelhamid Ezzerg, Sebastian Cygert, Ammar Abbas, Piotr Bilinski, Roberto Barra-Chicote, Daniel Korzekwa, Jaime Lorenzo-Trueba   

 Statistical Machine Translation  
  Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer  
  Paul-Ambroise Duquenne, Holger Schwenk, Benoît Sagot   
  Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters  
  Proyag Pal, Brian Thompson, Yogesh Virkar, Prashant Mathur, Alexandra Chronopoulou, Marcello Federico   
  StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation  
  Kun Song, Yi Ren, Yi Lei, Chunfeng Wang, Kun Wei, Lei Xie, Xiang Yin, Zejun Ma   
  Joint Speech Translation and Named Entity Recognition  
  Marco Gaido, Sara Papi, Matteo Negri, Marco Turchi   
  Analysis of Acoustic information in End-to-End Spoken Language Translation  
  Gerard Sant, Carlos Escolano   
  LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers  
  Peidong Wang, Eric Sun, Jian Xue, Yu Wu, Long Zhou, Yashesh Gaur, Shujie Liu, Jinyu Li   

 Self-Supervised Learning in ASR  
  DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models  
  Yifan Peng, Yui Sudo, Shakeel Muhammad, Shinji Watanabe   
  Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations  
  Salah Zaiem, Titouan Parcollet, Slim Essid   
  Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition  
  Zhao Yang, Dianwen Ng, Chong Zhang, Xiao Fu, Rui Jiang, Wei Xi, Yukun Ma, Chongjia Ni, Eng Siong Chng, Bin Ma, Jizhong Zhao   
  O-1: Self-training with Oracle and 1-best Hypothesis  
  Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran, Kartik Audhkhasi   
  MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets  
  Ziyang Ma, Zhisheng Zheng, Changli Tang, Yujin Wang, Xie Chen   
  Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models  
  Léa-Marie Lam-Yee-Mui, Lucas Ondel Yang, Ondřej Klejch   

 Prosody  
  Chinese EFL Learners’ Perception of English Prosodic Focus  
  Xinya Zhang, Ying Chen   
  Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English  
  Thomas Sostarics, Jennifer Cole   
  Tonal coarticulation as a cue for upcoming prosodic boundary  
  Jianjing Kuang, May Pik Yu Chan, Nari Rhee   
  Alignment of Beat Gestures and Prosodic Prominence in German  
  Sophie Repp, Lara Muhtz, Johannes Heim   
  Creak Prevalence and Prosodic Context in Australian English  
  Hannah White, Joshua Penney, Andy Gibson, Anita Szakay, Felicity Cox   
  Speech reduction: position within French prosodic structure  
  Kübra Bodur, Roxane Bertrand, James S. German, Stéphane Rauzy, Corinne Fredouille, Christine Meunier   

 Speech Production  
  Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels  
  Ziyu Zhu, Yujie Chi, Zhao Zhang, Kiyoshi Honda, Jianguo Wei   
  Speech Synthesis from Articulatory Movements Recorded by Real-time MRI  
  Yuto Otani, Shun Sawada, Hidefumi Ohmura, Kouichi Katsurada   
  The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN  
  Zheng Yuan, Aldo Pastore, Dorina de Jong, Hao Xu, Luciano Fadiga, Alessandro D'Ausilio   
  Did you see that? Exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants  
  James Mahshie, Michael Larsen   

 Dysarthric Speech Assessment  
  Automatic assessments of dysarthric speech: the usability of acoustic-phonetic features  
  Loes van Bemmel, Chiara Pesenti, Xue Wei, Helmer Strik   
  Classification of Multi-class Vowels and Fricatives From Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity  
  Chowdam Venkata Thirumala Kumar, Tanuka Bhattacharjee, Yamini Belur, Atchayaram Nalini, Ravi Yadav, Prasanta Kumar Ghosh   
  Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation  
  Jinzi Qi, Hugo Van hamme   
  Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation  
  Enno Hermann, Mathew Magimai.-Doss   
  Latent Phrase Matching for Dysarthric Speech  
  Dianna Yee, Colin Lea, Jaya Narain, Zifang Huang, Lauren Tooley, Jeffrey P. Bigham, Leah Findlater   
  Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification  
  Eun Jung Yeo, Kwanghee Choi, Sunhee Kim, Minhwa Chung   

 Speech Coding: Transmission and Enhancement  
  CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding  
  Youqiang Zheng, Li Xiao, Weiping Tu, Yuhong Yang, Xinmeng Xu   
  Target Speech Extraction with Conditional Diffusion Model  
  Naoyuki Kamo, Marc Delcroix, Tomohiro Nakatani   
  Towards Fully Quantized Neural Networks For Speech Enhancement  
  Elad Cohen, Hai Victor Habi, Arnon Netzer   
  Complex Image Generation SwinTransformer Network for Audio Denoising  
  Youshan Zhang, Jialu Li   

 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 1  
  Using Text Injection to Improve Recognition of Personal Identifiers in Speech  
  Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, Bhuvana Ramabhadran   
  Investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a Finnish model  
  Tamas Grosz, Yaroslav Getman, Ragheb Al-Ghezi, Aku Rouhe, Mikko Kurimo   
  Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech  
  Jan Lehečka, Jan Švec, Josef V. Psutka, Pavel Ircing   
  Iteratively Improving Speech Recognition and Voice Conversion  
  Mayank Kumar Singh, Naoya Takahashi, Naoyuki Onoe   
  LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems  
  Kavan Fatehi, Ayse Kucukyilmaz   
  TranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition  
  Hongfei Xue, Qijie Shao, Peikun Chen, Pengcheng Guo, Lei Xie, Jie Liu   
  Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR  
  Zelin Wu, Tsendsuren Munkhdalai, Pat Rondon, Golan Pundak, Khe Chai Sim, Christopher Li   
  GhostRNN: Reducing State Redundancy in RNN with Cheap Operations  
  Hang Zhou, Xiaoxu Zheng, Yunhe Wang, Michael Bi Mi, Deyi Xiong, Kai Han   
  Task-Agnostic Structured Pruning of Speech Representation Models  
  Haoyu Wang, Siyuan Wang, Wei-Qiang Zhang, Suo Hongbin, Yulong Wan   
  Factual Consistency Oriented Speech Recognition  
  Naoyuki Kanda, Takuya Yoshioka, Yang Liu   
  Multi-Head State Space Model for Speech Recognition  
  Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, Mark J. F. Gales   
  Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search  
  Yingying Gao, Shilei Zhang, Zihao Cui, Chao Deng, Junlan Feng   
  Probing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration  
  Kinan Martin, Jon Gauthier, Canaan Breiss, Roger Levy   
  Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers  
  Philip Harding, Sibo Tong, Simon Wiesler   

 Analysis of Speech and Audio Signals 1  
  Robust Prototype Learning for Anomalous Sound Detection  
  Xiao-Min Zeng, Yan Song, Ian McLoughlin, Lin Liu, Li-Rong Dai   
  A multimodal prototypical approach for unsupervised sound classification  
  Saksham Singh Kushwaha, Magdalena Fuentes   
  Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms  
  Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang   
  Adapting Language-Audio Models as Few-Shot Audio Learners  
  Jinhua Liang, Xubo Liu, Haohe Liu, Huy Phan, Emmanouil Benetos, Mark D. Plumbley, Wenwu Wang   
  TFECN: Time-Frequency Enhanced ConvNet for Audio Classification  
  Mengwei Wang, Zhe Yang   
  Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection  
  Won-Gook Choi, Joon-Hyuk Chang   
  Fine-tuning Audio Spectrogram Transformer with Task-aware Adapters for Sound Event Detection  
  Kang Li, Yan Song, Ian McLoughlin, Lin Liu, Jin Li, Li-Rong Dai   
  Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness  
  Dianwen Ng, Yang Xiao, Jia Qi Yip, Zhao Yang, Biao Tian, Qiang Fu, Eng Siong Chng, Bin Ma   
  Few-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes  
  Wei Xie, Yanxiong Li, Qianhua He, Wenchang Cao, Tuomas Virtanen   
  Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion  
  Mohammad Hassan Vali, Tom Bäckström   
  Topological Data Analysis for Speech Processing  
  Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev   
  Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation  
  Kangwook Jang, Sungnyun Kim, Se-Young Yun, Hoirin Kim   
  Personalized Acoustic Scene Classification in Ultra-low Power Embedded Devices Using Privacy-preserving Data Augmentation  
  Timm Koppelmann, Semih Agcaer, Rainer Martin   
  Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection  
  Wei-Cheng Lin, Luca Bondi, Shabnam Ghaffarzadegan   
  Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning  
  Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren   
  Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds  
  Hejing Zhang, Jian Guan, Qiaoxi Zhu, Feiyang Xiao, Youde Liu   
  Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions  
  Yifei Xin, Yuexian Zou   
  Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data  
  Suhas BN, Sarah Rajtmajer, Saeed Abdullah   
  Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech  
  Shijun Wang, Jón Guðnason, Damian Borth   
  Towards Multi-Lingual Audio Question Answering  
  Swarup Ranjan Behera, Pailla Balakrishna Reddy, Achyut Mani Tripathi, Megavath Bharadwaj Rathod, Tejesh Karavadi   

 Speech Recognition: Architecture, Search, and Linguistic Components 1  
  Diacritic Recognition Performance in Arabic ASR  
  Hanan Aldarmaki, Ahmad Ghannam   
  Personalization for BERT-based Discriminative Speech Recognition Rescoring  
  Jari Kolehmainen, Yile Gu, Aditya Gourav, Prashanth Gurunath Shivakumar, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko   
  On the N-gram Approximation of Pre-trained Language Models  
  Aravind Krishnan, Jesujoba O. Alabi, Dietrich Klakow   
  Record Deduplication for Entity Distribution Modeling in ASR Transcripts  
  Tianyu Huang, Chung Hoon Hong, Carl Wivagg, Kanna Shimizu   
  Learning When to Trust Which Teacher for Weakly Supervised ASR  
  Aakriti Agrawal, Milind Rao, Anit Kumar Sahu, Gopinath Chennupati, Andreas Stolcke   
  Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer  
  Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma   

 Speech Recognition: Technologies and Systems for New Applications 1  
  Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model  
  Puyuan Peng, Shang-Wen Li, Okko Räsänen, Abdelrahman Mohamed, David Harwath   
  Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization  
  Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath   
  Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys  
  Roger K. Moore, Ricard Marxer   
  Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling  
  Ramon Sanabria, Ondřej Klejch, Hao Tang, Sharon Goldwater   
  CASA-ASR: Context-Aware Speaker-Attributed ASR  
  Mohan Shi, Zhihao Du, Qian Chen, Fan Yu, Yangze Li, Shiliang Zhang, Jie Zhang, Li-Rong Dai   
  Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams  
  Shun Takahashi, Sakriani Sakti   
  AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark  
  Gaobin Yang, Jun Du, Maokui He, Shutong Niu, Baoxiang Li, Jiakui Li, Chin-Hui Lee   
  Distilling knowledge from Gaussian process teacher to neural network student  
  Jeremy H. M. Wong, Huayun Zhang, Nancy F. Chen   
  Segmental SpeechCLIP: Utilizing Pretrained Image-text Models for Audio-Visual Learning  
  Saurabhchand Bhati, Jesús Villalba, Laureano Moro-Velazquez, Thomas Thebaud, Najim Dehak   
  Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili  
  Christiaan Jacobs, Nathanaël Carraz Rakotonirina, Everlyn Asiko Chimoto, Bruce A. Bassett, Herman Kamper   
  Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning  
  Ruan van der Merwe, Herman Kamper   
  Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems  
  Martin Poláček, Petr Červa, Jindřich Žďánský, Lenka Weingartová   
  Language Agnostic Data-Driven Inverse Text Normalization  
  Szu-Jui Chen, Debjyoti Paul, Yutong Pang, Peng Su, Xuedong Zhang   
  How to Estimate Model Transferability of Pre-Trained Speech Models?  
  Zih-Ching Chen, Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Shuo-Yiin Chang, Rohit Prabhavalkar, Hung-yi Lee, Tara Sainath   
  Transcribing Speech as Spoken and Written Dual Text Using an Autoregressive Model  
  Mana Ihori, Hiroshi Sato, Tomohiro Tanaka, Ryo Masumura, Saki Mizuno, Nobukatsu Hojo   

 Lexical and Language Modeling for ASR  
  NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning  
  Kamer Ali Yuksel, Thiago Castro Ferreira, Golara Javadi, Mohamed Al-Badrashiny, Ahmet Gunduz   
  Scaling Laws for Discriminative Speech Recognition Rescoring Models  
  Yile Gu, Prashanth Gurunath Shivakumar, Jari Kolehmainen, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko   
  Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition  
  Hong Liu, Zhaobiao Lv, Zhijian Ou, Wenbo Zhao, Qing Xiao   
  Memory Augmented Lookup Dictionary Based Language Modeling for Automatic Speech Recognition  
  Yukun Feng, Ming Tu, Rui Xia, Chuanzeng Huang, Yuxuan Wang   
  Memory Network-Based End-To-End Neural ES-KMeans for Improved Word Segmentation  
  Yu Iwamoto, Takahiro Shinozaki   
  Retraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation  
  Yui Sudo, Kazuya Hata, Kazuhiro Nakadai   

 Language Identification and Diarization  
  Lightweight and Efficient Spoken Language Identification of Long-form Audio  
  Winstead Zhu, Md Iftekhar Tanveer, Yang Janet Liu, Seye Ojumu, Rosie Jones   
  End to End Spoken Language Diarization with Wav2vec Embeddings  
  Jagabandhu Mishra, Jayadev N Patil, Amartya Chowdhury, Mahadeva Prasanna   
  Efficient Spoken Language Recognition via Multilabel Classification  
  Oriol Nieto, Zeyu Jin, Franck Dernoncourt, Justin Salamon   
  Description and Analysis of ABC Submission to NIST LRE 2022  
  Pavel Matejka, Anna Silnova, Josef Slavíček, Ladislav Mosner, Oldřich Plchot, Michal Klčo, Junyi Peng, Themos Stafylakis, Lukáš Burget   
  Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation  
  Tanel Alumäe, Kunnar Kukk, Viet-Bac Le, Claude Barras, Abdel Messaoudi, Waad Ben Kheder   
  Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22  
  Jesús Villalba, Jonas Borgstrom, Maliha Jahan, Saurabh Kataria, Leibny Paola Garcia, Pedro Torres-Carrasquillo, Najim Dehak   

 Speech Quality Assessment  
  DeePMOS: Deep Posterior Mean-Opinion-Score of Speech  
  Xinyu Liang, Fredrik Cumlin, Christian Schüldt, Saikat Chatterjee   
  The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study  
  Ashwini Dasare, Pradyoth Hegde, Supritha Shetty, Deepak K T   
  A no-reference speech quality assessment method based on neural network with densely connected convolutional architecture  
  Wuxuan Gong, Jing Wang, Yitong Liu, Hongwen Yang   
  Probing Speech Quality Information in ASR Systems  
  Bao Thang Ta, Minh Tu Le, Nhat Minh Le, Van Hai Do   
  Preference-based training framework for automatic speech quality assessment using deep neural network  
  Cheng-Hung Hu, Yusuke Yasuda, Tomoki Toda   
  Crowdsourced Data Validation for ASR Training  
  Wannaphong Phatthiyaphaibun, Chompakorn Chaksangchaichot, Thanawin Rakthammanon, Ekapol Chuangsuwanich, Sarana Nutanong   

 Feature Modeling for ASR  
  Re-investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods  
  Zhouyuan Huo, Khe Chai Sim, Dongseong Hwang, Tsendsuren Munkhdalai, Tara Sainath, Pedro M. Mengibar   
  Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training  
  Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue   
  InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition  
  Zhi-Hao Lai, Tian-Hao Zhang, Qi Liu, Xinyuan Qian, Li-Fang Wei, Feng Chen, Song-Lu Chen, Xu-Cheng Yin   
  Transductive Feature Space Regularization for Few-shot Bioacoustic Event Detection  
  Yizhou Tan, Haojun Ai, Shengchen Li, Feng Zhang   
  Incorporating L2 Phonemes Using Articulatory Features for Robust Speech Recognition  
  Jisung Wang, Haram Lee, Myungwoo Oh   
  On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning  
  Titouan Parcollet, Shucong Zhang, Rogier van Dalen, Alberto Gil C. P. Ramos, Sourav Bhattacharya   

 Interfacing Speech Technology and Phonetics  
  Phonemic competition in end-to-end ASR models  
  Louis ten Bosch, Martijn Bentum, Lou Boves   
  Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics  
  Vincent Hughes, Jessica Wormald, Paul Foulkes, Philip Harrison, Finnian Kelly, David van der Vloed, Poppy Welch, Chenzi Xu   
  Exploring Graph Theory Methods For the Analysis of Pronunciation Variation in Spontaneous Speech  
  Bernhard C. Geiger, Barbara Schuppler   
  Automatic Speaker Recognition performance with matched and mismatched female bilingual speech data  
  Bryony Nuttall, Philip Harrison, Vincent Hughes   

 Speech Synthesis: Multilinguality  
  FACTSpeech: Speaking a Foreign Language Pronunciation Using Only Your Native Characters  
  Hong-Sun Yang, Ji-Hoon Kim, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Shuk-Jae Choi, Hyung-Yong Kim   
  Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model  
  Hoyeon Lee, Hyun-Wook Yoon, Jong-Hwan Kim, Jae-Min Kim   
  DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech  
  Sen Liu, Yiwei Guo, Chenpeng Du, Xie Chen, Kai Yu   
  Generating Multilingual Gender-Ambiguous Text-to-Speech Voices  
  Konstantinos Markopoulos, Georgia Maniati, Georgios Vamvoukakis, Nikolaos Ellinas, Georgios Vardaxoglou, Panos Kakoulidis, Junkwang Oh, Gunu Jho, Inchul Hwang, Aimilios Chalamandaris, Pirros Tsiakoulis, Spyros Raptis   
  RAD-MMM: Multilingual Multiaccented Multispeaker Text To Speech  
  Rohan Badlani, Rafael Valle, Kevin J. Shih, João Felipe Santos, Siddharth Gururani, Bryan Catanzaro   
  Multilingual context-based pronunciation learning for Text-to-Speech  
  Giulia Comini, Sam Ribeiro, Fan Yang, Heereen Shim, Jaime Lorenzo-Trueba   

 Speech Emotion Recognition 1  
  Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition  
  Minh Tran, Yufeng Yin, Mohammad Soleymani   
  The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers  
  Huang-Cheng Chou, Lucas Goncalves, Seong-Gyun Leem, Chi-Chun Lee, Carlos Busso   
  A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model  
  Mohammad Ibrahim Malik, Siddique Latif, Raja Jurdak, Björn W. Schuller   
  Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack  
  Basmah Alsenani, Tanaya Guha, Alessandro Vinciarelli   
  Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition  
  James Tavernor, Matthew Perez, Emily Mower Provost   
  Stable Speech Emotion Recognition with Head-k-Pooling Loss  
  Chaoyue Ding, Jiakui Li, Daoming Zong, Baoxiang Li, Tian-Hao Zhang, Qunyan Zhou   

 Show and Tell: Health applications and emotion recognition  
  A Personalised Speech Communication Application for Dysarthric Speakers  
  Matthew Gibson, Ievgen Karaulov, Oleksii Zhelo, Filip Jurcicek   
  Video Multimodal Emotion Recognition System for Real World Applications  
  Sun-Kyung Lee, Jong-Hwan Kim   
  Promoting Mental Self-Disclosure in a Spoken Dialogue System  
  Mahdin Rohmatillah, Bobbi Aditya, Li-Jen Yang, Bryan Gautama Ngo, Willianto Sulaiman, Jen-Tzung Chien   
  "Select language, modality or put on a mask!" Experiments with Multimodal Emotion Recognition  
  Paweł Bujnowski, Bartłomiej Kuźma, Bartłomiej Paziewski, Jacek Rutkowski, Joanna Marhula, Zuzanna Bordzicka, Piotr Andruszkiewicz   
  My Vowels Matter: Formant Automation Tools for Diverse Child Speech  
  Hannah Valentine, Joel MacAuslan, Maria Grigos, Marisha Speights   
  NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments  
  Nicky Chong-White, Arun Sebastian, Jorge Mejia   
  When Words Speak Just as Loudly as Actions: Virtual Agent Based Remote Health Assessment Integrating What Patients Say with What They Do  
  Vikram Ramanarayanan, David Pautler, Lakshmi Arbatti, Abhishek Hosamath, Michael Neumann, Hardik Kothare, Oliver Roesler, Jackson Liscombe, Andrew Cornish, Doug Habberstad, Vanessa Richter, David Fox, David Suendermann-Oeft, Ira Shoulson   
  Stuttering Detection Application  
  Kowshik Siva Sai Motepalli, Vamshiraghusimha Narasinga, Harsha Pathuri, Hina Khan, Sangeetha Mahesh, Ajish K. Abraham, Anil Kumar Vuppala   
  Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games  
  Mario Zusag, Laurin Wagner   
  Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for Cognitive Monitoring  
  Jacob Solinsky, Raymond Finzel, Martin Michalowski, Serguei Pakhomov   
  5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids  
  Ankit Gupta, Abhijeet Bishnu, Mandar Gogate, Kia Dashtipour, Tughrul Arslan, Ahsan Adeel, Amir Hussain, Tharmalingam Ratnarajah, Mathini Sellathurai   
  Towards Two-point Neuron-inspired Energy-efficient Multimodal Open Master Hearing Aid  
  Mohsin Raza, Adewale Adetomi, Khubaib Ahmed, Amir Hussain, Tughrul Arslan, Ahsan Adeel   

 Spoken Dialog Systems and Conversational Analysis 1  
  FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding  
  Xuxin Cheng, Wanshi Xu, Ziyu Yao, Zhihong Zhu, Yaowei Li, Hongxiang Li, Yuexian Zou   
  C²A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding  
  Xuxin Cheng, Ziyu Yao, Zhihong Zhu, Yaowei Li, Hongxiang Li, Yuexian Zou   
  Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets  
  Henry Weld, Sijia Hu, Siqu Long, Josiah Poon, Soyeon Han   
  Semantic Enrichment Towards Efficient Speech Representations  
  Gaëlle Laperrière, Ha Nguyen, Sahar Ghannay, Bassam Jabaian, Yannick Estève   
  Tensor decomposition for minimization of E2E SLU model toward on-device processing  
  Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, Emiru Tsunoo, Shinji Watanabe   
  DiffSLU: Knowledge Distillation Based Diffusion Model for Cross-Lingual Spoken Language Understanding  
  Tianjun Mao, Chenghong Zhang   
  Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding  
  Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe   
  Contrastive Learning Based ASR Robust Knowledge Selection For Spoken Dialogue System  
  Zhiyuan Zhu, Yusheng Liao, Yu Wang, Yunfeng Guan   
  Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space  
  Seongmin Park, Jinkyu Seo, Jihwa Lee   
  An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding  
  Umberto Cappellazzo, Daniele Falavigna, Alessio Brutti   
  Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning  
  Zhenhe Wu, Xiaoguang Yu, Meng Chen, Liangqing Wu, Jiahao Ji, Zhoujun Li   
  Personalized Predictive ASR for Latency Reduction in Voice Assistants  
  Andreas Schwarz, Di He, Maarten Van Segbroeck, Mohammed Hethnawi, Ariya Rastrow   
  Compositional Generalization in Spoken Language Understanding  
  Avik Ray, Yilin Shen, Hongxia Jin   
  Sampling bias in NLU models: Impact and Mitigation  
  Zefei Li, Anil Ramakrishna, Anna Rumshisky, Andy Rosenbaum, Saleh Solta, Rahul Gupta   
  5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair  
  Jiarui Lu, Bo-Hsiang Tseng, Joel Ruben Antony Moniz, Site Li, Xueyun Zhu, Hong Yu, Murat Akbacak   
  Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation  
  Xiaohan Shi, Xingfeng Li, Tomoki Toda   
  WhiSLU: End-to-End Spoken Language Understanding with Whisper  
  Minghan Wang, Yinglu Li, Jiaxin Guo, Xiaosong Qiao, Zongyao Li, Hengchao Shang, Daimeng Wei, Shimin Tao, Min Zhang, Hao Yang   

 Speech Coding and Enhancement 1  
  Biophysically-inspired single-channel speech enhancement in the time domain  
  Chuan Wen, Sarah Verhulst   
  On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer  
  Md Asif Jalal, Pablo Peso Parada, Jisi Zhang, Mete Ozay, Karthikeyan Saravanan, Myoungji Han, Jung In Lee, Seokyeong Jung   
  How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning  
  Hye-jin Shim, Rosa Gonzalez Hautamäki, Md Sahidullah, Tomi Kinnunen   
  CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram  
  Zhifeng Kong, Wei Ping, Ambrish Dantrey, Bryan Catanzaro   
  A Two-stage Progressive Neural Network for Acoustic Echo Cancellation  
  Zhuangqi Chen, Xianjun Xia, Cheng Chen, Xianke Wang, Yanhong Leng, Li Chen, Roberto Togneri, Yijian Xiao, Piao Ding, Shenyi Song, Pingjian Zhang   
  An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec  
  Linping Xu, Jiawei Jiang, Dejun Zhang, Xianjun Xia, Li Chen, Yijian Xiao, Piao Ding, Shenyi Song, Sixing Yin, Ferdous Sohel   
  Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations  
  Shucong Zhang, Malcolm Chadwick, Alberto Gil C. P. Ramos, Titouan Parcollet, Rogier van Dalen, Sourav Bhattacharya   
  CFTNet: Complex-valued Frequency Transformation Network for Speech Enhancement  
  Nursadul Mamun, John H. L. Hansen   
  Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement  
  Hejung Yang, Hong-Goo Kang   
  Multi-mode Neural Speech Coding Based on Deep Generative Networks  
  Wei Xiao, Wenzhe Liu, Meng Wang, Shan Yang, Yupeng Shi, Yuyong Kang, Dan Su, Shidong Shang, Dong Yu   
  Streaming Dual-Path Transformer for Speech Enhancement  
  Soo Hyun Bae, Seok Wan Chae, Youngseok Kim, Keunsang Lee, Hyunjin Lim, Lae-Hoon Kim   
  Sequence-to-Sequence Multi-Modal Speech In-Painting  
  Mahsa Kadkhodaei Elyaderani, Shahram Shirani   
  Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression  
  Hao Zhang, Meng Yu, Yuzhong Wu, Tao Yu, Dong Yu   
  Differentially Private Adapters for Parameter Efficient Acoustic Modeling  
  Chun-Wei Ho, Chao-Han Huck Yang, Sabato Marco Siniscalchi   
  Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation  
  Rui-Chen Zheng, Yang Ai, Zhen-Hua Ling   
  Consonant-emphasis Method Incorporating Robust Consonant-section Detection to Improve Intelligibility of Bone-conducted speech  
  Yasufumi Uezu, Sicheng Wang, Teruki Toya, Masashi Unoki   
  Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss  
  Hiroshi Sato, Ryo Masumura, Tsubasa Ochiai, Marc Delcroix, Takafumi Moriya, Takanori Ashihara, Kentaro Shinayama, Saki Mizuno, Mana Ihori, Tomohiro Tanaka, Nobukatsu Hojo   
  Perceptual Improvement of Deep Neural Network (DNN) Speech Coder Using Parametric and Non-parametric Density Models  
  Joon Byun, Seungmin Shin, Jongmo Sung, Seungkwon Beack, Youngcheol Park   
  DeFT-AN RT: Real-time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window  
  Dongheon Lee, Dayun Choi, Jung-Woo Choi   

 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 2  
  A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer  
  Kyungmin Lee, Haeri Kim, Sichen Jin, Jinhwan Park, Youngho Han   
  Attention Gate Between Capsules in Fully Capsule-Network Speech Recognition  
  Kyungmin Lee, Hyeontaek Lim, Mun-Hwan Lee, Hong-Gee Kim   
  ML-SUPERB: Multilingual Speech Universal PERformance Benchmark  
  Jiatong Shi, Dan Berrebbi, William Chen, En-Pei Hu, Wei-Ping Huang, Ho-Lam Chung, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe   
  General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization  
  Dohee Kim, Daeyeol Shim, Joon-Hyuk Chang   
  Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition  
  Keke Zhao, Peng Song, Shaokai Li, Wenming Zheng   
  Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data  
  Takafumi Moriya, Hiroshi Sato, Tsubasa Ochiai, Marc Delcroix, Takanori Ashihara, Kohei Matsuura, Tomohiro Tanaka, Ryo Masumura, Atsunori Ogawa, Taichi Asami   
  Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition  
  Yist Y. Lin, Tao Han, Haihua Xu, Van Tung Pham, Yerbolat Khassanov, Tze Yuang Chong, Yi He, Lu Lu, Zejun Ma   
  Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers  
  Nithish Muthuchamy Selvaraj, Xiaobao Guo, Adams Kong, Bingquan Shen, Alex Kot   
  Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding  
  Tian-Hao Zhang, Hai-Bo Qin, Zhi-Hao Lai, Song-Lu Chen, Qi Liu, Feng Chen, Xinyuan Qian, Xu-Cheng Yin   
  Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation  
  Zheng Liang, Zheshu Song, Ziyang Ma, Chenpeng Du, Kai Yu, Xie Chen   
  Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts  
  Dongji Gao, Matthew Wiesner, Hainan Xu, Leibny Paola Garcia, Daniel Povey, Sanjeev Khudanpur   
  DCCRN-KWS: An Audio Bias Based Model for Noise Robust Small-Footprint Keyword Spotting  
  Shubo Lv, Xiong Wang, Sining Sun, Long Ma, Lei Xie   
  OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition  
  Li Fu, Siqi Li, Qingtao Li, Fangzhu Li, Liping Deng, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He   
  Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition  
  Maurits Bleeker, Pawel Swietojanski, Stefan Braun, Xiaodan Zhuang   
  Rehearsal-Free Online Continual Learning for Automatic Speech Recognition  
  Steven Vander Eeckt, Hugo Van hamme   

 Speech Recognition: Technologies and Systems for New Applications 2  
  Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring  
  Kaiqi Fu, Shaojun Gao, Shuju Shi, Xiaohai Tian, Wei Li, Zejun Ma   
  Disentangling the Contribution of Non-native Speech in Automated Pronunciation Assessment  
  Shuju Shi, Kaiqi Fu, Yiwei Gu, Xiaohai Tian, Shaojun Gao, Wei Li, Zejun Ma   
  A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning  
  Hyungshin Ryu, Sunhee Kim, Minhwa Chung   
  Assessing Intelligibility in Non-native Speech: Comparing Measures Obtained at Different Levels  
  Xing Wei, Roeland van Hout, Catia Cucchiarini, Danielle Reuvekamp, Helmer Strik   
  End-to-End Word-Level Pronunciation Assessment with MASK Pre-training  
  Yukang Liang, Kaitao Song, Shaoguang Mao, Huiqiang Jiang, Luna Qiu, Yuqing Yang, Dongsheng Li, Linli Xu, Lili Qiu   
  A Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment  
  Fu-An Chao, Tien-Hong Lo, Tzu-I Wu, Yao-Ting Sung, Berlin Chen   
  Automatic Prediction of Language Learners' Listenability Using Speech and Text Features Extracted from Listening Drills  
  Yingxiang Gao, Jaehyun Choi, Nobuaki Minematsu, Noriko Nakanishi, Daisuke Saito   
  Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer  
  Ram C. M. C. Shekar, Mu Yang, Kevin Hirschi, Stephen Looney, Okim Kang, John H. L. Hansen   
  Adapting an Unadaptable ASR System  
  Rao Ma, Mengjie Qian, Mark J. F. Gales, Kate M. Knill   
  Addressing Cold Start Problem for End-to-end Automatic Speech Scoring  
  Jungbae Park, Seungtaek Choi   
  Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings  
  Sam Ribeiro, Giulia Comini, Jaime Lorenzo-Trueba   
  Orthography-based Pronunciation Scoring for Better CAPT Feedback  
  Caitlin Richter, Ragnar Pálsson, Luke O'Brien, Kolbrún Friðriksdóttir, Branislav Bédi, Eydís Huld Magnúsdóttir, Jón Guðnason   
  Zero-Shot Automatic Pronunciation Assessment  
  Hongfu Liu, Mingqian Shi, Ye Wang   
  Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese  
  Tuong Tu Huu, Viet Thanh Pham, Thi Thu Trang Nguyen, Thai Lai Dao   

 Keynote 2  
  Beyond the AI hype: Balancing Innovation and Social Responsibility  
  Virginia Dignum   

 Paralinguistics 1  
  Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach  
  Georg Stemmer, Paulo Lopez Meyer, Juan Del Hoyo Ontiveros, Jose Lopez, Hector A. Cordourier, Tobias Bocklet   
  Detection of Laughter and Screaming Using the Attention and CTC Models  
  Takuto Matsuda, Yoshiko Arimoto   
  Capturing Formality in Speech Across Domains and Languages  
  Debasmita Bhattacharya, Jie Chi, Julia Hirschberg, Peter Bell   
  Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio  
  Jialu Li, Mark Hasegawa-Johnson, Nancy L. McElwain   
  Cues to next-speaker projection in conversational Swedish: Evidence from reaction times  
  Kathrin Feindt, Martina Rossi, Ghazaleh Esfandiari-Baiat, Axel G. Ekström, Margaret Zellers   
  Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech  
  Areej Buker, Huda Alsofyani, Alessandro Vinciarelli   

 Speech Enhancement and Denoising  
  Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation  
  Sefik Emre Eskimez, Takuya Yoshioka, Alex Ju, Min Tang, Tanel Pärnamaa, Huaming Wang   
  TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective  
  Andong Li, Weixin Meng, Guochen Yu, Wenzhe Liu, Xiaodong Li, Chengshi Zheng   
  MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement  
  Yulong Wang, Xueliang Zhang   
  Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement  
  Zilu Guo, Jun Du, Chin-Hui Lee, Yu Gao, Wenbin Zhang   
  Multi-input Multi-output Complex Spectral Mapping for Speaker Separation  
  Hassan Taherian, Ashutosh Pandey, Daniel Wong, Buye Xu, DeLiang Wang   
  Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain  
  Maurice Oberhag, Daniel Neudek, Rainer Martin, Tobias Rosenkranz, Henning Puder   

 Speech Synthesis: Evaluation  
  Listener sensitivity to deviating obstruents in WaveNet  
  Ayushi Pandey, Jens Edlund, Sébastien Le Maguer, Naomi Harte   
  How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics  
  Joonyong Park, Shinnosuke Takamichi, Tomohiko Nakamura, Kentaro Seki, Detai Xin, Hiroshi Saruwatari   
  MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors  
  Joshua Camp, Tom Kenter, Lev Finkelstein, Rob Clark   
  RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting  
  Hui Wang, Shiwan Zhao, Xiguang Zheng, Yong Qin   
  Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users  
  Gerda Ana Melnik-Leroy, Gediminas Navickas   
  Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech  
  Erica Cooper, Junichi Yamagishi   

 End-to-end Spoken Dialog Systems  
  Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding  
  Mutian He, Philip N. Garner   
  Improving End-to-End SLU performance with Prosodic Attention and Distillation  
  Shangeth Rajaa   
  Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding  
  Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer   
  Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language Understanding  
  Lingyan Huang, Tao Li, Haodong Zhou, Qingyang Hong, Lin Li   
  ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding  
  Vishal Sunder, Eric Fosler-Lussier, Samuel Thomas, Hong-Kwang J Kuo, Brian Kingsbury   
  GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering  
  Xuxin Cheng, Zhihong Zhu, Ziyu Yao, Hongxiang Li, Yaowei Li, Yuexian Zou   

 Biosignal-enabled Spoken Communication  
  Obstructive Sleep Apnea Detection using Pre-trained Speech Representations  
  Kaibo Zhang, Lili Cao, Yiming Ding, Yanru Li, Chao Zhang, Ji Wu, Demin Han   
  EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network  
  Ruicong Wang, Siqi Cai, Haizhou Li   
  Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video  
  Rachel Beeson, Korin Richmond   
  Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG  
  Kai Yang, Zhuang Xie, Di Zhou, Longbiao Wang, Gaoyan Zhang   
  Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG  
  Soowon Kim, Young-Eun Lee, Seo-Hyun Lee, Seong-Whan Lee   
  Towards Ultrasound Tongue Image prediction from EEG during speech production  
  Tamás Gábor Csapó, Frigyes Viktor Arthur, Péter Nagy, Ádám Boncz   
  Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks  
  László Tóth, Amin Honarmandi Shandiz, Gábor Gosztolya, Tamás Gábor Csapó   
  STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks  
  Kevin Scheck, Tanja Schultz   
  Spanish Phone Confusion Analysis for EMG-Based Silent Speech Interfaces  
  Inge Salomons, Eder del Blanco, Eva Navas, Inma Hernáez   
  Hybrid Silent Speech Interface Through Fusion of Electroencephalography and Electromyography  
  Huiyan Li, Mingyi Wang, Han Gao, Shuo Zhao, Guang Li, You Wang   

 Neural-based Speech and Acoustic Analysis  
  Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?  
  Eklavya Sarkar, Mathew Magimai.-Doss   
  Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains  
  Jinjin Cai, Sudip Vhaduri, Xiao Luo   
  Background-aware Modeling for Weakly Supervised Sound Event Detection  
  Yifei Xin, Dongchao Yang, Yuexian Zou   
  How to (Virtually) Train Your Speaker Localizer  
  Prerak Srivastava, Antoine Deleforge, Archontis Politis, Emmanuel Vincent   
  MMER: Multimodal Multi-task Learning for Speech Emotion Recognition  
  Sreyan Ghosh, Utkarsh Tyagi, S Ramaneswaran, Harshvardhan Srivastava, Dinesh Manocha   
  A Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds  
  Tanmay Khandelwal, Rohan Kumar Das   

 DiGo - Dialog for Good: Speech and Language Technology for Social Good  
  A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication  
  Michael Neumann, Hardik Kothare, Doug Habberstad, Vikram Ramanarayanan   
  Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation  
  Angus Addlesee, Marco Damonte   
  Developing Speech Processing Pipelines for Police Accountability  
  Anjalie Field, Prateek Verma, Nay San, Jennifer L. Eberhardt, Dan Jurafsky   
  Prosody-controllable Gender-ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception  
  Éva Székely, Joakim Gustafson, Ilaria Torre   
  Affective attributes of French caregivers' professional speech  
  Jean-Luc Rouas, Yaru Wu, Takaaki Shochi   

 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 3  
  ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion  
  Edresson Casanova, Christopher Shulby, Alexander Korolev, Arnaldo Candido Junior, Anderson da Silva Soares, Sandra Aluísio, Moacir Antonelli Ponti   
  Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition  
  Yue Gu, Zhihao Du, Shiliang Zhang, Qian Chen, Jiqing Han   
  Target Vocabulary Recognition Based on Multi-Task Learning with Decomposed Teacher Sequences  
  Aoi Ito, Tatsuya Komatsu, Yusuke Fujita, Yusuke Kida   
  Wave to Syntax: Probing spoken language models for syntax  
  Gaofei Shen, Afra Alishahi, Arianna Bisazza, Grzegorz Chrupała   
  Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR  
  Burin Naowarat, Philip Harding, Pasquale D'Alterio, Sibo Tong, Bashar Awwad Shiekh Hasan   
  Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation  
  Ziyang Ma, Zhisheng Zheng, Guanrou Yang, Yu Wang, Chao Zhang, Xie Chen   
  SlothSpeech: Denial-of-service Attack Against Speech Recognition Models  
  Mirazul Haque, Rutvij Shah, Simin Chen, Berrak Sisman, Cong Liu, Wei Yang   
  CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition  
  Zhihan Wang, Feng Hou, Ruili Wang   
  Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation  
  Li-Fang Lai, Nicole Holliday   
  Can Contextual Biasing Remain Effective with Whisper and GPT-2?  
  Guangzhi Sun, Xianrui Zheng, Chao Zhang, Philip C. Woodland   
  Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation  
  Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino   
  Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition  
  Xiaodong Cui, George Saon, Brian Kingsbury   
  MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition  
  Jiamin Xie, John H. L. Hansen   
  Adapting Multi-Lingual ASR Models for Handling Multiple Talkers  
  Chenda Li, Yao Qian, Zhuo Chen, Naoyuki Kanda, Dongmei Wang, Takuya Yoshioka, Yanmin Qian, Michael Zeng   
  Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition  
  Dianwen Ng, Chong Zhang, Ruixi Zhang, Yukun Ma, Trung Hieu Nguyen, Chongjia Ni, Shengkui Zhao, Qian Chen, Wen Wang, Eng Siong Chng, Bin Ma   
  Model-Internal Slot-triggered Biasing for Domain Expansion in Neural Transducer ASR Models  
  Yiting Lu, Philip Harding, Kanthashree Mysore Sathyendra, Sibo Tong, Xuandi Fu, Jing Liu, Feng-Ju Chang, Simon Wiesler, Grant P. Strimel   
  Delay-penalized CTC Implemented Based on Finite State Transducer  
  Zengwei Yao, Wei Kang, Fangjun Kuang, Liyong Guo, Xiaoyu Yang, Yifan Yang, Long Lin, Daniel Povey   

 Speech Recognition: Architecture, Search, and Linguistic Components 2  
  Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation  
  Jiaxu Zhu, Weinan Tong, Yaoxun Xu, Changhe Song, Zhiyong Wu, Zhao You, Dan Su, Dong Yu, Helen Meng   
  Knowledge Distillation Approach for Efficient Internal Language Model Estimation  
  Zhipeng Chen, Haihua Xu, Yerbolat Khassanov, Yi He, Lu Lu, Zejun Ma, Ji Wu   
  Language Model Personalization for Improved Touchscreen Typing  
  Jiban Adhikary, Keith Vertanen   
  Blank Collapse: Compressing CTC Emission for the Faster Decoding  
  Minkyu Jung, Ohhyeok Kwon, Seunghyun Seo, Soonshin Seo   
  Improving Joint Speech-Text Representations Without Alignment  
  Cal Peyser, Zhong Meng, Rohit Prabhavalkar, Andrew Rosenberg, Tara Sainath, Michael Picheny, Kyunghyun Cho, Ke Hu   
  Leveraging Cross-Utterance Context For ASR Decoding  
  Robert Flynn, Anton Ragni   
  Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation  
  Minglun Han, Feilong Chen, Jing Shi, Shuang Xu, Bo Xu   
  Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition  
  Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe   
  A Neural Time Alignment Module for End-to-End Automatic Speech Recognition  
  Dongcheng Jiang, Chao Zhang, Philip C. Woodland   
  Accelerating Transducers through Adjacent Token Merging  
  Yuang Li, Yu Wu, Jinyu Li, Shujie Liu   
  Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition  
  Siyuan Feng, Ming Tu, Rui Xia, Chuanzeng Huang, Yuxuan Wang   
  Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition  
  Wenxuan Wang, Guodong Ma, Yuke Li, Binbin Du   
  Embedding Articulatory Constraints for Low-resource Speech Recognition Based on Large Pre-trained Model  
  Jaeyoung Lee, Masato Mimura, Tatsuya Kawahara   
  Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning  
  Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe   
  SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings  
  Alexandra Antonova, Evelina Bakhturina, Boris Ginsburg   
  Text Injection for Capitalization and Turn-Taking Prediction in Speech Models  
  Shaan Bijwadia, Shuo-Yiin Chang, Weiran Wang, Zhong Meng, Hao Zhang   
  Confidence-based Ensembles of End-to-End Speech Recognition Models  
  Igor Gitman, Vitaly Lavrukhin, Aleksandr Laptev, Boris Ginsburg   
  Unsupervised Code-switched Text Generation from Parallel Text  
  Jie Chi, Brian Lu, Jason Eisner, Peter Bell, Preethi Jyothi, Ahmed M. Ali   
  A Binary Keyword Spotting System with Error-Diffusion Based Feature Binarization  
  Dingyi Wang, Mengjie Luo, Lin Li, Xiaoqin Wang, Shushan Qiao, Yumei Zhou   
  Language-universal Phonetic Encoder for Low-resource Speech Recognition  
  Siyuan Feng, Ming Tu, Rui Xia, Chuanzeng Huang, Yuxuan Wang   
  A Lexical-aware Non-autoregressive Transformer-based ASR Model  
  Chong-En Lin, Kuan-Yu Chen   
  Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions  
  Joshua Jansen van Vüren, Thomas Niesler   

 Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1  
  Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text  
  Jerome R. Bellegarda   
  ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition  
  Yuanchao Li, Zeyu Zhao, Ondřej Klejch, Peter Bell, Catherine Lai   
  BASS: Block-wise Adaptation for Speech Summarization  
  Roshan Sharma, Siddhant Arora, Kenneth Zheng, Shinji Watanabe, Rita Singh, Bhiksha Raj   
  Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus  
  Meena M. C. Shekar, John H. L. Hansen   
  Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion  
  Tianfang Yan, Kikuo Maekawa, Yukiko Nota, Masayuki Hirata   
  A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition  
  Xiaoheng Zhang, Yang Li   
  Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm  
  Cristian Chivriga, Rinita Roy   
  Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction  
  Adhiraj Banerjee, Vipul Arora   
  Query Based Acoustic Summarization for Podcasts  
  Samantha Kotey, Rozenn Dahyot, Naomi Harte   
  Spot Keywords From Very Noisy and Mixed Speech  
  Ying Shi, Dong Wang, Lantian Li, Jiqing Han, Shi Yin   
  Knowledge Distillation on Joint Task End-to-End Speech Translation  
  Khandokar Md. Nayem, Ran Xue, Ching-Yun Chang, Akshaya Vishnu Kudlu Shanbhogue   
  Investigating Pre-trained Audio Encoders in the Low-Resource Condition  
  Hao Yang, Jinming Zhao, Gholamreza Haffari, Ehsan Shareghi   
  Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target  
  Guan-Wei Wu, Guan-Ting Lin, Shang-Wen Li, Hung-yi Lee   

 Speech, Voice, and Hearing Disorders 1  
  Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test  
  Eungbeom Kim, Yunkee Chae, Jaeheon Sim, Kyogu Lee   
  Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition  
  Katerina Papadimitriou, Gerasimos Potamianos   
  Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features  
  Monica Gonzalez-Machorro, Pascal Hecker, Uwe D. Reichel, Helly N. Hammer, Robert Hoepner, Lisa Pedrotti, Alisha Zmutt, Hesam Sagha, Johan van Beek, Florian Eyben, Dagmar M. Schuller, Björn W. Schuller, Bert Arnrich   
  Whisper Features for Dysarthric Severity-Level Classification  
  Siddharth Rathod, Monil Charola, Akshat Vora, Yash Jogi, Hemant A. Patil   
  A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning  
  Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, Brian MacWhinney   
  Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra  
  Zhengjun Yue, Erfan Loweimi, Zoran Cvetkovic   
  A Stutter Seldom Comes Alone – Cross-Corpus Stuttering Detection as a Multi-label Problem  
  Sebastian P. Bayerl, Dominik Wagner, Ilja Baumann, Florian Hönig, Tobias Bocklet, Elmar Nöth, Korbinian Riedhammer   
  Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis  
  Tanuka Bhattacharjee, Anjali Jayakumar, Yamini Belur, Atchayaram Nalini, Ravi Yadav, Prasanta Kumar Ghosh   
  DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model  
  Helin Wang, Thomas Thebaud, Jesús Villalba, Myra Sydnor, Becky Lammers, Najim Dehak, Laureano Moro-Velazquez   
  CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice  
  Ramin Hedeshy, Raphael Menges, Steffen Staab   
  Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation  
  Massa Baali, Ibrahim Almakky, Shady Shehata, Fakhri Karray   
  Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling  
  Theodoros Kouzelis, Georgios Paraskevopoulos, Athanasios Katsamanis, Vassilis Katsouros   
  Glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo Parkinson's disease and Huntington's disease  
  Michal Novotný, Tereza Tykalová, Michal Šimek, Tomáš Kouba, Jan Rusz   
  An Analysis of Glottal Features of Chronic Kidney Disease Speech and Its Application to CKD Detection  
  Jihyun Mun, Sunhee Kim, Myeong Ju Kim, Jiwon Ryu, Sejoong Kim, Minhwa Chung   
  Weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels  
  Varun Belagali, Achuth Rao, Prasanta Kumar Ghosh   

 Speech Recognition: Technologies and Systems for New Applications 3  
  An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition  
  Zhengyang Li, Chenwei Liang, Timo Lohrenz, Marvin Sach, Björn Möller, Tim Fingscheidt   
  A Novel Self-training Approach for Low-resource Speech Recognition  
  Satwinder Singh, Feng Hou, Ruili Wang   
  FunASR: A Fundamental End-to-End Speech Recognition Toolkit  
  Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao Du, Shiliang Zhang   
  Streaming Audio-Visual Speech Recognition with Alignment Regularization  
  Pingchuan Ma, Niko Moritz, Stavros Petridis, Christian Fuegen, Maja Pantic   
  SparseVSR: Lightweight and Noise Robust Visual Speech Recognition  
  Adriana Fernandez-Lopez, Honglie Chen, Pingchuan Ma, Alexandros Haliassos, Stavros Petridis, Maja Pantic   
  Multimodal Speech Recognition for Language-Guided Embodied Agents  
  Allen Chang, Xiaoyuan Zhu, Aarav Monga, Seoho Ahn, Tejas Srinivasan, Jesse Thomason   

 Spoken Term Detection and Voice Search  
  Matching Latent Encoding for Audio-Text based Keyword Spotting  
  Kumari Nishu, Minsik Cho, Devang Naik   
  Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource  
  Sudhakar P, Sreenivasa K. Rao, Pabitra Mitra   
  On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation  
  Gene-Ping Yang, Yue Gu, Qingming Tang, Dongsu Du, Yuzong Liu   
  Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics  
  Umberto Michieli, Pablo Peso Parada, Mete Ozay   
  Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data  
  Seunghan Yang, Byeonggeun Kim, Kyuhong Shim, Simyoung Chang   
  Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability  
  Chouchang Yang, Yashas Malur Saidutta, Rakshith Sharma Srinivasa, Ching-Hua Lee, Yilin Shen, Hongxia Jin   

 Models for Streaming ASR  
  Enhancing the Unified Streaming and Non-streaming Model with Contrastive Learning  
  Yuting Yang, Yuke Li, Binbin Du   
  ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs  
  Xingchen Song, Di Wu, Binbin Zhang, Zhendong Peng, Bo Dang, Fuping Pan, Zhiyong Wu   
  Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation  
  Hanbyul Kim, Seunghyun Seo, Lukas Lee, Seolki Baek   
  DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer  
  Goeric Huybrechts, Srikanth Ronanki, Xilai Li, Hadis Nosrati, Sravan Bodapati, Katrin Kirchhoff   
  Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer  
  Kyuhong Shim, Jinkyu Lee, Simyoung Chang, Kyuwoong Hwang   
  Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition  
  Tianyi Xu, Zhanheng Yang, Kaixun Huang, Pengcheng Guo, Ao Zhang, Biao Li, Changru Chen, Chao Li, Lei Xie   

 Source Separation  
  Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model  
  Héctor Martel, Julius Richter, Kai Li, Xiaolin Hu, Timo Gerkmann   
  Remixing-based Unsupervised Source Separation from Scratch  
  Kohei Saijo, Tetsuji Ogawa   
  CAPTDURE: Captioned Sound Dataset of Single Sources  
  Yuki Okamoto, Kanta Shimonishi, Keisuke Imoto, Kota Dohi, Shota Horiguchi, Yohei Kawaguchi   
  Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources  
  Hokuto Munakata, Ryu Takeda, Kazunori Komatani   
  Multi-Channel Speech Separation with Cross-Attention and Beamforming  
  Ladislav Mosner, Oldřich Plchot, Junyi Peng, Lukáš Burget, Jan "Honza" Černocký   
  Background-Sound Controllable Voice Source Separation  
  Deokjun Eom, Woo Hyun Nam, Kyung-Rae Kim   

 Speech and Language in Health: From Remote Monitoring to Medical Conversations 1  
  An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients  
  Daniel Escobar-Grisales, Tomás Arias-Vergara, Cristian David Ríos-Urrego, Elmar Nöth, Adolfo M. García, Juan Rafael Orozco-Arroyave   
  Personalization for Robust Voice Pathology Detection in Sound Waves  
  Khanh-Tung Tran, Truong Hoang, Duy Khuong Nguyen, Hoang D. Nguyen, Xuan-Son Vu   
  Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders  
  Helen Meng, Brian Mak, Man-Wai Mak, Helene Fung, Xianmin Gong, Timothy Kwok, Xunying Liu, Vincent Mok, Patrick Wong, Jean Woo, Xixin Wu, Ka Ho Wong, Shensheng Xu, Naijun Zheng, Ranzo Huang, Jiawen Kang, Xiaoquan Ke, Junan Li, Jinchao Li, Yi Wang   
  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder  
  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost   
  FTA-net: A Frequency and Time Attention Network for Speech Depression Detection  
  Qifei Li, Dong Wang, Yiming Ren, Yingming Gao, Ya Li   
  Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data  
  Salvatore Fara, Orlaith Hickey, Alexandra Georgescu, Stefano Goria, Emilia Molimpakis, Nicholas Cummins   
  Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition  
  Tianzi Wang, Shoukang Hu, Jiajun Deng, Zengrui Jin, Mengzhe Geng, Yi Wang, Helen Meng, Xunying Liu   
  Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models.  
  Edward L. Campbell, Judith Dineley, Pauline Conde, Faith Matcham, Katie M. White, Carolin Oetzmann, Sara Simblett, Stuart Bruce, Amos A. Folarin, Til Wykes, Srinivasan Vairavan, Richard J. B. Dobson, Laura Docio-Fernandez, Carmen Garcia-Mateo, Vaibhav A. Narayan, Matthew Hotopf, Nicholas Cummins   
  Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma  
  Shabnam Ghaffarzadegan, Luca Bondi, Ho-Hsiang Wu, Sirajum Munir, Kelly J. Shields, Samarjit Das, Joseph Aracri   
  Automatic Assessment of Alzheimer's across Three Languages Using Speech and Language Features  
  Paula A. Pérez-Toro, Tomás Arias-Vergara, Franziska Braun, Florian Hönig, Carlos A. Tobón-Quintero, David Aguillón, Francisco Lopera, Liliana Hincapié-Henao, Maria Schuster, Korbinian Riedhammer, Andreas Maier, Elmar Nöth, Juan Rafael Orozco-Arroyave   
  On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition  
  Mengzhe Geng, Xurong Xie, Rongfeng Su, Jianwei Yu, Zengrui Jin, Tianzi Wang, Shujie Hu, Zi Ye, Helen Meng, Xunying Liu   
  Relationship between LTAS-based spectral moments and acoustic parameters of hypokinetic dysarthria in Parkinson’s disease  
  Jan Svihlik, Vojtěch Illner, Petr Kryze, Mário Sousa, Paul Krack, Elina Tripoliti, Robert Jech, Jan Rusz   
  Respiratory distress estimation in human-robot interaction scenario  
  Eduardo Alvarado, Nicolás Grágeda, Alejandro Luzanto, Rodrigo Mahu, Jorge Wuth, Laura Mendoza, Richard Stern, Néstor Becerra Yoma   
  Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on?  
  Emma Reyner-Fuentes, Esther Rituerto-González, Isabel Trancoso, Carmen Peláez-Moreno   
  Whisper Encoder features for Infant Cry Classification  
  Monil Charola, Aastha Kachhi, Hemant A. Patil   

 Speech Perception  
  A neural architecture for selective attention to speech features  
  Nika Jurov, William Idsardi, Naomi H. Feldman   
  Quantifying Informational Masking due to Masker Intelligibility in Same-talker Speech-in-speech Perception  
  Mingyue Huo, Yinglun Sun, Dan Fogerty, Yan Tang   
  On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions  
  Santiago Cuervo, Ricard Marxer   
  Predicting Perceptual Centers Located at Vowel Onset in German Speech Using Long Short-Term Memory Networks  
  Felicia Schulz, Mirella De Sisto, M. Paula Roncaglia-Denissen, Peter Hendrix   
  Exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal  
  Martin Cooke, María Luisa García Lecumberri   
  Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese  
  Mafuyu Kitahara, Naoya Watabe, Hiroto Noguchi, Chuyu Huang, Ayako Hashimoto, Ai Mizoguchi   

 Phonetics and Phonology: Languages and Varieties  
  The emergence of obstruent-intrinsic f0 and VOT as cues to the fortis/lenis contrast in West Central Bavarian  
  Jasmin Pöhnlein, Felicitas Kleber   
  〈'〉 in Tsimane': a Preliminary Investigation  
  William N. Havard, Yaya Sy, Camila Scaff, Loann Peurey, Alejandrina Cristia   
  Segmental features of Brazilian (Santa Catarina) Hunsrik  
  Dennis Hoffmann, Maria O'Reilly   
  Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English  
  Louise Ratko, Joshua Penney, Felicity Cox   
  Increasing aspiration of word-medial fortis plosives in Swiss Standard German  
  Franka Zebe   
  Lexical Stress and Velar Palatalization in Italian: A spatio-temporal Interaction  
  Bowei Shao, Philipp Buech, Anne Hermes, Maria Giavazzi   

 Paralinguistics 2  
  Speaker Embeddings as Individuality Proxy for Voice Stress Detection  
  Zihan Wu, Neil Scheidwasser-Clow, Karl El Hajal, Milos Cernak   
  From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion  
  Jingyao Wu, Ting Dang, Vidhyasaharan Sethu, Eliathamby Ambikairajah   
  Turbo your multi-modal classification with contrastive learning  
  Zhiyu Zhang, Da Liu, Shengqiang Liu, Anna Wang, Jie Gao, Yali Li   
  Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition  
  Georgios Ioannides, Michael Owen, Andrew Fletcher, Viktor Rozgic, Chao Wang   
  SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition  
  Ruiteng Zhang, Jianguo Wei, Xugang Lu, Yongwei Li, Junhai Xu, Di Jin, Jianhua Tao   
  On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition  
  Lokesh Bansal, S. Pavankumar Dubagunta, Malolan Chetlur, Pushpak Jagtap, Aravind Ganapathiraju   
  Speaking State Decoder with Transition Detection for Next Speaker Prediction  
  Shao-Hao Lu, Yun-Shao Lin, Chi-Chun Lee   
  What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation  
  Yuki Kitagishi, Naohiro Tawara, Atsunori Ogawa, Ryo Masumura, Taichi Asami   
  Effects of perceived gender on the perceived social function of laughter  
  Joop Arts, Khiet P. Truong   
  Implicit phonetic information modeling for speech emotion recognition  
  Tilak Purohit, Bogdan Vlasenko, Mathew Magimai.-Doss   
  Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters  
  Seong-Gyun Leem, Daniel Fulford, Jukka-Pekka Onnela, David Gard, Carlos Busso   
  Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions  
  Yang Liu, Haoqin Sun, Geng Chen, Qingyue Wang, Zhen Zhao, Xugang Lu, Longbiao Wang   
  Preference Learning Labels by Anchoring on Consecutive Annotations  
  Abinay Reddy Naini, Ali N. Salman, Carlos Busso   
  Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks  
  Orchid Chetia Phukan, Arun Balaji Buduru, Rajesh Sharma   
  Learning Local to Global Feature Aggregation for Speech Emotion Recognition  
  Cheng Lu, Hailun Lian, Wenming Zheng, Yuan Zong, Yan Zhao, Sunan Li   
  Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition  
  Xuechen Wang, Shiwan Zhao, Yong Qin   

 Speaker and Language Identification 1  
  Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition  
  Viet Thanh Pham, Xuan Thai Hoa Nguyen, Vu Hoang, Thi Thu Trang Nguyen   
  What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model  
  Mu Yang, Ram C. M. C. Shekar, Okim Kang, John H. L. Hansen   
  The 2022 NIST Language Recognition Evaluation  
  Yooyoung Lee, Craig Greenberg, Eliot Godard, Asad A. Butt, Elliot Singer, Trang Nguyen, Lisa Mason, Douglas Reynolds   
  Description and analysis of the KPT system for NIST Language Recognition Evaluation 2022  
  Salvatore Sarni, Sandro Cumani, Sabato Marco Siniscalchi, Andrea Bottino   
  ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention  
  Jia Qi Yip, Duc-Tuan Truong, Dianwen Ng, Chong Zhang, Yukun Ma, Trung Hieu Nguyen, Chongjia Ni, Shengkui Zhao, Eng Siong Chng, Bin Ma   
  Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification  
  Jiadi Yao, Chengdong Liang, Zhendong Peng, Binbin Zhang, Xiao-Lei Zhang   
  Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech  
  Vishwanath Pratap Singh, Md Sahidullah, Tomi Kinnunen   
  Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification  
  Spandan Dey, Premjeet Singh, Goutam Saha   
  A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model  
  Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Narsis A. Kiani, David Gomez-Cabrero, Jesper N. Tegner   
  HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing  
  Pablo Andrés Tamayo Flórez, Rubén Manrique, Bernardo Pereira Nunes   
  Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory  
  Rui Li, Zhiwei Xie, Haihua Xu, Yizhou Peng, Hexin Liu, Hao Huang, Eng Siong Chng   
  Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory  
  Bei Liu, Haoyu Wang, Yanmin Qian   
  Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance  
  Sourya Dipta Das, Yash Vadi, Abhishek Unnam, Kuldeep Yadav   
  pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe  
  Hervé Bredin   
  Model Compression for DNN-based Speaker Verification Using Weight Quantization  
  Jingyu Li, Wei Liu, Zhaoyang Zhang, Jiong Wang, Tan Lee   
  Multi-resolution Approach to Identification of Spoken Languages and To Improve Overall Language Diarization System Using Whisper Model  
  Bhavik Vachhani, Dipesh Singh, Rustom Lawyer   
  Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms  
  Chang Zeng, Xin Wang, Xiaoxiao Miao, Erica Cooper, Junichi Yamagishi   
  Dynamic Fully-Connected Layer for Large-Scale Speaker Verification  
  Zhida Song, Liang He, Baowei Zhao, Minqiang Xu, Yu Zheng   

 Show and Tell: Speech tools, speech enhancement, speech synthesis  
  DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement  
  Hendrik Schröter, Alberto N. Escalante-B., Tobias Rosenkranz, Andreas Maier   
  Nkululeko: Machine Learning Experiments on Speaker Characteristics Without Programming  
  Felix Burkhardt, Florian Eyben, Björn W. Schuller   
  Sp1NY: A Quick and Flexible Speech Visualisation Tool in Python  
  Sébastien Le Maguer, Mark Anderson, Naomi Harte   
  Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0  
  Niamh Corkey, Johannah O'Mahony, Simon King   
  So-to-Speak: An Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS  
  Éva Székely, Siyang Wang, Joakim Gustafson   
  Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking  
  Takayuki Arai, Tsukasa Yoshinaga, Akiyoshi Iida   
  Show & Tell: Voice Activity Projection and Turn-taking  
  Erik Ekstedt, Gabriel Skantze   
  Real Time Detection of Soft Voice for Speech Enhancement  
  Hector A. Cordourier, Georg Stemmer, Sinem Aslan, Tobias Bocklet, Himanshu Bhalla   
  Data Augmentation for Diverse Voice Conversion in Noisy Environments  
  Avani Tanna, Michael Saxon, Amr El Abbadi, William Yang Wang   
  Application for Real-time Audio-Visual Speech Enhancement  
  Mandar Gogate, Kia Dashtipour, Amir Hussain   

 Speech Synthesis and Voice Conversion  
  Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction  
  Eunseop Yoon, Hee Suk Yoon, Dhananjaya Gowda, SooHwan Eom, Daehyeok Kim, John Harvill, Heting Gao, Mark Hasegawa-Johnson, Chanwoo Kim, Chang D. Yoo   
  Streaming Parrotron for on-device speech-to-speech conversion  
  Oleg Rybakov, Fadi Biadsy, Xia Zhang, Liyang Jiang, Phoenix Meadowlark, Shivani Agrawal   
  Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech  
  Zein Shaheen, Tasnima Sadekova, Yulia Matveeva, Alexandra Shirshova, Mikhail Kudinov   
  E2E-S2S-VC: End-To-End Sequence-To-Sequence Voice Conversion  
  Takuma Okamoto, Tomoki Toda, Hisashi Kawai   
  DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer  
  Yerin Choi, Myoung-Wan Koo   
  Voice Conversion With Just Nearest Neighbors  
  Matthew Baas, Benjamin van Niekerk, Herman Kamper   
  CFVC: Conditional Filtering for Controllable Voice Conversion  
  Kou Tanaka, Takuhiro Kaneko, Hirokazu Kameoka, Shogo Seki   
  DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding  
  Ziqian Ning, Yuepeng Jiang, Pengcheng Zhu, Jixun Yao, Shuai Wang, Lei Xie, Mengxiao Bi   
  Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion  
  Yun Chen, Lingxiao Yang, Qi Chen, Jian-Huang Lai, Xiaohua Xie   
  ALO-VC: Any-to-any Low-latency One-shot Voice Conversion  
  Bohan Wang, Damien Ronssin, Milos Cernak   
  Evaluating and reducing the distance between synthetic and real speech distributions  
  Christoph Minixhofer, Ondřej Klejch, Peter Bell   
  Decoupling Segmental and Prosodic Cues of Non-native Speech through Vector Quantization  
  Waris Quamer, Anurag Das, Ricardo Gutierrez-Osuna   
  VC-T: Streaming Voice Conversion Based on Neural Transducer  
  Hiroki Kanagawa, Takafumi Moriya, Yusuke Ijima   
  Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion  
  Suhita Ghosh, Arnab Das, Yamini Sinha, Ingo Siegert, Tim Polzehl, Sebastian Stober   
  ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed  
  Meiying Chen, Zhiyao Duan   
  Reverberation-Controllable Voice Conversion Using Reverberation Time Estimator  
  Yeonjong Choi, Chao Xie, Tomoki Toda   
  Cross-utterance Conditioned Coherent Speech Editing  
  Cheng Yu, Yang Li, Weiqin Zu, Fanglei Sun, Zheng Tian, Jun Wang   

 Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2  
  MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information  
  Jianrong Wang, Yuchen Huo, Li Liu, Tianyi Xu, Qi Li, Sen Li   
  CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition  
  Lantian Li, Xiaolou Li, Haoyu Jiang, Chen Chen, Ruihai Hou, Dong Wang   
  Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion  
  Yuhang Li, Xiao Wei, Yuke Si, Longbiao Wang, Xiaobao Wang, Jianwu Dang   
  Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer  
  Wooseok Shin, Hyun Joon Park, Jin Sob Kim, Dongwon Kim, Seungjin Lee, Sung Won Han   
  Boosting Punctuation Restoration with Data Generation and Reinforcement Learning  
  Viet Dac Lai, Abel Salinas, Hao Tan, Trung Bui, Quan Tran, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, Thien Huu Nguyen   
  J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences  
  Yi-Fen Liu, Xiang-Li Lu   
  Towards Cross-Language Prosody Transfer for Dialog  
  Jonathan E. Avila, Nigel G. Ward   
  Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models  
  Santosh Kesiraju, Marek Sarvaš, Tomáš Pavlíček, Cécile Macaire, Alejandro Ciuba   
  ITALIC: An Italian Intent Classification Dataset  
  Alkis Koudounas, Moreno La Quatra, Lorenzo Vaiani, Luca Colomba, Giuseppe Attanasio, Eliana Pastor, Luca Cagliero, Elena Baralis   
  Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation  
  Janine Rugayan, Giampiero Salvi, Torbjørn Svendsen   
  How ChatGPT is Robust for Spoken Language Understanding?  
  Guangpeng Li, Lu Chen, Kai Yu   
  GigaST: A 10,000-hour Pseudo Speech Translation Corpus  
  Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, Jun Cao   
  Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism  
  Jiaxin Fan, Yong Zhang, Hanzhang Li, Jianzong Wang, Zhitao Li, Sheng Ouyang, Ning Cheng, Jing Xiao   
  Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool  
  Per Fallgren, Jens Edlund   
  PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts  
  Yunxiang Li, Pengfei Liu, Xixin Wu, Helen Meng   
  Speech-to-Face Conversion Using Denoising Diffusion Probabilistic Models  
  Shuhei Kato, Taiichi Hashimoto   
  Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation  
  Yuta Nishikawa, Satoshi Nakamura   

 Novel Transformer Models for ASR  
  Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants  
  Martin Radfar, Paulina Lyskawa, Brandon Trujillo, Yi Xie, Kai Zhen, Jahn Heymann, Denis Filimonov, Grant P. Strimel, Nathan Susanj, Athanasios Mouchtaris   
  Intra-ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition  
  Dohee Kim, Jieun Choi, Joon-Hyuk Chang   
  A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks  
  Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe   
  HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition  
  Florian Mai, Juan Zuluaga-Gomez, Titouan Parcollet, Petr Motlicek   
  Memory-augmented conformer for improved end-to-end long-form ASR  
  Carlos Carvalho, Alberto Abad   
  Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems  
  Mingyu Cui, Jiawen Kang, Jiajun Deng, Xi Yin, Yutao Xie, Xie Chen, Xunying Liu   

 Speaker Recognition 1  
  An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification  
  Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Qian Chen, Jiajun Qi   
  A Study on Visualization of Voiceprint Feature  
  Jian Zhang, Liang He, Xiaochen Guo, Jing Ma   
  VoxTube: a multilingual speaker recognition dataset  
  Ivan Yakovlev, Anton Okhotnikov, Nikita Torgashov, Rostislav Makarov, Yuri Voevodin, Konstantin Simonchik   
  Visualizing Data Augmentation in Deep Speaker Recognition  
  Pengqi Li, Lantian Li, Askar Hamdulla, Dong Wang   

 Cross-lingual and Multilingual ASR  
  Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition  
  Zhilong Zhang, Wei Wang, Yanmin Qian   
  UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR  
  Wei Wang, Yanmin Qian   
  Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes  
  Kevin Glocker, Aaricia Herygers, Munir Georges   
  Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system  
  Li Li, Dongxing Xu, Haoran Wei, Yanhua Long   
  Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages  
  Andrew Rouditchenko, Sameer Khurana, Samuel Thomas, Rogerio Feris, Leonid Karlinsky, Hilde Kuehne, David Harwath, Brian Kingsbury, James Glass   
  DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model  
  Haoyu Wang, Siyuan Wang, Wei-Qiang Zhang, Jinfeng Bai   

 Voice Conversion  
  Emotional Voice Conversion with Semi-Supervised Generative Modeling  
  Hai Zhu, Huayi Zhan, Hong Cheng, Ying Wu   
  Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation  
  Ha-Yeong Choi, Sang-Hoon Lee, Seong-Whan Lee   
  S2CD: Self-heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion  
  Pengfei Wei, Xiang Yin, Chunfeng Wang, Zhonghao Li, Xinghua Qu, Zhiqiang Xu, Zejun Ma   
  Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion  
  Le Xu, Rongxiu Zhong, Ying Liu, Huibao Yang, Shilei Zhang   
  Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation  
  Zhonghua Liu, Shijun Wang, Ning Chen   
  End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions  
  Wonjune Kang, Mark Hasegawa-Johnson, Deb Roy   

 Speech and Language in Health: From Remote Monitoring to Medical Conversations 2  
  Classifying Dementia in the Presence of Depression: A Cross-Corpus Study  
  Franziska Braun, Sebastian P. Bayerl, Paula A. Pérez-Toro, Florian Hönig, Hartmut Lehfeld, Thomas Hillemacher, Elmar Nöth, Tobias Bocklet, Korbinian Riedhammer   
  Exploiting Cross-Domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition  
  Shujie Hu, Xurong Xie, Mengzhe Geng, Mingyu Cui, Jiajun Deng, Guinan Li, Tianzi Wang, Helen Meng, Xunying Liu   
  Multi-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?  
  Dominik Wagner, Ilja Baumann, Franziska Braun, Sebastian P. Bayerl, Elmar Nöth, Korbinian Riedhammer, Tobias Bocklet   
  Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression  
  Hardik Kothare, Michael Neumann, Jackson Liscombe, Jordan Green, Vikram Ramanarayanan   
  Use of Speech Impairment Severity for Dysarthric Speech Recognition  
  Mengzhe Geng, Zengrui Jin, Tianzi Wang, Shujie Hu, Jiajun Deng, Mingyu Cui, Guinan Li, Jianwei Yu, Xurong Xie, Xunying Liu   
  MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones  
  Mohammed Mosuily, Lindsay Welch, Jagmohan Chauhan   
  Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization  
  Siyuan Chen, Colin A. Grambow, Mojtaba Kadkhodaie Elyaderani, Alireza Sadeghi, Federico Fancellu, Thomas Schaaf   
  Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals  
  Jinhan Wang, Vijay Ravi, Abeer Alwan   
  PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images  
  Kubilay Can Demir, Tobias Weise, Matthias May, Axel Schmid, Andreas Maier, Seung Hee Yang   
  Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis  
  Michael Neumann, Hardik Kothare, Vikram Ramanarayanan   
  The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech  
  Adria Mallol-Ragolta, Nils Urbach, Shuo Liu, Anton Batliner, Björn W. Schuller   
  Towards Reference Speech Characterization for Health Applications  
  Catarina Botelho, Alberto Abad, Tanja Schultz, Isabel Trancoso   
  Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors  
  Cristian David Ríos-Urrego, Jan Rusz, Elmar Nöth, Juan Rafael Orozco-Arroyave   
  Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech  
  Judith Dineley, Ewan Carr, Faith Matcham, Johnny Downs, Richard J. B. Dobson, Thomas F. Quatieri, Nicholas Cummins   

 Pathological Speech Analysis 1  
  Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) Using a Novel Remote Speech Assessment App  
  Leif Simmatis, Timothy Pommeé, Yana Yunusova   
  On the Use of High Frequency Information for Voice Pathology Classification  
  David Martínez, Dayana Ribas, Eduardo Lleida   
  Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora?  
  Anna Favaro, Tianyu Cao, Thomas Thebaud, Jesus Villalba, Ankur Butala, Najim Dehak, Laureano Moro-Velazquez   
  Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features  
  Sudarsana Reddy Kadiri, Manila Kodali, Paavo Alku   
  Comparison of acoustic measures of dysphonia in Parkinson's disease and Huntington's disease: Effect of sex and speaking task  
  Michal Šimek, Tomáš Kouba, Michal Novotný, Tereza Tykalová, Jan Rusz   
  Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses  
  Lucía Gómez-Zaragozá, Simone Wills, Cristian Tejedor-Garcia, Javier Marín-Morales, Mariano Alcañiz, Helmer Strik   

 Multimodal Speech Emotion Recognition  
  LanSER: Language-Model Supported Speech Emotion Recognition  
  Taesik Gong, Josh Belanich, Krishna Somandepalli, Arsha Nagrani, Brian Eoff, Brendan Jou   
  Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition  
  Jiachen Luo, Huy Phan, Joshua Reiss   
  Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition  
  Eimear Stanley, Eric DeMattos, Anita Klementiev, Piotr Ozimek, Georgia Clarke, Michael Berger, Dimitri Palaz   
  Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information  
  Zhongjie Li, Gaoyan Zhang, Longbiao Wang, Jianwu Dang   
  Meta-domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-sentiment Predictions  
  Zhi Li, Ryu Takeda, Takahiro Hara   
  SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition  
  Ziping Zhao, Tian Gao, Haishuai Wang, Björn W. Schuller   

 Speech Coding and Enhancement 2  
  PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement  
  Xinmeng Xu, Weiping Tu, Yuhong Yang   
  Exploring the Interactions Between Target Positive and Negative Information for Acoustic Echo Cancellation  
  Chang Han, Xinmeng Xu, Weiping Tu, Yuhong Yang, Yajie Liu   
  Iterative autoregression: a novel trick to improve your low-latency speech enhancement model  
  Pavel Andreev, Nicholas Babaev, Azat Saginbaev, Ivan Shchekotov, Aibek Alanov   
  A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models  
  Pin-Jui Ku, Chao-Han Huck Yang, Sabato Siniscalchi, Chin-Hui Lee   
  Domain Adaptation for Speech Enhancement in a Large Domain Gap  
  Lior Frenkel, Jacob Goldberger, Shlomo E. Chazan   
  SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks  
  Vasily Zadorozhnyy, Qiang Ye, Kazuhito Koishida   
  A Mask Free Neural Network for Monaural Speech Enhancement  
  Liang Liu, Haixin Guan, Jinlong Ma, Wei Dai, Guangyong Wang, Shaowei Ding   
  A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech  
  Li-Wei Chen, Yao-Fei Cheng, Hung-Shin Lee, Yu Tsao, Hsin-Min Wang   
  A Simple RNN Model for Lightweight, Low-compute and Low-latency Multichannel Speech Enhancement in the Time Domain  
  Ashutosh Pandey, Ke Tan, Buye Xu   
  High Fidelity Speech Enhancement with Band-split RNN  
  Jianwei Yu, Hangting Chen, Yi Luo, Rongzhi Gu, Chao Weng   
  Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information  
  Jiuxin Lin, Peng Wang, Heinrich Dinkel, Jun Chen, Zhiyong Wu, Zhiyong Yan, Yongqing Wang, Junbo Zhang, Yujun Wang   
  DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement  
  Anton Kovalyov, Kashyap Patel, Issa Panahi   
  Speaker-Aware Anti-spoofing  
  Xuechen Liu, Md Sahidullah, Kong Aik Lee, Tomi Kinnunen   
  Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine  
  Shoko Araki, Ayako Yamamoto, Tsubasa Ochiai, Kenichi Arai, Atsunori Ogawa, Tomohiro Nakatani, Toshio Irino   
  EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement  
  Marvin Sach, Jan Franzen, Bruno Defraene, Kristoff Fluyt, Maximilian Strake, Wouter Tirry, Tim Fingscheidt   
  HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control  
  JungPhil Park, Jeong-Hwan Choi, Yungyeo Kim, Joon-Hyuk Chang   
  MSAF: A Multiple Self-Attention Field Method for Speech Enhancement  
  Minghang Chu, Jing Wang, Yaoyao Ma, Zhiwei Fan, Mengtao Yang, Chao Xu, Zhi Tao, Di Wu   
  Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression  
  Hangting Chen, Jianwei Yu, Yi Luo, Rongzhi Gu, Weihua Li, Zhuocheng Lu, Chao Weng   
  ABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression  
  Yixin Wan, Yuan Zhou, Xiulian Peng, Kai-Wei Chang, Yan Lu   
  PLCMOS – A Data-driven Non-intrusive Metric for The Evaluation of Packet Loss Concealment Algorithms  
  Lorenz Diener, Marju Purin, Sten Sootla, Ando Saabas, Robert Aichner, Ross Cutler   

 Phonetics, Phonology, and Prosody 1  
  Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading  
  Petra Wagner, Simon Betz   
  Comparing first spectral moment of Australian English /s/ between straight and gay voices using three analysis window sizes  
  Tünde Szalay, John Holik, Duy Duong Nguyen, James Morandini, Catherine J. Madill   
  Universal Automatic Phonetic Transcription into the International Phonetic Alphabet  
  Chihiro Taguchi, Yusuke Sakai, Parisa Haghani, David Chiang   
  Voice Twins: Discovering Extremely Similar-sounding, Unrelated Speakers  
  Linda Gerlach, Kirsty McDougall, Finnian Kelly, Anil Alexander   
  Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts  
  Hannah Hedegard, Andrea Fröhlich, Fabian Tomaschek, Carina Steiner, Adrian Leemann   
  Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers  
  Mathilde Hutin, Liesbeth Degand, Marc Allassonnière-Tang   
  Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings  
  Robert Essery, Philip Harrison, Vincent Hughes   
  An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline  
  Emily P. Ahn, Gina-Anne Levow, Richard A. Wright, Eleanor Chodroff   
  The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features  
  Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj   
  Beatboxing Kick Drum Kinematics  
  Reed Blaylock, Shrikanth Narayanan   
  Effects of hearing loss and amplification on Mandarin consonant perception  
  Huali Zhou, Xianming Bei, Nengheng Zheng, Qinglin Meng   
  An Acoustic Analysis of Fricative Variation in Three Accents of English  
  Roland Adams, Calbert Graham   
  Acoustic cues to stress perception in Spanish – a mismatch negativity study  
  Karolina Broś   
  Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings  
  Mitko Sabev, Bistra Andreeva, Christoph Gabriel, Jonas Gruenke   
  An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations  
  Shelly Jain, Priyanshi Pal, Anil Kumar Vuppala, Prasanta Kumar Ghosh, Chiranjeevi Yarra   
  Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels Based on Difference Thresholds  
  Hye-Sook Park, Sunhee Kim   
  Evaluation of delexicalization methods for research on emotional speech  
  Nicolas Audibert, Francesca Carbone, Maud Champagne-Lavau, Aurélien Said Housseini, Caterina Petrone   

 Spoken Dialog Systems and Conversational Analysis 2  
  Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN)  
  Jay Kejriwal, Štefan Beňuš   
  Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks  
  Jay Kejriwal, Štefan Beňuš, Lina M. Rojas-Barahona   
  Parsing dialog turns with prosodic features in English  
  Elizabeth Nielsen, Mark Steedman, Sharon Goldwater   
  Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness Using Dynamic-Prompt-Tune  
  Toshiki Muromachi, Yoshinobu Kano   
  Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder  
  Tahiya Chowdhury, Veronica Romero, Amanda Stent   
  Efficient Multimodal Neural Networks for Trigger-less Voice Assistants  
  Sai Srujana Buddi, Utkarsh Oggy Sarawgi, Tashweena Heeramun, Karan Sawnhey, Ed Yanosik, Saravana Rathinam, Saurabh Adya   
  Rapid Lexical Alignment to a Conversational Agent  
  Rachel Ostrand, Victor S. Ferreira, David Piorkowski   
  Multimodal Turn-Taking Model Using Visual Cues for End-of-Utterance Prediction in Spoken Dialogue Systems  
  Fuma Kurata, Mao Saeki, Shinya Fujie, Yoichi Matsuyama   
  Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer  
  Nobukatsu Hojo, Saki Mizuno, Satoshi Kobashikawa, Ryo Masumura, Mana Ihori, Hiroshi Sato, Tomohiro Tanaka   
  Improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay  
  Jin Sakuma, Shinya Fujie, Huaibo Zhao, Tetsunori Kobayashi   
  Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition  
  Keulbit Kim, Namhyun Cho   
  A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting  
  Haotian Wang, Jun Du, Hengshun Zhou, Chin-Hui Lee, Yuling Ren, Jiangjiang Zhao   
  Abusive Speech Detection in Indic Languages Using Acoustic Features  
  Anika A. Spiesberger, Andreas Triantafyllopoulos, Iosif Tsangko, Björn W. Schuller   
  Listening To Silences In Contact Center Conversations Using Textual Cues  
  Digvijay Anil Ingle, Ayush Kumar, Jithendra Vepa   
  I Learned Error, I Can Fix It! : A Detector-Corrector Structure for ASR Error Calibration  
  Heui-Yeen Yeen, Min-Ju Kim, Myoung-Wan Koo   
  Verbal and nonverbal feedback signals in response to increasing levels of miscommunication  
  Maeva Garnier, Eric Le Ferrand, Fabien Ringeval   
  Speech-Based Classification of Defensive Communication: A Novel Dataset and Results  
  Shahin Amiriparian, Lukas Christ, Regina Kushtanova, Maurice Gerczuk, Alexandra Teynor, Björn W. Schuller   
  Quantifying the perceptual value of lexical and non-lexical channels in speech  
  Sarenne Wallbridge, Peter Bell, Catherine Lai   
  Relationships Between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown  
  Kazuya Tsubokura, Yurie Iribe, Norihide Kitaoka   
  Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition  
  Huan Zhao, Bo Li, Zixing Zhang   

 Analysis of Speech and Audio Signals 2  
  Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network  
  Zhiheng Liao, Feifei Xiong, Juan Luo, Minjie Cai, Eng Siong Chng, Jinwei Feng, Xionghu Zhong   
  Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement  
  Xin Ren, Juan Luo, Xionghu Zhong, Minjie Cai   
  Anomalous Sound Detection Based on Sound Separation  
  Kanta Shimonishi, Kota Dohi, Yohei Kawaguchi   
  Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices  
  Vitória S. Fahed, Emer P Doheny, Madeleine M Lowery   
  GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos  
  Youngdo Ahn, Chengyi Wang, Yu Wu, Jong Won Shin, Shujie Liu   
  Wav2ToBI: a new approach to automatic ToBI transcription  
  Wanyue Zhai, Mark Hasegawa-Johnson   
  Joint-Former: Jointly Regularized and Locally Down-sampled Conformer for Semi-supervised Sound Event Detection  
  Lijian Gao, Qirong Mao, Ming Dong   
  Towards Attention-based Contrastive Learning for Audio Spoof Detection  
  Chirag Goel, Surya Koppisetti, Ben Colman, Ali Shahriyari, Gaurav Bharaj   
  Masked Audio Modeling with CLAP and Multi-Objective Learning  
  Yifei Xin, Xiulian Peng, Yan Lu   
  Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems  
  Manuele Rusci, Tinne Tuytelaars   
  Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing  
  Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza   
  Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR  
  W. Ronny Huang, Hao Zhang, Shankar Kumar, Shuo-Yiin Chang, Tara Sainath   
  Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features  
  Théo Mariotte, Anthony Larcher, Silvio Montrésor, Jean-Hugh Thomas   
  Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection  
  Jing Li, Yanhua Long, Yijie Li, Dongxing Xu   
  Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works  
  Juan Carlos Martínez-Sevilla, María Alfaro-Contreras, Jose J. Valero-Mas, Jorge Calvo-Zaragoza   
  Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers  
  Yuan Gong, Sameer Khurana, Leonid Karlinsky, James Glass   
  Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer  
  Jingran Gong, Ning Chen   
  Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection  
  Yuankun Xie, Haonan Cheng, Yutian Wang, Long Ye   
  Application of Knowledge Distillation to Multi-Task Speech Representation Learning  
  Mine Kerpicci, Van Nguyen, Shuhua Zhang, Erik Visser   
  DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes  
  Xilin Jiang, Yinghao Aaron Li, Nima Mesgarani   
  Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization  
  Antonio Almudévar, Alfonso Ortega, Luis Vicente, Antonio Miguel, Eduardo Lleida   
  FlexiAST: Flexibility is What AST Needs  
  Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak   
  MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization  
  Ji Won Yoon, Seok Min Kim, Nam Soo Kim   
  Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention  
  Xubo Liu, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H. Tang, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang   

 Speech Coding: Privacy  
  Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health  
  Apiwat Ditthapron, Emmanuel O. Agu, Adam C. Lammert   
  eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network  
  Bajian Xiang, Hongkun Liu, Zedong Wu, Su Shen, Xiangdong Zhang   
  Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement  
  Junyu Wang   
  Privacy-preserving Representation Learning for Speech Understanding  
  Minh Tran, Mohammad Soleymani   
  Vocoder drift in x-vector–based speaker anonymization  
  Michele Panariello, Massimiliano Todisco, Nicholas Evans   
  Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems  
  Michele Panariello, Wanying Ge, Hemlata Tak, Massimiliano Todisco, Nicholas Evans   

 Analysis of Neural Speech Representations  
  Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?  
  Salah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, Mirco Ravanelli   
  An extension of disentanglement metrics and its application to voice  
  Olivier Zhang, Olivier Le Blouch, Nicolas Gengembre, Damien Lolive   
  An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech  
  Badr M. Abdullah, Mohammed Maqsood Shaik, Bernd Möbius, Dietrich Klakow   
  SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?  
  Takanori Ashihara, Takafumi Moriya, Kohei Matsuura, Tomohiro Tanaka, Yusuke Ijima, Taichi Asami, Marc Delcroix, Yukinori Honma   
  Comparison of GIF- and SSL-based Features in Pathological-voice Detection  
  Akira Sasou, Yang Chen   
  What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions  
  Hanyu Meng, Vidhyasaharan Sethu, Eliathamby Ambikairajah   

 End-to-end ASR  
  End-to-End Joint Target and Non-Target Speakers ASR  
  Ryo Masumura, Naoki Makishima, Taiga Yamane, Yoshihiko Yamazaki, Saki Mizuno, Mana Ihori, Mihiro Uchida, Keita Suzuki, Hiroshi Sato, Tomohiro Tanaka, Akihiko Takashima, Satoshi Suzuki, Takafumi Moriya, Nobukatsu Hojo, Atsushi Ando   
  Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition  
  Xianzhao Chen, Yist Y. Lin, Kang Wang, Yi He, Zejun Ma   
  Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-level Timestamp Prediction  
  Naoki Makishima, Keita Suzuki, Satoshi Suzuki, Atsushi Ando, Ryo Masumura   
  Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition  
  Yuchen Hu, Nana Hou, Chen Chen, Eng Siong Chng   
  Multi-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition  
  Xuefei Wang, Yanhua Long, Yijie Li, Haoran Wei   
  Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator  
  Vladimir Bataev, Roman Korostik, Evgeny Shabalin, Vitaly Lavrukhin, Boris Ginsburg   

 Spoken Language Understanding, Summarization, and Information Retrieval  
  Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling  
  He Huang, Jagadeesh Balam, Boris Ginsburg   
  Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models  
  Heerin Yang, Seung-won Hwang, Jungmin So   
  Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization  
  Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Tomohiro Tanaka, Takatomo Kano, Atsunori Ogawa, Marc Delcroix   
  Audio Retrieval with WavText5K and CLAP Training  
  Soham Deshmukh, Benjamin Elizalde, Huaming Wang   
  Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding  
  Umberto Cappellazzo, Muqiao Yang, Daniele Falavigna, Alessio Brutti   
  Contrastive Disentangled Learning for Memory-Augmented Transformer  
  Jen-Tzung Chien, Shang-En Li   

 Invariant and Robust Pre-trained Acoustic Models  
  ProsAudit, a prosodic benchmark for self-supervised speech models  
  Maureen de Seyssel, Marvin Lavechin, Hadrien Titeux, Arthur Thomas, Gwendal Virlet, Andrea Santos Revilla, Guillaume Wisniewski, Bogdan Ludusan, Emmanuel Dupoux   
  Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces  
  Oli Danyi Liu, Hao Tang, Sharon Goldwater   
  Evaluating context-invariance in unsupervised speech representations  
  Mark Hallap, Emmanuel Dupoux, Ewan Dunbar   
  CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning  
  Chutong Meng, Junyi Ao, Tom Ko, Mingxuan Wang, Haizhou Li   
  Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering  
  Heng-Jui Chang, Alexander H. Liu, James Glass   
  Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder  
  Jingru Lin, Xianghu Yue, Junyi Ao, Haizhou Li   

 Pathological Speech Analysis 2  
  A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer  
  Mary Paterson, James Moor, Luisa Cutillo   
  ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection  
  Pingyue Zhang, Mengyue Wu, Kai Yu   
  Automated Multiple Sclerosis Screening Based on Encoded Speech Representations  
  José Egas-López, Veronika Svindt, Judit Bóna, Ildikó Hoffmann, Gábor Gosztolya   
  Cross-Lingual Features for Alzheimer’s Dementia Detection from Speech  
  Thomas Melistas, Lefteris Kapelonis, Nikos Antoniou, Petros Mitseas, Dimitris Sgouropoulos, Theodoros Giannakopoulos, Athanasios Katsamanis, Shrikanth Narayanan   
  Careful Whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification  
  Mario Zusag, Laurin Wagner, Theresa Bloder   
  Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer  
  Jenthe Thienpondt, Caroline M. Speksnijder, Kris Demuynck   

 Speech Synthesis: Representation Learning  
  Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech  
  Hyungchan Yoon, Seyun Um, Changhwan Kim, Hong-Goo Kang   
  Adapter-Based Extension of Multi-Speaker Text-To-Speech Model for New Speakers  
  Cheng-Ping Hsieh, Subhankar Ghosh, Boris Ginsburg   
  SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis  
  Ramanan Sivaguru, Vasista Sai Lodagala, S Umesh   
  UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data  
  Heeseung Kim, Sungwon Kim, Jiheum Yeom, Sungroh Yoon   
  LightVoc: An Upsampling-Free GAN Vocoder Based On Conformer And Inverse Short-time Fourier Transform  
  Dinh Son Dang, Tung Lam Nguyen, Bao Thang Ta, Tien Thanh Nguyen, Thi Ngoc Anh Nguyen, Dang Linh Le, Nhat Minh Le, Van Hai Do   
  ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings  
  Yuki Saito, Shinnosuke Takamichi, Eiji Iimori, Kentaro Tachibana, Hiroshi Saruwatari   

 Speech Perception, Production, and Acquisition 1  
  Human Transcription Quality Improvement  
  Jian Gao, Hanbo Sun, Cheng Cao, Zheng Du   
  The effect of masking noise on listeners’ spectral tilt preferences  
  Olympia Simantiraki, Yannis Pantazis, Martin Cooke   
  The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners  
  Anais Tran Ngoc, Fanny Meunier, Julien Meyer   
  Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali)  
  Puja Bharati, Sabyasachi Chandra, Shayamal Kumar Das Mandal   
  The effect of stress on Mandarin tonal perception in continuous speech for Spanish-speaking learners  
  Lixia Hao, Qi Gong, Jinsong Zhang   
  Combining acoustic and aerodynamic data collection: A perceptual evaluation of acoustic distortions  
  Amélie Elmerich, Jiayin Gao, Angelique Amelot, Lise Crevier-Buchman, Shinji Maeda   
  Estimating virtual targets for lingual stop consonants using general Tau theory  
  Benjamin Elie, Alice Turk   
  Using Random Forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing  
  Mark Gibson   
  An Improved End-to-End Audio-Visual Speech Recognition Model  
  Sheng Yang, Zheng Gong, Jia Kang   
  What influences the foreign accent strength? Phonological and grammatical errors in the perception of accentedness  
  Sarah Wesołek, Piotr Gulgowski, Joanna Błaszczak, Marzena Żygis   
  Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence  
  Lena-Marie Huttner, Noël Nguyen, Martin J. Pickering   
  Emotion Prompting for Speech Emotion Recognition  
  Xingfa Zhou, Min Li, Lan Yang, Rui Sun, Xin Wang, Huayi Zhan   
  Speech-in-Speech Recognition is Modulated by Familiarity to Dialect  
  Jessica L. L. Chin, Elena Talevska, Mark Antoniou   
  BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions  
  Jie Zhang, QingTian Xu, Qiu-Shi Zhu, Zhen-Hua Ling   
  Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study  
  Zuzanna Miodonska, Claartje Levelt, Natalia Mocko, Michał Kręcichwost, Agata Sage, Pawel Badura   

 Speaker and Language Identification 2  
  Reversible Neural Networks for Memory-Efficient Speaker Verification  
  Bei Liu, Yanmin Qian   
  ECAPA++: Fine-grained Deep Embedding Learning for TDNN Based Speaker Verification  
  Bei Liu, Yanmin Qian   
  TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection  
  Chenglong Wang, Jiangyan Yi, Jianhua Tao, Chu Yuan Zhang, Shuai Zhang, Ruibo Fu, Xun Chen   
  Fooling Speaker Identification Systems with Adversarial Background Music  
  Chu-Xiao Zuo, Jia-Yi Leng, Wu-Jun Li   
  Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification  
  Jianchen Li, Jiqing Han, Shiwen Deng, Tieran Zheng, Yongjun He, Guibin Zheng   
  Target Active Speaker Detection with Audio-visual Cues  
  Yidi Jiang, Ruijie Tao, Zexu Pan, Haizhou Li   
  Improving End-to-End Neural Diarization Using Conversational Summary Representations  
  Samuel J. Broughton, Lahiru Samarakoon   
  Phase perturbation improves channel robustness for speech spoofing countermeasures  
  Yongyi Zang, You Zhang, Zhiyao Duan   
  Improving training datasets for resource-constrained speaker recognition neural networks  
  Pierre-Michel Bousquet, Mickael Rouvier   
  Instance-based Temporal Normalization for Speaker Verification  
  Thanathai Lertpetchpun, Ekapol Chuangsuwanich   
  On the robustness of wav2vec 2.0 based speaker recognition systems  
  Sergey Novoselov, Galina Lavrentyeva, Anastasia Avdeeva, Vladimir Volokhov, Nikita Khmelev, Artem Akulov, Polina Leonteva   
  P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification  
  Xiyuan Wang, Fangyuan Wang, Bo Xu, Liang Xu, Jing Xiao   
  Group GMM-ResNet for Detection of Synthetic Speech Attacks  
  Zhenchun Lei, Yan Wen, Yingen Yang, Changhong Liu, Minglei Ma   
  Robust Training for Speaker Verification against Noisy Labels  
  Zhihua Fang, Liang He, Hanhan Ma, Xiaochen Guo, Lin Li   
  Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization  
  Ye-Rin Jeoung, Jeong-Hwan Choi, Ju-Seok Seong, Jehyun Kyung, Joon-Hyuk Chang   
  Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022  
  Zhengyang Chen, Bing Han, Xu Xiang, Houjun Huang, Bei Liu, Yanmin Qian   
  Describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition  
  Imen Ben-Amor, Jean-François Bonastre, Benjamin O'Brien, Pierre-Michel Bousquet   
  Range-Based Equal Error Rate for Spoof Localization  
  Lin Zhang, Xin Wang, Erica Cooper, Nicholas Evans, Junichi Yamagishi   
  Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection  
  Nowshin Tabassum, Tasfia Tabassum, Fardin Saad, Tahiya Sultana Safa, Hasan Mahmud, Md. Kamrul Hasan   
  Powerset multi-class cross entropy loss for neural speaker diarization  
  Alexis Plaquet, Hervé Bredin   
  A Method of Audio-Visual Person Verification by Mining Connections between Time Series  
  Peiwen Sun, Shanshan Zhang, Zishan Liu, Yougen Yuan, Taotao Zhang, Honggang Zhang, Pengfei Hu   

 Speech Recognition: Architecture, Search, and Linguistic Components 3  
  A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization  
  Edward Fish, Umberto Michieli, Mete Ozay   
  Modeling Dependent Structure for Utterances in ASR Evaluation  
  Zhe Liu, Fuchun Peng   
  ASR for Low Resource and Multilingual Noisy Code-Mixed Speech  
  Tushar Verma, Atul Shree, Ashutosh Modi   
  Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System  
  Xian Shi, Haoneng Luo, Zhifu Gao, Shiliang Zhang, Zhijie Yan   
  Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish  
  Lukas Mateju, Jan Nouza, Petr Červa, Jindrich Zdansky, Frantisek Kynych   
  Two Stage Contextual Word Filtering for Context Bias in Unified Streaming and Non-streaming Transducer  
  Zhanheng Yang, Sining Sun, Xiong Wang, Yike Zhang, Long Ma, Lei Xie   
  Towards continually learning new languages  
  Quan Pham, Jan Niehues, Alex Waibel   
  N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space  
  Rao Ma, Mark J. F. Gales, Kate M. Knill, Mengjie Qian   
  SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge  
  Jiaxu Zhu, Changhe Song, Zhiyong Wu, Helen Meng   
  miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge  
  Haris Gulzar, Monikka Roslianna Busto, Takeharu Eda, Katsutoshi Itoyama, Kazuhiro Nakadai   
  CoMFLP: Correlation Measure Based Fast Search on ASR Layer Pruning  
  Wei Liu, Zhiyuan Peng, Tan Lee   
  Exploration on HuBERT with Multiple Resolution  
  Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe   
  Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding  
  Zi Yang, Samridhi Choudhary, Siegfried Kunzmann, Zheng Zhang   
  Word-level Confidence Estimation for CTC Models  
  Burin Naowarat, Thananchai Kongthaworn, Ekapol Chuangsuwanich   
  Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages  
  Devang Kulshreshtha, Saket Dingliwal, Brady Houston, Sravan Bodapati   
  Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition  
  Zhisheng Zheng, Ziyang Ma, Yu Wang, Xie Chen   
  4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders  
  Yui Sudo, Shakeel Muhammad, Brian Yan, Jiatong Shi, Shinji Watanabe   
  Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition  
  Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Yu Tsao   
  Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition  
  Zhiyun Fan, Linhao Dong, Chen Shen, Zhenlin Liang, Jun Zhang, Lu Lu, Zejun Ma   
  Mixture-of-Expert Conformer for Streaming Multilingual ASR  
  Ke Hu, Bo Li, Tara Sainath, Yu Zhang, Françoise Beaufays   
  Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switchboard Corpus  
  Zhaoqing Li, Tianzi Wang, Jiajun Deng, Junhao Xu, Shoukang Hu, Xunying Liu   
  Compressed MoE ASR Model Based on Knowledge Distillation and Quantization  
  Yuping Yuan, Zhao You, Shulin Feng, Dan Su, Yanchun Liang, Xiaohu Shi, Dong Yu   

 Acoustic Model Adaptation for ASR  
  Factorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems  
  Jiajun Deng, Guinan Li, Xurong Xie, Zengrui Jin, Mingyu Cui, Tianzi Wang, Shujie Hu, Mengzhe Geng, Xunying Liu   
  Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition  
  Wei Wang, Xun Gong, Hang Shao, Dongning Yang, Yanmin Qian   
  Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition  
  Samuel Cahyawijaya, Holy Lovenia, Willy Chung, Rita Frieske, Zihan Liu, Pascale Fung   
  Modular Domain Adaptation for Conformer-Based Streaming ASR  
  Qiujia Li, Bo Li, Dongseong Hwang, Tara Sainath, Pedro M. Mengibar   
  Don’t Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters  
  Anshu Bhatia, Sanchit Sinha, Saket Dingliwal, Karthik Gopalakrishnan, Sravan Bodapati, Katrin Kirchhoff   
  SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization  
  Changhun Kim, Joonhyung Park, Hajin Shim, Eunho Yang   

 Speech Synthesis: Expressivity  
  A Generative Framework for Conversational Laughter: Its 'Language Model' and Laughter Sound Synthesis  
  Hiroki Mori, Shunya Kimura   
  Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis  
  Weiqin Li, Shun Lei, Qiaochu Huang, Yixuan Zhou, Zhiyong Wu, Shiyin Kang, Helen Meng   
  Beyond Style: Synthesizing Speech with Pragmatic Functions  
  Harm Lameris, Joakim Gustafson, Éva Székely   
  eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many Fine-Grained Prosody Transfer  
  Ammar Abbas, Sri Karlapati, Bastian Schnell, Penny Karanasou, Marcel Granero Moya, Amith Nagaraj, Ayman Boustati, Nicole Peinelt, Alexis Moinet, Thomas Drugman   

 Multi-modal Systems  
  BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion  
  Ahana Deb, Sayan Nag, Ayan Mahapatra, Soumitri Chattopadhyay, Aritra Marik, Pijush Kanti Gayen, Shankha Sanyal, Archi Banerjee, Samir Karmakar   
  Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning  
  Sara Kashiwagi, Keitaro Tanaka, Qi Feng, Shigeo Morishima   
  Whistle-to-text: Automatic recognition of the Silbo Gomero whistled language  
  Agata Jakubiak   
  A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus  
  Lufei Gao, Shan Huang, Li Liu   
  Visually grounded few-shot word acquisition with fewer shots  
  Leanne Nortje, Benjamin van Niekerk, Herman Kamper   
  JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection  
  Li Zhou, Zhenyu Liu, Zixuan Shangguan, Xiaoyan Yuan, Yutong Li, Bin Hu   

 Question Answering from Speech  
  Prompt Guided Copy Mechanism for Conversational Question Answering  
  Yong Zhang, Zhitao Li, Jianzong Wang, Yiming Gao, Ning Cheng, Fengying Yu, Jing Xiao   
  Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants  
  Pedro Faustini, Besnik Fetahu, Giuseppe Castellucci, Anjie Fang, Oleg Rokhlenko, Shervin Malmasi   
  On Monotonic Aggregation for Open-domain QA  
  Sang-eun Han, Yeonseok Jeong, Seung-won Hwang, Kyungjae Lee   
  Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection  
  Minh Van Nguyen, Kishan KC, Toan Nguyen, Thien Huu Nguyen, Ankit Chadha, Thuy Vu   
  Multi-Scale Attention for Audio Question Answering  
  Guangyao Li, Yixin Xu, Di Hu   
  Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers  
  Feilong Chen, Minglun Han, Jing Shi, Shuang Xu, Bo Xu   

 Multi-talker Methods in Speech Processing  
  SEF-Net: Speaker Embedding Free Target Speaker Extraction Network  
  Bang Zeng, Suo Hongbin, Yulong Wan, Ming Li   
  Cascaded encoders for fine-tuning ASR models on overlapped speech  
  Richard Rose, Oscar Chang, Olivier Siohan   
  TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition  
  Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zalán Borsos, Marco Tagliasacchi, Neil Zeghidour, John R. Hershey   
  Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator  
  Lingwei Meng, Jiawen Kang, Mingyu Cui, Haibin Wu, Xixin Wu, Helen Meng   
  Time-domain Transformer-based Audiovisual Speaker Separation  
  Vahid Ahmadi Kalkhorani, Anurag Kumar, Ke Tan, Buye Xu, DeLiang Wang   
  Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization  
  Marc Delcroix, Naohiro Tawara, Mireia Diez, Federico Landini, Anna Silnova, Atsunori Ogawa, Tomohiro Nakatani, Lukáš Burget, Shoko Araki   
  Unsupervised Adaptation with Quality-Aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization  
  Shutong Niu, Jun Du, Maokui He, Chin-Hui Lee, Baoxiang Li, Jiakui Li   
  BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR  
  Yuhao Liang, Fan Yu, Yangze Li, Pengcheng Guo, Shiliang Zhang, Qian Chen, Lei Xie   
  Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation  
  Chenyang Gao, Yue Gu, Ivan Marsic   
  Joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones  
  Clément Gaultier, Tobias Goehring   
  Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach  
  Midia Yousefi, Naoyuki Kanda, Dongmei Wang, Zhuo Chen, Xiaofei Wang, Takuya Yoshioka   
  GPU-accelerated Guided Source Separation for Meeting Transcription  
  Desh Raj, Daniel Povey, Sanjeev Khudanpur   
  Overlap Aware Continuous Speech Separation without Permutation Invariant Training  
  Linfeng Yu, Wangyou Zhang, Chenda Li, Yanmin Qian   
  Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition  
  Wangyou Zhang, Yanmin Qian   
  Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression  
  Ju Lin, Niko Moritz, Ruiming Xie, Kaustubh Kalgaonkar, Christian Fuegen, Frank Seide   
  Mixture Encoder for Joint Speech Separation and Recognition  
  Simon Berger, Peter Vieting, Christoph Boeddeker, Ralf Schlüter, Reinhold Haeb-Umbach   

 Sociophonetics  
  Aberystwyth English Pre-aspiration in Apparent Time  
  Míša Michaela Hejná, Adèle Jatteau   
  Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role  
  Yanting Sun, Hongwei Ding   
  Sociodemographic and Attitudinal Effects on Dialect Speakers’ Articulation of the Standard Language: Evidence from German-Speaking Switzerland  
  Carina Steiner, Dieter Studer-Joho, Corinne Lanthemann, Andrin Büchler, Adrian Leemann   
  Vowel Normalisation in Latent Space for Sociolinguistics  
  James Burridge   

 Speaker and Language Diarization  
  Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor  
  Zhengyang Chen, Bing Han, Shuai Wang, Yanmin Qian   
  Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism  
  Rimita Lahiri, Tiantian Feng, Rajat Hebbar, Catherine Lord, So Hyun Kim, Shrikanth Narayanan   
  The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments  
  Shikha Baghel, Shreyas Ramoji, Sidharth, Ranjana H, Prachi Singh, Somil Jain, Pratik Roy Chowdhuri, Kaustubh Kulkarni, Swapnil Padhi, Deepu Vijayasenan, Sriram Ganapathy   
  Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction  
  Rohit Paturi, Sundararajan Srinivasan, Xiang Li   
  The SpeeD--ZevoTech submission at DISPLACE 2023  
  Gabriel Pirlogeanu, Dan Oneata, Alexandru-Lucian Georgescu, Horia Cucu   
  End-to-End Neural Speaker Diarization with Absolute Speaker Loss  
  Chao Wang, Jie Li, Xiang Fang, Jian Kang, Yongxiang Li   

 Speech Emotion Recognition 2  
  A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation  
  Ya-Tse Wu, Yuan-Ting Chang, Shao-Hao Lu, Jing-Yi Chuang, Chi-Chun Lee   
  MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer  
  Ya-Tse Wu, Chi-Chun Lee   
  The co-use of laughter and head gestures across speech styles  
  Bogdan Ludusan, Marin Schröer, Martina Rossi, Petra Wagner   
  EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition  
  Haiyang Sun, Zheng Lian, Bin Liu, Ying Li, Jianhua Tao, Licai Sun, Cong Cai, Meng Wang, Yuan Cheng   
  Pre-Finetuning for Few-Shot Emotional Speech Recognition  
  Maximillian Chen, Zhou Yu   
  Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations  
  Wen Wu, Chao Zhang, Philip C. Woodland   
  Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection  
  Chandrashekhar Lavania, Sanjiv Das, Xin Huang, Kyu J. Han   
  Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews  
  Sergio Burdisso, Esaú Villatoro-Tello, Srikanth Madikeri, Petr Motlicek   
  Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh  
  Catarina Branco, Isabel Trancoso, Paulo Infante, Khiet P. Truong   
  Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition  
  Yuanbo Fang, Xiaofen Xing, Xiangmin Xu, Weibin Zhang   
  Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models  
  Danilo de Oliveira, Navin Raj Prabhu, Timo Gerkmann   
  Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining  
  Yuan Gao, Chenhui Chu, Tatsuya Kawahara   
  Investigating Acoustic Cues for Multilingual Abuse Detection  
  Yash Thakran, Vinayak Abrol   
  A novel frequency warping scale for speech emotion recognition  
  Premjeet Singh, Goutam Saha   
  Multi-Scale Temporal Transformer For Speech Emotion Recognition  
  Zhipeng Li, Xiaofen Xing, Yuanbo Fang, Weibin Zhang, Hengsheng Fan, Xiangmin Xu   
  Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario  
  Nicolás Grágeda, Eduardo Alvarado, Rodrigo Mahu, Carlos Busso, Néstor Becerra Yoma   
  A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation  
  Dehua Tao, Tan Lee, Harold Chui, Sarah Luk   

 Show and Tell: Language learning and educational resources  
  A Unified Framework to Improve Learners' Skills of Perception and Production Based on Speech Shadowing and Overlapping  
  Nobuaki Minematsu, Noriko Nakanishi, Yingxiang Gao, Haitong Sun   
  Speak & Improve: L2 English Speaking Practice Tool  
  Diane Nicholls, Kate M. Knill, Mark J. F. Gales, Anton Ragni, Paul Ricketts   
  Measuring prosody in child speech using SoapBox Fluency API  
  Mauro Nicolao, Brenda McGuirk, Declan Moore, Niall Mullally, Lora Lynn O’Mahony, Emma O’Neill, Amelia C. Kelly   
  Teaching Non-native Sound Contrasts using Visual Biofeedback  
  Shawn Nissen   
  Large-Scale Automatic Audiobook Creation  
  Brendan Walsh, Mark Hamilton, Greg Newby, Xi Wang, Serena Ruan, Sheng Zhao, Lei He, Shaofei Zhang, Eric Dettinger, William T. Freeman, Markus Weimer   
  QVoice: Arabic Speech Pronunciation Learning Application  
  Yassine El Kheir, Fouad Khnaisser, Shammur Absar Chowdhury, Hamdy Mubarak, Shazia Afzal, Ahmed M. Ali   
  Asking Questions: an Innovative Way to Interact with Oral History Archives  
  Jan Švec, Martin Bulín, Adam Frémund, Filip Polák   
  DisfluencyFixer: A tool to enhance Language Learning through Speech To Speech Disfluency Correction  
  Vineet Bhat, Preethi Jyothi, Pushpak Bhattacharyya   
  Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages  
  Anusha Prakash, Arun Kumar, Ashish Seth, Bhagyashree Mukherjee, Ishika Gupta, Jom Kuriakose, Jordan F, K V Vikram, Mano R Kumar M, Metilda Sagaya Mary, Mohammad Wajahat, Mohana N, Mudit Batra, Navina K, Nihal John George, Nithya Ravi, Pruthwik Mishra, Sudhanshu Srivastava, Vasista Sai Lodagala, Vandan Mujadia, Kada Sai Venkata Vineeth, Vrunda N. Sukhadia, Dipti Sharma, Hema Murthy, Pushpak Bhattacharyya, S Umesh, Rajeev Sangal   
  MyVoice: Arabic Speech Resource Collaboration Platform  
  Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed M. Ali   
  Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-Based Educational Artifact  
  Daniel D. Hromada, Hyungjoong Kim   

 Analysis of Speech and Audio Signals 3  
  Time-frequency Domain Filter-and-sum Network for Multi-channel Speech Separation  
  Zhewen Deng, Yi Zhou, Hongqing Liu   
  Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation  
  Debang Liu, Tianqi Zhang, Mads Græsbøll Christensen, Ying Wei, Zeliang An   
  An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention  
  Junyu Wang   
  Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)  
  Waradon Phokhinanan, Nicolas Obin, Sylvain Argentieri   
  Contrastive Learning based Deep Latent Masking for Music Source Separation  
  Jihyun Kim, Hong-Goo Kang   
  Speaker Extraction with Detection of Presence and Absence of Target Speakers  
  Ke Zhang, Marvin Borsdorf, Zexu Pan, Haizhou Li, Yangjie Wei, Yi Wang   
  PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network  
  Qinghua Liu, Meng Ge, Zhizheng Wu, Haizhou Li   
  Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning  
  Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer   
  Image-driven Audio-visual Universal Source Separation  
  Chenxing Li, Ye Bai, Yang Wang, Feng Deng, Yuanyuan Zhao, Zhuo Zhang, Xiaorui Wang   
  Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource MNMF with Localization Prior  
  Mieszko Fraś, Marcin Witkowski, Konrad Kowalczyk   
  SDNet: Stream-attention and Dual-feature Learning Network for Ad-hoc Array Speech Separation  
  Honglong Wang, Chengyun Deng, Yanjie Fu, Meng Ge, Longbiao Wang, Gaoyan Zhang, Jianwu Dang, Fei Wang   
  Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization  
  Min-Sang Baek, Joon-Young Yang, Joon-Hyuk Chang   
  Multi-channel separation of dynamic speech and sound events  
  Takuya Fujimura, Robin Scheibler   
  Rethinking the Visual Cues in Audio-Visual Speaker Extraction  
  Junjie Li, Meng Ge, Zexu Pan, Rui Cao, Longbiao Wang, Jianwu Dang, Shiliang Zhang   
  Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator  
  Shaoxiang Dang, Tetsuya Matsumoto, Yoshinori Takeuchi, Hiroaki Kudo   
  Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers  
  Younggwan Kim, Hyungjun Lim, Kiho Yeom, Eunjoo Seo, Hoodong Lee, Stanley Jungkyu Choi, Honglak Lee   
  SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking  
  Jae-Heung Cho, Joon-Hyuk Chang   
  Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement  
  Zhao Yang, Dianwen Ng, Xizhe Li, Chong Zhang, Rui Jiang, Wei Xi, Yukun Ma, Chongjia Ni, Jizhong Zhao, Bin Ma, Eng Siong Chng   
  FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization  
  Yabo Wang, Bing Yang, Xiaofei Li   
  A Neural State-Space Modeling Approach to Efficient Speech Separation  
  Chen Chen, Chao-Han Huck Yang, Kai Li, Yuchen Hu, Pin-Jui Ku, Eng Siong Chng   
  Locate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation  
  Yanjie Fu, Meng Ge, Honglong Wang, Nan Li, Haoran Yin, Longbiao Wang, Gaoyan Zhang, Jianwu Dang, Chengyun Deng, Fei Wang   
  Monaural Speech Separation Method Based on Recurrent Attention with Parallel Branches  
  Xue Yang, Changchun Bao, Xu Zhang, Xianhong Chen   
  Ontology-aware Learning and Evaluation for Audio Tagging  
  Haohe Liu, Qiuqiang Kong, Xubo Liu, Xinhao Mei, Wenwu Wang, Mark D. Plumbley   

 Speech Coding and Enhancement 3  
  Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing  
  Hye-jin Shim, Jee-weon Jung, Tomi Kinnunen   
  Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement  
  Bunlong Lay, Simon Welker, Julius Richter, Timo Gerkmann   
  Complex-valued neural networks for voice anti-spoofing  
  Nicolas M. Müller, Philip Sperl, Konstantin Böttinger   
  DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation  
  Nicolae Catalin Ristea, Evgenii Indenbom, Ando Saabas, Tanel Pärnamaa, Jegor Guzhvin, Ross Cutler   
  Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement  
  Ryosuke Sawata, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Takashi Shibuya, Shusuke Takahashi, Yuki Mitsufuji   
  HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders  
  Doyeon Kim, Soo-Whan Chung, Hyewon Han, Youna Ji, Hong-Goo Kang   
  MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra  
  Ye-Xin Lu, Yang Ai, Zhen-Hua Ling   
  TridentSE: Guiding Speech Enhancement with 32 Global Tokens  
  Dacheng Yin, Zhiyuan Zhao, Chuanxin Tang, Zhiwei Xiong, Chong Luo   
  Detection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features  
  Chenglong Wang, Jiangyan Yi, Jianhua Tao, Chu Yuan Zhang, Shuai Zhang, Xun Chen   
  Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions  
  Sandipana Dowerah, Ajinkya Kulkarni, Romain Serizel, Denis Jouvet   
  Two-Stage Voice Anonymization for Enhanced Privacy  
  Francesco Nespoli, Daniel Barreda, Jöerg Bitzer, Patrick A. Naylor   
  Personalized Dereverberation of Speech  
  Ruilin Xu, Gurunandan Krishnan, Changxi Zheng, Shree K. Nayar   
  Weighted Von Mises Distribution-based Loss Function for Real-time STFT Phase Reconstruction Using DNN  
  Nguyen Binh Thien, Yukoh Wakabayashi, Yuting Geng, Kenta Iwai, Takanobu Nishiura   
  Deep Multi-Frame Filtering for Hearing Aids  
  Hendrik Schröter, Tobias Rosenkranz, Alberto N. Escalante-B., Andreas Maier   
  Aligning Speech Enhancement for Improving Downstream Classification Performance  
  Yan Xiong, Visar Berisha, Chaitali Chakrabarti   
  DNN-based Parameter Estimation for MVDR Beamforming and Post-filtering  
  Minseung Kim, Sein Cheong, Jong Won Shin   
  FRA-RIR: Fast Random Approximation of the Image-source Method  
  Yi Luo, Jianwei Yu   
  Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement  
  Haibin Wu, Ke Tan, Buye Xu, Anurag Kumar, Daniel Wong   
  Harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model  
  Xiaohuai Le, Tong Lei, Li Chen, Yiqing Guo, Chao He, Cheng Chen, Xianjun Xia, Hua Gao, Yijian Xiao, Piao Ding, Shenyi Song, Jing Lu   

 Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 3  
  How Does Pretraining Improve Discourse-Aware Translation?  
  Zhihong Huang, Longyue Wang, Siyou Liu, Derek F. Wong   
  PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction  
  Ziji Zhang, Zhehui Wang, Rajesh Kamma, Sharanya Eswaran, Narayanan Sadagopan   
  Model-assisted Lexical Tone Evaluation of three-year-old Chinese-speaking Children by also Considering Segment Production  
  Shu-Chuan Tseng, Yi-Fen Liu, Xiang-Li Lu   
  Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding  
  Yi Xuan Tan, Navonil Majumder, Soujanya Poria   
  Joint Time and Frequency Transformer for Chinese Opera Classification  
  Qiang Li, Beibei Hu   
  AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination  
  Myunghun Jung, Hoirin Kim   
  Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective  
  Mohammad Arvan, A. Seza Doğruöz, Natalie Parde   
  An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Sorpus  
  Astik Biswas, Abdelmoumene Boumadane, Stephane Peillon, Gildas Bleas   
  Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition  
  Shi-wook Lee   
  Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text  
  Parnia Bahar, Mattia Di Gangi, Nick Rossenbach, Mohammad Zeineldeen   
  Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin  
  Pin-Jie Lin, Muhammed Saeed, Ernie Chang, Merel Scholman   
  Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition  
  Eesung Kim, Aditya Jajodia, Cindy Tseng, Divya Neelagiri, Taeyeon Ki, Vijendra Raj Apsingekar   
  PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords  
  Yong-Hyeok Lee, Namhyun Cho   
  Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning  
  Zhihong Zhu, Xuxin Cheng, Dongsheng Chen, Zhiqi Huang, Hongxiang Li, Yuexian Zou   
  AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation  
  Sara Papi, Marco Turchi, Matteo Negri   
  Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff  
  Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar   
  Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages  
  Claytone Sikasote, Kalinda Siaminwe, Stanly Mwape, Bangiwe Zulu, Mofya Phiri, Martin Phiri, David Zulu, Mayumbo Nyirenda, Antonios Anastasopoulos   

 Anti-Spoofing for Speaker Verification  
  Towards Single Integrated Spoofing-aware Speaker Verification Embeddings  
  Sung Hwan Mun, Hye-jin Shim, Hemlata Tak, Xin Wang, Xuechen Liu, Md Sahidullah, Myeonghun Jeong, Min Hyun Han, Massimiliano Todisco, Kong Aik Lee, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen, Nam Soo Kim, Jee-weon Jung   
  Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification  
  Qing Wang, Jixun Yao, Ziqian Wang, Pengcheng Guo, Lei Xie   
  Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion  
  Rui Liu, Jinhua Zhang, Guanglai Gao, Haizhou Li   
  Robust Audio Anti-spoofing Countermeasure with Joint Training of Front-end and Back-end Models  
  Xingming Wang, Bang Zeng, Suo Hongbin, Yulong Wan, Ming Li   
  Improved DeepFake Detection Using Whisper Features  
  Piotr Kawa, Marcin Plata, Michał Czuba, Piotr Szymański, Piotr Syga   
  DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures  
  Mengao Zhang, Ke Xu, Hao Li, Lei Wang, Chengfang Fang, Jie Shi   

 Speech Coding: Intelligibility  
  On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR  
  Sankaran Panchapagesan, Turaj Zakizadeh Shabestary, Arun Narayanan   
  Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation  
  Jean-Marie Lemercier, Julian Tobergte, Timo Gerkmann   
  UnSE: Unsupervised Speech Enhancement Using Optimal Transport  
  Wenbin Jiang, Fei Wen, Yifan Zhang, Kai Yu   
  MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation  
  Jun Chen, Wei Rao, Zilin Wang, Jiuxin Lin, Yukai Ju, Shulin He, Yannan Wang, Zhiyong Wu   
  Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement  
  Julitta Bartolewska, Stanisław Kacprzak, Konrad Kowalczyk   
  Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction  
  Wenzhe Liu, Yupeng Shi, Jun Chen, Wei Rao, Shulin He, Andong Li, Yannan Wang, Zhiyong Wu   

 Resources for Spoken Language Processing  
  Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech  
  Elena Ryumina, Dmitry Ryumin, Maxim Markitantov, Heysem Kaya, Alexey Karpov   
  MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset  
  Mikołaj Pudo, Mateusz Wosik, Adam Cieślak, Justyna Krzywdziak, Bozena Lukasiak, Artur Janicki   
  MD3: The Multi-Dialect Dataset of Dialogues  
  Jacob Eisenstein, Vinodkumar Prabhakaran, Clara Rivera, Dorottya Demszky, Devyani Sharma   
  MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation  
  Mohamed Anwar, Bowen Shi, Vedanuj Goswami, Wei-Ning Hsu, Juan Pino, Changhan Wang   
  Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition  
  Artit Suwanbandit, Burin Naowarat, Orathai Sangpetch, Ekapol Chuangsuwanich   
  HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation  
  Cihan Xiao, Henry Li Xinyuan, Jinyi Yang, Dongji Gao, Matthew Wiesner, Kevin Duh, Sanjeev Khudanpur   

 New Computational Strategies for ASR Training and Inference  
  A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference  
  Dhanush Bekal, Karthik Gopalakrishnan, Karel Mundnich, Srikanth Ronanki, Sravan Bodapati, Katrin Kirchhoff   
  Distillation Strategies for Discriminative Speech Recognition Rescoring  
  Prashanth Gurunath Shivakumar, Jari Kolehmainen, Yile Gu, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko   
  Another Point of View on Visual Speech Recognition  
  Baptiste Pouthier, Laurent Pilati, Giacomo Valenti, Charles Bouveyron, Frederic Precioso   
  RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech Recognition  
  Wei Zhou, Eugen Beck, Simon Berger, Ralf Schlüter, Hermann Ney   
  Streaming Speech-to-Confusion Network Speech Recognition  
  Denis Filimonov, Prabhat Pandey, Ariya Rastrow, Ankur Gandhe, Andreas Stolcke   
  Accurate and Structured Pruning for Efficient Automatic Speech Recognition  
  Huiqiang Jiang, Li Lyna Zhang, Yuang Li, Yu Wu, Shijie Cao, Ting Cao, Yuqing Yang, Jinyu Li, Mao Yang, Lili Qiu   

 MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech  
  MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization  
  Victoria Y. H. Chua, Hexin Liu, Leibny Paola Garcia, Fei Ting Woon, Jinyi Wong, Xiangyu Zhang, Sanjeev Khudanpur, Andy W. H. Khong, Justin Dauwels, Suzy J. Styles   
  Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech  
  Shashi Kant Gupta, Sushant Hiray, Prashant Kukde   
  Improving wav2vec2-based Spoken Language Identification by Learning Phonological Features  
  Mostafa Shahin, Zheng Nan, Vidhyasaharan Sethu, Beena Ahmed   
  Language Identification Networks for Multilingual Everyday Recordings  
  Kiran Praveen, Balaji Radhakrishnan, Kamini Sabu, Abhishek Pandey, Mahaboob Ali Basha Shaik   
  Investigating model performance in language identification: beyond simple error statistics  
  Suzy J. Styles, Victoria Y. H. Chua, Fei Ting Woon, Hexin Liu, Leibny Paola Garcia, Sanjeev Khudanpur, Andy W. H. Khong, Justin Dauwels   

 Health-Related Speech Analysis  
  Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings  
  Manila Kodali, Sudarsana Reddy Kadiri, Paavo Alku   
  The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances  
  Alexander Kathan, Andreas Triantafyllopoulos, Shahin Amiriparian, Sabrina Milkus, Alexander Gebhard, Jonas Hohmann, Pauline Muderlak, Jürgen Schottdorf, Björn W. Schuller, Richard Musil   
  Analysis and automatic prediction of exertion from speech: Contrasting objective and subjective measures collected while running  
  Andreas Triantafyllopoulos, Alexander Gebhard, Alexander Kathan, Maurice Gerczuk, Shahin Amiriparian, Björn W. Schuller   
  The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection  
  Fuxiang Tao, Anna Esposito, Alessandro Vinciarelli   
  Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation  
  Marina Eni, Ilan Dinstein, Yaniv Zigel   
  Acoustic characteristics of depression in older adults' speech: the role of covariates  
  Carmen Mijnders, Esther Janse, Paul Naarding, Khiet P. Truong   

 Automatic Audio Classification and Audio Captioning  
  Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning  
  Jianyuan Sun, Xubo Liu, Xinhao Mei, Volkan Kılıç, Mark D. Plumbley, Wenwu Wang   
  Adapting a ConvNeXt Model to Audio Classification on AudioSet  
  Thomas Pellegrini, Ismail Khalfaoui-Hassani, Etienne Labbé, Timothée Masquelier   
  Few-shot Class-incremental Audio Classification Using Stochastic Classifier  
  Yanxiong Li, Wenchang Cao, Jialong Li, Wei Xie, Qianhua He   
  Enhance Temporal Relations in Audio Captioning with Sound Event Detection  
  Zeyu Xie, Xuenan Xu, Mengyue Wu, Kai Yu   

 Speech Perception, Production, and Acquisition 2  
  First Language Effects on Second Language Perception: Evidence from English Low-vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners  
  Sijia Zhang   
  Motor Control Similarity Between Speakers Saying “A Souk” Using Inverse Atlas Tongue Modeling  
  Ursa Maity, Fangxu Xing, Jerry Prince, Maureen Stone, El Fakhri Georges, Jonghye Woo, Sidney Fels   
  Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models  
  Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien   
  A Relationship Between Vocal Fold Vibration and Droplet Production  
  Tsukasa Yoshinaga, Takayuki Arai, Akiyoshi Iida   
  Audio, Visual and Audiovisual intelligibility of vowels produced in noise  
  Maeva Garnier   
  Optimal control of speech with context-dependent articulatory targets  
  Benjamin Elie, Juraj Šimko, Alice Turk   
  Computational modeling of auditory brainstem responses derived from modified speech  
  Tzu-Han Zoe Cheng, Paul Calamia   
  Leveraging Label Information for Multimodal Emotion Recognition  
  Peiying Wang, Sunlu Zeng, Junqing Chen, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He   
  Improving End-to-End Modeling For Mandarin-English Code-Switching Using Lightweight Switch-Routing Mixture-of-Experts  
  Fengyun Tan, Chaofeng Feng, Tao Wei, Shuai Gong, Jinqiang Leng, Wei Chu, Jun Ma, Shaojun Wang, Jing Xiao   
  Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges  
  Zhao Zhang, Ju Zhang, Ziyu Zhu, Yujie Chi, Kiyoshi Honda, Jianguo Wei   
  Adaptation to predictive prosodic cues in non-native standard dialect  
  Sabine Gosselke Berthelsen   
  Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions  
  Alan Archer-Boyd, Rainer Martin   
  Second language identification of Vietnamese tones by native Mandarin learners  
  Juqiang Chen, Ailing Qin, Hui Chang, Hua Chen   
  Nasal vowel production and grammatical processing in French-speaking children with cochlear implants and normal-hearing peers.  
  Sophie Fagniart, Véronique Delvaux, Brigitte Charlier, Bernard Harmegnies, Anne Huberlant, Myriam Piccaluga, Kathy Huet   
  Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech  
  Zechen Zhang, Xihong Wu, Jing Chen   
  L2-Mandarin regional accent variability during Mandarin tone-word training facilitates English listeners’ subsequent tone categorizations  
  Yanping Li, Michael D. Tyler, Denis Burnham, Catherine T. Best   
  HumanDiffusion: diffusion model using perceptual gradients  
  Yota Ueda, Shinnosuke Takamichi, Yuki Saito, Norihiro Takamune, Hiroshi Saruwatari   
  Queer Events, Relationships, and Sports: Does Topic Influence Speakers’ Acoustic Expression of Sexual Orientation?  
  Sven Kachel, Manuel Pöhlmann, Christine Nussbaum   

 Speech Synthesis  
  Epoch-Based Spectrum Estimation for Speech  
  Jón Guðnason, Guolin Fang, Mike Brookes   
  OverFlow: Putting flows on top of neural transducers for better TTS  
  Shivam Mehta, Ambika Kirkland, Harm Lameris, Jonas Beskow, Éva Székely, Gustav Eje Henter   
  ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation  
  Ambuj Mehrish, Abhinav Ramesh Kashyap, Li Yingting, Navonil Majumder, Soujanya Poria   
  Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis  
  Won-Gook Choi, So-Jeong Kim, TaeHo Kim, Joon-Hyuk Chang   
  UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model  
  Anastasiia Iashchenko, Pavel Andreev, Ivan Shchekotov, Nicholas Babaev, Dmitry Vetrov   
  Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech  
  Hyungchan Yoon, Changhwan Kim, Eunwoo Song, Hyun-Wook Yoon, Hong-Goo Kang   
  Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge  
  Wenhao Guan, Tao Li, Yishuang Li, Hukai Huang, Qingyang Hong, Lin Li   
  Towards Robust FastSpeech 2 by Modelling Residual Multimodality  
  Fabian Kögel, Bac Nguyen, Fabien Cardinaux   
  Real time spectrogram inversion on mobile phone  
  Oleg Rybakov, Marco Tagliasacchi, Yunpeng Li, Liyang Jiang, Xia Zhang, Fadi Biadsy   
  Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis  
  Seongyeon Park, Bohyung Kim, Tae-Hyun Oh   
  A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic  
  Dan Wells, Korin Richmond, William Lamb   
  Self-Supervised Solution to the Control Problem of Articulatory Synthesis  
  Paul K. Krug, Peter Birkholz, Branislav Gerazov, Daniel R. van Niekerk, Anqi Xu, Yi Xu   
  Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis  
  Joun Yeop Lee, Jae-Sung Bae, Seongkyu Mun, Jihwan Lee, Ji-Hyun Lee, Hoon-Young Cho, Chanwoo Kim   
  ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models  
  Minki Kang, Wooseok Han, Sung Ju Hwang, Eunho Yang   
  Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding  
  Muyang Du, Chuan Liu, Jiaxing Qi, Junjie Lai   
  Intelligible Lip-to-Speech Synthesis with Speech Units  
  Jeongsoo Choi, Minsu Kim, Yong Man Ro   
  Parameter-Efficient Learning for Text-to-Speech Accent Adaptation  
  Li-Jen Yang, Chao-Han Huck Yang, Jen-Tzung Chien   
  Controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age  
  Ziya Khan, Lovisa Wihlborg, Cassia Valentini-Botinhao, Oliver Watts   
  FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs  
  Won Jang, Dan Lim, Heayoung Park   
  iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN  
  Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Shogo Seki   
  VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design  
  Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim   
  Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme  
  Hieu-Thi Luong, Junichi Yamagishi   

 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 4  
  Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR  
  Kaushal Bhogale, Sai Sundaresan, Abhigyan Raman, Tahir Javed, Mitesh M. Khapra, Pratyush Kumar   
  Domain Adaptive Self-supervised Training of Automatic Speech Recognition  
  Cong-Thanh Do, Rama Doddipatla, Mohan Li, Thomas Hain   
  There is more than one kind of robustness: Fooling Whisper with adversarial examples  
  Raphael Olivier, Bhiksha Raj   
  MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations  
  Calum Heggan, Tim Hospedales, Sam Budgett, Mehrdad Yaghoobi   
  Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute  
  William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe   
  Blank-regularized CTC for Frame Skipping in Neural Transducer  
  Yifan Yang, Xiaoyu Yang, Liyong Guo, Zengwei Yao, Wei Kang, Fangjun Kuang, Long Lin, Xie Chen, Daniel Povey   
  The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR  
  Kaousheik Jayakumar, Vrunda N. Sukhadia, A Arunkumar, S Umesh   
  Improving RNN-Transducers with Acoustic LookAhead  
  Vinit S. Unni, Ashish Mittal, Preethi Jyothi, Sunita Sarawagi   
  Everyone has an accent  
  Nina Markl, Catherine Lai   
  Some Voices are Too Common: Building Fair Speech Recognition Systems Using the CommonVoice Dataset  
  Lucas Maison, Yannick Estève   
  Information Magnitude Based Dynamic Sub-sampling for Speech-to-text  
  Yuhao Zhang, Chenghao Gao, Kaiqi Kou, Chen Xu, Tong Xiao, Jingbo Zhu   

 Keynote 3  
  What’s in a Rise? The Relevance of Intonation for Attention Orienting  
  Martine Grice   

 Speech Synthesis: Controllability and Adaptation  
  HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer  
  Sang-Hoon Lee, Ha-Yeong Choi, Hyung-Seok Oh, Seong-Whan Lee   
  VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer  
  Yongmao Zhang, Heyang Xue, Hanzhao Li, Lei Xie, Tingwei Guo, Ruixiong Zhang, Caixia Gong   
  EdenTTS: A Simple and Efficient Parallel Text-to-speech Architecture with Collaborative Duration-alignment Learning  
  Youneng Ma, Junyi He, Meimei Wu, Guangyue Hu, Haojun Fei   
  Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations  
  Wenbin Wang, Yang Song, Sanjay Jha   
  Speech inpainting: Context-based speech synthesis guided by video  
  Juan Felipe Montesinos, Daniel Michelsanti, Gloria Haro, Zheng-Hua Tan, Jesper Jensen   
  STEN-TTS: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework  
  Chung Tran, Chi Mai Luong, Sakriani Sakti   

 Search Methods and Decoding Algorithms for ASR  
  Average Token Delay: A Latency Metric for Simultaneous Translation  
  Yasumasa Kano, Katsuhito Sudoh, Satoshi Nakamura   
  Automatic Speech Recognition Transformer with Global Contextual Information Decoder  
  Yukun Qian, Xuyi Zhuang, Mingjiang Wang   
  Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training  
  Yui Sudo, Shakeel Muhammad, Yifan Peng, Shinji Watanabe   
  Prefix Search Decoding for RNN Transducers  
  Kiran Praveen, Advait Vinay Dhopeshwarkar, Abhishek Pandey, Balaji Radhakrishnan   
  WhisperX: Time-Accurate Speech Transcription of Long-Form Audio  
  Max Bain, Jaesung Huh, Tengda Han, Andrew Zisserman   
  Implementing Contextual Biasing in GPU Decoder for Online ASR  
  Iuliia Nigmatulina, Srikanth Madikeri, Esaú Villatoro-Tello, Petr Motlicek, Juan Zuluaga-Gomez, Karthik Pandia, Aravind Ganapathiraju   

 Speech Signal Analysis  
  MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion  
  Woo-Jin Chung, Doyeon Kim, Soo-Whan Chung, Hong-Goo Kang   
  Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset  
  Ahmed Adel Attia, Mark Tiede, Carol Espy-Wilson   
  Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study  
  Melanie Jouaiti, Pippa Kirby, Ravi Vaidyanathan   
  Improved Contextualized Speech Representations for Tonal Analysis  
  Jiahong Yuan, Xingyu Cai, Kenneth Church   
  A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence  
  Siddarth Chandrasekar, Arvind Ramesh, Tilak Purohit, Prasanta Kumar Ghosh   
  FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals  
  Eray Eren, Lee Ngee Tan, Abeer Alwan   

 Speech Emotion Recognition 3  
  Improving Joint Speech and Emotion Recognition Using Global Style Tokens  
  Jehyun Kyung, Ju-Seok Seong, Jeong-Hwan Choi, Ye-Rin Jeoung, Joon-Hyuk Chang   
  Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute  
  Ryotaro Nagase, Takahiro Fukumori, Yoichi Yamashita   
  Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition  
  Shenjie Jiang, Peng Song, Shaokai Li, Keke Zhao, Wenming Zheng   
  Dual Memory Fusion for Multimodal Speech Emotion Recognition  
  Darshana Prisayad, Tharindu Fernando, Sridha Sridharan, Simon Denman, Clinton Fookes   
  Hybrid Dataset for Speech Emotion Recognition in Russian Language  
  Vladimir Kondratenko, Nikolay Karpov, Artem Sokolov, Nikita Savushkin, Oleg Kutuzov, Fyodor Minkin   
  Speech Emotion Recognition using Decomposed Speech via Multi-task Learning  
  Jia-Hao Hsu, Chung-Hsien Wu, Yu-Hung Wei   

 Connecting Speech-science and Speech-technology for Children's Speech  
  Prospective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders  
  Nina R Benway, Jonathan L Preston   
  Classifying Rhoticity of /ɹ/ in Speech Sound Disorder using Age-and-Sex Normalized Formants  
  Nina R Benway, Jonathan L Preston, Asif Salekin, Yi Xiao, Harshit Sharma, Tara McAllister   
  Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /ɹ/ in Child Speech Sound Disorders  
  Nina R Benway, Yashish M Siriwardena, Jonathan L Preston, Elaine Hitchcock, Tara McAllister, Carol Espy-Wilson   
  Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report  
  Timothy Piton, Enno Hermann, Angela Pasqualotto, Marjolaine Cohen, Mathew Magimai.-Doss, Daphné Bavelier   
  Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children’s Speech  
  Christopher Gebauer, Lars Rumberg, Hanna Ehlert, Ulrike Lüdtke, Joern Ostermann   
  Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition  
  Lars Rumberg, Christopher Gebauer, Hanna Ehlert, Maren Wallbaum, Ulrike Lüdtke, Joern Ostermann   
  BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models  
  Marvin Lavechin, Yaya Sy, Hadrien Titeux, María Andrea Cruz Blandón, Okko Räsänen, Hervé Bredin, Emmanuel Dupoux, Alejandrina Cristia   
  Data augmentation for children ASR and child-adult speaker classification using voice conversion methods  
  Shuyang Zhao, Mittul Singh, Abraham Woubie, Reima Karhila   
  Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children  
  Vishwas M. Shetty, Steven M. Lulich, Abeer Alwan   
  Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism  
  Yahan Yang, Sunghye Cho, Maxine Covello, Azia Knox, Osbert Bastani, James Weimer, Edgar Dobriban, Robert Schultz, Insup Lee, Julia Parish-Morris   
  An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities  
  Alexander Johnson, Hariram Veeramani, Natarajan Balaji Shankar, Abeer Alwan   
  An Analysis of Goodness of Pronunciation for Child Speech  
  Xinwei Cao, Zijian Fan, Torbjørn Svendsen, Giampiero Salvi   
  Measuring Language Development From Child-centered Recordings  
  Yaya Sy, William N. Havard, Marvin Lavechin, Emmanuel Dupoux, Alejandrina Cristia   
  Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children Based on Speech Intelligibility Using a Machine Learning Approach  
  Hiuching Hung, Paula A. Pérez-Toro, Tomás Arias-Vergara, Andreas Maier, Elmar Nöth   
  Speech Breathing Behavior During Pauses in Children  
  Delphine Charuau, Béatrice Vaxelaire, Rudolph Sock   
  Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings  
  Anfeng Xu, Rajat Hebbar, Rimita Lahiri, Tiantian Feng, Lindsay Butler, Lue Shen, Helen Tager-Flusberg, Shrikanth Narayanan   
  Measuring Phonological Precision in Children with Cleft Lip and Palate  
  Tomás Arias-Vergara, Elizabeth Londoño-Mora, Paula A. Pérez-Toro, Maria Schuster, Elmar Nöth, Juan Rafael Orozco-Arroyave, Andreas Maier   
  A Study on Using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children  
  Si-Ioi Ng, Cymie Wing-Yee Ng, Tan Lee   
  Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate  
  Ilja Baumann, Dominik Wagner, Franziska Braun, Sebastian P. Bayerl, Elmar Nöth, Korbinian Riedhammer, Tobias Bocklet   

 Dialog Management  
  Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning  
  Mingyu Derek Ma, Jiun-Yu Kao, Shuyang Gao, Arpit Gupta, Di Jin, Tagyoung Chung, Nanyun Peng   
  An Autoregressive Conversational Dynamics Model for Dialogue Systems  
  Matthew McNeill, Rivka Levitan   
  Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos  
  Chiori Hori, Puyuan Peng, David Harwath, Xinyu Liu, Kei Ota, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux   
  Speech Aware Dialog System Technology Challenge (DSTC11)  
  Hagen Soltau, Izhak Shafran, Mingqiu Wang, Abhinav Rastogi, Jeffrey Zhao, Ye Jia, Wei Han, Yuan Cao, Aramys Miranda   
  Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision  
  Yucheng Cai, Hong Liu, Zhijian Ou, Yi Huang, Junlan Feng   
  Tracking Must Go On : Dialogue State Tracking with Verified Self-Training  
  Jihyun Lee, Chaebin Lee, Yunsu Kim, Gary Geunbae Lee   

 Speaker Recognition 2  
  Ordered and Binary Speaker Embedding  
  Jiaying Wang, Xianglong Wang, Namin Wang, Lantian Li, Dong Wang   
  Self-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition  
  Saurabh Kataria, Jesús Villalba, Laureano Moro-Velazquez, Thomas Thebaud, Najim Dehak   
  Curriculum Learning for Self-supervised Speaker Verification  
  Hee-Soo Heo, Jee-weon Jung, Jingu Kang, Young-ki Kwon, Bong-Jin Lee, You Jin Kim, Joon Son Chung   
  Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification  
  Ziyang Zhang, Wu Guo, Bin Gu   
  A Teacher-Student Approach for Extracting Informative Speaker Embeddings From Speech Mixtures  
  Tobias Cord-Landwehr, Christoph Boeddeker, Cătălin Zorilă, Rama Doddipatla, Reinhold Haeb-Umbach   
  Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification  
  Theo Lepage, Reda Dehak   

 Phonetics, Phonology, and Prosody 2  
  Nonbinary American English speakers encode gender in vowel acoustics  
  Maxwell Hope, Charlotte Ward, Jason Lilley   
  Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study  
  Jared Sharp, Matthew Faytak, Hasutai Fei Xiong Liu   
  Using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech  
  Georgina Brown, Christin Kirchhübel, Ramiz Cuthbert   
  Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones  
  Yishan Huang   
  Discovering Phonetic Feature Event Patterns in Transformer Embeddings  
  Patrick Cormac English, John D. Kelleher, Julie Carson-Berndsen   
  A System for Generating Voice Source Signals that Implements the Transformed LF-model Parameter Control  
  Zihan Wang, Christer Gobl   
  Speaker-independent Speech Inversion for Estimation of Nasalance  
  Yashish M Siriwardena, Carol Espy-Wilson, Suzanne Boyce, Mark Tiede, Liran Oren   
  Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect  
  Yiying Hu, Hui Feng, Qinghua Zhao, Aijun Li   
  Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic  
  Amel Issa   
  Mapping Phonemes to Acoustic Symbols and Codes Using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank  
  Ashwin Rao   
  Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners  
  Lindun Ge, Min Xu, Hongwei Ding   
  (Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-prosodic Features  
  Anneliese Kelterer, Margaret Zellers, Barbara Schuppler   
  Vowel reduction by Greek-speaking children: The effect of stress and word length  
  Polychronia Christodoulidou, Katerina Nicolaidis, Dimitrios Stamovlasis   
  Pitch distributions in a very large corpus of spontaneous Finnish speech  
  Mietta Lennes, Minnaleena Toivola   
  Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective  
  Jacek Kudera, Katharina Zahner-Ritter, Jakob Engel, Nathalie Elsässer, Philipp Hutmacher, Carolin Worstbrock   

 Speech Synthesis: Expressivity  
  Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions  
  Florian Lux, Pascal Tilli, Sarina Meyer, Ngoc Thang Vu   
  Dual Audio Encoders Based Mandarin Prosodic Boundary Prediction by Using Multi-Granularity Prosodic Representations  
  Ruishan Li, Yingming Gao, Yanlu Xie, Dengfeng Ke, Jinsong Zhang   
  NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS  
  Dongchao Yang, Songxiang Liu, Helin Wang, Jianwei Yu, Chao Weng, Yuexian Zou   
  MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy  
  Ya-Jie Zhang, Wei Song, Yanghao Yue, Zhengchen Zhang, Youzheng Wu, Xiaodong He   
  Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS  
  Tankala Pavan Kalyan, Preeti Rao, Preethi Jyothi, Pushpak Bhattacharyya   
  CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation  
  Yuhao Cui, Xiongwei Wang, Zhongzhou Zhao, Wei Zhou, Haiqing Chen   
  Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations  
  Yoori Oh, Juheon Lee, Yoseob Han, Kyogu Lee   
  Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis  
  Tu Anh Nguyen, Wei-Ning Hsu, Antony D'Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux   
  ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios  
  Yuyue Wang, Huan Xiao, Yihan Wu, Ruihua Song   
  Neural Speech Synthesis with Enriched Phrase Boundaries  
  Marie Kunešová, Jindřich Matoušek   
  Cross-lingual Prosody Transfer for Expressive Machine Dubbing  
  Jakub Swiatkowski, Duo Wang, Mikolaj Babianski, Patrick Lumban Tobing, Ravichander Vipperla, Vincent Pollet   
  Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception  
  Mikey Elmers, Johannah O'Mahony, Éva Székely   
  Accentor: An Explicit Lexical Stress Model for TTS Systems  
  Diana Geneva, Georgi Shopov, Kostadin Garov, Maria Todorova, Stefan Gerdjikov, Stoyan Mihov   
  A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers  
  Slava Shechtman, Raul Fernandez   
  Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model  
  Xiang Li, Songxiang Liu, Max W. Y. Lam, Zhiyong Wu, Chao Weng, Helen Meng   
  Prosody Modeling with 3D Visual Information for Expressive Video Dubbing  
  Zhihan Yang, Shansong Liu, Xu Li, Haozhe Wu, Zhiyong Wu, Ying Shan, Jia Jia   
  LightClone: Speaker-guided Parallel Subnet Selection for Few-shot Voice Cloning  
  Jie Wu, Jian Luan, Yujun Wang   
  EE-TTS: Emphatic Expressive TTS with Linguistic Information  
  Yi Zhong, Chen Zhang, Xule Liu, Chenxi Sun, Weishan Deng, Haifeng Hu, Zhongqian Sun   
  Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS  
  Sewade Ogun, Vincent Colotte, Emmanuel Vincent   
  ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading  
  Yujia Xiao, Shaofei Zhang, Xi Wang, Xu Tan, Lei He, Sheng Zhao, Frank K. Soong, Tan Lee   
  PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions  
  Guanghou Liu, Yongmao Zhang, Yi Lei, Yunlin Chen, Rui Wang, Lei Xie, Zhifei Li   
  Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models  
  Yusheng Tian, Guangyan Zhang, Tan Lee   

 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 5  
  Towards Multi-task Learning of Speech and Speaker Recognition  
  Nik Vaessen, David A. van Leeuwen   
  Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR  
  Zeyu Zhao, Peter Bell   
  2-bit Conformer quantization for automatic speech recognition  
  Oleg Rybakov, Phoenix Meadowlark, Shaojin Ding, David Qiu, Jian Li, David Rim, Yanzhang He   
  Time-Domain Speech Enhancement for Robust Automatic Speech Recognition  
  Yufeng Yang, Ashutosh Pandey, DeLiang Wang   
  Multi-channel multi-speaker transformer for speech recognition  
  Guo Yifan, Tian Yao, Suo Hongbin, Wan Yulong   
  Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion  
  Zhe Ye, Terui Mao, Li Dong, Diqun Yan   
  Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR  
  Shogo Miwa, Atsuhiko Kai   
  Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network  
  Kaixun Huang, Ao Zhang, Zhanheng Yang, Pengcheng Guo, Bingshen Mu, Tianyi Xu, Lei Xie   
  Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think  
  Tina Raissi, Christoph Lüscher, Moritz Gunz, Ralf Schlüter, Hermann Ney   
  MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for speech recognition  
  Xiaohuan Zhou, Jiaming Wang, Zeyu Cui, Shiliang Zhang, Zhijie Yan, Jingren Zhou, Chang Zhou   
  Biased Self-supervised Learning for ASR  
  Florian L. Kreyssig, Yangyang Shi, Jinxi Guo, Leda Sari, Abdel-rahman Mohamed, Philip C. Woodland   
  A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions  
  Zhao Yang, Dianwen Ng, Chong Zhang, Rui Jiang, Wei Xi, Yukun Ma, Chongjia Ni, Jizhong Zhao, Bin Ma, Eng Siong Chng   
  wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting  
  Ranzo Huang, Brian Mak   
  BAT: Boundary aware transducer for memory-efficient and low-latency ASR  
  Keyu An, Xian Shi, Shiliang Zhang   
  Bayes Risk Transducer: Transducer with Controllable Alignment Prediction  
  Jinchuan Tian, Jianwei Yu, Hangting Chen, Brian Yan, Chao Weng, Dong Yu, Shinji Watanabe   
  Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition  
  Belen Alastruey, Lukas Drude, Jahn Heymann, Simon Wiesler   

 Speech, Voice, and Hearing Disorders 2  
  Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding  
  Sanjana Sankar, Denis Beautemps, Frédéric Elisei, Olivier Perrotin, Thomas Hueber   
  Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task  
  Yingyang Wang, Min Xu, Jing Shao, Lan Wang, Nan Yan   
  Cochlear-implant Listeners Listening to Cochlear-implant Simulated Speech  
  Fanhui Kong, Nengheng Zheng, Xianren Wang, Hao He, Jan W. H. Schnupp, Qinglin Meng   
  Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection  
  Olivia M. Murton, Abigail E. Haenssler, Marc F. Maffei, Kathryn P. Connaghan, Jordan Green   
  Score-balanced Loss for Multi-aspect Pronunciation Assessment  
  Heejin Do, Yunsu Kim, Gary Geunbae Lee   
  Federated Learning for Secure Development of AI Models for Parkinson’s Disease Detection Using Speech from Different Languages  
  Soroosh Tayebi Arasteh, Cristian David Ríos-Urrego, Elmar Nöth, Andreas Maier, Seung Hee Yang, Jan Rusz, Juan Rafael Orozco-Arroyave   
  F0inTFS: A lightweight periodicity enhancement strategy for cochlear implants  
  Huali Zhou, Fanhui Kong, Nengheng Zheng, Qinglin Meng   
  Differentiating acoustic and physiological features in speech for hypoxia detection  
  Benjamin O'Brien, Adrien Gresse, Jean-Baptise Billaud, Guilhem Belda, Jean-François Bonastre   
  Mandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features  
  Hsin-Hao Chen, Yung-Lun Chien, Ming-Chi Yen, Shu-Wei Tsai, Tai-shih Chi, Hsin-Min Wang, Yu Tsao   
  Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion  
  Yung-Lun Chien, Hsin-Hao Chen, Ming-Chi Yen, Shu-Wei Tsai, Hsin-Min Wang, Yu Tsao, Tai-shih Chi   
  Which aspects of motor speech disorder are captured by Mel Frequency Cepstral Coefficients? Evidence from the change in STN-DBS conditions in Parkinson’s disease  
  Vojtěch Illner, Petr Krýže, Jan Švihlík, Mário Sousa, Paul Krack, Elina Tripoliti, Robert Jech, Jan Rusz   
  Detecting Manifest Huntington's Disease Using Vocal Data  
  Vinod Subramanian, Namhee Kwon, Raymond Brueckner, Nate Blaylock, Henry O'Connell, Luis Sierra, Clementina Ullman, Karen Hildebrand, Simon Laganiere   
  Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models  
  Minchuan Chen, Chenfeng Miao, Jun Ma, Shaojun Wang, Jing Xiao   

 Speech Activity Detection and Modeling  
  GL-SSD: Global and Local Speech Style Disentanglement by vector quantization for robust sentence boundary detection in speech stream  
  Kuncai Zhang, Wei Zhou, Pengcheng Zhu, Haiqing Chen   
  Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction  
  Mohan Shi, Yuchun Shu, Lingyun Zuo, Qian Chen, Shiliang Zhang, Jie Zhang, Li-Rong Dai   
  Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions  
  Prithvi R.R. Gudepu, Jayesh M. Koroth, Kamini Sabu, Mahaboob Ali Basha Shaik   
  Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets  
  Denise Moussa, Germans Hirsch, Sebastian Wankerl, Christian Riess   
  Real-Time Causal Spectro-Temporal Voice Activity Detection Based on Convolutional Encoding and Residual Decoding  
  Jingyuan Wang, Jie Zhang, Li-Rong Dai   
  SVVAD: Personal Voice Activity Detection for Speaker Verification  
  Zuheng Kang, Jianzong Wang, Junqing Peng, Jing Xiao   

 Multilingual Models for ASR  
  Learning Cross-lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition  
  Muhammad Umar Farooq, Thomas Hain   
  AfriNames: Most ASR Models "Butcher" African Names  
  Tobi Olatunji, Tejumade Afonja, Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Chris Chinenye Emezue, Amina Mardiyyah Rufai, Sahib Singh   
  Towards Dialect-inclusive Recognition in a Low-resource Language: Are Balanced Corpora the Answer?  
  Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, Ailbhe Ní Chasaide   
  Svarah: Evaluating English ASR Systems on Indian Accents  
  Tahir Javed, Sakshi Joshi, Vignesh Nagarajan, Sai Sundaresan, Janki Nawale, Abhigyan Raman, Kaushal Bhogale, Pratyush Kumar, Mitesh M. Khapra   
  N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition  
  Bashar Talafha, Abdul Waheed, Muhammad Abdul-Mageed   
  The MALACH Corpus: Results with End-to-End Architectures and Pretraining  
  Michael Picheny, Qin Yang, Daiheng Zhang, Lining Zhang   

 Speech Enhancement and Bandwidth Expansion  
  Unsupervised speech enhancement with deep dynamical generative speech and noise models  
  Xiaoyu Lin, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda   
  Noise-Robust Bandwidth Expansion for 8K Speech Recordings  
  Yin-Tse Lin, Bo-Hao Su, Chi-Han Lin, Shih-Chan Kuo, Jyh-Shing Roger Jang, Chi-Chun Lee   
  mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra  
  Chenhao Shuai, Chaohua Shi, Lu Gan, Hongqing Liu   
  Zoneformer: On-device Neural Beamformer For In-car Multi-zone Speech Separation, Enhancement and Echo Cancellation  
  Yong Xu, Vinay Kothapally, Meng Yu, Shixiong Zhang, Dong Yu   
  Low-complexity Broadband Beampattern Synthesis using Array Response Control  
  Jiayi Xu, Jian Li, Weixin Meng, Xiaodong Li, Chengshi Zheng   
  A GAN Speech Inpainting Model for Audio Editing Software  
  Haixin Zhao   

 Articulation  
  Deep Speech Synthesis from MRI-Based Articulatory Representations  
  Peter Wu, Tingle Li, Yijing Lu, Yubin Zhang, Jiachen Lian, Alan W Black, Louis Goldstein, Shinji Watanabe, Gopala K. Anumanchipalli   
  Learning to Compute the Articulatory Representations of Speech with the MIRRORNET  
  Yashish M Siriwardena, Carol Espy-Wilson, Shihab Shamma   
  Generating high-resolution 3D real-time MRI of the vocal tract  
  Martin Strauch, Antoine Serrurier   
  Exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion  
  Jesuraj Bandekar, Sathvik Udupa, Prasanta Kumar Ghosh   

 Neural Processing of Speech and Language: Encoding and Decoding the Diverse Auditory Brain  
  MEG Encoding using Word Context Semantics in Listening Stories  
  Subba Reddy Oota, Nathan Trouvain, Frederic Alexandre, Xavier Hinaut   
  Investigating the cortical tracking of speech and music with sung speech  
  Giorgia Cantisani, Amirhossein Chalehchaleh, Giovanni Di Liberto, Shihab Shamma   
  Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment  
  Oskar Keding, Emina Alickovic, Martin A. Skoglund, Maria Sandsten   
  Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?  
  Subba Reddy Oota, Veeral Agarwal, Mounika Marreddy, Manish Gupta, Raju Bapi   
  Exploring Auditory Attention Decoding using Speaker Features  
  Zelin Qiu, Jianjun Gu, Dingding Yao, Junfeng Li   
  Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries  
  Akshara Soman, Vidhi Sinha, Sriram Ganapathy   
  Similar Hierarchical Representation of Speech and Other Complex Sounds In the Brain and Deep Residual Networks: An MEG Study  
  Tzu-Han Zoe Cheng, Kuan-Lin Chen, Juliane Schubert, Ya-Ping Chen, Tim Brown, John Iversen   
  Effects of spectral degradation on the cortical tracking of the speech envelope  
  Alexis Deighton MacIntyre, Tobias Goehring   
  Effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals  
  Ignacio Calderon De Palma, Laura S. Lopez, Alejandro Lopez Valdes   

 Perception of Paralinguistics  
  Transfer Learning for Personality Perception via Speech Emotion Recognition  
  Yuanchao Li, Peter Bell, Catherine Lai   
  A stimulus-organism-response model of willingness to buy from advertising speech using voice quality  
  Mizuki Nagano, Yusuke Ijima, Sadao Hiroya   
  Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition  
  David Doukhan, Simon Devauchelle, Lucile Girard-Monneron, Mía Chávez Ruz, V. Chaddouk, Isabelle Wagner, Albert Rilliard   
  Influence of Personal Traits on Impressions of One's Own Voice  
  Hikaru Yanagida, Yusuke Ijima, Naohiro Tawara   
  Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence  
  Ambika Kirkland, Joakim Gustafson, Éva Székely   
  Cross-linguistic Emotion Perception in Human and TTS Voices  
  Iona Gessinger, Michelle Cohn, Benjamin R. Cowan, Georgia Zellou, Bernd Möbius   

 Technologies for Child Speech Processing  
  Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech  
  Richeng Duan   
  Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics  
  Bo Molenaar, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik   
  An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read  
  Yu Bai, Ferdy Hubers, Catia Cucchiarini, Roeland van Hout, Helmer Strik   
  Adaptation of Whisper models to child speech recognition  
  Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, Horia Cucu   

 Show and Tell: Media and commercial applications  
  Let's Give a Voice to Conversational Agents in Virtual Reality  
  Michele Yin, Gabriel Roccabruna, Abhinav Azad, Giuseppe Riccardi   
  FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator  
  Massa Baali, Ahmed M. Ali   
  Video Summarization Leveraging Multimodal Information for Presentations  
  Hanchao Liu, Dapeng Chen, Rongjun Li, Wenyuan Xue, Wei Peng   
  What questions are my customers asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls  
  Varun Nathan, Devashish Deshpande, Ayush Kumar, Cijo George, Jithendra Vepa   
  COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery  
  Rishabh Tripathi, Digvijay Anil Ingle, Ayush Kumar, Cijo George, Jithendra Vepa   
  NeMo Forced Aligner and its application to word alignment for subtitle generation  
  Elena Rastorgueva, Vitaly Lavrukhin, Boris Ginsburg   
  CauSE: Causal Search Engine for Understanding Contact-Center Conversations  
  Anup Pattnaik, Tanay Narshana, Aashraya Sachdeva, Cijo George, Jithendra Vepa   
  Tailored Real-Time Call Summarization System for Contact Centers  
  Aashraya Sachdeva, Sai Nishanth Padala, Anup Pattnaik, Varun Nathan, Cijo George, Ayush Kumar, Jithendra Vepa   
  Federated Learning Toolkit with Voice-based User Verification Demo  
  Prathamesh Mandke, Rachel Oberst, Matthias Reisser, Avĳit Chakraborty, Christos Louizos, Joseph Soriaga, Daniel Madrigal, Andre Manoel, Nalin Singal, Jeff Omhover, Robert Sim   
  Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models  
  Liam Dugan, Anshul Wadhawan, Kyle Spence, Chris Callison-Burch, Morgan McGuire, Victor Zordan   
  Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser  
  Namhyun Cho, Sunmin Kim, Yoseb Kang, Heeman Kim   
  Cross-lingual/Cross-channel Intent Detection in Contact-Center Conversations  
  Suraj Agrawal, Aashraya Sachdeva, Soumya Jain, Cijo George, Jithendra Vepa   

 Speaker and Language Identification 3  
  One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification  
  Jungwoo Heo, Chan-yeong Lim, Ju-ho Kim, Hyun-seo Shin, Ha-Jin Yu   
  Defense Against Adversarial Attacks on Audio DeepFake Detection  
  Piotr Kawa, Marcin Plata, Piotr Syga   
  A conformer-based classifier for variable-length utterance processing in anti-spoofing  
  Eros Rosello, Alejandro Gomez-Alanis, Angel M. Gomez, Antonio Peinado   
  Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification  
  Feng Wang, Lingyan Huang, Tao Li, Qingyang Hong, Lin Li   
  CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice  
  Juan Zuluaga-Gomez, Sara Ahmed, Danielius Visockas, Cem Subakan   
  From adaptive score normalization to adaptive data normalization for speaker verification systems  
  Sandro Cumani, Salvatore Sarni   
  CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking  
  Hui Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, Qian Chen   
  North Sámi Dialect Identification with Self-supervised Speech Models  
  Sofoklis Kakouros, Katri Hiovain-Asikainen   
  Encoder-decoder Multimodal Speaker Change Detection  
  Jee-weon Jung, Soonshin Seo, Hee-Soo Heo, Geonmin Kim, You Jin Kim, Young-ki Kwon, Minjae Lee, Bong-Jin Lee   
  Disentangled Representation Learning for Multilingual Speaker Recognition  
  Kihyun Nam, Youkyum Kim, Jaesung Huh, Hee-Soo Heo, Jee-weon Jung, Joon Son Chung   
  A Compact End-to-End Model with Local and Global Context for Spoken Language Identification  
  Fei Jia, Nithin Rao Koluguri, Jagadeesh Balam, Boris Ginsburg   
  On the Robustness of Arabic Speech Dialect Identification  
  Peter Sullivan, AbdelRahim Elmadany, Muhammad Abdul-Mageed   
  Adaptive Neural Network Quantization For Lightweight Speaker Verification  
  Haoyu Wang, Bei Liu, Yifei Wu, Yanmin Qian   
  Adversarial Diffusion Probability Model For Cross-domain Speaker Verification Integrating Contrastive Loss  
  Xinmei Su, Xiang Xie, Fengrun Zhang, Chenguang Hu   
  Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model  
  Aoi Ito, Shota Horiguchi   
  Label Aware Speech Representation Learning For Language Identification  
  Shikhar Vashishth, Shikhar Bharadwaj, Sriram Ganapathy, Ankur Bapna, Min Ma, Wei Han, Vera Axelrod, Partha Talukdar   
  Exploring the Impact of Back-End Network on Wav2vec 2.0 for Dialect Identification  
  Qibao Luo, Ruohua Zhou   
  Improving Speaker Verification with Self-Pretrained Transformer Models  
  Junyi Peng, Oldřich Plchot, Themos Stafylakis, Ladislav Mosner, Lukáš Burget, Jan "Honza" Černocký   
  Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches  
  Vinicius Ribeiro, Yiteng Huang, Yuan Shangguan, Zhaojun Yang, Li Wan, Ming Sun   

 Analysis of Speech and Audio Signals 4  
  What do self-supervised speech representations encode? An analysis of languages, varieties, speaking styles and speakers  
  Julian Linke, Mate Kadar, Gergely Dosinszky, Peter Mihajlik, Gernot Kubin, Barbara Schuppler   
  A Compressed Synthetic Speech Detection Method with Compression Feature Embedding  
  Jinghong Zhang, Xiaowei Yi, Xianfeng Zhao   
  Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning  
  Yucong Zhang, Suo Hongbin, Yulong Wan, Ming Li   
  MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment  
  Zitong Li, Wei Li   
  A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation  
  Xipin Wei, Junhui Chen, Zirui Zheng, Li Guo, Lantian Li, Dong Wang   
  MTANet: Multi-band Time-frequency Attention Network for Singing Melody Extraction from Polyphonic Music  
  Yuan Gao, Ying Hu, Liusong Wang, Hao Huang, Liang He   
  Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network  
  Wang Chunhui, Chang Zeng, Xing He   
  Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?  
  Mohammad Shaique Solanki, Ashutosh Bharadwaj, Jeevan Kylash, Prasanta Kumar Ghosh   
  Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation Using Improved Differentiable Automatic Data Augmentation  
  Toki Sugiura, Hiromitsu Nishizaki   
  A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis  
  Li Xiao, Xiuping Yang, Xinhong Li, Weiping Tu, Xiong Chen, Weiyan Yi, Jie Lin, Yuhong Yang, Yanzhen Ren   
  RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music  
  Haojie Wei, Xueke Cao, Tangpeng Dan, Yueguo Chen   
  Spatialization Quality Metric for Binaural Speech  
  Pranay Manocha, Israel Dejene Gebru, Anurag Kumar, Dejan Markovic, Alexander Richard   
  AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds  
  Arka Roy, Udit Satija   
  Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification  
  Sangmin Bae, June-Woo Kim, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim, Se-Young Yun   
  Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance  
  Vanessa Richter, Michael Neumann, Jordan Green, Brian Richburg, Oliver Roesler, Hardik Kothare, Vikram Ramanarayanan   
  Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation  
  Guy Yariv, Itai Gat, Lior Wolf, Yossi Adi, Idan Schwartz   
  Obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach  
  Hector E. Romero, Ning Ma, Guy J. Brown, Sam Johnson   
  Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning  
  Yifu Sun, Xulong Zhang, Jianzong Wang, Ning Cheng, Kaiyu Hu, Jing Xiao   

 Speech Synthesis: Multilinguality; Evaluation  
  The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech  
  Phat Do, Matt Coler, Jelske Dĳkstra, Esther Klabbers   
  Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages  
  Phat Do, Matt Coler, Jelske Dĳkstra, Esther Klabbers   
  Robust Feature Decoupling in Voice Conversion by Using Locality-Based Instance Normalization  
  Yewei Gu, Xianfeng Zhao, Xiaowei Yi   
  Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network  
  Dongya Jia, Qiao Tian, Kainan Peng, Jiaxin Li, Yuanzhe Chen, Mingbo Ma, Yuping Wang, Yuxuan Wang   
  Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis  
  Erik Ekstedt, Siyang Wang, Éva Székely, Joakim Gustafson, Gabriel Skantze   
  GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech  
  Yahuan Cong, Haoyu Zhang, Haopeng Lin, Shichao Liu, Chunfeng Wang, Yi Ren, Xiang Yin, Zejun Ma   
  Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities  
  Yusuke Yasuda, Tomoki Toda   
  LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus  
  Yuma Koizumi, Heiga Zen, Shigeki Karita, Yifan Ding, Kohei Yatabe, Nobuyuki Morioka, Michiel Bacchiani, Yu Zhang, Wei Han, Ankur Bapna   
  UniFLG: Unified Facial Landmark Generator from Text or Speech  
  Kentaro Mitsui, Yukiya Hono, Kei Sawada   
  XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech  
  Linh The Nguyen, Thinh Pham, Dat Quoc Nguyen   
  ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus  
  Ajinkya Kulkarni, Atharva Kulkarni, Sara Abedalmon'em Mohammad Shatnawi, Hanan Aldarmaki   
  Diffusion-based accent modelling in speech synthesis  
  Kamil Deja, Georgi Tinchev, Marta Czarnowska, Marius Cotescu, Jasha Droppo   
  Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration  
  Rustem Yeshpanov, Saida Mussakhojayeva, Yerbolat Khassanov   
  CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation  
  Siheng Zhang, Xingjun Tan, Yanqiang Lei, Xianxiang Wang, Zhizhong Zhang, Yuan Xie   
  Improving Bilingual TTS Using Language And Phonology Embedding With Embedding Strength Modulator  
  Fengyu Yang, Jian Luan, Meng Meng, Yujun Wang   
  High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units  
  Junchen Lu, Berrak Sisman, Mingyang Zhang, Haizhou Li   
  PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text  
  Yang Yu, Matthew Perez, Ankur Bapna, Fadi Haik, Siamak Tazari, Yu Zhang   
  Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer  
  Jakub Swiatkowski, Duo Wang, Mikolaj Babianski, Giuseppe Coccia, Patrick Lumban Tobing, Ravichander Vipperla, Viacheslav Klimkov, Vincent Pollet   
  Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously  
  Cheng-Han Chiang, Wei-Ping Huang, Hung-yi Lee   
  Speaker-independent neural formant synthesis  
  Pablo Pérez Zarazaga, Zofia Malisz, Gustav Eje Henter, Lauri Juvela   
  CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center  
  Yuki Saito, Eiji Iimori, Shinnosuke Takamichi, Kentaro Tachibana, Hiroshi Saruwatari   
  SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion  
  Orian Sharoni, Roee Shenberg, Erica Cooper   

 Search papers       
  
 Article |  

 ×  Keynote 1 ISCA Medallist   
 Speech Synthesis: Prosody and Emotion   
 Statistical Machine Translation   
 Self-Supervised Learning in ASR   
 Prosody   
 Speech Production   
 Dysarthric Speech Assessment   
 Speech Coding: Transmission and Enhancement   
 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 1   
 Analysis of Speech and Audio Signals 1   
 Speech Recognition: Architecture, Search, and Linguistic Components 1   
 Speech Recognition: Technologies and Systems for New Applications 1   
 Lexical and Language Modeling for ASR   
 Language Identification and Diarization   
 Speech Quality Assessment   
 Feature Modeling for ASR   
 Interfacing Speech Technology and Phonetics   
 Speech Synthesis: Multilinguality   
 Speech Emotion Recognition 1   
 Show and Tell: Health applications and emotion recognition   
 Spoken Dialog Systems and Conversational Analysis 1   
 Speech Coding and Enhancement 1   
 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 2   
 Speech Recognition: Technologies and Systems for New Applications 2   
 Keynote 2   
 Paralinguistics 1   
 Speech Enhancement and Denoising   
 Speech Synthesis: Evaluation   
 End-to-end Spoken Dialog Systems   
 Biosignal-enabled Spoken Communication   
 Neural-based Speech and Acoustic Analysis   
 DiGo - Dialog for Good: Speech and Language Technology for Social Good   
 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 3   
 Speech Recognition: Architecture, Search, and Linguistic Components 2   
 Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1   
 Speech, Voice, and Hearing Disorders 1   
 Speech Recognition: Technologies and Systems for New Applications 3   
 Spoken Term Detection and Voice Search   
 Models for Streaming ASR   
 Source Separation   
 Speech and Language in Health: From Remote Monitoring to Medical Conversations 1   
 Speech Perception   
 Phonetics and Phonology: Languages and Varieties   
 Paralinguistics 2   
 Speaker and Language Identification 1   
 Show and Tell: Speech tools, speech enhancement, speech synthesis   
 Speech Synthesis and Voice Conversion   
 Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2   
 Novel Transformer Models for ASR   
 Speaker Recognition 1   
 Cross-lingual and Multilingual ASR   
 Voice Conversion   
 Speech and Language in Health: From Remote Monitoring to Medical Conversations 2   
 Pathological Speech Analysis 1   
 Multimodal Speech Emotion Recognition   
 Speech Coding and Enhancement 2   
 Phonetics, Phonology, and Prosody 1   
 Spoken Dialog Systems and Conversational Analysis 2   
 Analysis of Speech and Audio Signals 2   
 Speech Coding: Privacy   
 Analysis of Neural Speech Representations   
 End-to-end ASR   
 Spoken Language Understanding, Summarization, and Information Retrieval   
 Invariant and Robust Pre-trained Acoustic Models   
 Pathological Speech Analysis 2   
 Speech Synthesis: Representation Learning   
 Speech Perception, Production, and Acquisition 1   
 Speaker and Language Identification 2   
 Speech Recognition: Architecture, Search, and Linguistic Components 3   
 Acoustic Model Adaptation for ASR   
 Speech Synthesis: Expressivity   
 Multi-modal Systems   
 Question Answering from Speech   
 Multi-talker Methods in Speech Processing   
 Sociophonetics   
 Speaker and Language Diarization   
 Speech Emotion Recognition 2   
 Show and Tell: Language learning and educational resources   
 Analysis of Speech and Audio Signals 3   
 Speech Coding and Enhancement 3   
 Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 3   
 Anti-Spoofing for Speaker Verification   
 Speech Coding: Intelligibility   
 Resources for Spoken Language Processing   
 New Computational Strategies for ASR Training and Inference   
 MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech   
 Health-Related Speech Analysis   
 Automatic Audio Classification and Audio Captioning   
 Speech Perception, Production, and Acquisition 2   
 Speech Synthesis   
 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 4   
 Keynote 3   
 Speech Synthesis: Controllability and Adaptation   
 Search Methods and Decoding Algorithms for ASR   
 Speech Signal Analysis   
 Speech Emotion Recognition 3   
 Connecting Speech-science and Speech-technology for Children's Speech   
 Dialog Management   
 Speaker Recognition 2   
 Phonetics, Phonology, and Prosody 2   
 Speech Synthesis: Expressivity   
 Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 5   
 Speech, Voice, and Hearing Disorders 2   
 Speech Activity Detection and Modeling   
 Multilingual Models for ASR   
 Speech Enhancement and Bandwidth Expansion   
 Articulation   
 Neural Processing of Speech and Language: Encoding and Decoding the Diverse Auditory Brain   
 Perception of Paralinguistics   
 Technologies for Child Speech Processing   
 Show and Tell: Media and commercial applications   
 Speaker and Language Identification 3   
 Analysis of Speech and Audio Signals 4   
 Speech Synthesis: Multilinguality; Evaluation

78. ISC_3 conference:
Skip to content      
   
  Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 
   
     Menu    
 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 

 Newsletter   

  Contact   

  Our Story   

  Submissions   

 CONNECTING THE DOTS  

 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 
   
     Menu    
 Submissions | Research Paper 
  Research Poster 
  Project Poster 
  Women in HPC Poster 
  Birds of a Feather 
  Workshop 
  Tutorial 
  Awards | Jack Dongarra Early Career Award 
  Hans Meuer Award 
  Research Poster Award 
  Program | Invited program 
  Contributed program 
  Vendor Program 
  Student Cluster Competition 
  Student Volunteer Program 
  Exhibition | To Exhibit 
  To Visit 
  Attendance | Travel & Visa 
  Accommodation 
  Hamburg 
  News & Media 
  Our Story 
  Newsletter 
  Contact 
  Our Story 
  Submissions 

 SHAPE THE  
  DISCUSSION!  

 SUBMISSIONS  

 Conference & exhibition   
  JUNE 10-12  
  Tutorials & Workshops   
  JUNE 13  

 SUBMISSIONS  

 ISC 2025 provides a dynamic platform for scientists and engineers to showcase their work and engage with a community of over 3,500 HPC enthusiasts. Join us as a contributor!   

 FOR THE COMMUNITY BY THE COMMUNITY   
   
 You have the chance to influence the ISC 2025 research paper presentations, tutorials, workshops, and more. As a contributor, you can guide the discussions in a way that helps to transform high performance computing and also gain recognition for your research efforts.  

 BECOME A REVIEWER!   
 ISC invites experienced reviewers to express interest in evaluating submissions for our contributed program. Your expertise will be invaluable in guiding the program’s direction.  
 Join our review team by applying here!    

      RESEARCH PAPER      

      POSTER      

      BIRDS OF A FEATHER      

      TUTORIAL      

      WORKSHOP      

 DEADLINES  

 CONTRIBUTED PROGRAM  

 Submission Deadlines | Notification of Acceptance 
 Workshops with proceedings | closed | December 6, 2024 
 Research Papers | December 10, 2024 | February 28, 2025 
 Regular Workshops | December 11, 2024 | January 28, 2025 
 Tutorials | December 12, 2024 | February 14, 2025 
 Research Posters | January 16, 2025 | February 26, 2025 
 Project Posters | January 16, 2025 | February 26, 2025 
 Birds of a Feather | January 22, 2025 | February 24, 2025 

 RESEARCH PAPER SUBMISSION  

 RESEARCH PAPER CHAIR  
 Amanda Randles, Duke University, USA  

 Proceedings CHAIR  
 Tobias Weinzierl, Durham University, UK  

 RESEARCH PAPER Deputy CHAIR  
 Hatem Ltaief, KAUST, Saudi Arabia  

 Proceedings Deputy CHAIR  
 Erin Carson, Charles University, Prague  

 The research papers sessions provide great opportunities for engineers and scientists from academia, industry, and government to come together and exchange ideas on significant topics, trends, and discoveries that shape the future of high performance computing, machine learning, data analytics, and quantum computing.  

      READ MORE      

 Please note:  
  the Call for Research Paper is closed!  

  Dec 10, 2024 11:59 pm AoE   
 Full Submission Deadline  

 FEB 28, 2025   
 Notification of Acceptance  

 JUN 10-12, 2025   
 Research Paper Sessions  

 All Dates   
        Feb 8-12, 2025   
 Author Rebuttals  

 MAY 19, 2025   
 Short pre-recorded Video due  

 MAR 31, 2025   
 Camera-Ready Submission  

 Poster SUBMISSION  

 The ISC poster sessions provide students, early-career researchers, and established researchers from around the world with the opportunity to present their latest findings, progress, and breakthroughs to an audience engaged in HPC and applied domains, including AI and quantum computing.  
  The ISC Research Poster Session, ISC Project Poster Session, and the Women in HPC Poster Session are all inviting proposals!  

 RESEARCH POSTER  
 The ISC Research Poster program welcomes poster proposals from scientists in industry and academia, including students, to share their latest research findings.  

      READ MORE      

 PROJECT POSTER  
 Submitting a project poster lets you showcase fundamental ideas, methodology, and initial work. Researchers with upcoming or recently funded projects and those with fresh project ideas are encouraged to submit their work.  

      READ MORE      

 WOMAN IN HPC POSTER  
 The WHPC poster session at ISC spotlights women and underrepresented individuals in early stages of their HPC careers.  

      READ MORE      

 BIRDS OF A FEATHER  

 BOF CHAIR  
 David Boehme, Lawrence Livermore National Laboratory, USA  

 BoF Deputy Chair  
 Carsten Trinitis, TUM, Germany  

 Birds of a Feather (BoF) sessions offer an informal platform for dynamic discussions among peers, experts, leaders, and anyone from the HPC community with shared interests. Each 60-minute BoF session addresses current HPC topics and is guided by one or more experts in their field.  

      READ MORE      

 Jan 22, 2025 11:59 pm AoE   
 Full Submission Deadline  

 FEB 24, 2025   
 Notification of Acceptance  

 JUN 10-12, 2025   
 BoF Sessions  

 All Dates   
        MAY 16, 2025   
 Submission of Final List of Speakers Deadline  

 TUTORIAL SUBMISSION  

 TUTORIAL CHAIR  
 Diana Moise, HPE, Switzerland  

 Tutorial Deputy Chair  
 Hartwig Anzt, TUM, Germany  

 The ISC tutorials are interactive courses and collaborative learning experiences focusing on key topics of high performance computing, machine learning, data analytics and quantum computing.  
 Renowned experts in their respective fields will give attendees a comprehensive introduction to the topic as well as providing a closer look at specific problems.  
 Tutorials are encouraged to include a “hands-on” component to allow attendees to practice prepared materials.  

      READ MORE      

 DEC 12, 2024 11:59 pm AoE   
 Full Submission Deadline  

 FEB 14, 2025   
 Notification of Acceptance  

 FRIDAY (new), JUNE 13, 2025   
 Half-day: 9:00 am - 1:00 pm, 2:00 pm - 6:00 pm  
  Full-day: 9:00 am - 6:00 pm  

 All Dates   
        May 28, 2025   
 Working Materials for Tutorial Attendees due  

 Workshop SUBMISSION  

 Workshop Chair  
 Sarah Neuwirth, Johannes Gutenberg University Mainz, Germany  

 Proceedings CHAIR  
 Tobias Weinzierl, Durham University, UK  

 Workshop Deputy Chair  
 Arnab K. Paul, Birla Institute of Technology and Science, Pilani, India  

 Proceedings Deputy CHAIR  
 Erin Carson, Charles University, Prague  

 The ISC workshops aim to offer attendees a concentrated and comprehensive forum for presentations, conversations, and engagement on a specific topic.  

      READ MORE      

 DEC 11, 2024 11:59 PM AoE   
 Submission Deadline Regular Workshops  

 CLOSED   
 Submission Deadline Workshops with Proceedings  

 FRIDAY (new), JUNE 13, 2025   
 Half-day: 9:00 am - 1:00 pm, 2:00 pm - 6:00 pm  
  Full-day: 9:00 am - 6:00 pm  

        ALL DATES      

 Awards  

 Awards  

 ISC High Performance bestows awards annually to recognize research excellence by individuals and research groups from the global HPC communities.  

 JACK DONGARRA EARLY CAREER AWARD  

 The ISC High Performance Jack Dongarra Early Career Award and Lecture Series  
  is an annual event that honors the remarkable contributions of Professor  
  Jack Dongarra to the field of high performance computing and the community.  
 The award includes a certificate of recognition and a cash prize of 5,000 Euros.   

      Find out more      

 HANS MEUER AWARD  

 The award honors the best research paper submitted to the conference’s Research Papers Committee. This award has been introduced in the memory of the late Dr. Hans Meuer, general chair of the ISC conference from 1986 through 2014, and co-founder of the TOP500 project.  
 The Hans Meuer Award includes a cash prize of 3,000 Euros and an award certificate.   

 RESEARCH POSTER AWARD  

 The research poster award recognizes three outstanding research posters as selected by members of the posters committee.  
 The award includes a cash prize of 500 Euros (first place), 300 Euros (second place) and 200 Euros (third place)   

        AWARD WINNER ISC 2024      

 Contact  

 MS. ISABEL GRÄBNER   
 Conference Program Manager  

      send request      

 Contact  

 MS. ISABEL GRÄBNER​   
 Conference Program Manager  

      send request      

 MS. TANJA GRÜNTER   
 Conference Program Manager  

      send request      

 PLATINUM SPONSORS  

 GOLD SPONSORS  

 SITEMAP 
  CONTACT 
  IMPRINT 
  PRIVACY 
   
 SITEMAP 
  CONTACT 
  IMPRINT 
  PRIVACY 

 Linkedin      Flickr      Youtube

79. CAINE_0 conference:
Skip to content    

 ISCA CAINE   
 International Conference on Computer Applications in Industry and Engineering  

   Menu   CAINE 2023 
  Registration 
  Final Paper Submission 
  Keynote 
  Program 
  CFP 
  Submission 
  Organizers 
    
 Search for:        

   Search   Search for:        

 CAINE 2023  
  
 36th International Conference on   
  Computer Applications in Industry and Engineering   
 October 16 – 18, 2023   
 Sponsored by the International Society for Computers and Their Applications (ISCA)  
  In conjunction with  
  32nd International Conference on Software Engineering and Data Engineering (SEDE 2023)  
  
 Proceedings will be published in EasyChair EPiC Series in Computing, that is open access and included in some major citation indexes such as Crossref, Scopus, Microsoft Academic, and Google Scholar.  
 Note: The conference will be held remotely with online presentations and the conference proceedings will be published as planned.  
 The 36th International Conference on Computer Applications in Industry and Engineering (CAINE 2023) provides an international forum for presentation and discussion of research on computer applications in industry and engineering. The conference also includes a Best Paper Award. CAINE 2023 will feature contributed papers as well as a keynote speech. Papers will be accepted into oral presentation sessions.  

 Search   Search    
   
 Posts  

 Archive  
 No archives to show.   

 Copyright. All rights reserved.   
 Proudly powered by WordPress  |  Education Hub by WEN Themes

80. CAINE_1 conference:
Skip to content    

 ISCA CAINE    

 Menu   CAINE 2024 
  CFP 
  Special Sessions 
  Submission 
  Registration 
  Final Paper Submission 
  Keynote 
  Program 
  Organizers 
  Past conferences 

 CAINE 2024  
 ISCA CAINE  >> CAINE 2024   

 37th International Conference on   
  Computer Applications in Industry and Engineering   
 October 21 – 22, 2024   
 Hilton San Diego Airport/Harbor Island, San Diego, California, USA    
  
 Sponsored by the International Society for Computers and Their Applications (ISCA)  
  In conjunction with  
  33rd International Conference on Software Engineering and Data Engineering (SEDE 2024)  
  
 Proceedings will be published by Springer Nature   
 The 37th International Conference on Computer Applications in Industry and Engineering (CAINE 2024) provides an international forum for presentation and discussion of research on computer applications in industry and engineering. The conference also includes a Best Paper Award. CAINE 2024 will feature contributed papers as well as a keynote speech. Papers will be accepted into oral presentation sessions.  

 Conference WordPress Theme  By WP Elemento  
   
 Proudly powered by WordPress

81. SOFA_0 conference:
IEEE Sofa-Org 
  About us 
  Contact-Us 

 SOFA Conferences   
 International Workshop On Soft Computing  

     IEEE Sofa-Org 
  About us 
  Contact-Us 

  International Workshop on Soft Computing Applications (SOFA Conferences)  
 International Workshop  

 SOFA Conferences  
 Soft computing (SC) is a collection of methodologies that are trying to cope with the main disadvantage of the conventional (hard) computing: the poor performances when working in uncertain conditions. The fundamental idea of soft computing is to emulate the human like reasoning. The classic constituents of SC are fuzzy logic, neural network theory and probabilistic reasoning, but new methods are continuously emerging: belief networks, genetic algorithms, anytime algorithms, chaos theory, some parts of learning theory, etc. Due to the large variety and complexity of the domain, the constituting methods of SC are not competing for a comprehensive ultimate solution. Instead they are complementing each other, for dedicated solutions adapted to each specific problem. Hundreds of concrete applications are already available in control, decision making, pattern recognition and robotics. The SC systems are tolerant to imprecision, uncertainty, and partial truth. Their main advantages are tractability, robustness, and low cost implementations. At the same time SC is a major developing vector of the Artificial Intelligence.  

 PREVIOUS & UPCOMING SOFA WORKSHOPS  

 Former organizer of  previous conferences Trivent (Hungary)   

 Sofa 2022 
 Sofa 2020 
 Sofa 2018 
 Sofa 2016 
 Sofa 2014 
 Sofa 2012 
 Sofa 2010 
 Sofa 2009 
 Sofa 2007 
 Sofa 2005 

 © Copyright 2020 SOFA Conferences. All Rights Reserved.

82. CAINE_2 conference:
ISCA 2023    
 Submit Work | Call for Papers 
  Submission Guidelines 
  HotCRP Submission Site 
  Industry Track 
  Camera-Ready Guidelines 
  Call for Workshops & Tutorials 
  Artifact Evaluation 
  Program | Main Program 
  Workshops & Tutorials 
  Attend | Venue & Hotel 
  Register for the Conference 
  Visa Information 
  Student Travel Grants 
  Excursion 
  Childcare 
  Committees | Organizing Committee 
  Program Committee 
  Steering Committee 
  Code of Conduct 
  CARES 
  @ISCAConfOrg 
  iscaconf.org 
  Home 

  ISCA 2023  
  The 49 th  International Symposium on Computer Architecture  
 June 17–21, 2023  
 Orlando, FL, USA  

 Important Dates  
   
 Abstract Deadline: | November 14, 2022 at 11:59 PM AoE 
  Full Paper Deadline: | November 21, 2022 at 11:59 PM AoE 
  Industry Track Abstract Deadline: | January 6, 2023 at 11:59 PM AoE 
  Industry Track Full Paper Deadline: | January 13, 2023 at 11:59 PM AoE 
  Workshop/Tutorial Proposal Submission Deadline (Extended): | February 3, 2023 
  Workshop/Tutorial Notification (Extended): | February 17, 2023 
  Rebuttal/Revision period: | February 6 - 20, 2023 
  Industry Track Rebuttal/Revision period: | February 23 - 27, 2023 
  Author Notification: | March 9, 2023 

 The International Symposium on Computer Architecture  (ISCA) is the premier forum for new ideas and experimental results in computer architecture. The conference specifically seeks particularly forward-looking and novel submissions. In 2023, the 50 th  edition of ISCA will be held in Orlando at the Marriott World Center Orlando during June 17 – 21, 2023. ISCA 2023 is an FCRC  event, held alongside with 13 other conferences to be held from June 17 – 23, 2023.  
  Submission guidelines: Regular Track  • Industry Track   
  View the Calls for Papers: Regular Track  • Industry Track   
  Upload Abstracts and Papers to HotCRP: Regular Track  • Industry Track   
  View the Call for Workshops & Tutorials   

  Announcements  
 ISCA'23 Proceeding is now available!  
 June 17, 2023   
   
 You can view ISCA'23 Proceeding here  .  
   
 Lightning Talks on Youtube!  
 June 16, 2023   
   
 Three-minute lightning talks of the accepted papers can be watched on ACM SIGARCH  YouTube channel.  
   
 PLDI / ISCA Opening Reception!  
 June 15, 2023   
   
 Opening Reception will be held on June 18, 6:00 PM - 8:00 PM at Royal room/hall.  
   
 Student Travel Grants are available!  
 April 26, 2023   
   
 Visit student travel grants  page for detailed information and important dates.  
   
 Workshop/Tutorial program is available!  
 March 30, 2023   
   
 The workshops and tutorials  page shows the schedule and links to organizers' pages (main conference program coming soon).  
   
 Free childcare program is available.  
 March 27, 2023   
   
 FCRC has arranged a free and on-site childcare program. Please follow the FCRC childcare page  for more details and registration.  
   
 ISCA-2023 Registration is open!  
 March 24, 2023   
   
 Registration for ISCA 2023 is now open! View the registration  page for pricing and a link to register via Cvent. See the visa information  page for instructions on visa support letters. See you in Orlando, FL!  
   
 Artifact Submission is now open  
 March 14, 2023   
   
 HotCRP link for submitting artifcats for evaluation is now open. Artifact Evaluation  .  
   
 Travel grants are available for Undergrad Architecture Mentoring (uArch) Workshop!  
 March 14, 2023   
   
 Please contact the workshop  organizers for more details.  

 Tweets   

  Platinum Sponsors  

  National Science Foundation    

  Gold Sponsors  

  Alphabet    
   
  Futurewei    

  Arm    
   
  UCF    

  Ant Research    
   
  Qualcomm    

  Silver Sponsors  

  IBM    
   
  VMware    

  KSC/Astronaut Sponsors  

 Astronauts brought to you by  

  Futurewei    

  Microsoft    

  © International Symposium on Computer Architecture   
 IEEE Privacy Policy

83. SOFA_1 conference:
選擇語言   
 English 
  繁體中文 
  简体中文 

 :::相關網站  
 回首頁 
  售票網 
  企業採購 
  福利平台 
  海外專館 
    
 :::會員服務|快速功能   
 會員登入 | 黃金會員 
   前往會員專區 
  我的電子書櫃 
  訂單查詢 
  瀏覽記錄 
  下次再買 
  可訂購時通知 
  本月獨享 
  可用E-Coupon 0  張 
  可用單品折價券 0  張 
  可用購物金 0  元 
  可用 OPENPOINT 0  點 
  登出 
  訂單查詢 
  購物車( 0  ) 
  電子書櫃 
  繁體 

     展開廣告       
 關閉廣告         

 :::網站搜尋   
  全部      
          
 全部 
  圖書 
  電子書 
  有聲書 
  訂閱 
  影音 
  美妝 
  保健 
  服飾 
  鞋包配件 
  美食 
  家居生活 
  餐廚生活 
  設計文具 
  無印良品 
  星巴克 
  3C 
  家電 
  日用 
  休閒生活 
  婦幼生活 
  電子票證 
  寵物生活 
  票券 
  玲廊滿藝 
  故宮精品 
  雜誌 
  售票 
  海外專館 
  快速到貨 
  禮物卡 

 必讀暢銷榜 
  天天爆殺 
  今日66折 
  每日簽到 
  禮物卡 
  現領折價券 
  全站分類   
   
 電子書 
  兒童館 
  旅遊戶外 
  家居日用 
  美妝個清 
  健康運動 
  品牌旗艦 

 旗艦品牌 
  中文書 | ． | 簡體 | ． | 外文 
  電子書 | ． | 有聲 | ． | 訂閱 
  雜誌 | ． | 日文書 
  CD | ． | DVD | ． | 黑膠 
  線上藝廊 
  文具 | ． | 動漫 
  日用品 | ． | 婦幼玩具 
  彩妝 | ． | 保養 | ． | 洗沐 
  鞋包 | ． | 黃金 | ． | 服飾 
  3C | ． | 手機 | ． | 電玩 
  家電 | ． | 視聽 
  美食 | ． | 生鮮 | ． | 保健 
  寵物 | ． | 家居 | ． | 餐廚 
  運動 | ． | 戶外 | ． | 旅用 
  禮券 | ． | 票證 | ． | 票券 

 外文書    
 新書 
  選書 
  排行榜 
  特價書 
  讀者書評 
  分類總覽 
  童書分齡推薦 

 博客來 
  外文書 
  自然科普 
  科技與應用科學 
  工程總論與技術 
  商品介紹 

 18th International Conference on Soft Computing Models in Industrial and Environmental Applications (Soco 2023): Salamanca, Spain, Sep 5th-7th, 2023 P  

 編者： | García Bringas, Pablo,Pérez García, Hilde,Martínez de Pisón, Francisco Javier 
  原文出版社： | Springer 
  出版日期：2023/08/31 
  語言：英文 
    
 定價： | 10199 | 元 

 分期價：(除不盡餘數於第一期收取) 分期說明   
 可接受VISA, Master, JCB, 聯合信用卡 

 3期0利率 | 每期 3399 | 6期0利率 | 每期 1699 

 運送方式： 
  臺灣與離島 
  海外 
  可配送點： | 台灣、蘭嶼、綠島、澎湖、金門、馬祖 
  可配送點： | 台灣、蘭嶼、綠島、澎湖、金門、馬祖 
  可配送點： | 全球 
  可配送點： | 香港、澳門、新加坡、馬來西亞、菲律賓 

 載入中...   
 我要評鑑      
   
 分享 

 上頁   下頁             

  內容簡介  

 This book of Advances in Intelligent and Soft Computing contains accepted papers presented at SOCO 2023 conference held in the beautiful and historic city of Salamanca (Spain) in September 2023.  
 Soft computing represents a collection or set of computational techniques in machine learning, computer science, and some engineering disciplines, which investigate, simulate, and analyze very complex issues and phenomena.  
 After a through peer-review process, the 18th SOCO 2023 International Program Committee selected 61 papers which are published in these conference proceedings and represents an acceptance rate of 60%. In this relevant edition, a particular emphasis was put on the organization of special sessions. Seven special sessions were organized related to relevant topics such as: Time Series Forecasting in Industrial and Environmental Applications, Technological Foundations and Advanced Applications of Drone Systems, Soft Computing Methods in Manufacturing and Management Systems, Efficiency and Explainability in Machine Learning and Soft Computing, Machine Learning and Computer Vision in Industry 4.0, Genetic and Evolutionary Computation in Real World and Industry, and Soft Computing and Hard Computing for a Data Science Process Model.  
  
 The selection of papers was extremely rigorous to maintain the high quality of the conference. We want to thank the members of the Program Committees for their hard work during the reviewing process. This is a crucial process for creating a high-standard conference; the SOCO conference would not exist without their help.  

  詳細資料  
 ISBN：9783031425356 
  規格：平裝 / 34頁 / 普通級 / 初版 
  出版地：美國 
  本書分類： | 自然科普 | > | 科技與應用科學 | > | 工程總論與技術 

 最近瀏覽商品  

 相關活動  
    
  購物說明  
 外文館商品版本：商品之書封，為出版社提供之樣本。實際出貨商品，以出版社所提供之現有版本為主。關於外文書裝訂、版本上的差異，請參考【  外文書的小知識  】。   
   
  調貨時間：無庫存之商品，在您完成訂單程序之後，將以空運的方式為您下單調貨。原則上約14~20個工作天可以取書(若有將延遲另行告知)。為了縮短等待的時間，建議您將外文書與其它商品分開下單，以獲得最快的取貨速度，但若是海外專案進口的外文商品，調貨時間約1~2個月。    
   
  若您具有法人身份為常態性且大量購書者，或有特殊作業需求，建議您可洽詢「 企業採購   」。    
   
  退換貨說明      
   
  會員所購買的商品均享有到貨十天的猶豫期（含例假日）。退回之商品必須於猶豫期內寄回。    
   
  辦理退換貨時，商品必須是全新狀態與完整包裝(請注意保持商品本體、配件、贈品、保證書、原廠包裝及所有附隨文件或資料的完整性，切勿缺漏任何配件或損毀原廠外盒)。退回商品無法回復原狀者，恐將影響退貨權益或需負擔部分費用。     
   
  訂購本商品前請務必詳閱商品 退換貨原則  。     

 同類商品新上架  
 1. | Biosynthetic Engineering of Natural Products 
  2. | Atlas of Flexible Bronchoscopy, Second Edition 
  3. | Hamilton Bailey’’s Emergency Surgery, 14th Edition 
  4. | Mastering Endocrinology and Diabetes: A Structured Guide for Clinical Practice 
  5. | Determination of Radioactivity in Terrestrial Environmental Samples: From the Basics to Complex Case Studies 
    
 本類新品熱銷  
 1. | Nexus: A Brief History of Information Networks from the Stone Age to AI 
  2. | Nexus: A Brief History of Information Networks from the Stone Age to AI 
  3. | Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity 
  4. | Floral Tarot: Access the Wisdom of Flowers: 78-Card Deck and Guidebook 
  5. | Determined: A Science of Life Without Free Will 
    
 本類暢銷榜  
 1. | Nexus: A Brief History of Information Networks from the Stone Age to AI 
  2. | Nexus: A Brief History of Information Networks from the Stone Age to AI 
  3. | Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity 
  4. | Elon Musk 
  5. | 貓貓掃地機器人Desktop Cat Vac 

 博客來  
  App 下載 
  博客來  
  電子書App 下載 
  博客來  
  Facebook粉絲專頁 
  博客來  
  YouTube 
  博客來  
  LINE 官方帳號 
   切換行動版    
   
 關於我們  關於博客來   關於PCSC   人才募集   利害關係人專區    企業合作  企業採購   福利平台   成為供應商   AP策略聯盟   異業合作   廣告刊登    好站連結  OKAPI 閱讀生活誌   青春博客來   高中生書店   售票網    會員服務  會員專區   加入會員   會員分級   查詢帳號密碼    客服中心   常見問題    線上客服    客服信箱    週一 ~ 五 08:00 - 19:00 
  週六日、例假日 09:00 - 18:00 
    客服專線 02-26535588 
  傳 真 02-27885008 

 得獎認證  

 博客來數位科技股份有限公司  統編 : 96922355  地址：臺灣 115 台北市南港區八德路四段768巷1弄18號B1之1  食品業者登錄字號：A-196922355-00000-9   
 Copyright © since 1995 books.com.tw All Rights Reserved.  
   
 資訊安全 
  服務條款 
  隱私權政策

84. IWSM Mensura_0 conference:
Skip to content    

 Search for:      

 Montréal, Sept. 30-Oct. 4, 2024 
  About the conference 
  Conference history 

 Montréal, September 30 – October 4, 2023  iwsm-mensura    2024-08-08T21:02:53+00:00      

 The Joint Conference of the 33 rd  International Workshop on Software Measurement (IWSM) and the 18 th  International Conference on Software Process and Product Measurement (MENSURA), will be held on September 30 – October 4, 2024 in Montréal, Canada.  

 Previous edition   
 On September 14-15, 2023 the previous edition  of the IWSM Mensura conference was held in Rome, Italy. See under History  more about all the previous editions, back to the first edition in 1990.   

 2024 
  Important dates 
  Program 
  Registration 
  Tourism & Transport 
  Venue 
  Where to stay 

 About IWSM Mensura  
 The IWSM Mensura  conference is the result of the joining of forces of the International Workshop on Software Measurement  and the International Conference on Software Process and Product Measurement  . Together they form the conference where new ideas from the world of academic research meet practical improvements from industry on topics of measuring software.  

 Each year practitioners and researchers from all over the world gather together to learn about new developments, test new ideas and exchange possible new solutions and applications. more    
 If you like the content, please share it with your network on Twitter, Facebook or LinkedIn using #IWSM2023  .  

 In 2024 we will be in  

 © Copyright 2012 - 2024   
   
 X   LinkedIn     

 Page load link  Translate »    

 Go to Top

85. CAINE_3 conference:
Events 
  Organizers 
  Venues 
  Add Event 
  Subscribe | Subscribe | Sign In 
  Hotels 
  Car Rental 
  Tours & Transport 
  Event Tickets 

 The 38th International Conference on Computer Applications in Industry and Engineering (CAINE 2025)  
 Oct 2025 

 Find Your Ideal Hotel   

 Promote this Event   

 Home 
  The 38th International Conference on Computer Applications in Industry and Engineering (CAINE 2025) 

 The event has no final dates - See other events instead:  
    
 17th International Conference on Computer and Automation Engineering (ICCAE 2025)   
  20-22 Mar 2025  Perth, Australia    

 30th International Conference on Engineering, Science, Technology & Industrial Applications (ESTIA 2025)   
  31 Mar - 02 Apr, 2025  Istanbul, Turkey    

 4th International Conference on Intelligent Systems Design and Engineering Applications (ISDEA 2025)   
  19-21 Apr 2025  Seoul, South Korea    
   
 More Events    

 Description  
 Topics  
 Computer-Aided Design /Manufacturing 
  Image/Signal Processing 
  Agent-Based Systems 
  Information Assurance 
  Autonomous Systems 
  Information Systems/Databases 
  Big Data Analytics 
  Internet and Web-Based Systems 
  Bioinformatics, Biomedical Systems/Engineering 
  Knowledge-based Systems 
  Mobile Computing 
  Computer Architecture/VLSI 
  Multimedia Applications 
  Computer Graphics and Animation 
  Neural Networks 
  Computer Modeling/Simulation 
  Pattern Recognition/Computer Vision 
  Computer Security 
  Rough Set and Fuzzy Logic 
  Computers in Education 
  Robotics Computer 
  Computers in Healthcare 
  Fuzzy Logic and 
  Networks 
  Sensor Networks 
  Control Systems 
  Scientific Computing 
  Data Communications 
  Software Engineering/CASE 
  Data Mining 
  Visualization 
  Distributed Systems 
  Wireless Networks and Communication 
  Embedded Systems 

 More Details  
 Organizer:   
 ISCA - International Society for Computers and their Applications   
   
 Website:   

 Promote this Event   

  Remind   
   Review   
   Article   

 Future Events  
 The 38th International Conference on Computer Applications in Industry and Engineering (CAINE 2025) - Oct 2025, | (36110) 
  Past Events  
 The 37th International Conference on Computer Applications in Industry and Engineering (CAINE 2024) - 21-22 Oct 2024, Hilton San Diego Airport/Harbor Island Hotel, California, United States | (43496) 
  The 36th International Conference on Computer Applications in Industry and Engineering (CAINE 2023) - 16-18 Oct 2023, Online Event | (36111) 

 Important  
 Please, check "The International Conference on Computer Applications in Industry and Engineering (CAINE)" official website for possible changes, before making any traveling arrangements   

 Event Categories  
 Science:  Computer Science, Engineering   
   
 Technology:  Industrial technology, Software & Applications   

 Other Events with Similar Categories  
 Science Conferences    

 Technology Conferences    

 Engineering Conferences    

 Computer Science Conferences    

 Software & Applications Conferences    

 Industrial technology Conferences    

 Novel Technologies Conferences    

 Science and Technology Conferences    

 Science Education Conferences    

 Soft Computing Conferences    

 Software Conferences    

 Applications Conferences    

 Apps Conferences    

 Saas Conferences    

 CAD CAM Conferences    

 Automation Technologies Conferences    

 Coding Theory Conferences    

 Computation Theory Conferences    

 Distributed Computing Conferences    

 Formal Methods Conferences    

 Fusion Engineering Conferences    

 Grid Computing Conferences    

 Heat Transfer Conferences    

 High Performance Computing Conferences    

 Human-centered Computing Conferences    

 Industrial Automation Conferences    

 Intelligent Control Conferences    

 Manufacturing Technologies Conferences    

 Molecular Engineering Conferences    

 Neural Networks Conferences    

 Scientific Computing Conferences    

 Computer Applications Conferences    

 Helicopter Engineering Conferences    

 Intelligent Systems Conferences    

 Microarchitecture Conferences    

 Structural Analysis Conferences    

 Structural Engineering Conferences    

 Systems Engineering Conferences    

 Technology and Science Conferences    

 Applied Cryptography Conferences    

 Applied Mechanics Conferences    

 Ceramic Engineering Conferences    

 Computer Architecture Conferences    

 Computer Mathematics Conferences    

 Control Engineering Conferences    

 Dynamics Conferences    

 Electromechanics Conferences    

 Engineering Management Conferences    

 Green Computing Conferences    

 Hydraulic Engineering Conferences    

 Industrial Engineering Conferences    

 Manufacturing Engineering Conferences    

 Mechanical Design Conferences    

 Mechatronics Engineering Conferences    

 Municipal Engineering Conferences    

 Nuclear Engineering Conferences    

 Parallel Computing Conferences    

 Pervasive Computing Conferences    

 Quantum Computing Conferences    

 Thermal Engineering Conferences    

 Ubiquitous Computing Conferences    

 Applied Engineering Conferences    

 Automation Engineering Conferences    

 Computational Methods Conferences    

 Computer Engineering Conferences    

 Computer Technology Conferences    

 Earthquake Engineering Conferences    

 Evolutionary Computation Conferences    

 Frontier Computing Conferences    

 Naval Engineering Conferences    

 Parallel Processing Conferences    

 Process Engineering Conferences    

 Speech Processing Conferences    

 Chemical and Materials Engineering Conferences    

 Civil and Architectural Engineering Conferences    

 Civil and Environmental Engineering Conferences    

 Computer and Information Engineering Conferences    

 Electrical and Computer Engineering Conferences    

 Industrial and Manufacturing Engineering Conferences    

 Industrial and Systems Engineering Conferences    

 Mechanical and Industrial Engineering Conferences    

 Mechanical and Materials Engineering Conferences    

 Mechanical and Mechatronics Engineering Conferences    

 Nuclear and Quantum Engineering Conferences    

 Information Theory Conferences    

 Engineer Conferences    

 Mechanical Engineering Conferences    

 Computational Conferences    

 Ui Conferences    

 Informatics Conferences    

 Bioinformatics Conferences    

 Sap Conferences    

 Algorithms Conferences    

 Oracle Conferences    

 Web Science Conferences    

 Other Events with Similar Location or Organizer  
    
 Events by ISCA - International Society for Computers and their Applications    

 Featured Conferences & Exhibitions  
    
  Biotechnology for the Non-Biotechnologist  

 Virtual (Online)   
 02-06 Dec 2024   

  International Conference of Leadership Business and Managment (ICLBM)  

 San Francisco   
 14-16 Dec 2024   

  International Crop Science Conference & Exhibition  

 Dubai   
 21-22 Jan 2025   

  74th New Orleans Academy of Ophthalmology (NOAO) Annual Symposium 2025  

 New Orleans   
 21-23 Feb 2025   

  ConveyUX  

 Seattle   
 25-27 Feb 2025   

  INTED2025 - The 19th International Technology, Education and Development Conference  

 Valencia   
 03-05 Mar 2025   

  EnviroTech Athens 2025  

 Athens   
 09-12 Mar 2025   

  35th Annual Art and Science of Health Promotion Conference  

 Scottsdale   
 04-05 Apr 2025   

  The Learning Ideas Conference 2025  

 New York City   
 11-13 Jun 2025   

  Primary Care Hawaii Conference - Caring for the Active and Athletic Patient  

 Koloa   
 21-25 Jul 2025   

 Articles / News / Press releases  
    
 Wood-Plastic Composites    

 Mechatronics    

 MES & Process Minds - Exclusive interview with speaker - Neil Roche from    

 An Approach to the Diagnosis of Transformer’s Insulating Oil    

 Work study engineering    

 Comparative Speed Control Study Using PID And fuzzy Logic Controller    

    Home  |  Conferences  |  Exhibitions  |  Organizers  |  Venues  |  Add Conference  |  Subscribe  |  Contact  |  Venues Directory      
   
 Share   

 Follow   

 Terms and Conditions  Privacy Policy  Cookies Policy    
   
 © 2024 Conference Locate (Clocate)   

 Search for Events  

 Search    
   
 Clear

86. IWSM Mensura_1 conference:
Skip to content    

 Search for:      

 Montréal, Sept. 30-Oct. 4, 2024 
  About the conference 
  Conference history 

 2023  iwsm-mensura    2024-02-14T14:24:21+00:00  Rome 2023  
 In 2023 the IWSM Mensura conference was held in Rome on September 14 and 15.  
 Pictures of the conference are available on Pinterest   

 Organization  
 Steering Committee  
 Alain Abran, University of Québec / ÉTS, Montréal (Québec), Canada  
  Onur Demirors, Izmir Institute of Technology, Izmir, Turkey  
  Reiner R. Dumke, Otto-von-Guericke-University, Magdeburg, Germany  
 General Chairs  
 Filippo De Carli, GUFPI-ISMA, Italy  
  Luigi Buglione, GUFPI-ISMA / DXC Technology  
 Program Chairs  
 Gabriele De Vito, University of Salerno, Italy  
  Filomena Ferrucci, University of Salerno, Italy  
  Carmine Gravino, University of Salerno, Italy  
 Workshop and Tutorial Chair  
 Sergio Di Martino, University of Napoli Federico II, Italy  
 Organization Committee  
 Biagio Nocito, GUFPI-ISMA, Italy  
  Filippo De Carli, GUFPI-ISMA, Italy  
  Guido Moretto, GUFPI-ISMA, Italy  
  Luigi Buglione, GUFPI-ISMA / DXC Technology  
  Paola Billia, GUFPI-ISMA, Italy  
 Finance Chair  
 Gianfranco Lanza, GUFPI-ISMA, Italy  
 Web Chairs  
 Frank Vogelezang, IDC Metri, the Netherlands  
  Samet Tenekeci, Izmir Institute of Technology, Turkey  
 Publicity Chairs  
 Francesco Casillo, University of Salerno, Italy  
  Giusy Annunziata, University of Salerno, Italy  
  Luigi Libero Lucio Starace, University of Napoli Federico II, Italy  
 Proceedings Chairs  
 Huseyin Unlu, Izmir Institute of Technology, Turkey  
  Gorkem Kilinc Soylu, Izmir Institute of Technology, Turkey  
 Program Committee  
  
 Alain | Abran | École de Technologie Supérieure | UQAM | Canada 
 J. Rafael | Aguilar Cisneros | Universidad Popular Autónoma del Estado de Puebla | México 
 Sousuke | Amasaki | Okayama Prefectural University | Japan 
 Lefteris | Angelis | Aristotle University of Thessaloniki | Greece 
 Monalessa | Barcellos | UFES | Brazil 
 Luigi | Buglione | GUFPI-ISMA and DXC Technology | Italy 
 Panagiota | Chatzipetrou | Örebro University School of Business | Sweden 
 Laila | Cheikhi | ENSIAS | Morocco 
 Marcus | Ciolkowski | QAware GmbH | Germany 
 Tayana | Conte | Federal University of Amazonas | Brazil 
 Beata | Czarnacka-Chrobot | Warsaw School Of Economics | Poland 
 Maya | Daneva | University of Twente | the Netherlands 
 María | De León Sigg | Universidad Autónoma de Zacatecas | México 
 Onur | Demirörs | Izmir Institute of Technology | Turkey 
 Sergio | Di Martino | University of Naples “Federico II” | Italy 
 Reiner | Dumke | University of Magdeburg | Germany 
 Christof | Ebert | Vector Consulting | Germany 
 Peter | Fagg | UK Software Measurement Association | United Kingdom 
 Thomas | Fehlmann | Euro Project Office | Switzerland 
 Dan | Galorath | Galorath | USA 
 Vahid | Garousi | Queen’s University Belfast | United Kingdom 
 Cigdem | Gencel | Free University of Bolzano | Italy 
 Giammaria | Giordano | University of Salerno | Italy 
 Görkem | Giray | Izmir Institute of Technology | Turkey 
 Tuna | Hacaloğlu | Atılım University | Turkey 
 Colin | Hammond | Scopemaster Ltd | United Kingdom 
 Jens | Heidrich | Fraunhofer IESE | Germany 
 Helena | Holmström Olsson | Malmö University | Sweden 
 Paul | Hussein | European Securities and Markets Authority (ESMA) | France 
 Amy | Kearney | USA 
 Andrzej | Kobylinski | Warsaw School of Economics | Poland 
 Rob | Kusters | Eindhoven University Of Technology | the Netherlands 
 Stefano | Lambiase | University of Salerno | Italy 
 Luigi | Lavazza | Università degli Studi dell’Insubria | Italy 
 Ruchika | Malhotra | Delhi Technological University | India 
 Emilia | Mendes | Blekinge Institute of Technology | Sweden 
 Arlene | Minkiewicz | PRICE Systems | United Kingdom 
 Sandro | Morasca | University of Insubria | Italy 
 Simona | Motogna | Babes-Bolyai University, Cluj-Napoca | Romania 
 Donatien K. | Moulla | University of Maroua | Cameroon 
 Özden | Özcan Top | Middle East Technical University, Ankara | Turkey 
 Valeria | Pontillo | University of Salerno | Italy 
 Nicolas | Porta | Daimler TSS | Germany 
 Luigi | Quaranta | University of Bari | Italy 
 Federica | Sarro | University College London | United Kingdom 
 Asma | Sellami | University of Sfax | Tunisia 
 Giulia | Sellitto | University of Salerno | Italy 
 Hassan | Soubra | German University In Cairo | Egypt 
 Miroslaw | Staron | University of Gothenburg | Sweden 
 Davide | Taibi | University of Oulu | Finland 
 Ayça | Tarhan | Hacettepe University | Turkey 
 Selma | Tekir | Izmir Institute of Technology | Turkey 
 Erdir | Ungan | Université du Québec à Montréal | Canada 
 Monica | Villavicencio | ESPOL | Ecuador 
 Frank | Vogelezang | METRI | COSMIC | the Netherlands 

 About IWSM Mensura  
 The IWSM Mensura  conference is the result of the joining of forces of the International Workshop on Software Measurement  and the International Conference on Software Process and Product Measurement  . Together they form the conference where new ideas from the world of academic research meet practical improvements from industry on topics of measuring software.  

 Each year practitioners and researchers from all over the world gather together to learn about new developments, test new ideas and exchange possible new solutions and applications. more    
 If you like the content, please share it with your network on Twitter, Facebook or LinkedIn using #IWSM2023  .  

 In 2024 we will be in  

 © Copyright 2012 - 2024   
   
 X   LinkedIn     

 Page load link  Translate »    

 Go to Top

87. IWSM Mensura_2 conference:
Vol-3543   
  urn:nbn:de:0074-3543-7  Copyright © 2023 for the individual papers by the papers' authors. Copyright © 2023  for the volume as a collection by its editors. This volume and its papers are published under the Creative Commons License Attribution 4.0 International ( CC BY 4.0  )  . 

  IWSM-MENSURA 2023    
  Joint Proceedings IWSM and MENSURA 2023   
  
  Joint Proceedings of the 32nd International Workshop on Software Measurement (IWSM) and the 17th International Conference on Software Process and Product Measurement (MENSURA)   
   
 Rome, Italy, September 14-15, 2023  .  
  
  Edited by   
 Gabriele De Vito   *  
  Filomena Ferrucci   *  
  Carmine Gravino   *  
   
 * University of Salerno  , The Department of Computer Science/DI, 84084 Fisciano SA, Italy  

  Table of Contents  
 Organization Committee 
  Preface 
  Regular Papers   
 An Analysis of Students’ Teams Scores at the 2022 Software Estimation Challenge | Donatien Koulla Moulla | , | Jean-Marc Desharnais | , | Alain Abran 
  Functional Size Measurement for X86 Assembly Programs | Donatien Koulla Moulla | , | Abdel Aziz Kitikil | , | Ernest Mnkandla | , | Hassan Soubra | , | Alain Abran 
  AI-based Fault-proneness Metrics for Source Code Changes | Francesco Altiero | , | Anna Corazza | , | Sergio Di Martino | , | Adriano Peron | , | Luigi L. L. Starace 
  Software Development Effort Estimation Using Function Points and Simpler Functional Measures: A Comparison | Luigi Lavazza | , | Angela Locoro | , | Roberto Meli 
  A Method for Metric Management at a Large-Scale Agile Software Development Organization | Pascal Philipp | , | Franziska Tobisch | , | Leon Menzel | , | Florian Matthes 
  Functional Size Measurement Automation for IoT Edge Devices | Salma Salem | , | Hassan Soubra 
  Understanding Developer Practices and Code Smells Diffusion in AI-Enabled Software: A Preliminary Study | Giammaria Giordano | , | Giusy Annunziata | , | Andrea De Lucia | , | Fabio Palomba 
  Starting a New REST API Project? A Performance Benchmark of Frameworks and Execution Environments | Sergio Di Meglio | , | Luigi Libero Lucio Starace | , | Sergio Di Martino 
  An Empirical Study on the Performance of Vulnerability Prediction Models Evaluated Applying Real-world Labelling | Giulia Sellitto | , | Alexandra Sheykina | , | Fabio Palomba | , | Andrea De Lucia 
  Metrics and Models for Developer Collaboration Analysis in Microservice-Based Systems. A Systematic Mapping Study | Xiaozhou Li | , | Amr S. Abdelfattah | , | Ruoyu Su | , | Joseph Lee | , | Ernesto Aponte | , | Rachel Koerner | , | Tomas Cerny | , | Davide Taibi 
  Short Papers   
 Size Measurement and Effort Estimation in Microservice-based Projects: Results from Pakistan | Görkem Kılınç Soylu | , | Hüseyin Ünlü | , | Isra Shafique Ahmad | , | Onur Demirörs 
  A Maturity Model Guidance Approach for Integration Testing of Avionics Software | Gülsüm Güngör | , | Ayça Kolukısa Tarhan 
  Industry Papers   
 Resolving the Historical Confusions about the Meaning of Software Size and Its Use for Project Effort Estimation (Extended Abstract) | Charles Symons 
  The ABC Model: Coming “Back to the Future” in (ICT) Contracts | Luigi Buglione 
    
  2023-09-23: submitted by Gabriele De Vito metadata incl. bibliographic data published under Creative Commons CC0   
  2023-11-09  : published on CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073) | valid HTML5  |

88. VISIGRAPP_0 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
   
 Information  Conference Details  Technical Program  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Doctoral Consortium  Partners  Academic Partners  Industrial Partners  Exhibitors  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Abstracts  Awards    
  
 Sponsored by:    

 INSTICC is Member of:    

 Logistics:    

 The purpose of VISIGRAPP is to bring together researchers and practitioners interested in both theoretical advances and applications of computer vision, computer graphics and information visualization. VISIGRAPP is composed of four co-located conferences, each specialized in at least one of the aforementioned main knowledge areas.  

  Although the conference is back to the normal mode (i.e., in-person) speakers are allowed to present remotely if unable to travel to the venue (hybrid support).    

  GRAPP   
  18th International Conference on Computer Graphics Theory and Applications    
   
 Program Co-chairs    
  A. Augusto Sousa  , FEUP/INESC TEC, Portugal  
  Thomas Bashford-Rogers  , University of Warwick, United Kingdom   

  HUCAPP   
  7th International Conference on Human Computer Interaction Theory and Applications    
   
 Program Co-chairs    
  Alexis Paljic  , Mines Paristech, France  
  Mounia Ziat  , Bentley University, United States   

  IVAPP   
  14th International Conference on Information Visualization Theory and Applications    
   
 Program Co-chairs    
  Christophe Hurter  , French Civil Aviation University (ENAC), France  
  Helen C. Purchase  , Monash University, Australia   

  VISAPP   
  18th International Conference on Computer Vision Theory and Applications    
   
 Program Co-chairs    
  Petia Radeva  , Universitat de Barcelona, Spain  
  Giovanni Maria Farinella  , Università di Catania, Italy   

   Conference Chair    
 Kadi Bouatouch  ,  IRISA, University of Rennes 1, France   

  Keynote Speakers   
 Alexandru Telea  ,  Utrecht University, Netherlands   
  Ferran Argelaguet  ,  Institut National de Recherche en Informatique et en Automatique (INRIA), France   
  Vincent Hayward  ,  Sorbonne University, France   
  Liang Zheng  ,  Australian National University, Australia   

 Tutorials   
 First Person (Egocentric) Vision   
  Instructors:  Francesco Ragusa and Antonino Furnari  

 Publications:     
  All papers presented at the conference venue  
  will be available at the SCITEPRESS Digital Library   
  ( consult SCITEPRESS  Ethics of Publication  )  

  It is planned to publish a short list of revised and  
  extended versions of presented papers with  
  Springer in a CCIS Series book   

  A short list of best papers will be invited  
  for a post-conference special issue of the  
  Springer Nature Computer Science Journal   

 Endorsed by:    

 In Cooperation with:    

 Proceedings will be submitted for evaluation for indexing by:    

  Web of Science/Conference Proceedings Citation Index     

 © 2024  INSTICC

89. IWSM Mensura_3 conference:
Skip to main menu 
  Skip to main content 
  Knowledge base: University of Lodz    

 Settings and your account  
 default font size  A | default font size 
  bigger font size  A | bigger font size 
  big font size  A | big font size 
  High contrast    tonality | High contrast  Normal contrast 
  Change language to: Polish  Zmień język na: Polski | Change language to polish 
  Log in 

 Main menu  
 menu  menu_open  Main menu   Start    Start 
  Research teams arrow_drop_down | Profiles 
  Scientific centers and research teams 
  Research Outputs arrow_drop_down | Publications 
  Research data 
  Research potential arrow_drop_down | Patents 
  Projects 
  Promotion arrow_drop_down | Organized conferences 
  Own magazines (published by institution) 
  Multimedia 
  More arrow_drop_down | Cooperation 
  Achievements 
  Activities 
  ScienceON arrow_drop_down | About the site 
  For employees 
  Training 

 You are here:  
 Start 
   Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM and Mensura combined from 2007) | (IWSM Mensura) 

 Back   Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM and Mensura combined from 2007)  
 Short name   IWSM Mensura  Full name   Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM and Mensura combined from 2007)  Identifiers in external systems   Identyfikator PBN MongoId: 5cac621c878c287715741acf  Scientific discipline   information and communication technology (ICT)   ; computer and information sciences (CIS)    Category   Ministerial score list    Indicators   Conference Ministerial score   : 2019   = 20.000 - 2024   = 20.000  Points   20   nonverified conference event in 2019  Related publications (1)     
 Refresh page       
 Refresh page   

     Uniform Resource Identifier  https://son.uni.lodz.pl/info/conferenceseries/WUT3100d2e9887649a2afca7e84c06c616e/     
 URN  urn:uni-lodz-prod:WUT3100d2e9887649a2afca7e84c06c616e      
  Back    

  Expand/Collapse AddToAny          

 Projekt Uczelnianego Systemu Informacji o Osiągnięciach UŁ współfinansowany z funduszy Unii Europejskiej w ramach konkursu NCBR  
 Additional information  
 Partners  
 © 2024 Powered by Omega-PSIR     

     Confirmation    
  Are you sure?    
  Yes    Cancel     
   
 Report incorrect data on this page      

 clipboard   
     
 Refresh clipboard       
 Refresh clipboard

90. VISIGRAPP_1 conference:
Home  Log In  Contacts  FAQs  INSTICC Portal    
   
 Documents  Actions  On-line Registration  Registration Fees  Deadlines and Policies  Submit Paper  Submit Abstract  Guidelines  Templates  Glossary  Author's Login  Reviewer's Login  Ethics of Review  Information  Conference Details  Important Dates  Call for Papers  Program Committee  Event Chairs  Keynote Lectures  Best Paper Awards  Satellite Events  Workshops  Special Sessions  Tutorials  Demos  Panels  Travel and Accommodation  Conference Venue  About the Region  Reaching the City  Visa Information  Hotel Reservation  Partners  Academic Partners  Industrial Partners  Institutional Partners  Media Partners  Partner Events  Publication Partners  Previous Conferences  Websites  Abstracts  Invited Speakers  Awards  Books Published    
  
 Sponsored by:    

 INSTICC is Member of:    

 Logistics:    

 VISAPP is part of VISIGRAPP    , the 20th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications.  
  Registration to VISAPP allows free access to all other VISIGRAPP conferences.   
   
  VISIGRAPP 2025 will be held in conjunction with MODELSWARD 2025    and ROBOVIS 2025    .   
  Registration to VISIGRAPP allows free access to the MODELSWARD and ROBOVIS conferences (as a non-speaker).  

  Although the conference is back to the normal mode (i.e., in-person) speakers are allowed to present remotely if unable to travel to the venue (hybrid support).    

 Upcoming Submission Deadlines  
 Late-Breaking Submission Deadline:  December 16, 2024    
 Abstracts Track Submission:  December 18, 2024    

  (See Important Dates for more information)    
 The International Conference on Computer Vision Theory and Applications aims at becoming a major point of contact between researchers, engineers and practitioners on the area of computer vision methods, systems and applications. Several tracks cover all different aspects related to computer vision, including: Image and Video Processing and Analysis; Image and Video Understanding; Motion, Tracking and Stereo Vision; Mobile and Egocentric Vision for Humans and Robots; and Applications and Services.    
 Conference Areas  
 1  .  Image and Video Processing and Analysis   
  
  2  .  Image and Video Understanding   
  
  3  .  Motion, Tracking, and 3D Vision   
  
  4  .  Mobile, Egocentric, and Robotic Vision   
  
  5  .  Applications and Services   

  Conference Co-chairs    
 Kadi Bouatouch  ,  IRISA, University of Rennes 1, France   
  A. Augusto Sousa  ,  FEUP/INESC TEC, Portugal   

 PROGRAM CO-CHAIRS    
 Petia Radeva  ,  Universitat de Barcelona, Spain   
  Antonino Furnari  ,  University of Catania, Italy   

 ASSOCIATE CHAIRS   
  Dario Allegra  , University of Catania, Italy - Image and Video Processing and Analysis    
  Gianluigi Ciocca  , University of Milano-Bicocca, Italy - Image and Video Processing and Analysis    
  Mariella Dimiccoli  , Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Spain - Image and Video Understanding    
  Fabio Galasso  , Sapienza University of Rome, Italy - Image and Video Understanding    
  Matteo Dunnhofer  , University of Udine, Italy - Motion, Tracking, and 3D Vision    
  Jun Sato  , Nagoya Institute of Technology, Japan - Motion, Tracking, and 3D Vision    
  Dimitri Ognibene  , Università degli studi Milano-Bicocca, Italy and University of Essex, United Kingdom - Mobile, Egocentric, and Robotic Vision    
  Francesco Ragusa  , University of Catania, Italy - Mobile, Egocentric, and Robotic Vision    
  Eduardo Aguilar Torres  , Universidad Catolica del Norte, Chile - Applications and Services    
  George Azzopardi  , University of Groningen, Netherlands and University of Malta, Malta - Applications and Services    

  Keynote Speakers   
 Julien Pettré  ,  Inria, France   
  Daniel Archambault  ,  Newcastle University, United Kingdom   
  Katherine J. Kuchenbecker  ,  Max Planck Institute for Intelligent Systems, Stuttgart, Germany   
  Diane Larlus  ,  Naver Labs Europe, France   

 Publications:     
  All papers presented at the conference venue  
  will be available at the SCITEPRESS Digital Library   
  ( consult SCITEPRESS  Ethics of Publication  )  

  It is planned to publish a short list of revised and  
  extended versions of presented papers with  
  Springer in a CCIS Series book  (final approval pending)  

 Technically co-sponsored by:   

 Endorsed by:    

 In Cooperation with:    

 Proceedings will be submitted for evaluation for indexing by:    

  Web of Science/Conference Proceedings Citation Index     

 © 2024  INSTICC

91. K-CAP_0 conference:
K-CAP 2023  
 The Twelfth International Conference on Knowledge Capture  
 December 5 - 7, 2023  
 Pensacola, Florida, USA  

 Knowledge has played a fundamental role since the inception of artificial intelligence. While the forms in which algorithms have leveraged knowledge have evolved over time, the need for efficient representations is ever more critical. Indeed, recent advances of AI, such as the stunning performance of large language models have relied on the large amount of data available on the Web. There is growing agreement among researchers that it's important to look beyond the sheer volume of data, and instead also prioritize the development of methods that are accurate, precise, and efficient for capturing knowledge.  
 The International Conference on Knowledge Capture, K-CAP, aims at bringing together an interdisciplinary group of researchers on a diverse set of topics with interest in the development of knowledge capture. This involves the design and development of formalisms, methods and tools that enable efficient and precise extraction and organization of knowledge from different sources and for different modalities of use including, for example, automated reasoning, machine learning and human-machine teaming.  
  
 To enable a vibrant and constructive discussion on scalable and precise knowledge capture, K-CAP 2023, calls for the participation of researchers from diverse areas of artificial intelligence, including, but not limited to, knowledge representation and reasoning, knowledge acquisition, semantic web, intelligent user interfaces for knowledge acquisition and retrieval, query processing and question answering over heterogeneous knowledge bases, novel evaluation paradigms, problem-solving and reasoning, ethics and AI, explainability, neurosymbolic AI, agents, information extraction from structured or unstructured data, machine learning and representation learning, information enrichment and visualization, as well as researchers interested in cyber-infrastructures to foster the publication, retrieval, reuse, and integration of data.  
 K-CAP is an in-person conference, and remote participation will not be supported.  
 Conference venue   

 Sponsors

92. K-CAP_1 conference:
Upcoming Conference   
  
 K-CAP 2025   
  Thirteenth International Conference on Knowledge Capture  
  December 10 - 12, 2025  
  Wright State University, Ohio, USA 

 Previous K-CAP Conferences   
  
 K-CAP 2023   
  Twelfth International Conference on Knowledge Capture  
  December 5 - 7, 2023  
  Pensacola, Florida, USA 
 K-CAP 2021   
  Eleventh International Conference on Knowledge Capture  
  December 2 - 3, 2021  
  Virtual Conference 
 K-CAP 2019   
  Tenth International Conference on Knowledge Capture  
  November, 2019  
  Marina del Rey, California 

 K-CAP 2017   
  Ninth International Conference on Knowledge Capture  
  4th - 6th December, 2017  
  Austin, TX 

 K-CAP 2015   
  Eighth International Conference on Knowledge Capture  
  October, 2015  
  Palisades, NY 

 K-CAP 2013   
  Seventh International Conference on Knowledge Capture  
  Jun. 23-26, 2013  
  Banff, Alberta 

 K-CAP 2011   
  Sixth International Conference on Knowledge Capture  
  Jun. 25-29, 2011  
  Banff, Alberta 

 K-CAP 2009   
  Fifth International Conference on Knowledge Capture  
  Sep. 1-4, 2009  
  Redondo Beach, CA 

 K-CAP 2007   
  Fourth International Conference on Knowledge Capture  
  Oct. 28-31, 2007  
  Whistler, BC 

 K-CAP 2005   
  Third International Conference on Knowledge Capture  
  Oct. 23-25, 2005  
  Banff, Alberta 

 K-CAP 2003   
  Second International Conference on Knowledge Capture  
  Oct. 23-25, 2003  
  Florida, USA 

 K-CAP 2001   
  First International Conference on Knowledge Capture  
  Oct. 21-23, 2001  
  Victoria, B.C, Canada 

 Motivation for the K-CAP conference series 

 The K-CAP conference series is sponsored by  
  the Association for Computing Machinery's  
  Special Interest Group on Artificial Intelligence    
   
 For more information about k-cap.org, send email to info@k-cap.org

93. VISIGRAPP_2 conference:
Cookies Policy   
 The website need some cookies and similar means to function. If you permit us, we will use those means to collect data on your visits for aggregated statistics to improve our service. Find out More    
 Accept  Reject    
 '   

 Menu 
    
 Português 
  Inglês 
  Spread the word.  
  Share this page. 
  Facebook 
  Twitter 
  Linked in 
  Copy Url 
   Institution 
  Research | Research Domains   Artificial Intelligence 
  Bioengineering 
  Communications 
  Computer Science and Engineering 
   
  Photonics 
  Power and Energy Systems 
  Robotics 
  Systems Engineering and Management 
    RESEARCH CENTERS   
   
        Porto, Portugal   +351 222 094 000  info@inesctec.pt 
  Innovation | Innovation / Tec4   TEC4AGRO-FOOD 
  TEC4ENERGY 
  TEC4HEALTH 
  TEC4INDUSTRY 
  TEC4SEA 
  TECPARTNERSHIPS 
   
  Available Technologies          Porto, Portugal   +351 222 094 000  info@inesctec.pt 
  Laboratories | Research Laboratories 
  iilab 
  Communication | News 
  Events 
  Media 
  Newsletter 
         Porto, Portugal   +351 222 094 000  info@inesctec.pt 
  Work with us 
  Contacts 

 Home 
  Events 
  VISIGRAPP 

 Event  VISIGRAPP  

 Read Full Description    
 Join the 20th International Joint Conference on Computer Vision, Imaging, and Computer Graphics Theory and Applications (VISIGRAPP) in Porto from February 26-28, 2025.  

 Read Full Description    

 Presentation   

 VISIGRAPP  

 The 20th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP) will take place from 26-28 February, 2025, in Porto.  
  
 Download the flyer  | Download the poster   
  
 The purpose of VISIGRAPP is to bring together researchers and practitioners interested in both theoretical advances and applications of computer vision, computer graphics, human-computer interaction and information visualization.  
 VISIGRAPP comprises four specialized conferences:  
 GRAPP - Conference on Computer Graphics Theory and Applications 
  HUCCAP - Conference on Human Computer Interaction Theory and Applications 
  IVAPP - Conference on Information Visualization Theory and Applications 
  VISAPP - Conference on Computer Vision Theory and Applications 
   
 Conference Co-Chairs  
  
 Kadi Bouatouch  , IRISA, University of Rennes  
 A. Augusto Sousa  , DEI, FEUP/INESC TEC  

 Upcoming Submission Deadlines  
  
 Regular Paper Submission: October 2, 2024  
 Position/Regular Paper Submission: November 13, 2024  
 Doctoral Consortium Paper Submission: December 18, 2024  

 Details   

 Details  

 Start | 26th February 2025 
  Promoters | SCITEVENTS, INSTICC 
  City | Porto 
    
 End | 28th February 2025 
  Website | https://visigrapp.scitevents.org/ 

 navigation   
 Institution 
  Research 
  Research 
  Communication 
  Communication 
  Laboratories 
  Work with us 
  Contacts 

 poles   
 FCUP 
  FEUP 
  ISEP 
  UMINHO 
  UTAD 

 Contacts   
 T. +351 222 094 000  
  E. info@inesctec.pt   
 INESC TEC  
  Campus da FEUP  
  Rua Dr. Roberto Frias  
  4200 - 465 Porto  
  Portugal  

 newsletter   
 Subscribe    

 associates   

 Nuclei   

 Financing   

 Projects funded by   

 INESC TEC - All Rights Reserved 2024 Privacy Policy    
 by NQ Digital Agency    

  Spread the word.  
  Share this page. 
  Facebook 
  Twitter 
  Linked in 
  Google plus 
  Copy Url 

  Português 
  Inglês 

  Close   

 Institution 
  Research  | Research Domains   Artificial Intelligence 
  Bioengineering 
  Communications 
  Computer Science and Engineering 
  Photonics 
  Power and Energy Systems 
  Robotics 
  Systems Engineering and Management 
   RESEARCH CENTERS 
  Innovation 
  Communication  | News 
  Events 
  Mediass 
  Success  
  Stories 
  BIP 
  Work with us 
  Contacts 
    
      Porto, Portugal    
 +351 222 094 000   
 info@inesctec.pt    

     
     

 Subscribe 
  Remove 
    
 Subscribe 
  Remove 

 BIP is INESC TEC’s monthly electronic newsletter.   
 It is one of the institution’s most important communication tools, featuring news and articles about the science and technology made by INESC TEC, always with an informal, light, fresh and yet authentic and educational tone. It is not politically correct, nor does it intend to be the voice of the Board of Directors.   

 Subscribe INESC TEC’s Newsletter   
  First Name*     
   
 Last Name*     

 E-mail Address*       

 Organization     

  The data submitted through this form will be used exclusively for the monthly sending of the newsletter BIP – Bulletin INESC TEC, and will not, under any circumstances, be shared with third parties. If you choose to, you can easily unsubscribe from the newsletter by following the link presented in the footer. In that case, your data will be automatically deleted from our information system. If you need to update your contact information or clarify any questions related to the newsletter, please send an email to scom@inesctec.pt. By submitting this form, you give permission to the use of your personal data according to the conditions above.   

 Human check: Leave this field empty    

 Apply  Cancel    

     

 Thank you...   
   
 Close    
   
 Your subscripiton was complete. Thank you.  

     
     

 Subscribe 
  Remove 
    
 Subscribe 
  Remove 

 Remove your e-mail address form our mailing list   
 Ao remover a subscrição deixará de receber as nossas newsletters mensais.   

 Unsubscribe INESC TEC’s Newsletter   
  E-mail Address*      

 Remove  Cancel    

     

 Thank you...   
   
 Close    
   
 Your subscription has been successfully removed.  

   Search      

 Access to the Final Selection Minute  
 The access to the final selection minute is only available to applicants.  
 Please check the confirmation e-mail of your application to obtain the access code.  
   
 Enter your Access Code  
   Access Code*     

 Cancel  Continue    

     
     

 Contact us   

 Send Message   
   First Name*     
   
 Last Name*     

 E-mail Address*       
   
 Telephone Number*     

  Subject*    

 Message       

 Human check: Leave this field empty     

 Send  Cancel    

     

 Thank you...   
   
 Close    
   
 Sent with success. Thank you.

94. K-CAP_2 conference:
K-Cap 2013  
 23-26 June 2013  
 Banff, Canada  

 You can become a sponsor of K-CAP 2013  by clicking on this banner and give your support to science starting at $1!   

 Home 
  About K-CAP 
  News 
  Important Dates 
  Contact 
  Sponsor Opportunities 
  Programme 
  Main Conference 
  Invited Speakers 
  Accepted Papers 
  Workshop 
  Tutorial 
  Registration 
  Conference 
  Accommodation and transport 
  Calls 
  Datathon 
  Call for Research and Application Papers 
  Call for Workshops and Tutorials 
  Submissions 
  Committees 
  Organising Committee 
  Programme Committee 
  Steering Committee 

 In today’s technology-driven society, effective access to and use of information is a key enabler for progress. Driven by the demands for knowledge-based applications and the unprecedented availability of information on the Web, the study of knowledge capture is of crucial importance. Knowledge capture involves the extraction of useful knowledge from vast and diverse online sources as well as its acquisition directly from human experts.  
     
 Researchers and practitioners who work in the area of knowledge capture traditionally participate in several distinct communities, including knowledge engineering, machine learning, natural language processing, humancomputer interaction, artificial intelligence, and the Semantic Web. K-CAP 2013 will provide a forum that brings together members of disparate research communities who are interested in efficiently capturing knowledge from a variety of sources and in creating representations that can be useful for automated reasoning, analysis, and other forms of machine processing, as well as to support users in knowledge-intensive collaborative tasks.  
     
 In addition, the wealth of information available and generated on the Web, both in structured, linked data forms and in unstructured information sources, brings the need for new methods to make knowledge emerge from such sources, following the recent democratisation, and somehow industrialisation, of areas such as open data, linked data and the Semantic Web. Such new methods must draw both from the traditional knowledge acquisition techniques and from more computational approaches, from large-scale data-mining, statistical analysis, data analytics, etc. For this reason, we are making the special theme of the K-CAP 2013 conference:  

 Knowledge Capture in the Age of Massive Web Data   
  
    This will be reflected both through encouraging paper submissions showing works on the way to deal with the scale, but also the distribution, heterogeneity, incompleteness, inconsistency and variety of both sources and results when trying to make knowledge emerge from massive Web data, including data published on the Web, as well as generated through the Web.  
     
 We will also, for the first time in the K-CAP conference series, introduce a special Application Track  , where submissions are invited to present results tested and deployed in real-life settings. The application track will pay special attention to in-use applications for the generation, management, and reuse of large amounts of existing data and knowledge resources on the Web. Data and knowledge-intensive applications in a variety of domains are welcome, including the Web of Data, mobile and sensor networks, large-scale scientific data management and reuse, open government and e-participation, social networks, and enterprise data management and business intelligence. We encourage submissions in such domains, dealing with topics including: big data capture, representation and analytics, crowd-sourcing for data generation and problem-solving, hybrid approaches combining knowledge engineering and machine learning, data and service dynamicity, heterogeneity and decay, and innovative user interfaces. Especially interesting are applications with available datasets and in-place data-driven business models.  

 News | Hjalmar Gislason to deliver a Keynote Speech at KCAP 2013 
  Crowd-funding KCAP 2013! 
  KCAP2013 Datathon – Registration now open 
  Tags | facebook  KCAP 2013  organisation  twitter  web  website 
  Follow Us 
  kcap2013 | Due to extreme weather conditions in the region of Calgary we have had to cancel #KCAP2013  . Contact info @kcap2013  .org for more info. Pls RT 
  kcap2013 | iSOCO is one of our Gold sponsors at #KCAP2013  http://t.co/7wlBuMX2ni  #semanticweb  #artificialintelligence 
  kcap2013 | HP join the list of gold sponsors at #KCAP2013  http://t.co/ddB1ZuA2Kb  #semanticweb  #artificialintelligence 

 Home

95. VizSec_0 conference:
Toggle navigation          
 VizSec 2023 Home 
  Sponsorship 
  News 
  Past Years 
  Data 
  About 

  VizSec 2023  
 Welcome to 20th IEEE Symposium on Visualization for Cyber Security  
 VizSec 2023 will be held in conjunction with IEEE VIS Conference  on Sunday, October 22, 2023 2:00pm - 5:00pm in Melbourne, Australia. VizSec brings together researchers and practitioners in information visualization and security to address the specific needs of the cyber security community through new and insightful visualization techniques.  
 To register for VizSec, see the IEEE VIS registration page  .  
 Questions?  Please email chair@vizsec.org  for questions regarding VizSec 2023.  
  Program  
 All times are local Time (AEDT, UTC+11)  
 14:00 - 15:15   
 Session 1: Opening, Keynote and Best Paper  
  moderated by Lyndsey Franklin   

 14:00 - 14:10   
 Opening Remarks  by Lyndsey Franklin    
   
 14:10 - 14:55   
 Keynote:  
  Security-aware data provenance to support cybersecurity situational awareness  by Carsten Rudolph  , Monash University, Oceania Cyber Security Centre OCSC  .  
  Please see below  for bio.   
   
 14:55- 15:15   
 FuzzPlanner: Visually Assisting the Design of Firmware Fuzzing Campaigns  
  by Emilio Coppa, Alessio Izzillo, Riccardo Lazzeretti and Simone Lenti    
   
 15:15 - 15:25   
 Break    

 15:25 - 16:15   
 Session 2: Paper Session  
  moderated by tba   

 15:25- 15:40   
 Exploring the Representation of Cyber-Risk Data Through Sketching  
  by Thomas Miller, Miriam Sturdee and Daniel Prince    
   
 15:40- 15:55   
 PassViz: A Visualisation System for Analysing Leaked Passwords  
  by Samuel Charles Parker, Haiyue Yuan and Shujun Li    
   
 15:55- 16:05   
 Visualizing Comparisons of Bill of Materials  
  by Rebecca Jones and Lucas Tate    
   
 16:05- 16:15   
 Vis-SAGA: Visual Analytics for Situational Awareness of Grid Anomalies  
  by Graham Johnson, Kenny Gruchalla, Michael Ingram, Nalinrat Guba, Robert Cruickshank and Scott Caruso    
   
 16:15 - 16:25   
 Break    

 16:25 - 17:00   
 Session 3: Presentation Session  
  moderated by tba   

 16:25 - 16:40   
 Reimagining SOC Security Visualization with Cyber Next Platform  
  by Lalit Mohan Sanagavarapu    
   
 16:40 - 16:55   
 Network Narratives: Weaving Observations and Intentions in Security Visualization  
  by Larry Chan and Kuhu Gupta    
   
 16:55 - 17:00   
 Closing  
  by Lyndsey Franklin    

 Keynote  
 Security-aware data provenance to support cybersecurity situational awareness  
 Carsten Rudolph  
  Monash University, Oceania Cyber Security Centre OCSC   
 Abstract  
 Cybersecurity in distributed processes and systems often relies on a combination of individual local security controls and security protocols to protect communication. This means that in individual steps in the process there is limited situational awareness on the current security posture of the participating entities and on the protections applied to data in the process. Thus, people interacting with the systems don't have sufficient information to estimate cybersecurity risks at real-time. Security-aware provenance data can potentially improve this situation and existing research shows that various security meta-data is, in principle, available, but currently not used. One challenge is the sheer amount of available information and it is obvious that automation and suitable visualisation would be required to enable humans to digest this data and to react on changes in risks.  
  Biography  
 Carsten Rudolph is Professor for Cybersecurity and Deputy Dean of the Faculty of IT, Monash University, and Director for Research of the Oceania Cyber Security Centre OCSC in Melbourne, Australia. Before joining Monash, he was Head of Research Department for Trust and Compliance at the Fraunhofer Institute for Secure Information Technology SIT, Darmstadt, Germany and lead several large-scale European research initiatives. His research concentrates on information security, formal methods, cryptographic protocols, security of machine learning and human aspects of security with a strong focus interdisciplinary topics. He contributes to the development of secure solutions for different areas, such as digital health or future energy networks. Further, he drives scientific exchange between cybersecurity, law and organisational informatics. Another focus of his research is on nation-level cybersecurity maturity and policy development. Dr. Rudolph has established the OCSC and oversees the OCSC’s program for national cybersecurity maturity reviews in the Pacific region, a collaboration with Oxford University.  
   
  VizSec 2023 Call for Papers  
 We cordially invite you to participate in the 20th IEEE Symposium on Visualization for Cyber Security (VizSec) 2023. VizSec brings together researchers and practitioners from academia, government, and industry to address the needs of the cybersecurity community through new and insightful visualization and analysis techniques. VizSec provides an excellent venue for fostering greater exchange and new collaborations on a broad range of security- and privacy-related topics.  
 VizSec will be held as an Associated Event with IEEE VIS taking place October 22 - 27 in Melbourne, Australia. As an Associated Event, the VizSec program will be an in-person  event.  
 The purpose of VizSec is to explore effective and scalable visual interfaces for security domains such as network security, computer forensics, reverse engineering, insider threat detection, cryptography, privacy, user-assisted attacks prevention, compliance management, wireless security, secure coding, penetration testing, and other related research topics.  
 We are soliciting Technical (Full), Short, Position Papers and Presentation-Only (Non Archival) submissions.  
 Technical Papers  
 Full papers describing novel contributions in security visualization are solicited. Papers may present techniques, applications, theory, analysis, experiments, or evaluations. We encourage the submission of papers on technologies and methods that promise to improve cyber security practices, including, but not limited to:  
 Situation awareness and/or understanding 
  Incident handling including triage, exploration, correlation, and response 
  Computer forensics 
  Machine learning & explainable AI for cybersecurity 
  Adversarial machine learning visualization 
  Adversarial visualization 
  Visual analytics for cybersecurity 
  Data protection & privacy 
  Blockchain performance & security 
  Cybersecurity in critical infrastructure 
  Collaborative visualization for cybersecurity & provenance 
  Decision support for security configuration and deployment 
  Reverse engineering and malware analysis 
  Vulnerability management 
  Multiple data source analysis 
  Analyzing information requirements for computer network defense 
  Evaluation and/or user testing of systems 
  Criteria for assessing the effectiveness of cyber security visualizations, for security goals or human factors 
  System modeling and network analysis 
  Usable Security and Privacy 
  Software security 
  Mobile application security 
  Social networking privacy and security 
  Training & education 
  Visualization for privacy engineering 
   
  Submissions including tests and evaluations of the proposed tools and techniques are also considered particularly desirable. If possible, making test data available will also be considered positively. If you do not have real-world data to demonstrate your visualization, you may be interested in looking at the VizSec dataset links  . Please see past years  proceedings and websites to see the quality and examples of previous submissions. Previous VizSec papers may also be found on the VizSec Papers Browser  and on IEEE Xplore.  
 Short Papers  
 Short papers describing initial research results, concise research contributions, or incremental work on the above topics even including practical applications of security visualization, are solicited. We encourage the submission of papers discussing the introduction of cyber security visualizations into operational context including, but not limited to:  
 Cases where visualization made positive contributions towards meeting operational needs 
  Gaps or negative outcomes from visualization deployments 
  Situations where visualization was not utilized, but could have had a positive impact 
  Lessons learned from operational engagements 
  Insights gained from the transition process 
   
  Cyber security practitioners from industry, as well as the research community, are encouraged to submit case studies.  
 Position Papers  
 Position papers focus on topics needing or calling for discussion or reconsideration of forgotten topics relevant for the VizSec community. They should report a clear position on the target topic and should also suggest a proposal or actions regarding the target topic. They should stimulate imaginative and hypothesis-driven research. In particular, we encourage submissions both from researchers inside the visualization community and outside, such as researchers and practitioners from pure cybersecurity domains who currently do not broadly employ visualization in their work.  
 Presentation-Only  
 Presentation submissions are non-archival submissions and may describe late-breaking results, work in progress, preliminary results, demonstrations that present concrete examples of successful applications of visualization techniques in industry settings, practical challenges, insights gained from experience, best practices, future directions, and emerging trends relevant to the VizSec community. There will be a dedicated session where speakers can showcase the latest developments in the field, share their findings, present future directions and challenges, and discuss with conference participants. We encourage submissions from both academia and industry, as this will enable fruitful exchanges and foster collaboration between these two communities. Our goal is to create a forum where attendees can learn from each other, discuss emerging trends, and share insights on addressing practical challenges academia and industry face.  
 Awards  
 There will be an award for the best full paper from the accepted program. This award will be given to the paper judged to have the highest overall quality as determined by the program committee. Key elements of the selection process include whether papers include evaluation, repeatable results, and open-source data or software. We plan to recommend the best full paper in a special issue of the journal, IEEE Transactions on Visualization and Computer Graphics (TVCG).  
 Paper Submissions   
 The VizSec 2023 Proceedings will be published by IEEE. Submissions must be formatted using the IEEE VGTC conference proceedings template that can be found at: https://tc.computer.org/vgtc/publications/conference/   
 VizSec full papers are limited to 9 pages of content plus an additional 2 pages of references. Papers may be shorter than this but must make a similar contribution to a longer paper. Reviewers are not required to read the appendices or any pages past the maximum. Short papers must be at most 4 pages plus 1 page of references. Position papers must be at most 4 pages total including references.  
 Submissions not meeting these guidelines will be rejected without consideration of their merit. Reviews are single-blind, so authors may include names and affiliations in their submissions. Submitted papers must not substantially overlap papers that have been published or that are simultaneously submitted to a journal or a conference with proceedings. Authors of accepted papers must guarantee that their papers will be presented at the conference.  
 Guidelines for Artificial Intelligence (AI)-Generated Text   
  The use of artificial intelligence (AI)–generated text in an article shall be disclosed in the acknowledgements section of any paper submitted. The sections of the paper that use AI-generated text shall have a citation to the AI system used to generate the text.  
 How to submit:   
  Go to https://new.precisionconference.com/submissions  , then make a new submission to Society: VGTC, Conference/Journal: VIS 2023, Track: VIS 2023 VizSec  
 Presentation Submissions   
 Presentation-only submissions must be 2-page proposals including a presentation title, a brief presentation abstract (maximum 30 words), a description of the presentation topic, a description of the target audience, and biographies of the presenters. Presenters may also submit an accompanying slide or other supplementary materials by following the IEEE VIS formatting guidelines.  
 How to submit:   
  Go to https://new.precisionconference.com/submissions  , then make a new submission to Society: VGTC, Conference/Journal: VIS 2023, Track: VIS 2023 VizSec Presentation-only.  
  Important Dates  
 All deadlines are at 11:59pm (23:59) Anywhere on Earth (AoE).  
 Paper deadlines:  Extended!   
 June 30, 2023   July 12, 2023   Submission deadline for full, short, and position papers  
 July 21, 2023   Author notifications  
 Aug 08, 2023   Camera ready submissions and copyright forms due  
 Presentation deadlines:   
 Aug 11, 2023   Submission deadline presentation abstract  
 Sep 01, 2023   Presenter notifications  
 Oct 10, 2023   Presentation Draft Submission  
 VizSec will be held on Sunday, October 22, 2023  .  
   
 Committees  
 Organizing Committee  
 Lyndsey Franklin  , | General Chair | Pacific Northwest National Laboratory 
  Xumeng Wang, | Program Co-Chair | Nankai University 
  Aritra Dasgupta, | Program Co-Chair | New Jersey Institute of Technology 
  Adrian Komandina, | Publication Chair | University of Zagreb 
  Kuhu Gupta, | Presentation Track Chair | Illumio 
  TBA, | Publicity Chair 
  Alex Ulmer  , | Web Chair | Fraunhofer IGD 
  Program Committee  
 Arnav Aghav | Arizona State University 
  Marco Angelini | University of Rome "La Sapienza" 
  Anjana Arunkumar | Arizona State University 
  Kaustav Bhattacharjee | Pacific Northwest National Laboratory 
  Siming Chen | Fudan University 
  Robertas Damaševičiu | Kaunas University of Technology 
  Xiaohan Ding | Virginia Tech 
  Mai Elshehaly | University of London 
  Mohammad Ghoniem | Luxembourg Institute of Science and Technology 
  Steven Gomez | MIT Lincoln Laboratory 
  Robert Gove | CrowdStrike 
  Ridhima Gupta | Databricks 
  Jörn Kohlhammer | Fraunhofer IGD 
  Frédéric Majorczyk | DGA-MI 
  Jiacheng Pan | Zhejiang University 
  Christopher Simpson | National University 
  David Trimm | U.S. Department of Defense 
  Alex Ulmer | Fraunhofer IGD 
  Steering Committee  
 Marco Angelini | University of Rome La Sapienza 
  Dustin Arendt | Pacific Northwest National Laboratory 
  Chris Bryan | Arizona State University 
  Robert Gove | CrowdStrike 
  Jörn Kohlhammer | Fraunhofer IGD 
  Diane Staheli | MIT Lincoln Laboratory 
   Supporters  

   Get in Touch  
 Email chair@vizsec.org  for questions regarding the latest VizSec event. For general questions, email info@vizsec.org  or post to our Google group.   
  Follow @vizsec  on Twitter!  
   
  VizSec News  
 VizSec 2023 Call for Papers and Deadlines 
  VizSec 2023 Website 
  VizSec 2022 Program 
  VizSec 2022 Call for Papers and Deadlines 
  VizSec 2022 Website 
  VizSec 2021 Program 
  VizSec 2021 Call for Papers 
  VizSec 2021 Website 
  Inclusivity and Diversity scholarship 
  VizSec 2020 Registration 
  VizSec 2020 Call for Papers 
  VizSec 2019: Program Schedule and Posters Announced 
  Submission Deadline Extended to June 19th, 2019 
  Chris Oehmen to Keynote at VizSec 2019 
  VizSec 2019 Call for Papers 
  VizSec 2018 Videos and Proceeding Available 
  VizSec 2017 Videos Posted 
  VizSec 2017 Program Posted 
  VizSec 2017 Deadline Extended 
  VizSec 2017 Best Paper Award 
  VizSec 2017 Call for Papers 
  VizSec 2016 papers are in the IEEE DL 
  Program Schedule Available 
  Jay Jacob's to Provide Keynote 
  VizSec 2016 Registration 
  VizSec 2016 Call for Papers 
  VizSec 2015 papers are in the IEEE DL 
  Papers for VizSec 2015 
  VizSec 2015 Program Schedule 
  VizSec 2015 Call for Papers 
    
  Past VizSec Events  
 2022    Hybrid Conference, Oklahoma City, USA   2021    Virtual Conference   2020    Virtual Conference   2019    Vancouver, Canada   2018    Berlin, Germany   2017    Phoenix, AZ, USA   2016    Baltimore, MD, USA   2015    Chicago, Illinois, USA   2014    Paris, France   2013    Atlanta GA, USA   2012    Seattle, WA, USA   2011    Pittsburgh, PA, USA   2010    Ottawa, ON, Canada   2009    Atlantic City, NJ, USA   2008    Cambridge, MA, USA   2007    Sacramento, CA   2006    Washington, DC, USA   2005    Minneapolis, MN, USA   2004    Washington, DC, USA      
 View all »    

 © The VizSec team; content licensed under CC BY 3.0

96. K-CAP_3 conference:


97. VizSec_1 conference:
Toggle navigation          
 VizSec 2023 Home 
  Sponsorship 
  News 
  Past Years 
  Data 
  About 

 IEEE Symposium on Visualization for Cyber Security  

  VizSec  
 The IEEE Symposium on Visualization for Cyber Security (VizSec) is a forum that brings together researchers and practitioners from academia, government, and industry to address the needs of the cyber security community through new and insightful visualization and analysis techniques. VizSec provides an excellent venue for fostering greater exchange and new collaborations on a broad range of security and privacy related topics.  
 VizSec 2023 will be held on Sunday, October 22, 2023  , in conjunction with IEEE VIS  . Learn more about the event on the VizSec 2023 event page  .  

 Proceedings Browser  
 For a visual overview of past papers, please see the Proceedings Browser  from long-time contributor Fabian Fischer.  
   
   Get in Touch  
 Email chair@vizsec.org  for questions regarding the latest VizSec event. For general questions, email info@vizsec.org  or post to our Google group.   
  Follow @vizsec  on Twitter!  
   
  VizSec News  
 VizSec 2023 Call for Papers and Deadlines 
  VizSec 2023 Website 
  VizSec 2022 Program 
  VizSec 2022 Call for Papers and Deadlines 
  VizSec 2022 Website 
  VizSec 2021 Program 
  VizSec 2021 Call for Papers 
  VizSec 2021 Website 
  Inclusivity and Diversity scholarship 
  VizSec 2020 Registration 
  VizSec 2020 Call for Papers 
  VizSec 2019: Program Schedule and Posters Announced 
  Submission Deadline Extended to June 19th, 2019 
  Chris Oehmen to Keynote at VizSec 2019 
  VizSec 2019 Call for Papers 
  VizSec 2018 Videos and Proceeding Available 
  VizSec 2017 Videos Posted 
  VizSec 2017 Program Posted 
  VizSec 2017 Deadline Extended 
  VizSec 2017 Best Paper Award 
  VizSec 2017 Call for Papers 
  VizSec 2016 papers are in the IEEE DL 
  Program Schedule Available 
  Jay Jacob's to Provide Keynote 
  VizSec 2016 Registration 
  VizSec 2016 Call for Papers 
  VizSec 2015 papers are in the IEEE DL 
  Papers for VizSec 2015 
  VizSec 2015 Program Schedule 
  VizSec 2015 Call for Papers 
    
  Past VizSec Events  
 2022    Hybrid Conference, Oklahoma City, USA   2021    Virtual Conference   2020    Virtual Conference   2019    Vancouver, Canada   2018    Berlin, Germany   2017    Phoenix, AZ, USA   2016    Baltimore, MD, USA   2015    Chicago, Illinois, USA   2014    Paris, France   2013    Atlanta GA, USA   2012    Seattle, WA, USA   2011    Pittsburgh, PA, USA   2010    Ottawa, ON, Canada   2009    Atlantic City, NJ, USA   2008    Cambridge, MA, USA   2007    Sacramento, CA   2006    Washington, DC, USA   2005    Minneapolis, MN, USA   2004    Washington, DC, USA      
 View all »    

 © The VizSec team; content licensed under CC BY 3.0

98. W2GIS_0 conference:
JavaScript must be enabled to use the system

99. VISIGRAPP_3 conference:
Anmelden 
  Registrierung 
  Deutsch  English 
  Español 
  Português 
  Français 

     Dom 
  Najlepsze kategorie | CAREER & MONEY 
  PERSONAL GROWTH 
  POLITICS & CURRENT AFFAIRS 
  SCIENCE & TECH 
  HEALTH & FITNESS 
  LIFESTYLE 
  ENTERTAINMENT 
  BIOGRAPHIES & HISTORY 
  FICTION 
  Najlepsze historie 
  Najlepsze historie 
  Dodaj historię 
  Moje historie 

 Home 
  Computer Vision, Imaging and Computer Graphics Theory and Applications. 16th International Joint Conference, VISIGRAPP 2021 Virtual Event, February 8–10, 2021 Revised Selected Papers 9783031254765, 9783031254772 

 Computer Vision, Imaging and Computer Graphics Theory and Applications. 16th International Joint Conference, VISIGRAPP 2021 Virtual Event, February 8–10, 2021 Revised Selected Papers 9783031254765, 9783031254772   
   
  851    81    87MB    
  English   Pages [382]   Year 2023    
  Report DMCA / Copyright    
  DOWNLOAD FILE   
   
 Polecaj historie   

 Computer Vision, Imaging and Computer Graphics Theory and Applications: 14th International Joint Conference, VISIGRAPP 2019, Prague, Czech Republic, ... in Computer and Information Science, 1182) 3030415899, 9783030415891  
 This book constitutes thoroughly revised and selected papers from the 14th International Joint Conference on Computer Vi  
  158    89    Read more   

 Computer Vision, Imaging and Computer Graphics – Theory and Applications: 12th International Joint Conference, VISIGRAPP 2017, Porto, Portugal, February 27 – March 1, 2017, Revised Selected Papers [1st ed.] 978-3-030-12208-9, 978-3-030-12209-6  
 This book constitutes thoroughly revised and selected papers from the 12th International Joint Conference on Computer Vi  
  648    98    73MB    Read more   

 Computer Vision, Imaging and Computer Graphics Theory and Applications: 13th International Joint Conference, VISIGRAPP 2018 Funchal–Madeira, Portugal, January 27–29, 2018, Revised Selected Papers [1st ed.] 978-3-030-26755-1;978-3-030-26756-8  
 This book constitutes thoroughly revised and selected papers from the 13th International Joint Conference on Computer Vi  
  674    27    84MB    Read more   

 Biomedical Engineering Systems and Technologies. 14th International Joint Conference, BIOSTEC 2021 Virtual Event, February 11–13, 2021 Revised Selected Papers 9783031206634, 9783031206641  
  
  369    80    35MB    Read more   

 Robotics, Computer Vision and Intelligent Systems. First International Conference, ROBOVIS 2020, Virtual Event November 4–6, 2020, and Second International Conference ROBOVIS 2021, Virtual Event, October 27–28, 2021 Revised Selected Papers 9783031196492, 9783031196508  
  
  394    24    31MB    Read more   

 Information Systems and Design: Second International Conference, ICID 2021, Virtual Event, September 6–7, 2021, Revised Selected Papers (Communications in Computer and Information Science) 3030954935, 9783030954932  
 This volume constitutes selected papers from the Second International Conference on Information Systems and Design, ICID  
  122    33    Read more   

 Trends in Functional Programming: 22nd International Symposium, TFP 2021, Virtual Event, February 17–19, 2021, Revised Selected Papers (Lecture Notes in Computer Science, 12834) 303083977X, 9783030839772  
 This book constitutes revised selected papers from the 22nd International Symposium on Trends in Functional Programming,  
  151    51    6MB    Read more   

 Knowledge Discovery, Knowledge Engineering and Knowledge Management: 13th International Joint Conference, IC3K 2021 Virtual Event, October 25–27, 2021 Revised Selected Papers 3031359232, 9783031359231  
 This book constitutes the extended and revised versions of a set of selected papers from the 13th International Joint Co  
  569    82    11MB    Read more   

 Knowledge Discovery, Knowledge Engineering and Knowledge Management: 13th International Joint Conference, IC3K 2021, Virtual Event, October 25–27, 2021, Revised Selected Papers 9783031359248, 9783031359231, 3031359240  
 This book constitutes the extended and revised versions of a set of selected papers from the 13th International Joint Co  
  165    25    18MB    Read more   

 Computer-Human Interaction Research and Applications: 5th International Conference, CHIRA 2021, Virtual Event, October 28–29, 2021, and 6th International Conference, CHIRA 2022 Valletta, Malta, October 27–28, 2022, Revised Selected Papers 3031419618, 9783031419614  
 This post-conference book constitutes selected papers of the Fifth International Conference on Computer-Human Interactio  
  136    17    21MB    Read more   

 Author / Uploaded 
  A. Augusto de Sousa 
  Vlastimil Havran 
  Alexis Paljic 
  Tabitha Peck 
  Christophe Hurter 
  Helen Purchase 
  Giovanni Maria Farinella 
  Petia Radeva 
  Kadi Bouatouch 

 Table of contents :  
  Preface  
  Organization  
  Contents  
  Computer Graphics Theory and Applications  
  Impact of Avatar Representation in a Virtual Reality-Based Multi-user Tunnel Fire Simulator for Training Purposes  
  1 Introduction  
  2 Background  
  3 FrèjusVR  
  3.1 Devices and Technologies  
  3.2 Multi-user  
  3.3 Avatar Representation Techniques  
  3.4 Selected Procedure  
  4 Experimental Setup  
  4.1 Participants  
  4.2 Hardware  
  4.3 Methodology  
  4.4 Evaluation Criteria  
  5 Results and Discussion  
  5.1 Embodiment  
  5.2 Social Presence  
  5.3 Immersion and Presence  
  5.4 Direct Comparison  
  6 Conclusions and Future Work  
  References  
  Facade Layout Completion with Long Short-Term Memory Networks  
  1 Introduction  
  2 Related Work  
  3 Object Detection and Data Preparation  
  3.1 Random Data Generation  
  3.2 Object Detection  
  3.3 Data Preparation  
  4 Recurrent Neural Networks for Pattern Completion and Inpainting Mask Generation  
  4.1 Multi-Dimensional Long Short-Term Memory Network  
  4.2 Quasi Recurrent Neural Network  
  4.3 Rotated Multi-Dimensional Long Short-Term Memory Network  
  5 Network Training and Testing  
  6 Results  
  6.1 Layout Completion  
  6.2 Inpainting  
  6.3 Limitations  
  7 Conclusion and Future Work  
  References  
  Human Computer Interaction Theory and Applications  
  Generating Haptic Sensations over Spherical Surface  
  1 Introduction  
  2 Concept and Design of the Spherical Haptic Display  
  3 Methods  
  4 Results  
  4.1 Constructive Wave Interference  
  4.2 Combination of Peak Displacement Magnitudes  
  5 Providing Enhanced Immersion to Video Content Application Pilot Study  
  5.1 Pilot Study Background  
  5.2 Experimental Design  
  6 Experimental Procedure  
  7 Results  
  7.1 Pilot Study Discussion  
  7.2 Pilot Study Conclusion  
  8 Discussion and Future Work  
  9 Future Applications  
  10 Conclusions  
  References  
  Effects of Emotion-Induction Words on Memory and Pupillary Reactions While Viewing Visual Stimuli with Audio Guide  
  1 Introduction  
  2 Integration of Visual Information and Auditory Information  
  2.1 Memory Formation by Integrating Visual and Auditory Information  
  2.2 Emotion-Induction Words for Better Memory  
  3 Three Experiments for Investigating Effects of Emotion-Induction Words  
  3.1 Measurement Data  
  3.2 Word Impression Evaluation Experiment (Exp-W)  
  3.3 Sentence Impression Evaluation Experiment (Exp-S)  
  3.4 Video Appreciation Experiment (Exp-V)  
  4 Results  
  4.1 Impression of Emotion-Induction Words  
  4.2 Memory Score  
  5 Discussion  
  5.1 Memory Score Analysis  
  5.2 Pupil Diameter Analysis  
  5.3 Design Implications  
  6 Conclusion and Future Work  
  References  
  A Bimanual Flick-Based Japanese Software Keyboard Using Direct Kanji Input  
  1 Introduction  
  2 Related Work  
  3 Japanese Characters and Keyboards  
  3.1 Kana Characters  
  3.2 Elements of Kanji Characters  
  3.3 Japanese Keyboards  
  4 Proposed Method  
  4.1 Bimanual Flick  
  4.2 Kanji Layouts  
  4.3 Direct Kanji Input  
  4.4 Key Arrangement  
  4.5 Learning Support Functions  
  5 Implementation  
  6 Comparative Experiments  
  6.1 Sentence Input  
  6.2 Result of the Sentence Input Experiment  
  6.3 Kanji Conversion-Required Word Input  
  6.4 Result of the Kanji Conversion-Required Word Input Experiment  
  6.5 Subjective Evaluation and Participants' Comments  
  7 Experiment on Learning  
  7.1 Method  
  7.2 Result  
  7.3 Subjective Evaluation and Participants' Comments  
  8 Long-Term Experiment  
  8.1 Method  
  8.2 Result  
  9 Discussion  
  9.1 Comparative Experiment  
  9.2 Experiment on Learning  
  9.3 Long-Term Experiment  
  9.4 Direct Kanji Input  
  10 Conclusions and Future Work  
  References  
  Comparison of Cardiac Activity and Subjective Measures During Virtual Reality and Real Aircraft Flight  
  1 Introduction  
  2 Materials and Methods  
  2.1 Participants  
  2.2 Real Aircraft  
  2.3 Virtual Aircraft  
  2.4 Flight Scenario  
  2.5 Measures  
  2.6 Experimental Protocol  
  2.7 Data Analyses  
  3 Results  
  3.1 Flight Parameters  
  3.2 Subjective Measures  
  3.3 Physiological Measures  
  4 Discussion  
  4.1 Subjective Measures  
  4.2 Cardiac Activity  
  4.3 Motion Sickness  
  4.4 Conclusion  
  References  
  Information Visualization Theory and Applications  
  Improving Self-supervised Dimensionality Reduction: Exploring Hyperparameters and Pseudo-Labeling Strategies  
  1 Introduction  
  2 Background  
  3 SSNP Technique  
  4 Experimental Setup  
  4.1 Datasets  
  4.2 Projection Quality Metrics  
  4.3 Dimensionality Reduction Techniques Compared Against  
  4.4 Clustering Techniques for Pseudo-Labeling  
  4.5 Neural Network Hyperparameter Settings  
  5 Results  
  5.1 Quality on Synthetic Datasets  
  5.2 Quality on Real-World Datasets  
  5.3 Quality vs Clustering Hyperparameters  
  5.4 Quality vs Neural Network Settings  
  5.5 Computational Scalability  
  5.6 Inverse Projection  
  5.7 Data Clustering  
  5.8 Implementation Details  
  6 Discussion  
  7 Conclusion  
  References  
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
  1 Introduction  
  2 Related Work  
  3 Glyph Placement in a 2D Reference Space  
  3.1 Data Preprocessing  
  3.2 Latent Dirichlet Allocation on Source Code  
  3.3 Multidimensional Scaling  
  4 Visual Attributes of 3D Glyphs and Use Cases  
  4.1 City Metaphor  
  4.2 Forest Metaphor  
  4.3 Developer Avatars  
  5 System Design and Implementation Details  
  5.1 Layout Computation  
  5.2 Rendering  
  5.3 User Interaction  
  6 Conclusion  
  6.1 Discussion  
  6.2 Future Work  
  References  
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
  1 Introduction  
  2 The Region Packing Problem  
  2.1 Scale Measure  
  2.2 Region Alignment  
  2.3 Ordering Requirements  
  3 The Box Algorithm  
  4 The Rectpacking Algorithm  
  4.1 ONO Cases  
  5 The Improved Rectpacking Algorithm  
  5.1 Checking for Stackability  
  5.2 Revising the Width  
  6 A Constraint-Based Approach  
  7 Evaluation  
  7.1 Generated Models  
  7.2 Real World Models (SCCharts)  
  7.3 Hierarchical Graphs  
  8 Conclusion and Future Work  
  References  
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
  1 Introduction  
  2 Related Work  
  2.1 Health and Context Detection Using Smartphone-Sensed Data  
  2.2 Clustering Multivariate Data for Exploratory Data Analysis  
  2.3 Visual Analysis of Data Gathered In-the-Wild Using Ubiquitous Devices  
  3 Goal and Task Analysis  
  4 Our Visual Approach: PLEADES  
  4.1 Data Description and Clustering and Dimension Reduction Algorithm Selection and Features View  
  4.2 Clustering Results View (CRV)  
  4.3 Clusters View (CV)  
  4.4 Users View (UV)  
  4.5 F-Stats View (FSV)  
  4.6 Cluster Details View (CDV)  
  4.7 Daily Values View (DVV)  
  4.8 Feature Distribution Heatmap (FDH)  
  4.9 Saved Results View (SRV)  
  5 Evaluation with Use Cases  
  5.1 StudentLife (Dataset 1)  
  5.2 Use Case 1: Gaining a Quick Overview of the Data  
  5.3 Use Case 2: Examining Student Characteristics that Can Be Helpful for Insightful Comparisons  
  5.4 Use Case 3: Analyzing Graduate Vs. Undergraduate Students  
  5.5 ReadiSens (Dataset 2)  
  5.6 Use Case 4: Patterns of Presence in Primary Vs. Secondary Location  
  5.7 Use Case 5: Understanding Variances in Behaviors of Potentially Different Shift Workers  
  6 Evaluation with Experts  
  7 Discussion, Limitations and Future Directions  
  8 Conclusion  
  References  
  Towards Interactive Geovisualization Authoring Toolkit for Industry Use Cases  
  1 Introduction  
  2 Related Work  
  2.1 Geovisualization Types  
  2.2 Geovisualization Authoring Approaches  
  2.3 Limitations of Current Authoring Tools  
  3 Geovisto Prototype  
  4 Design Requirements Revision  
  4.1 Usability  
  4.2 Modularity  
  4.3 Configurability  
  4.4 Extensibility  
  4.5 Accessibility  
  5 Architecture  
  5.1 Core  
  5.2 API  
  6 Implementation  
  6.1 Layers  
  6.2 Controls and Utilities  
  7 Case Study: Logimic  
  8 Discussion  
  8.1 Advantages  
  8.2 Limitations  
  9 Conclusion and Future Work  
  9.1 Future Work  
  References  
  Computer Vision Theory and Applications  
  Global-first Training Strategy with Convolutional Neural Networks to Improve Scale Invariance  
  1 Introduction  
  2 Background  
  2.1 Global Features in Computer Vision  
  2.2 Pyramid Based Methods in CNNs for Scale-Invariant Classification  
  3 Stacked Filter CNN (SFCNN)  
  3.1 Stacked Filters Convolution (SFC) Module  
  3.2 Forward and Backward Propagation in SFC Module  
  4 Experiments  
  4.1 Datasets  
  4.2 VGG16 Network  
  4.3 SFC Hyper-Parameters  
  4.4 The SFCNN Network  
  4.5 Training Process  
  4.6 Scaled Images for Testing  
  4.7 Evaluation Metrics  
  5 Results and Discussion  
  5.1 Comparing Train and Test Statistics on Non-scaled Images  
  5.2 Evaluation of SFCNN on Scaled Images  
  6 Conclusion  
  References  
  Spline-Based Dense Medial Descriptors for Image Simplification Using Saliency Maps  
  1 Introduction  
  2 Related Work  
  2.1 Dense Medial Descriptors (DMD)  
  2.2 Spline-Based Medial Descriptors (SDMD)  
  2.3 Saliency Maps  
  3 Proposed 3S-DMD Method  
  3.1 User-Driven Saliency Map Generation  
  3.2 Saliency-Based Parameter Control  
  3.3 Saliency-Aware Quality Metric  
  4 Results  
  4.1 Evaluation Methodology  
  4.2 Effect of Parameters  
  4.3 Comparison with DMD and SDMD  
  4.4 Comparison with JPEG and JPEG 2000  
  5 Discussion  
  6 Conclusion  
  References  
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder for Multi-modal Neuroimaging Analysis  
  1 Introduction  
  2 Existing Work  
  2.1 Feature Extraction (FE)  
  2.2 Muti-view Representation Learning  
  2.3 Graph Representation Learning  
  3 Proposed Approach  
  3.1 Multi-modal Brain Graphs (BGs) Construction  
  3.2 Multi-modal Graph Auto-Encoder Based on the Attention Mechanism (MSCGATE)  
  3.3 Trace Regression Predictive Model  
  4 Experimental Results  
  4.1 InterTVA Data Description  
  4.2 Parameters Tuning  
  4.3 Performance Evaluation Metrics  
  4.4 Prediction Performance  
  5 Conclusion  
  References  
  Enhancing Backlight and Spotlight Images by the Retinex-Inspired Bilateral Filter SuPeR-B  
  1 Introduction  
  2 Background  
  3 SuPeR-B  
  4 Evaluation  
  4.1 Objective Evaluation  
  4.2 Subjective Evaluation  
  4.3 Comparison  
  5 Results  
  6 Conclusions  
  References  
  Rethinking RNN-Based Video Object Segmentation  
  1 Introduction  
  2 Related Work  
  3 Method  
  3.1 Hybrid Sequence-to-Sequence VOS  
  3.2 Bidirectional Architecture  
  3.3 Multi-task Training with Optical Flow Prediction  
  4 Experiments  
  4.1 Implementation Details  
  4.2 Experimental Results  
  4.3 Ablation on the Impact of Encoder Architecture  
  4.4 Ablation on the Impact of RNN Architecture  
  4.5 Ablation on Various Designs for the Fusion Block  
  5 Conclusion  
  References  
  Author Index   
 Citation preview   
  A. Augusto de Sousa · Vlastimil Havran · Alexis Paljic · Tabitha Peck · Christophe Hurter · Helen Purchase · Giovanni Maria Farinella · Petia Radeva · Kadi Bouatouch (Eds.) Communications in Computer and Information Science  
   
  1691  
   
  Computer Vision, Imaging and Computer Graphics Theory and Applications 16th International Joint Conference, VISIGRAPP 2021 Virtual Event, February 8–10, 2021 Revised Selected Papers  
   
  Communications in Computer and Information Science Editorial Board Members Joaquim Filipe Polytechnic Institute of Setúbal, Setúbal, Portugal Ashish Ghosh Indian Statistical Institute, Kolkata, India Raquel Oliveira Prates Federal University of Minas Gerais (UFMG), Belo Horizonte, Brazil Lizhu Zhou Tsinghua University, Beijing, China  
   
  1691  
   
  More information about this series at https://link.springer.com/bookseries/7899  
   
  A. Augusto de Sousa · Vlastimil Havran · Alexis Paljic · Tabitha Peck · Christophe Hurter · Helen Purchase · Giovanni Maria Farinella · Petia Radeva · Kadi Bouatouch (Eds.)  
   
  Computer Vision, Imaging and Computer Graphics Theory and Applications 16th International Joint Conference, VISIGRAPP 2021 Virtual Event, February 8–10, 2021 Revised Selected Papers  
   
  Editors A. Augusto de Sousa University of Porto Porto, Portugal  
   
  Vlastimil Havran Czech Technical University in Prague Prague, Czech Republic  
   
  Alexis Paljic Mines ParisTech Paris, France  
   
  Tabitha Peck Davidson College Davidson, NC, USA  
   
  Christophe Hurter French Civil Aviation University (ENAC) Toulouse, France  
   
  Helen Purchase Monash University Melbourne, Australia  
   
  Giovanni Maria Farinella University of Catania Catania, Italy Kadi Bouatouch IRISA, University of Rennes 1 Rennes, France  
   
  University of Glasgow Glasgow, UK Petia Radeva University of Barcelona Barcelona, Spain  
   
  ISSN 1865-0929 ISSN 1865-0937 (electronic) Communications in Computer and Information Science ISBN 978-3-031-25476-5 ISBN 978-3-031-25477-2 (eBook) https://doi.org/10.1007/978-3-031-25477-2 © Springer Nature Switzerland AG 2023 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland  
   
  Preface  
   
  This book includes extended and revised versions of a set of selected papers from the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2021), that was exceptionally held as an online event, due to COVID-19, on February 8–10, 2021. The purpose of VISIGRAPP is to bring together researchers and practitioners interested in both theoretical advances and applications of computer vision, computer graphics and information visualization. VISIGRAPP is composed of four co-located conferences, each specialized in at least one of the aforementioned main knowledge areas, namely GRAPP, IVAPP, HUCAPP and VISAPP. VISIGRAPP 2021 received 371 paper submissions from 53 countries, of which 4% were included in this book. The papers were selected by the event chairs and their selection is based on a number of criteria, including the classifications and comments provided by the program committee members, the session chairs’ assessment and also the program chairs’ global view of all papers included in the technical program. The authors of selected papers were then invited to submit a revised and extended version of their papers having at least 30% innovative material. The papers selected to be included in this book contribute to the understanding of relevant trends of current research on Computer Vision, Imaging and Computer Graphics Theory and Applications, including: Deep Learning for Visual Understanding, HighDimensional Data and Dimensionality Reduction, Information Visualization, Interactive Systems for Education and Training, Machine Learning Technologies for Vision, Software Visualization, Visualization Applications, Evaluation Paradigms and Frameworks, Early and Biologically-Inspired Vision, Deep Learning for Tracking, Impact of Avatar Representation in Virtual Reality-Based Simulations, and Techniques for Automatic 3D Cities Modeling. We would like to thank all the authors for their contributions and also the reviewers who helped ensuring the quality of this publication. February 2021  
   
  A. Augusto de Sousa Vlastimil Havran Alexis Paljic Tabitha Peck Christophe Hurter Helen Purchase Giovanni Maria Farinella Petia Radeva Kadi Bouatouch  
   
  Organization  
   
  Conference Co-chairs Jose Braz Kadi Bouatouch  
   
  Escola Superior de Tecnologia de Setúbal, Portugal IRISA, University of Rennes 1, France  
   
  Program Co-chairs GRAPP A. Augusto de Sousa Vlastimil Havran  
   
  FEUP/INESC TEC, Portugal Czech Technical University in Prague, Czech Republic  
   
  HUCAPP Alexis Paljic Tabitha Peck  
   
  Mines ParisTech, France Davidson College, USA  
   
  IVAPP Christophe Hurter Helen Purchase  
   
  French Civil Aviation University (ENAC), France Monash University, Australia and University of Glasgow, UK  
   
  VISAPP Giovanni Maria Farinella Petia Radeva  
   
  Università di Catania, Italy Universitat de Barcelona, Spain  
   
  GRAPP Program Committee Francisco Abad Marco Agus Benjamin Bringier Dimitri Bulatov  
   
  Maria Beatriz Carmo  
   
  Universidad Politécnica de Valencia, Spain Hamad Bin Khalifa University, Qatar Université de Poitiers, France Fraunhofer IOSB, Ettlingen - Fraunhofer Institute of Optronics, System Technologies and Image Exploitation, Germany LASIGE, Faculdade de Ciências, Universidade de Lisboa, Portugal  
   
  viii  
   
  Organization  
   
  Ozan Cetinaslan Parag Chaudhuri Antonio Chica Hwan-gue Cho Teodor Cioaca Ana Paula Cláudio António Coelho Vasco Costa Remi Cozot Paulo Dias Elmar Eisemann Marius Erdt Pierre-Alain Fayolle Jie Feng António Fernandes Leandro Fernandes Ioannis Fudos Davide Gadia Fabio Ganovelli Ignacio García-Fernández Miguel Gea Guillaume Gilet Enrico Gobbetti Stephane Gobron Abel Gomes Alexandrino Gonçalves Daniel Gonçalves Damiand Guillaume Marcelo Guimarães James Hahn Nancy Hitschfeld Sébastien Horna Samuel Hornus Andres Iglesias Alberto Jaspe-Villanueva Jean-Pierre Jessel Juan José Jiménez-Delgado  
   
  Instituto de Telecomunicações & University of Porto, Portugal Indian Institute of Technology Bombay, India Universitat Politècnica de Catalunya, Spain Pusan National University, Korea, Republic of SimCorp GmbH, Germany LASIGE, Faculdade de Ciências, Universidade de Lisboa, Portugal Faculdade de Engenharia da Universidade do Porto, Portugal INESC-ID, Portugal LISIC, University of Littoral, France Universidade de Aveiro, Portugal Delft University of Technology, The Netherlands Fraunhofer IDM@NTU, Singapore University of Aizu, Japan Peking University, China Universidade do Minho, Portugal Universidade Federal Fluminense, Brazil University of Ioannina, Greece Università Degli Studi Di Milano, Italy CNR, Italy Universidad de Valencia, Spain University of Granada, Spain Université de Sherbrooke, Canada CRS4, Italy HES-SO/Arc, Switzerland Universidade da Beira Interior, Portugal CIIC, ESTG, Polytechnic of Leiria, Portugal INESC-ID, Instituto Superior Técnico, Portugal CNRS/LIRIS, France Federal University of São Paulo/Master Program of Faculdade Campo Limpo Paulista, Brazil George Washington University, USA University of Chile, Chile University of Poitiers, France Inria, France University of Cantabria, Spain King Abdullah University of Science and Technology (KAUST), Saudi Arabia IRIT, Paul Sabatier University, Toulouse, France Universidad de Jaen, Spain  
   
  Organization  
   
  Xiaogang Jin Cláudio Jung Josef Kohout Maciej Kot Alejandro León Marco Livesu Helio Lopes Claus Madsen Luis Magalhães Stephen Mann Adérito Marcos Maxime Maria Ricardo Marroquim Miguel Melo Daniel Mendes Daniel Meneveaux Paulo Menezes Stéphane Mérillou João Moura Adolfo Muñoz Rui Nóbrega Deussen Oliver Lidia M. Ortega Georgios Papaioannou Félix Paulano-Godino Aruquia Peixoto João Pereira Christoph Peters Adrien Peytavie Paulo Pombinho Tomislav Pribanic Anna Puig Inmaculada Remolar Mickael Ribardière María Rivara Juan Roberto Jiménez Nuno Rodrigues  
   
  ix  
   
  Zhejiang University, China Universidade Federal do Rio Grande do Sul, Brazil University of West Bohemia, Czech Republic Dfinity, Japan University of Granada, Spain Italian National Research Council (CNR), Italy PUC-Rio, Brazil Aalborg University, Denmark Universidade do Minho, Portugal University of Waterloo, Canada University of Saint Joseph, Macao, China Université de Limoges - XLIM, France Delft University of Technology, The Netherlands INESC TEC, Portugal INESC-ID Lisboa, Portugal University of Poitiers, France Universidade de Coimbra, Portugal University of Limoges, France Universidade de Trás-os-Montes e Alto Douro, Portugal Universidad de Zaragoza, Spain DEI-FEUP, INESC TEC, Universidade do Porto, Portugal University of Konstanz, Germany University of Jaén, Spain Athens University of Economics and Business, Greece University of Jaén, Spain CEFET/RJ, Brazil Instituto Superior de Engenharia do Porto, Portugal KIT, Germany Claude Bernard University Lyon 1, France Universidade de Lisboa, Portugal University of Zagreb, Croatia University of Barcelona, Spain Universitat Jaume I, Spain University of Poitiers, XLIM, France Universidad de Chile, Chile University of Jaén, Spain Polytechnic Institute of Leiria, Portugal  
   
  x  
   
  Organization  
   
  Inmaculada Rodríguez Przemyslaw Rokita Teresa Romão Luís Romero Holly Rushmeier Beatriz Santos Luis Santos Basile Sauvage Vladimir Savchenko Rafael J. Segura Ricardo Sepulveda Marques Ana Serrano Frutuoso Silva Matthias Teschner Daniel Thalmann Torsten Ullrich Kuwait University Carlos Urbano Anna Ursyn Ling Xu Rita Zrour  
   
  University of Barcelona, Spain Warsaw University of Technology, Poland Universidade de Nova Lisboa, Portugal Instituto Politecnico de Viana do Castelo, Portugal Yale University, USA University of Aveiro, Portugal Universidade do Minho, Portugal University of Strasbourg, France Hosei University, Japan Universidad de Jaen, Spain Universitat de Barcelona, Spain Max Planck Institute for Informatics, Germany University of Beira Interior, Portugal University of Freiburg, Germany Ecole Polytechnique Federale de Lausanne, Switzerland Fraunhofer Austria Research GmbH, Austria Kuwait University, Kuwait Instituto Politécnico de Leiria, Portugal University of Northern Colorado, USA University of Houston-Downtown, USA XLIM, France  
   
  GRAPP Additional Reviewers Xavier Chermain Sandra Malpica Gustavo Patow  
   
  University of Strasbourg, France Universidad de Zaragoza, Spain Universitat de Girona, Spain  
   
  HUCAPP Program Committee Andrea Abate Cigdem Beyan Federico Botella Eva Cerezo Mathieu Chollet Yang-Wai Chow Cesar Collazos Damon Daylamani-Zad Juan Enrique Garrido Navarro Toni Granollers Michael Hobbs  
   
  University of Salerno, Italy Istituto Italiano di Tecnologia, Italy Miguel Hernandez University of Elche, Spain University of Zaragoza, Spain University of Glasgow, UK University of Wollongong, Australia Universidad del Cauca, Colombia Brunel University London, UK University of Lleida, Spain University of Lleida, Spain Deakin University, Australia  
   
  Organization  
   
  Francisco Iniesto Alvaro Joffre Uribe Quevedo Ahmed Kamel Chee Weng Khong Suzanne Kieffer Uttam Kokil Fabrizio Lamberti Chien-Sing Lee Tsai-Yen Li Flamina Luccio Sergio Lujan Mora José Macías Iglesias Guido Maiello Malik Mallem Troy McDaniel Vincenzo Moscato Evangelos Papadopoulos Florian Pecune Otniel Portillo-Rodriguez Brian Ravenet Juha Röning Andrea Sanna Trenton Schulz Alessandra Sciutti Fabio Solari Daniel Thalmann Gouranton Valérie Gualtiero Volpe  
   
  The Open University (UK) Centre for Research in Education and Educational Technology, UK University of Ontario Institute of Technology, Canada Concordia College, USA Multimedia University, Malaysia Université catholique de Louvain, Belgium Kennesaw State University, USA Politecnico di Torino, Italy Sunway University, Malaysia National Chengchi University, Taiwan, Republic of China Università Ca’ Foscari Venezia, Italy Universidad de Alicante, Spain Universidad Autónoma de Madrid, Spain Justus-Liebig University Gießen, Germany Université Paris Saclay, France Arizona State University, USA Università degli Studi di Napoli Federico II, Italy NTUA, Greece Glasgow University, UK Universidad Autonóma del Estado de México, Mexico Université Paris-Saclay - LIMSI-CNRS, France University of Oulu, Finland Politecnico di Torino, Italy Norwegian Computing Center, Norway Istituto Italiano di Tecnologia, Italy University of Genoa, Italy Ecole Polytechnique Federale de Lausanne, Switzerland Univ. Rennes, INSA Rennes, Inria, CNRS, IRISA, France Università degli Studi di Genova, Italy  
   
  HUCAPP Additional Reviewers Mattia Barbieri Antoine Costes Louise Devigne Jérémy Lacoche Maria Elena Lechuga Redondo  
   
  xi  
   
  Istituto Italiano di Tecnologia, Italy Inria, France INSA Rennes/Inria/IRISA, France Orange, France Istituto Italiano di Tecnologia, Italy  
   
  xii  
   
  Organization  
   
  IVAPP Program Committee Daniel Archambault Ayan Biswas David Borland Alexander Bornik  
   
  Swansea University, UK Los Alamos National Laboratory, USA University of North Carolina at Chapel Hill, USA Ludwig Boltzmann Institute for Archaeological Prospection and Virtual Archaeology, Austria Romain Bourqui University of Bordeaux, France Michael Burch University of Applied Sciences, Switzerland Guoning Chen University of Houston, USA Yongwan Chun University of Texas at Dallas, USA António Coelho Faculdade de Engenharia da Universidade do Porto, Portugal Danilo B. Coimbra Federal University of Bahia, Brazil Celmar da Silva University of Campinas, Brazil Georgios Dounias University of the Aegean, Greece Achim Ebert University of Kaiserslautern, Germany Mennatallah El-Assady University of Konstanz, Germany Danilo Eler São Paulo State University, Brazil Sara Fabrikant University of Zurich - Irchel, Switzerland Maria Cristina Ferreira de Oliveira University of São Paulo, ICMC, Brazil Johannes Fuchs University of Konstanz, Germany Enrico Gobbetti CRS4, Italy Randy Goebel University of Alberta, Canada Martin Graham Edinburgh Napier University, UK Lynda Hardman Centrum Wiskunde & Informatica; Utrecht University, The Netherlands Christian Heine Leipzig University, Germany Seokhee Hong University of Sydney, Australia Torsten Hopp Karlsruhe Institute of Technology, Germany Jie Hua University of Technology Sydney, Australia Jimmy Johansson Linköping University, Sweden Mark Jones Swansea University, UK Daniel Jönsson Linköping University, Sweden Ilir Jusufi Blekinge Institute of Technology, Sweden Bijaya Karki Louisiana State University, USA Sehwan Kim WorldViz LLC, USA Steffen Koch Universität Stuttgart, Germany Martin Kraus Aalborg University, Denmark Haim Levkowitz University of Massachusetts, Lowell, USA Giuseppe Liotta University of Perugia, Italy Rafael Martins Linnaeus University, Sweden Brescia Massimo INAF, Italy  
   
  Organization  
   
  Eva Mayr Kazuo Misue Ingela Nystrom Benoît Otjacques Jinah Park Fernando Paulovich Luc Pauwels Jürgen Pfeffer Renata Raidou Philip Rhodes Maria Riveiro Beatriz Santos Angel Sappa Gerik Scheuermann Falk Schreiber Juergen Symanzik Roberto Theron Christian Tominski Tea Tušar Thomas van Dijk Gilles Venturini Gunther Weber Marcel Worring Hsian-Yun Wu Hsu-Chun Yen Jianping Zeng Yue Zhang  
   
  Danube University Krems, Austria University of Tsukuba, Japan Uppsala University, Sweden Luxembourg Institute of Science and Technology (LIST), Luxembourg KAIST, Korea, Republic of Eindhoven University of Technology, The Netherlands University of Antwerp, Belgium Technische Universität München, Germany University of Groningen, The Netherlands University of Mississippi, USA University of Skövde, Sweden University of Aveiro, Portugal ESPOL Polytechnic University (Ecuador) and Computer Vision Center (Spain), Spain Universität Leipzig, Germany University of Konstanz, Germany and Monash University, Melbourne, Australia Utah State University, USA Universidad de Salamanca, Spain University of Rostock, Germany Jožef Stefan Institute, Slovenia Ruhr-Universität Bochum, Germany University of Tours, France Lawrence Berkeley National Laboratrory/UC Davis, USA University of Amsterdam, The Netherlands TU Wien, Austria National Taiwan University, Taiwan, Republic of China Microsoft, USA Oregon State University, USA  
   
  IVAPP Additional Reviewers Tiago Araújo Arindam Bhattacharya Romain Giot Jan-Henrik Haunert Stefan Jänicke Romain Vuillemot  
   
  xiii  
   
  Federal University of Pará, Brazil Independent Researcher, USA Université de Bordeaux, France University of Bonn, Germany University of Southern Denmark, Denmark LIRIS, France  
   
  xiv  
   
  Organization  
   
  VISAPP Program Committee Amr Abdel-Dayem Zahid Akhtar Vicente Alarcon-Aquino Enrique Alegre Dario Allegra Hugo Alvarez Danilo Avola Ariel Bayá Annà Belardinelli Fabio Bellavia Robert Benavente Yannick Benezeth Stefano Berretti Simone Bianco Ioan Marius Bilasco Giuseppe Boccignone Adrian Bors Larbi Boubchir Thierry Bouwmans Marius Brezovan Alfred Bruckstein Arcangelo Bruna Vittoria Bruni Giedrius Burachas Adrian Burlacu Alice Caplier Bruno Carvalho Dario Cazzato Oya Celiktutan Luigi Celona Krishna Chandramouli Ming-ching Chang Mulin Chen Samuel Cheng Manuela Chessa Chia Chong Kazimierz Choros Laurent Cohen Sara Colantonio  
   
  Laurentian University, Canada University of Memphis, USA Universidad de las Americas Puebla, Mexico Universidad de Leon, Spain University of Catania, Italy Vicomtech, Spain Sapienza University, Italy CONICET, Argentina Honda Research Institute, Germany Università degli Studi di Firenze, Italy Universitat Autònoma de Barcelona, Spain Université de Bourgogne, France University of Florence, Italy University of Milano Bicocca, Italy Lille University, France Università degli Studi di Milano, Italy University of York, UK University of Paris 8, France Université de La Rochelle, France University of Craiova, Romania Technion, Israel STMicroelectronics, Italy University of Rome La Sapienza, Italy SRI International, USA Gheorghe Asachi Technical University of Iasi, Romania GIPSA-lab, France Federal University of Rio Grande do Norte, Brazil Université du Luxembourg, Luxembourg King’s College London, UK University of Milan-Bicocca, Italy Queen Mary University of London, UK State University of New York, at Albany, USA Xidian University, China University of Oklahoma, USA University of Genoa, Italy Sunway University, Malaysia Wroclaw University of Science and Technology, Poland Université Paris Dauphine, France ISTI-CNR, Italy  
   
  Organization  
   
  Carlo Colombo Donatello Conte Nicholas Costen Pedro Couto António Cunha Claudio Cusano Mohammad Reza Daliri Christoph Dalitz Radu Danescu Roy Davies Kenneth Dawson-Howe Pedro de Rezende Konstantinos Delibasis Joachim Denzler Sotirios Diamantas Yago Diez Jana Dittmann Peter Eisert Mahmoud El-Sakka Shu-Kai Fan Hui Fang Mylène Farias Jorge Fernández-Berni  
   
  Laura Fernández-Robles Laura Florea Ignazio Gallo Alba Garcia Seco De Herrera Antonios Gasteratos Andrea Giachetti Dimitris Giakoumis Valerio Giuffrida Luiz Goncalves Manuel González-Hidalgo  
   
  xv  
   
  Università degli Studi di Firenze, Italy Université de Tours, France Manchester Metropolitan University, UK University of Trás-os-Montes e Alto Douro, Portugal UTAD - Universidade de Trás-os-Montes e Alto Douro, Portugal University of Pavia, Italy Iran University of Science and Technology, Iran Niederrhein University of Applied Sciences, Germany Technical University of Cluj-Napoca, Romania Royal Holloway, University of London, UK Trinity College Dublin, Ireland University of Campinas, Brazil University of Thessaly, Greece Friedrich Schiller University of Jena, Germany Tarleton State University, Texas A&M University System, USA Yamagata University, Japan Otto-von-Guericke-Universität Magdeburg, Germany Fraunhofer Institut für Nachrichtentechnik, Germany University of Western Ontario, Canada National Taipei University of Technology, Taiwan, Republic of China University of Loughborough, UK University of Brasilia, Brazil Institute of Microelectronics of Seville (IMSE-CNM), CSIC - Universidad de Sevilla, Spain Universidad de Leon, Spain University Politehnica of Bucharest, Romania University of Insubria, Varese, Italy University of Essex, UK Democritus University of Thrace, Greece Università di Verona, Italy CERTH-ITI, Aristotle University of Thessaloniki, Greece Edinburgh Napier University, UK Federal University of Rio Grande do Norte, Brazil Balearic Islands University, Spain  
   
  xvi  
   
  Organization  
   
  Bart Goossens Håkan Grahn Nikos Grammalidis Michael Greenspan Christiaan Gribble Levente Hajder Walid Hariri Aymeric Histace Wladyslaw Homenda Binh-Son Hua Hui-Yu Huang Laura Igual Francisco Imai Jiri Jan Tatiana Jaworska Xiaoyi Jiang Lucio Andre Jorge Martin Kampel Kenichi Kanatani Etienne Kerre Anastasios Kesidis Nahum Kiryati Nobuyuki Kita Itaru Kitahara Andrey Kopylov Adam Kortylewski Camille Kurtz Demetrio Labate Martin Lambers Mónica Larese Denis Laurendeau Isah A. Lawal Dah-jye Lee Marco Leo Xiuwen Liu Giosue Lo Bosco Angeles López Cristina Losada-Gutiérrez  
   
  imec - Ghent University, Belgium Blekinge Institute of Technology, Sweden Centre of Research and Technology Hellas, Greece Queen’s University, Canada Advanced Micro Devices, Inc., USA Eötvös Loránd University, Hungary Badji Mokhtar Annaba University, Algeria ETIS UMR CNRS 8051, France Warsaw University of Technology, Poland VinAI, Vietnam National Formosa University, Taiwan, Republic of China Universitat de Barcelona, Spain Apple Inc., USA University of Technology Brno, Czech Republic Polish Academy of Sciences, Poland University of Münster, Germany EMBRAPA, Brazil Vienna University of Technology, Austria Okayama University, Japan Ghent University, Belgium University of West Attica, Greece Tel Aviv University, Israel AIST (National Institute of Advanced Industrial Science and Technology), Japan University of Tsukuba, Japan Tula State University, Russian Federation Johns Hopkins University, USA Université de Paris, LIPADE, France University of Houston, USA University of Siegen, Germany CIFASIS-CONICET, National University of Rosario, Argentina Laval University, Canada Noroff University College, Norway Brigham Young University, USA CNR, Italy Florida State University, USA Università di Palermo, Italy Universitat Jaume I, Spain University of Alcalá, Spain  
   
  Organization  
   
  Ilias Maglogiannis Baptiste Magnier Emmanuel Marilly Jean Martinet José Martínez Sotoca Mitsuharu Matsumoto Mohamed Arezki Mellal Leonid Mestetskiy Cyrille Migniot Dan Mikami Steven Mills Filippo Milotta Pradit Mittrapiyanuruk Birgit Moeller Davide Moltisanti Bartolomeo Montrucchio Kostantinos Moustakas Dmitry Murashov  
   
  Yuta Nakashima Mikael Nilsson Shohei Nobuhara Yoshihiro Okada Félix Paulano-Godino Helio Pedrini Francisco José Perales Roland Perko Stephen Pollard Vijayakumar Ponnusamy Charalambos Poullis Antonis Protopsaltis Giovanni Puglisi Kumaradevan Punithakumar Naoufal Raissouni V. Rajinikanth Giuliana Ramella Francesco Rea  
   
  xvii  
   
  University of Piraeus, Greece LGI2P de l’Ecole des Mines d’ALES, France NOKIA - Bell Labs France, France University Cote d’Azur/CNRS, France Universitat Jaume I, Spain University of Electro-Communications, Japan M’Hamed Bougara University, Algeria Lomonosov Moscow State University, Russian Federation Université de Bourgogne - ImViA, France NTT, Japan University of Otago, New Zealand University of Catania, Italy Autodesk, Singapore Martin Luther University Halle-Wittenberg, Germany Nanyang Technological University, Singapore Politecnico di Torino, Italy University of Patras, Greece Federal Research Center "Computer Science and Control" of Russian Academy of Sciences, Russian Federation Osaka University, Japan Lund University, Sweden Kyoto University, Japan Kyushu University, Japan University of Jaén, Spain University of Campinas, Brazil UIB, Spain Joanneum Research, Austria HP Labs, UK SRM IST, Kattankulathur Campus, India Concordia University, Canada University of Western Macedonia, Greece University of Cagliari, Italy University of Alberta, Canada University Abdelmalek Essaadi (ENSA Tetuan), Morocco St. Josephs College Engineering, India CNR - Istituto per le Applicazioni del Calcolo “M. Picone”, Italy Istituto Italiano di Tecnologia, Italy  
   
  xviii  
   
  Organization  
   
  Joao Rodrigues Peter Rogelj Juha Röning Pedro Rosa Silvio Sabatini Ovidio Salvetti Andreja Samcovic K. C. Santosh Nickolas Sapidis Yann Savoye Marco Seeland Siniša Šegvic Oleg Seredin Fiorella Sgallari Shishir Shah Seppo Sirkemaa Robert Sitnik Andrzej Skalski Kenneth Sloan Ömer Soysal Amelia Carolina Sparavigna Mu-Chun Su Ryszard Tadeusiewicz Norio Tagawa Ricardo Torres Bruno Travençolo Carlos Travieso-González Du-Ming Tsai Javier Vazquez-Corral Luisa Verdoliva Nicole Vincent Panayiotis Vlamos Frank Wallhoff Tao Wang Wen-June Wang Laurent Wendling  
   
  University of the Algarve, Portugal University of Primorska, Slovenia University of Oulu, Finland Universidade Lusófona de Humanidades e Tecnologias de Lisboa, Portugal University of Genoa, Italy National Research Council of Italy - CNR, Italy University of Belgrade, Serbia University of South Dakota, USA University of Western Macedonia, Greece Liverpool John Moores University, UK Ilmenau University of Technology, Germany University of Zagreb, Croatia Tula State University, Russian Federation University of Bologna, Italy University of Houston, USA University of Turku, Finland Warsaw University of Technology, Poland AGH University of Science and Technology, Poland University of Alabama at Birmingham, USA Southeastern Louisiana University, USA Polytechnic University of Turin, Italy National Central University, Taiwan, Republic of China AGH University Science Technology, Poland Tokyo Metropolitan University, Japan Norwegian University of Science and Technology (NTNU), Norway Federal University of Uberlândia, Brazil Universidad de Las Palmas de Gran Canaria, Spain Yuan-Ze University, Taiwan, Republic of China University Pompeu Fabra, Spain University Federico II of Naples, Italy Université de Paris, France Ionian University, Greece Jade University of Applied Science, Germany BAE Systems, USA National Central University, Taiwan, Republic of China Paris Descartes University, France  
   
  Organization  
   
  Christian Wöhler Pengcheng Xi Jiangjian Xiao Alper Yilmaz Jang-Hee Yoo Sebastian Zambanini Pietro Zanuttigh Jie Zhang Zhigang Zhu Ju Zou  
   
  TU Dortmund University, Germany National Research Council Canada, Canada Ningbo Institute Material Technology & Engineering, CAS, China Ohio State University, USA ETRI, Korea, Republic of TU Wien, Austria University of Padova, Italy Newcastle University, UK City College of New York, USA University of Western Sydney, Australia  
   
  VISAPP Additional Reviewers George Azzopardi Yuwei Chen Gianluigi Ciocca Mariella Dimiccoli Fabio Galasso Luca Garello Andreas Kloukiniotis Riccardo La Grassa Nicola Landro Xiao Lin Fatima Saiz Álvaro Nikos Stagakis Filippo Stanco Rameez Ur Rahman  
   
  University of Groningen, The Netherlands and University of Malta, Malta SUNY Albany, USA University of Milano-Bicocca, Italy Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Spain Sapienza University of Rome, Italy Istituto Italiano di Tecnologia, Italy University of Patras, Greece University of Insubria, Italy University of Insubria, Italy Vicomtech, Spain Vicomtech, Spain University of Patras, Greece Università di Catania, Italy University of Rome, Sapienza, Italy  
   
  Invited Speakers Federico Tombari Dieter Schmalstieg Nathalie Henry Riche  
   
  Google and Technical University of Munich (TUM), Germany Graz University of Technology, Austria Microsoft Research, USA  
   
  xix  
   
  Contents  
   
  Computer Graphics Theory and Applications Impact of Avatar Representation in a Virtual Reality-Based Multi-user Tunnel Fire Simulator for Training Purposes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Davide Calandra, Filippo Gabriele Pratticò, Gianmario Lupini, and Fabrizio Lamberti Facade Layout Completion with Long Short-Term Memory Networks . . . . . . . . . Simon Hensel, Steffen Goebbels, and Martin Kada  
   
  3  
   
  21  
   
  Human Computer Interaction Theory and Applications Generating Haptic Sensations over Spherical Surface . . . . . . . . . . . . . . . . . . . . . . . Patrick Coe, Grigori Evreinov, Mounia Ziat, and Roope Raisamo Effects of Emotion-Induction Words on Memory and Pupillary Reactions While Viewing Visual Stimuli with Audio Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . Mashiho Murakami, Motoki Shino, Munenori Harada, Katsuko T. Nakahira, and Muneo Kitajima A Bimanual Flick-Based Japanese Software Keyboard Using Direct Kanji Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Yuya Nakamura and Hiroshi Hosobe  
   
  43  
   
  69  
   
  90  
   
  Comparison of Cardiac Activity and Subjective Measures During Virtual Reality and Real Aircraft Flight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 Patrice Labedan, Frédéric Dehais, and Vsevolod Peysakhovich Information Visualization Theory and Applications Improving Self-supervised Dimensionality Reduction: Exploring Hyperparameters and Pseudo-Labeling Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . 135 Artur André A. M. Oliveira, Mateus Espadoto, Roberto Hirata Jr., Nina S. T. Hirata, and Alexandru C. Telea Visualization of Source Code Similarity Using 2.5D Semantic Software Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 Daniel Atzberger, Tim Cech, Willy Scheibel, Daniel Limberger, and Jürgen Döllner  
   
  xxii  
   
  Contents  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing . . . . . . . . . . . . . . . 183 Sören Domrös, Daniel Lucas, Reinhard von Hanxleden, and Klaus Jansen Exploratory Data Analysis of Population Level Smartphone-Sensed Data . . . . . . 206 Hamid Mansoor, Walter Gerych, Abdulaziz Alajaji, Luke Buquicchio, Kavin Chandrasekaran, Emmanuel Agu, and Elke Rundensteiner Towards Interactive Geovisualization Authoring Toolkit for Industry Use Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 Jiˇrí Hynek and Vít Rusˇnák Computer Vision Theory and Applications Global-first Training Strategy with Convolutional Neural Networks to Improve Scale Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 Dinesh Kumar and Dharmendra Sharma Spline-Based Dense Medial Descriptors for Image Simplification Using Saliency Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 Jieying Wang, Leonardo de Melo, Alexandre X. Falcão, Jiˇrí Kosinka, and Alexandru Telea BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder for Multi-modal Neuroimaging Analysis . . . . . . . . . . . . . . . . . . . . . . 303 Refka Hanachi, Akrem Sellami, and Imed Riadh Farah Enhancing Backlight and Spotlight Images by the Retinex-Inspired Bilateral Filter SuPeR-B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 Michela Lecca Rethinking RNN-Based Video Object Segmentation . . . . . . . . . . . . . . . . . . . . . . . . 348 Fatemeh Azimi, Federico Raue, Jörn Hees, and Andreas Dengel Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367  
   
  Computer Graphics Theory and Applications  
   
  Impact of Avatar Representation in a Virtual Reality-Based Multi-user Tunnel Fire Simulator for Training Purposes Davide Calandra(B) , Filippo Gabriele Prattic`o , Gianmario Lupini, and Fabrizio Lamberti Politecnico di Torino, 10129 Turin, Italy {davide.calandra,filippogabriele.prattico, fabrizio.lamberti}@polito.it, [email protected]   
   
  Abstract. Virtual Reality (VR) technology is playing an increasingly important role in the field of training. The emergency domain, in particular, can benefit from various advantages of VR with respect to traditional training approaches. One of the most promising features of VR-based training is the possibility to share the virtual experience with other users. In multi-user training scenarios, the trainees have to be provided with a proper representation of both the other peers and themselves, with the aim of fostering mutual awareness, communication and cooperation. Various techniques for representing avatars in VR have been proposed in the scientific literature and employed in commercial applications. However, the impact of these techniques when deployed to multi-user scenarios for emergency training has not been extensively explored yet. In this work, two techniques for avatar representation in VR, i.e., no avatar (VR Kit only) and Full-Body reconstruction (blending of inverse kinematics and animations), are compared in the context of emergency training. Experiments were carried out in a training scenario simulating a road tunnel fire. The participants were requested to collaborate with a partner (controlled by an experimenter) to cope with the emergency, and aspects concerning perceived embodiment, immersion, and social presence were investigated. Keywords: Virtual Reality (VR) · Multi-user simulation · Emergency training · Road tunnel fire · Avatar representation  
   
  1 Introduction Virtual Reality (VR) technology allows the creation of arbitrarily wide and sophisticated Virtual Environments (VEs), enabling the possibility to develop training scenarios that are very complex, or very expensive, to be deployed in real-life [12]. One of the most prominent fields which has taken large advantage of VR is that of emergency training, as indicated by the huge number of literature works which investigated the role of this technology in the creation of effective scenarios for managing emergencies [2, 14, 21, 24, 29]). Among the various use cases investigated so far, c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 3–20, 2023. https://doi.org/10.1007/978-3-031-25477-2_1  
   
  4  
   
  D. Calandra et al.  
   
  fire emergency is indeed among the most representative and widely studied ones [1, 11, 13, 27]. As stated in [12], for near-future developments one of the most promising challenges posed by VR technology for firefighting training is represented by the transfer of findings from other domains regarding VR experiences involving multiple users. In multi-user scenarios, two or more users can engage from different locations and at the same time in a shared virtual experience [8]. Differently than in single-user experiences, in multi-user scenarios the avatar realism is critical for the development of collaborative VEs [3]. VR kits typically include an Head-Mounted Display (HMD) and two hand controllers, providing a synchronized visuomotor feedback to the user [25]. Hence, the only available sensory information is related to the position and orientation of the user’s head and hands, which is not sufficient for full-body motion capture [28]. By relying on this information, most of the single-user commercial VR experiences usually show only a virtual representation of the hand controllers (e.g., SteamVR Home1 ) or, in some cases, two floating hands/gloves aligned with the real hands (e.g., Oculus First Steps2 ) A previous work explored different visibility levels for the user’s own avatar in single-user experiences, and found no significant differences in terms of perceived embodiment between fully showing, partially hiding or not showing, along with the hand controllers, a virtual body for the VR user [25], confirming the above choices. Although these reduced avatar representations appeared be sufficient from a firstperson perspective, they may not be suitable for representing the other users’ avatars in shared experiences. In particular, for emergency training scenarios, these techniques may have a negative impact on the perceived realism of the simulated scenario and, consequently, on the training efficacy. An alternative technique to represent the avatar of a VR user consists in applying Inverse Kinematics (IK) to operate a body reconstruction targeted from head and hand sensors only [28]. From a first-person point of view, these techniques may increase the user’s embodiment thanks to the visuomotor correlation [20], but may also worsen it when the estimated pose is characterized by a low accuracy [33]. When seen from outside in multi-user scenarios, IK techniques also require to correctly manage the users’ legs, taking into account the user’s motion in the VE. This can be done by procedurally generating the gait (like, e.g., in Dead and Buried3 ) or by blending the IK outcome with animations (like, e.g., in VRChat4 ) Thanks to the possibility to show and manage a full representation of the user’s body, these techniques may be effective in guaranteeing an appropriate level of immersion and embodiment, especially in case of realism-oriented, multi-user emergency simulations for training purposes. The aim of this work is to study the impact of two avatar representation techniques, namely, the VR Kit only (no avatar, hereafter referred to as VK) and the Full-Body reconstruction obtained by blending IK and animations (hereafter referred to as FB), when used to represent VR users in a multi-user, emergency training experience. The 1 2 3 4  
   
  SteamVR Home: https://store.steampowered.com/app/250820/SteamVR/. Oculus First Steps: https://www.oculus.com/experiences/quest/1863547050392688. Dead and Buried: https://www.oculus.com/experiences/rift/1198491230176054/. VRChat: https://hello.vrchat.com/.  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  5  
   
  scenario, named Fr´ejusVR and presented in [8], consists of a VR road tunnel fire simulator provided with multi-user, multi-role and multi-technology capabilities. The scenario embeds a serious game that can be used as a training tool for firefighters and as a means for communicating correct procedures to civilians. This serious game, developed in the context of the PITEM RISK FOR5 project, provides a good test-bench for the considered avatar representation techniques. A user study involving 15 users was conducted, by requesting each of them to collaborate with another user (an experimenter) to respond to a fire emergency happening in the VE. All the study participants experienced both the avatar representation techniques, as they were asked to face two slightly different situations in the same simulation. Subjective measures were gathered through standard questionnaires to investigate the level of embodiment [15], social presence [5], as well as immersion and presence [17]. Results show that, even though the FB representation did not significantly increase the embodiment with respect to VK, it was judged as significantly better than the other technique in terms of ability to foster mutual awareness, mutual attention, mutual understanding, and immersion. Moreover, almost all the participants preferred the FB for representing the other user’s avatar in terms of aesthetics and multi-player interaction. No clear winner emerged regarding the representation of the user’s own avatar, neither in terms of usability and aesthetics, nor regarding the overall experience.  
   
  2 Background Nowadays, the use of VR technology to simulate emergency situations has been widely studied [19, 22, 23, 30]. Studies largely considered also the field of fire simulation. As reported in [23], tunnel fires are among the simulated scenarios that have been most commonly explored in the past, even though most of them were not exploited in multiuser experiences. To cope with this lack, a multi-user road tunnel fire simulator for training purposes was recently presented in [8], supporting multiple roles (civilian or emergency operators), various VR technologies (consumer VR kits, locomotion treadmills, and motion capture suits) that can be arranged in different configurations, as well as a real-time fire spreading logic, with the possibility to integrate Computational Fluid Dynamics (CFD) data for the smoke visualization. As mentioned before, a critical aspect of multi-user collaborative VEs is represented by the level of realism of employed avatars, as testified by a relevant number of works on the effects of the specific avatar visualization technique being adopted [3]. The authors of [31], for instance, presented an experimental method to investigate the effects of reduced social information and behavioral channels in immersive VEs with non-realistic FB avatars (mannequins). To this purpose, both physical and verbal interactions were executed in both VR and real-life, and then compared in terms of social presence, presence, attentional focus, and task performance. Results showed that the lack of realism of the humanoid avatars hindered the social interactions and possibly reduced the performance, although the authors stated that the lack of behavioral cues such as gaze and facial expressions could be partially compensated. 5  
   
  https://www.pitem-risk.eu/progetti/risk-for.  
   
  6  
   
  D. Calandra et al.  
   
  In [18], the main focus was to find out how useful was the implementation of highfidelity avatars in a multi-user VR-based learning environment. In particular, both educators and students were endowed with the possibility to access a shared VE by means of avatars. The educator’s avatar consisted of a high-fidelity representation (including facial cues and eye motion) and was motion-controlled in real-time by the educator. The student’s avatars was implemented as not anthropomorphic in order not to draw attention from the educator. The results of the study suggested that representing with high-fidelity avatars the subjects with important roles in the simulation can enhance the overall user experience for everyone participating in it. A similar approach was pursued in [4] regarding the use of avatars in rehabilitation scenarios, though with a different set of technologies. By using a Microsoft Kinect sensor coupled with virtual scenarios, the authors concentrated their efforts on reproducing human posture failures by monitoring avatars, with the aim to improve the posture of people in different rehabilitation stages. Results confirmed that the framework created had enough flexibility and precision, thus confirming that avatar representation can play a key role in a wide variety of scenarios. As reported in [32], several studies underlined that FB and Head and Hands avatar representations are the most widely used in the current body of literature. More broadly, the authors of [26] investigated the realism that an avatar should have to sufficiently appease the user’s tastes. Work in the field led to the definition of a standardized Embodiment Questionnaire [15] and to a branch of studies regarding avatar use in multi-user environments, focusing on factors such as social presence and social interactions [5]. Regarding the visualization of the avatar from a first-person point of view in VR, the authors of [25] studied the effect of the visual feedback of various body parts on the user experience and performance in an action-based game. The work considered as visual feedback a completely hidden body (except for the VK), a low visibility body (hands and forearms), and a medium visibility body (head, neck, trunk, forearms, hands, and tail for the lower limbs). Differently than some previous works, no significant differences between the three alternatives were observed in terms of perceived embodiment. Finally, regarding the research topic about avatar movements, works such as [10, 28] showed the promising capabilities of IK techniques for estimating the pose of humanoid avatars, whereas studies like [16] managed to achieve accurate results by using such IK methods on different body parts, in particular on head and hands, leaving the lower body movement to several animations cleverly blended together. Despite the wide number of works regarding the topic of avatar representation techniques for single and multi-user VR experiences, to the best of the authors’ knowledge, investigations about the impact of these techniques in the field of VR simulations for emergency training are still scarce.  
   
  3 Fr`ejusVR In this section, the fire training scenario used for the experimental activity is described, along with its configuration and the customization introduced for the purpose of the evaluation.  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  7  
   
  3.1 Devices and Technologies The application was developed with Unity 2018.4.36f and the SteamVR Software Development Kit (SDK), allowing the deployment to any OpenVR compatible VR system. Due to the wide extension of the depicted scenario (a road tunnel), additional stationary locomotion techniques [9] were included to overcome the limitations of room scale movements. Among them, the arm-swinging technique was selected for the considered experimental activity, since it did not require additional hardware and showed to outperform the other techniques in some previous investigations [6, 7]. 3.2 Multi-user Regarding the multi-user capabilities, the Unity legacy high-level network API (UNET) was used to support a client-server architecture. The host can be either one of the user or a dedicated non-VR machine to lower the computational load of the two VR clients. To complement body-to body communication, a VOIP channel is established between the users though the Dissonance VOIP asset for Unity6 , adding two additional UNET channels (one reliable and one unreliable). Position and rotation updating of network objects is performed 60 Hz, and interpolation is employed to smooth the transition between consecutive updates. The scenario can support different roles, i.e., civilians, firefighters, and truck driver(s), which can be either played by real users, Non-Player Characters (NPCs), or be deactivated. 3.3 Avatar Representation Techniques The training scenario provided two different avatars (male or female) to be chosen as civilian or truck driver, and a generic firefighter avatar for users playing as firefighting operators. Independent of the avatar selection in the main menu, the VR application was modified in order to allow the configuration of one of the avatar representation techniques considered in this study (Fig. 1), which are described below. VR Kit (VK). This technique did not require particular modifications to the application. For the own avatar, the SteamVR CameraRig Unity prefab automatically manages the visualization of the VR hand controllers and their real-time synchronization with the real ones. For the remote users, 3D meshes representing a generic VR HMD and the two hand controllers (VIVE wand7 ) were displayed in correspondence of the synchronized position and orientation of the other user’s head and hands (Fig. 1a). Full-Body (FB). To support the FB representation, the Unity asset named FinalIK by RootMotion8 was acquired and integrated in the scenario. The asset provides a VRoriented FB IK (VRIK) solution, which supports both procedural locomotion steps, 6  
   
  7 8  
   
  Dissonance Voice Chat: https://assetstore.unity.com/packages/tools/audio/dissonance-voicechat-70078. Vive wand controller: https://www.vive.com/eu/accessory/controller/. https://assetstore.unity.com/packages/tools/animation/final-ik-14290.  
   
  8  
   
  D. Calandra et al.  
   
  recommended for micro-movements on the spot, as well as an animated locomotion, and was designed for room-scale movements or faster techniques. Since the navigation of the tunnel scenario requires relatively fast virtual movements, the procedural locomotion of VRIK was discarded in favour of the animated one (Fig. 1b). For the local user, VRIK was configured to always apply the IK algorithm to the upper limbs, spine and head, whereas animated locomotion was automatically triggered when the HMD is moved on the horizontal (X, Z) plane, and the blending was adjusted on the basis of the movement speed. By default, the blended animation are managed with the Unity Mecanim animation system, through a 2D free-form directional blendtree integrating a set of standing animations (including idle, directional walking and running). Animations for movements in crouch position were not provided by the assets. For this reason, a second blend-tree implementing crouched movements was added, and the blending between the two trees was obtained on the basis of the user’s HMD position, normalized with respect to his or her height. Since the mentioned asset does not apply IK to fingers, hands were managed with an overriding layer in the Unity animator, in order to support hand gestures (i.e., open hand, fist, and pointing) which were activated based on the context. As a matter of example, if the user is pressing a button, the pointing animation will be temporarily showed. Similarly, if the user is walking around with the arm-swinging technique, the relative avatar will be displayed with his or her hands balled in tight fists. To manage the representation of the other user, the standard VRIK behaviour had to be modified to maintain a sufficient level of naturalness. In particular, to hide out the unnatural arm-swinging gesture while walking, the upper body IK was temporarily disabled by blending it with the full movement animation. Hence, when the additional locomotion technique is triggered by a user, his or her avatar on the other user’s machine will perform a FB walking animation. As soon as the user stops triggering the armswinging, the upper body IK is restored, and the animation is again applied to the lower limbs only. To avoid misalignments between grabbed objects and FB animations, the grabbing position is always adjusted to either coincide with the actual controllers (when the IK is enabled) or with the hands of the other avatar’s rig (when the arm-swinging is triggered and the FB animation is displayed). VRIK allows to manage the synchronization over the network of the FB IK by synchronizing the position and orientation of the user’s CameraRig (head and controller). To synchronize the additional functionalities mentioned before, a custom UNET network component was used. 3.4  
   
  Selected Procedure  
   
  As said, the considered training scenario provides different roles (civilian, firefighter, truck driver), each characterized by its own procedure and interactions (some of them depicted in Fig. 2). For the purpose of the current evaluation, it was decided to focus on the role of the civilian for both the users. The reasons behind this choice are manifold. The civilian role does not require previous knowledge (differently than for firefighters and, to some extent, the truck driver), it maximizes the percentage of time in which the two users can see each other and interact together (the two civilians can start the  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  9  
   
  Fig. 1. The two avatar representation techniques considered in the evaluation, as displayed to the participant in the form of mirrored avatar before starting the simulation.  
   
  experience inside the same car), and provides the highest level of flexibility for the execution of the procedure. In order to guarantee a good level of visibility, the CFD-based smoke simulation was disabled. Moreover, to avoid possibly confounding factors related to the visualization of the other peer, some NPC roles were disabled (the firefighters) or reduced to aesthetic features visible just at the end of the experience (the truck driver in the security shelter). Considering these modifications with respect to the complete experience detailed in [8], the considered procedure can be summarized as follows: 1. Both the civilians start on the same car, travelling from Italy to France, while the car radio is broadcasting the usual messages for tunnel users. 2. After some travel time during which the car occupants can communicate, and the passenger has access to the security brochure of the tunnel, a truck on fire is spotted in the opposite lane. In this situation, the driver can operate the brakes and stop the car at an arbitrary distance from the vehicle on fire. 3. As soon as the car stops, the two users can interact with the car interior, e.g. turning off the engine (Fig. 2a), enabling the hazard lights, or getting out of it using the doors’ handles. 4. Once outside the car, the two collaborating users may press one of the many SOS buttons (Fig. 2b) placed inside the tunnel to signal the accident, and then decide whether to head to the closer SOS shelter, which is beyond the truck, or turn back and reach a farther shelter. It should be noted that after getting out of the vehicle, a second car accident involving two civilian cars occurs behind the users’ vehicle, starting a second, more contained and less threatening fire. Hence, both ways will be partially occluded by damaged vehicles on fire. 5. If the two users opt for the first choice, they can take advantage of two SOS niches (Fig. 2c) on both sides of the tunnel right before the truck, provided with SOS telephones and extinguishers which can be freely used. The users can either try to extinguish the main fire (Fig. 2d), or directly run towards the selected shelter. Behind the  
   
  10  
   
  D. Calandra et al.  
   
  Fig. 2. Pictures of the the Fr`ejusVR scenario taken during the experimental phase.  
   
  truck on fire, the users will have to walk crouched to walk under a wooden plank (Fig. 2e) which fell, with others, from the truck load over a civilian car blocking the way to the shelter. 6. If the users head back to the other shelter, they will be again forced to crouch to pass under a metal rod (Fig. 2f) placed nearby the second accident. In this case, the closest extinguisher and SOS telephone will be inside the shelter. The users will be allowed to get back to the tunnel to try to extinguish the smaller fire. In this shelter, the users will meet an NPC character sitting on a bench near a locker containing a first aid kit and some water bottles. 7. In both cases, after opening the door one of the shelter (Fig. 2g), getting inside (Fig. 2h) and asking for help with the SOS telephone, the simulation ends (if the call was already done from a SOS niche in the tunnel, this step is not needed and the simulation is quickly terminated). The possibility to extinguish both fires was disabled; however, the users were not made aware of this aspect, and were left free to choose whether to try using the extinguishers, fail, and thus continue with the evacuation. The layout of the described version of the tunnel scenario is reported in Fig. 3.  
   
  4 Experimental Setup In this section, the setup used for the experimental activity and the adopted evaluation criteria are thoroughly explained.  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  11  
   
  Fig. 3. Layout of the modified version of the tunnel scenario with respect to the original one detailed in [8].  
   
  4.1 Participants The 15 participants (14 males, 1 female) were aged between 24 and 67. Most of them reported medium to high experience with video-games, VR and multi-player applications, but almost all of them had little to no experience with serious games for emergency training. 4.2 Hardware For the experiment, two HTC Vive Pro9 kit were employed, one worn by the participant, one by the experimenter playing the part of the second user. The VR scenario was run on two Intel i9-9820X machines, each equipped with 32GB of RAM and a NVIDIA GeForce RTX 2080 Ti video card. 4.3 Methodology The experiment was designed as a within-subjects study, with the avatar representation technique as independent variable. The participants were initially asked to fill in a demographic questionnaire aimed to assess their previous experience with the involved technologies. Then, they were introduced to the experiment, and told what they were supposed to do in the simulation. A sample footage of the experiments with both modalities is available to download10 : Each participant experienced the two representation techniques in a random order. Before starting each simulation run, they were asked to select, inside the VR application, one of the two available avatars for the civilians. After that, a mirrored version of the avatar with the currently used technique was displayed to the user to show how he 9 10  
   
  HTC Vive Pro: https://www.vive.com/eu/product/vive-pro/. http://tiny.cc/zmnnuz.  
   
  12  
   
  D. Calandra et al.  
   
  or she was going to be seen from the other user’s point of view. With the VK, the choice of the avatar was not relevant, since it was not going to be displayed as a virtual body, but only as 3D meshes representing the VR equipment. Then, for both the techniques, the participant was requested to choose the role of the driver. At this point, a second user, controlled by one of the experimenters, connected to the multi-user session and spawned as a second civilian sitting beside the driver, automatically starting the experience. During the simulation, the experimenter tried to follow a predefined set of actions, in order to force the other peer to perform all the intended actions and interactions, as well as to uniform the experience among the various participants. To limit learning effects, the experimenter followed two distinct scripts for the first and the second experience, whose main difference concerned the selection of the evacuation route. In particular, in the first run, the experimenter: 1. 2. 3. 4.  
   
  Performed interactions while observed by the participant. Suggested to head to the closer shelter beyond the truck. Ensured that the participant notices the SOS niches and interacts with them. Showed the participant how to surpass the main fire and the obstacles requiring to crouch. 5. Should the participant try to extinguish the fire, he or she encouraged him or her to give up after a while and reach the shelter. During the second run, the experimenter: 1. Suggested to turn back and head to the farther shelter beyond the car accident. 2. Showed the participant how to surpass the main fire and the obstacles requiring to crouch. 3. After reaching the shelter, he or she suggested to equip the extinguisher and try to deal with the smaller fire encountered on the way. 4. After a brief try, he or she suggested to head back to the shelter and wait there for the rescue team to arrive. It should be noted that these guidelines were not considered in a very strict way, as some interactions may be repeated, omitted or executed in a different order, depending of the participant’s collaborativeness during the simulation. After each run, the participant was asked to fill in the evaluation questionnaire detailed in the following. 4.4  
   
  Evaluation Criteria  
   
  As mentioned before, the participants were evaluated from a subjective perspective by means of a post-test questionnaire11 . The questionnaire was organized in four sections. In the first section, the participant was asked to fill in the Embodiment Questionnaire [15], in order to evaluate his or her level of embodiment in terms of body ownership, agency and motor control, tactile sensations, location of the body, external appearance and response to external stimuli (as in [25]). The second section corresponded to the Networked Minds Social Presence 11  
   
  http://tiny.cc/1nnnuz.  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  13  
   
  Fig. 4. Average results for the Embodiment Questionnaire [15] sub-scales (values normalized between 0 and 1). Statistically significant differences (p-value < 0.05) marked with a star (*) symbol.  
   
  Questionnaire [5], and it was aimed to assess the virtual representation of the other user’s avatar in terms of mutual awareness, attentional allocation, mutual understanding, behavioural interdependence, mutual assistance, and dependent actions (similarly to [31]); the only exception was the empathy category, which was not included, being the relative items not suitable for the considered use case. The third section included the immersion and presence section of the VRUSE [17] questionnaire. Finally, the last section, filled in after having completed both the runs, asked the participant to express his or her preference between the two representation techniques in terms of usability, aesthetics, multi-player interactions, and overall.  
   
  5 Results and Discussion The results obtained for the subjective metrics presented in the previous section were used to compare the VK and FB techniques. The Shapiro-Wilk test was used to analyze the normality of data. Since data were found to be non-normally distributed, the non-parametric Wilcoxon signed-rank test with 5% significance (p < 0.05) was used for studying statistical differences. 5.1 Embodiment For what it concerns the perceived embodiment, reported in Fig. 4, no significant differences were observed in most of the sub-scales, as well as for the total embodiment. For  
   
  14  
   
  D. Calandra et al.  
   
  the sake of readability, the sub-scales and the total embodiment, calculated as suggested in [15], have been normalized in a range between 0 and 1, being each of them originally characterized by different minimum and maximum values. The only exceptions are represented by the appearance (0.3 vs 0, p-value < 0.001) and response to external stimuli (0.18 vs 0.1, p-value = 0.003) sub scales, for which the FB was perceived as better than the VK. This result is in line with some previous literature works mentioned before, which did not find significant differences between showing or not showing the user’s avatar body [25]. Going into the details of scores assigned to the individual items, expressed on a 7-point Likert scale from −3 to 3 (from strongly disagree to strongly agree), the participants felt the FB as a representation of their body more than the VK (1.4 vs 0.27, p-value = 0.042); however, they also perceived a higher sense of having more than one body (0.73 vs −0.6, p = 0.01), confirming that the FB could have either positive or negative effects in terms of embodiment based on its accuracy [20, 33]. This is in agreement with the results got from the item, according to which, with the FB, the participants felt as if movements of the virtual representation were influencing their real movements more than with the VK (−0.13 vs −1.13, p = 0.0488). Moreover, concerning the initial part of the experience in which a mirrored version of the user’s avatar is displayed to the participant, FB appeared to be felt as the own body more than VK (0.73 vs −0.6, p = 0.0412). Regarding external appearance, with the FB the participants felt as if their real body was becoming an “avatar” body more than with the VK (0.8 vs −0.6, p = 0.003), that their real body was starting to take the posture of the avatar body (0.2 vs −1.13, p = 0.009), that at some point the virtual representation started to resembled more their real body in terms of shape and other visual features (0.13 vs −2.0, p = 0.003), and that they felt more like they were wearing different clothes than when they came to the laboratory (0 vs −2.26, p = 0.005) than with VK. For what it concerns the response to external stimuli, with the FB the participants felt as if virtual elements (fire, objects) could affect them more than with the VK (1.2 vs 0.33, p = 0.015), and had an higher feeling of being harmed by the fire (−0.07 vs −0.53, p = 0.015). 5.2  
   
  Social Presence  
   
  As for social presence, whose metrics are depicted in Fig. 5, the difference between the two techniques was more marked. Responses were given on a 7-point Likert scale in a range between 1 and 7 (from strongly disagree to strongly agree). Considering the various sub-scales, the participants perceived the VK as better than the FB in terms of mutual awareness (6.4 vs 5.43, p = 0.003), mutual attention (6.34 vs 5.81, p = 0.003), and mutual understanding (5.91 vs 5.44, p = 0.047). These results indicate that the use of a body to represent the other user’s avatar significantly improves multi-user cooperation. In particular, with the FB, the participants noticed more the presence of the other peer (1.33 vs 3.0, p = 0.001), they were more aware of themselves inside the VE (6.4 vs 4.93, p = 0.002), and they perceived the other peer as more aware of them (6.33 vs 5.4, p = 0.023) than with the VK. Moreover, with the FB, the participants felt less alone (1.2 vs 1.93, p = 0.046), and perceived the other peer as less lonely (1.4 vs 2.2, p = 0.039). The FB also helped the participants to pay higher attention to the other  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  15  
   
  Fig. 5. Average results for the Networked Minds Social Presence Questionnaire [5] sub-scales (7-point Likert scale from strong disagreement to strong agreement). Statistically significant differences (p < 0.05) marked with a star (*) symbol.  
   
  peer with respect to the VK (6.33 vs 5.47, p = 0.0312). Furthermore, with the VK, the participants tended to ignore the other individual more than with the FB (1.67 vs 2.73, p = 0.0117). Finally, FB allowed participants to express their opinions (6.00 vs 5.27, p = 0.0027), as well as to understand the other peers’ ones (5.93 vs 5.13, p = 0.04888) more than VK. 5.3 Immersion and Presence The results for the immersion and presence section, derived from [17], are reported in Fig. 6. According to the scores, expressed on a 5-point Likert scale between 1 and 5 (from strongly agree to strongly disagree), the FB was judged as better than the VK only in terms of immersion (4.86 vs 4.26, p = 0.031), but nothing can be said in terms of presence. This outcome can be related to the fact that the training experience, being oriented towards realism, benefited of a more realistic-looking avatar representation, which was provided by the FB. 5.4 Direct Comparison In the final section of the questionnaire, whose results are provided in Fig. 7, the participants were asked to express their preference between the VK and the FB for various aspects. Interestingly, the FB was judged as significantly better than the VK for the other user representation for the aesthetics (93.33% vs 6.66%, p = 0.001), regarding multi-player interactions (86.66% vs 13.33%, p = 0.010) and also overall (86.66% vs 13.33%, p = 0.010). However, none of the two techniques prevailed for what it concerned the  
   
  16  
   
  D. Calandra et al.  
   
  Fig. 6. Average results for the immersion and presence section of the VRUSE [17] questionnaire (5-point Likert scale from strong disagreement to strong agreement). Statistically significant differences (p-value < 0.05) marked with a star (*) symbol.  
   
  Fig. 7. Average results for the direct comparison section of the questionnaire. Statistically significant differences (p < 0.05) marked with a star (*) symbol. Indicate for each of the following aspect which version you preferred: #01. Regarding the usability (own avatar); #02. Regarding the aesthetics (own avatar); #03. Regarding the aesthetics (other avatar); #04. Regarding the multiplayer interactions; #05. Regarding own avatar; #06. Regarding the other avatar; #07. Regarding the overall user experience  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  17  
   
  own avatar representation. These mixed results for the own avatar are anyway in line with those regarding the embodiment, suggesting that the two representations of the own avatar may have a similar impact in multi-user experiences too.  
   
  6 Conclusions and Future Work In this work, two different techniques to represent users’ avatars in multi-user VR experiences were evaluated in the context of an emergency training simulation. In particular, a road tunnel fire simulation, presented in [8], was used as a test-bench for the considered evaluation. The first considered technique, widely used for the user’s representation in commercial, single-user VR applications, consists in not providing an avatar body, but only displaying the VR Kit (VK) equipment, i.e., the hand controllers and, in case of the other user, the HMD. The second technique, referred to as Full-Body (FB), makes use of a combination of IK algorithms and animation blending to operate a full humanoid reconstruction for the user’s avatar, targeted to the position and orientation of the user’s head and hands. A within-subject user study, involving 15 participants, showed different outcomes regarding how the own avatar and the other user’s avatar are considered. For what it concerns the own representation, no clear winners emerged in terms of embodiment or preference, although the FB was perceived significantly better than the VK in terms of appearance, response to external stimuli, and immersion. Regarding the other user’s avatar, the FB appeared to improve various social presence aspects, such as mutual awareness, mutual attention and mutual understanding. Furthermore, it was also preferred over the VK for the other user’s representation (in general, but also for the aesthetics and multi-player interactions). These findings suggest that multi-user training simulations would greatly benefit of the employment of a FB approach to represent the avatar of the other user inside the shared experience, as long as the outcome of the combined use of IK and animations produces a sufficiently believable and realistic result. The situation is different for what it concerns the own representation of the user, as the investigation produced mixed results. Comments provided by some participants at the end of the experience offer possible interpretations. In particular, some participants reported of being distracted by the FB avatar whenever the estimated pose of the body differed too much from the real one. In those cases, they would have preferred not to see the avatar at all, like with the VK. For this reason some users may still perceive VK as better than FB in these particular situations. Future developments will be devoted to widen the investigation, considering other avatar representation techniques, as well as including the other technologies (e.g., motion capture suits and leg sensors) and locomotion modalities (e.g., locomotion treadmills, walk-in-place, etc.) already supported by the considered training tool. Moreover, more complex situations could be developed, for example requiring the users to fulfill more collaboration-oriented tasks (e.g., interaction with objects that have to be handled by two users at the same time, tasks which can be only completed by working together, presence of more than two users in the simulation, etc.). To support the above scenarios, the set of animations used by the FB implementation may need to be extended, e.g., by introducing also prone crawling, jumping, climbing, and any other movement which  
   
  18  
   
  D. Calandra et al.  
   
  may have to be displayed in a collaborative emergency scenario. Furthermore, machine learning-based avatar representation techniques may be added to the comparison, also estimating their impact in terms of computational load with respect to the IK algorithms. Finally, the FB implementation may be integrated with facial and eye tracking capabilities, to further improve communication and expressiveness of the other user’s avatar, and to increase the perceived realism of the simulation. Acknowledgements. This work has been carried out in the frame of the VR@POLITO initiative.  
   
  References ¨ G¨oko˘glu, S.: Development of fire safety behavioral skills via virtual reality. 1. C ¸ akiro˘glu, U., Comput. Educ. 133, 56–68 (2019). https://doi.org/10.1016/j.compedu.2019.01.014 2. Andrade, M., Souto Maior, C., Silva, E., Moura, M., Lins, I.: Serious games & human reliability. The use of game-engine-based simulator data for studies of evacuation under toxic cloud scenario. In: Proceedings of Probabilistic Safety Assessment and Management (PSAM 14), pp. 1–12 (2018) 3. Bailenson, J.N., Yee, N., Merget, D., Schroeder, R.: The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction. Presence 15(4), 359–372 (2006). https:// doi.org/10.1162/pres.15.4.359 4. Benrachou, D.E., Masmoudi, M., Djekoune, O., Zenati, N., Ousmer, M.: Avatar-facilitated therapy and virtual reality: next-generation of functional rehabilitation methods. In: 2020 1st International Conference on Communications, Control Systems and Signal Processing (CCSSP), pp. 298–304 (2020). https://doi.org/10.1109/CCSSP49278.2020.9151528 5. Biocca, F., Harms, C., L. Gregg, J.: The networked minds measure of social presence: pilot test of the factor structure and concurrent validity. In: International Workshop on Presence, Philadelphia (2001) 6. Calandra, D., Billi, M., Lamberti, F., Sanna, A., Borchiellini, R.: Arm swinging vs treadmill: a comparison between two techniques for locomotion in virtual reality. In: Diamanti, O., Vaxman, A. (eds.) EG 2018 - Short Papers, pp. 53–56. The Eurographics Association (2018). https://doi.org/10.2312/egs.20181043 7. Calandra, D., Lamberti, F., Migliorini, M.: On the usability of consumer locomotion techniques in serious games: comparing arm swinging, treadmills and walk-in-place. In: Proceedings of 2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin), pp. 348–352 (2019). https://doi.org/10.1109/ICCE-Berlin47944.2019.8966165 8. Calandra, D., Prattic`o, F.G., Migliorini, M., Verda, V., Lamberti, F.: A multi-role, multi-user, multi-technology virtual reality-based road tunnel fire simulator for training purposes. In: Proceedings of 16th International Conference on Computer Graphics Theory and Applications (GRAPP 2021), pp. 96–105 (2021). https://doi.org/10.5220/0010319400960105 9. Cannav`o, A., Calandra, D., Prattic`o, F.G., Gatteschi, V., Lamberti, F.: An evaluation testbed for locomotion in virtual reality. IEEE Trans. Visual Comput. Graphics 27(3), 1871–1889 (2021). https://doi.org/10.1109/TVCG.2020.3032440 10. Caserman, P., Achenbach, P., G¨obel, S.: Analysis of inverse kinematics solutions for fullbody reconstruction in virtual reality. In: 2019 IEEE 7th International Conference on Serious Games and Applications for Health (SeGAH), pp. 1–8 (2019). https://doi.org/10.1109/ SeGAH.2019.8882429  
   
  Impact of Avatar Representation in a Virtual Reality  
   
  19  
   
  11. Corelli, F., Battegazzorre, E., Strada, F., Bottino, A., Cimellaro, G.P.: Assessing the usability of different virtual reality systems for firefighter training. In: Proceedings of 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (HUCAPP 2020), pp. 146–153 (2020). https://doi.org/10.5220/ 0008962401460153 12. Engelbrecht, H., Lindeman, R.W., Hoermann, S.: A SWOT analysis of the field of virtual reality for firefighter training. Front. Rob. AI 6, 101 (2019). https://doi.org/10.3389/frobt. 2019.00101 13. Syed Ali Fathima, S.J., Aroma, J.: Simulation of fire safety training environment using immersive virtual reality. Int. J. Recent Technol. Eng. (IJRTE) 7(4S), 347–350 (2019) 14. Feng, Z., Gonz´alez, V.A., Amor, R., Lovreglio, R., Cabrera-Guerrero, G.: Immersive virtual reality serious games for evacuation training and research: a systematic literature review. Comput. Educ. 127, 252–266 (2018). https://doi.org/10.1016/j.compedu.2018.09.002 15. Gonzalez-Franco, M., Peck, T.C.: Avatar embodiment. Towards a standardized questionnaire. Front. Rob. AI 5 (2018). https://doi.org/10.3389/frobt.2018.00074 16. Gu, L., Yin, L., Li, J., Wu, D.: A real-time full-body motion capture and reconstruction system for VR basic set. In: 2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), vol. 5, pp. 2087–2091 (2021). https://doi.org/ 10.1109/IAEAC50856.2021.9390617 17. Kalawsky, R.S.: VRUSE - a computerised diagnostic tool: for usability evaluation of virtual/synthetic environment systems. Appl. Ergon. 30(1), 11–25 (1999). https://doi.org/10. 1016/S0003-6870(98)00047-7 18. Kasapakis, V., Dzardanova, E.: Using high fidelity avatars to enhance learning experience in virtual learning environments. In: 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pp. 645–646 (2021). https://doi.org/10.1109/ VRW52623.2021.00205 19. Kinateder, M., et al.: Virtual reality for fire evacuation research. In: Proceedings of Federated Conference on Computer Science and Information Systems, pp. 313–321 (2014). https://doi. org/10.13140/2.1.3380.9284 20. Kokkinara, E., Slater, M.: Measuring the effects through time of the influence of visuomotor and visuotactile synchronous stimulation on a virtual body ownership illusion. Perception 43(1), 43–58 (2014). https://doi.org/10.1068/p7545, pMID: 24689131 21. Lamberti, F., De Lorenzis, F., Prattic`o, F.G., Migliorini, M.: An immersive virtual reality platform for training CBRN operators. In: Proceedings of 2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC), pp. 133–137 (2021). https://doi. org/10.1109/COMPSAC51774.2021.00030 22. Louka, M.N., Balducelli, C.: Virtual reality tools for emergency operation support and training. In: Proceedings International Conference on Emergency Management Towards Cooperation and Global Harmonization (TIEMS 2001), pp. 1–10 (06 2001) 23. Lovreglio, R.: Virtual and augmented reality for human behaviour in disasters: a review. In: Proceedings of Fire and Evacuation Modeling Technical Conference (FEMTC 2020), pp. 1–14 (2020) 24. Lu, X., Yang, Z., Xu, Z., Xiong, C.: Scenario simulation of indoor post-earthquake fire rescue based on building information model and virtual reality. Adv. Eng. Softw. 143, 102792 (2020). https://doi.org/10.1016/j.advengsoft.2020.102792 25. Lugrin, J.L., et al.: Any “body” there? Avatar visibility effects in a virtual reality game. In: 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 17–24 (2018). https://doi.org/10.1109/VR.2018.8446229 26. Molina, E., Jerez, A.R., G´omez, N.P.: Avatars rendering and its effect on perceived realism in virtual reality. In: 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR), pp. 222–225 (2020). https://doi.org/10.1109/AIVR50618.2020.00046  
   
  20  
   
  D. Calandra et al.  
   
  27. Mor´elot, S., Garrigou, A., Dedieu, J., N’Kaoua, B.: Virtual reality for fire safety training: influence of immersion and sense of presence on conceptual and procedural acquisition. Comput. Educ. 166, 104145 (2021). https://doi.org/10.1016/j.compedu.2021.104145 28. Parger, M., Mueller, J.H., Schmalstieg, D., Steinberger, M.: Human upper-body inverse kinematics for increased embodiment in consumer-grade virtual reality. In: Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology, VRST 2018. Association for Computing Machinery, New York (2018). https://doi.org/10.1145/3281505.3281529 29. Pedram, S., Palmisano, S., Skarbez, R., Perez, P., Farrelly, M.: Investigating the process of mine rescuers’ safety training with immersive virtual reality: a structural equation modelling approach. Comput. Educ. 153, 103891 (2020). https://doi.org/10.1016/j.compedu. 2020.103891 30. Prattic`o, F.G., De Lorenzis, F., Calandra, D., Cannav`o, A., Lamberti, F.: Exploring simulation-based virtual reality as a mock-up tool to support the design of first responders training. Appl. Sci. 11(16), 1–13 (2021). https://doi.org/10.3390/app11167527 31. Roth, D., et al.: Avatar realism and social interaction quality in virtual reality. In: 2016 IEEE Virtual Reality (VR), pp. 277–278 (2016). https://doi.org/10.1109/VR.2016.7504761 32. Sch¨afer, A., Reis, G., Stricker, D.: A survey on synchronous augmented, virtual and mixed reality remote collaboration systems (2021) 33. Steed, A., Frlston, S., Lopez, M.M., Drummond, J., Pan, Y., Swapp, D.: An ‘in the wild’ experiment on presence and embodiment using consumer virtual reality equipment. IEEE Trans. Visual Comput. Graphics 22(4), 1406–1414 (2016). https://doi.org/10.1109/TVCG. 2016.2518135  
   
  Facade Layout Completion with Long Short-Term Memory Networks Simon Hensel1(B) , Steffen Goebbels1 , and Martin Kada2 1  
   
  2  
   
  Institute for Pattern Recognition, Niederrhein University of Applied Sciences, Reinarzstrasse 49, Krefeld, Germany {simon.hensel,steffen.goebbels}@hs-niederrhein.de Institute of Geodesy and Geoinformation Science, Technische Universit¨at Berlin, Kaiserin-Augusta-Allee 104-106, Berlin, Germany [email protected]   
   
  Abstract. In a workflow creating 3D city models, facades of buildings can be reconstructed from oblique aerial images for which the extrinsic and intrinsic parameters are known. If the wall planes have already been determined, e.g., based on airborne laser scanning point clouds, facade textures can be computed by applying a perspective transform. These images given, doors and windows can be detected and then added to the 3D model. In this study, the “Scaled YOLOv4” neural network is applied to detect facade objects. However, due to occlusions and artifacts from perspective correction, in general not all windows and doors are detected. This leads to the necessity of automatically continuing the pattern of facade objects into occluded or distorted areas. To this end, we propose a new approach based on recurrent neural networks. In addition to applying the MultiDimensional Long Short-term Memory network and the Quasi Recurrent Neural Network, we also use a novel architecture, the Rotated Multi-Dimensional Long Short-term Memory network. This architecture combines four two-dimensional Multi-Dimensional Long Short-term Memory networks on rotated images. Independent of the 3D city model workflow, the three networks were additionally tested on the Graz50 dataset for which the Rotated Multi-Dimensional Long Short-term Memory network delivered better results than the other two networks. The facade texture regions, in which windows and doors are added to the set of initially detected facade objects, are likely to be occluded or distorted. Before equipping 3D models with these textures, inpainting should be applied to these regions which then serve as automatically obtained inpainting masks. Keywords: Deep learning · LSTM · Facade reconstruction · Structure completion · Image inpainting  
   
  1 Introduction Most current 3D city models were created based on airborne laser scanning point clouds. Whereas roofs are clearly visible, the point density of vertical walls is much lower, and walls might be occluded by roofs. Thus, facades are often represented as planar polygons without any details. The position and size of facade objects like windows and doors can be better obtained from oblique aerial images. By utilizing camera c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 21–40, 2023. https://doi.org/10.1007/978-3-031-25477-2_2  
   
  22  
   
  S. Hensel et al.  
   
  parameters, segments from these images can be mapped to become textures of facade polygons. These are low resolution images that might be distorted by occlusions and shadows. Since real facades are not exactly planar (e.g., due to balconies), artifacts from the perspective correction occur as well. However, windows and doors have to be detected on these distorted images in order to obtain semantic 3D facade models. For example, neural networks for object detection and instance segmentation, see Sect. 2, search for individual object instances but do not consider higher-level layout patterns. This also holds true for the state-of-the-art object detection network “Scaled YOLOv4” [31] that is applied in this study. Thus, relationships between windows are not taken into account to also find occluded object instances. But model knowledge can be used to add missing instances in a post-processing step. One way to do this is by applying split grammars that were introduced in [32] for building reconstruction. Such grammars consist of rules that describe the placement and orientation of objects. They allow filling in missing facade objects and also generating facades procedurally from scratch. However, the grammar rules must be appropriate for a particular building style. Individual facade layouts may require individual, manually created rules. Since it is difficult to find a suitable collection of rules, machine learning is an appropriate tool.  
   
  Fig. 1. Workflow for facade structure completion: Due to a flag, a satellite dish and a signpost, four windows could not be detected. They were inserted using an LSTM [14].  
   
  We propose the use of Recurrent Neural Networks (RNNs) with Long Short-term Memory (LSTM) to recover positions and sizes of missing windows and doors within a workflow shown in Fig. 1. These networks are commonly applied to time-dependent, one-dimensional input data, e.g., in the context of speech recognition. This leads to an input consisting of incomplete detection results, e.g., from the Scaled YOLOv4 network. These results are represented by (two-dimensional) bounding boxes. We obtain an irregular rectangular lattice (IRL) by extending the edges of the bounding boxes to straight lines. Thus, an IRL is a collection of horizontal and vertical lines. It is called “irregular” because the distances between the lines may vary. Bounding boxes of detected objects are not perfectly aligned, so we simplify the IRL depending on the image resolution. Lines are merged if they are closer than a resolution-specific threshold distance (see Sect. 3.3). The cells of the IRL are the rectangles that are bounded but not intersected by lines. In the beginning, cells are labeled with the initial detection results. They are either classified as background or as part of a facade object (window/door). The task of the LSTM is to correct the initial labels of the cells so that missing objects  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  23  
   
  are added. To this end, we compare results of the proposed Rotated Multi-Dimensional Long Short-term Memory network (RMD LSTM [14], see Sect. 4) with results of the Multi-Dimensional Long Short-term Memory (MD LSTM) [9] and the Quasi Recurrent Neural Network (QRNN), see [3], in Sect. 6. We are not only interested in the positions and sizes of windows and doors to create semantic 3D building models but also want to texture the models with the given oblique aerial images. Initially, undetected objects that could be added by the RNN approach help to identify occluded or distorted image regions. If a window or door is not detected in the distorted image, we assume that the object and its surrounding is occluded. Thus, an occlusion mask can be computed automatically as the union of all corresponding surroundings. Then, an inpainting algorithm can be applied to fill these regions. Inpainting is a standard computer vision technique that interpolates or extrapolates visual information to fill image regions that are typically defined by a binary masks such that edges and texture patterns should somehow fit with the given information, see, e.g., the overview paper [20]. Our approach to computing occlusion masks based on added objects differs from a more common technique based on segmenting objects that may occlude facades, see e.g. [5]. However, it is difficult to segment all objects that cause occlusions because a facade can be occluded by many different objects belonging to many object classes. This difficulty does not occur with our proposed method. This work builds on the foundation established in [14]. We have rewritten and extended individual sections. We also introduce a new use case in Sect. 6.2 where the proposed workflow is inserted into a facade inpainting process.  
   
  2 Related Work We are concerned with a workflow that generates semantic 3D building models. Such models are represented and exchanged with the open data model format CityGML see [10] or more recently with CityJSON1 . In the presented workflow, windows and doors have to be recognized in facade images. Convolutional Neural Networks (CNNs) have become a standard tool for image-based object detection and segmentation. ResNET [12] was a milestone in Deep Learning. It eliminated the well-known vanishing gradient problem that can occur when training very deep neural networks. Mask R-CNN [11] is an important network architecture for segmenting individual object instances. To this end, it applies an attention mechanism that consists of detecting object instances first. Then it performs segmentation within the bounding boxes of the detection results. In the RetinaNet [19], the concept of focal loss is implemented to distinguish between foreground and background. Not only when computational resources are limited, YOLO [23] is a commonly used network. The Scaled YOLOv4 network [31] provides differently scaled deep learning models, which are chosen according to a balance of image size, channels and layers. On the MC COCO dataset, the network reached an average precision of 64.8% and outperformed the EfficientDet neural network [27] with the former best average precision of 53.0%. 1  
   
  https://cityjson.org/specs/ (accessed: January 21, 2023).  
   
  24  
   
  S. Hensel et al.  
   
  We use bounding boxes of detected objects to generate an IRL. Unfortunately, the boxes may not be properly aligned. However, they often can be aligned by slightly shifting their position and changing their size. This can be done by solving a combinatorial optimization problem. Such a mixed integer linear program with integer and float variables is presented in [13]. By using predefined bounding box positions, the float variables can be eliminated so that a faster integer linear program can be applied, see [15].  
   
  Fig. 2. Examples of object detection using Scaled YOLOv4. (a) and (c) are images from the Graz50 dataset [24], (b) has a high resolution whereas the resolution of image (d) is low.  
   
  If occluded facade regions are known (e.g., by applying semantic segmentation of trees), image inpainting techniques can be applied to fill these regions prior to object detection. Vice versa, we use image inpainting with masks obtained from analyzing object detection results.  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  25  
   
  Diffusion based inpainting methods fill regions by propagating information from the boundary to the interior. Typically, they are applied to deal with small regions. If larger regions have to be filled, texture synthesis methods like example-based inpainting are used. General inpainting algorithms have been adopted to the completion of facade images. In [6], a Random Forest-based method is used to obtain a semantic segmentation of facade elements. The edges of the segment boundaries are used to create an IRL. The corresponding cells are initially labeled based on the semantic segmentation, similarly to our approach based on bounding boxes, see Sect. 3.3. The algorithm performs example-based inpainting by copying cell content to cells that have to be filled. To this end, the IRL is interpreted as an undirected graph and a graph labeling problem is solved to ensure structural consistency.  
   
  Fig. 3. Images (a) and (b) show an IRL representation of an example facade from the CMP dataset. Images (c) and (d) show the corresponding bounding boxes. IRL in (b) was computed by merging lines of the IRL in (a). [14]  
   
  The article [16] deals with another inpainting algorithm that is applied in the context of facade reconstruction. The algorithm detects line segments in an image showing edges. It then clusters line segments according to vanishing points of the corresponding lines. Image regions that are covered by line segments belonging to two different vanishing points are interpreted as 2D representations of a 3D plane. This information is then used to continue textures in connection with various cost functions that measure appearance, guidance, orthogonal direction, and proximity. With the advance of Generative Adversarial Networks (GANs), see [8], deep learning based inpainting techniques like the application of the Wasserstein GAN [2] in [33] have been developed. The EdgeConnect network [22] applies a GAN to edge images. Instead of using partial convolutions that are restricted to non-occluded pixels,  
   
  26  
   
  S. Hensel et al.  
   
  DeepFillv2 [34] learns how to apply a convolution mask that is also learned. This mechanism is called gated convolution. Batch normalization is a standard technique in neural network training. With regard to inpainting, features of occluded regions can lead to mean and variance shifts. Instead of batch normalization, a region normalization method to overcome this problem is proposed in [35]. Given an inpainting mask, the image is divided into multiple regions. Instead of performing normalization with the images in a batch, now features are normalized with respect to the image regions. For the application of general purpose inpainting algorithms, it is necessary to specify the regions that have to be filled. However, an automated 3D building reconstruction workflow requires automatically derived masks for facade textures. In [4], the mask is derived from a 3D-to-2D conversion of a facade point cloud. Regions without points define components of the mask. Subsequent to the definition of the mask, inpainting is performed with a GAN. Model knowledge is applied in [18] where object detection results are used to refine a mask obtained from a segmentation network. Based on object detection, patches are marked that have to be preserved by the inpainting algorithm. The patches define important facade structures. Our proposed workflow inter- and extrapolates a pattern of windows and doors. Thus, it has to know about typical facade layouts. Such layouts can be either learned (as proposed here) or explicitly given in terms of formal grammars like split grammars introduced in [32]. In [28], shape grammars are used to improve facade object segmentation. The terminal symbols of a grammar correspond with facade object classes. An initial probability map for terminal symbols is gained from a discriminatively trained classifier. In an iterative process, segmentation labels are optimized by applying grammar-based constrains. During this process an immediate and cumulative reward system is used for segmentations of windows, doors, and other facade objects which should better represent reality. Formal grammars can be also applied to other data than 2D images. For example, in [7] point cloud data is analyzed by counting the number of points along horizontal and vertical lines with a kernel density estimation. Openings like windows and doors are characterized by lower point counts. Thus, these data can be interpreted with a weighted attributed context-free grammar to refine a facade model. The attributes define semantic rules which are additionally weighted with probabilities. Since it is difficult to cover all architectural styles with explicitly defined grammars, we apply deep learning with recurrent neural networks. The concept of RNN and LSTM architectures is to equip a neural network with memory and access to previous predictions, cf. [26]. Such networks are often applied to time-dependent, one-dimensional data. For example, they are used for speech recognition and text understanding, cf. [21, 25]. With regard to such applications, they can outperform other network architectures, see [36]. Extensions of LSTMs are the MD LSTM [9] and the QRNN [3]. The proposed RMD LSTM is built upon the MD LSTM, cf. [14]. To our knowledge, LSTMs have not been applied to the problem of facade reconstruction by other research groups  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  27  
   
  so far. Due to memory being passed through multiple neurons and a serialized input of two-dimensional data, RNN and LSTM architectures have a high demand for hardware resources. Thus, the size of the input data has to be reduced, see Sect. 3.3.  
   
  3 Object Detection and Data Preparation This section explains how we prepared data from different data sources and used it for training and testing. While training data was generated randomly, testing data also contained facade layouts based on object detections computed with Scaled YOLOv4. 3.1 Random Data Generation Both the CMP dataset [30] and the Graz50 dataset [24] contain facade images and annotations for facade object classes. We restrict ourselves to windows and doors that are represented by rectangular regions taken from the datasets. To train and test the networks, window and door rectangles were removed randomly so that the networks had to add them again in their predictions. Hence, the networks did not see RGB images but only facade layouts consisting of window and door rectangles. This information was coded within an IRL, see Sect. 1. The CMP dataset consists of 606 facades whereas the Graz50 dataset provides 50 images with corresponding annotations for segmentation. The architectural style of CMP facades differs from the style of Graz50 facades. We utilized this to avoid overfitting as we trained only with the larger CMP dataset but tested with the Graz50 facade layouts. This also showed that the networks are able to complete facade layouts of arbitrary facade types. One could either generate static test and training data prior to applying the networks or one could generate input dynamically during training and testing by randomly removing facade objects. We apply both methods. To avoid storing huge numbers of facade layouts, we applied the latter technique to dynamically remove 20% of windows and doors within the training iterations. This can additionally help to avoid overfitting, cf. [29]. Since it is easier to compare results on static data, we tested on fixed data that was also generated by removing 20% of facade objects. 3.2 Object Detection In addition to training and testing with data generated from the CMP and Graz50 datasets by randomly removing facade objects, we also performed tests with results of real object detection. For this purpose, we used the Scaled YOLOv4 network [31], which we already mentioned in Sect. 1. For our purposes, we trained the Scaled YOLOv4 network on the CMP facade dataset to detect windows and doors. We only distinguished between objects and non-objects (binary detection). Subsequently, this trained network was used to generate additional test data. Figure 2 shows results for two Graz50 images, a manually taken photo and a facade derived from oblique aerial imaging.  
   
  28  
   
  S. Hensel et al.  
   
  3.3  
   
  Data Preparation  
   
  Instead of directly using rectangles representing doors and windows as input for the LSTM networks, a matrix is generated based on an IRL. An initial IRL is obtained from the straight lines which are defined by extending the edges of the rectangles, cf. Sect. 1. The cells of this lattice are labeled to represent background (label value 0) or windows and doors (label value 1), and the label values become entries of a matrix M . For example, the facade in Fig. 1 is represented by the matrix in Formula (1). ⎤ ⎡ 00000000000 ⎢0 1 0 0 0 0 0 1 0 1 0⎥ ⎥ ⎢ ⎢0 0 0 0 0 0 0 0 0 0 0⎥ ⎥ ⎢ ⎢0 1 0 1 0 1 0 1 0 1 0⎥ ⎥ ⎢ ⎥ (1) M =⎢ ⎢0 0 0 0 0 0 0 0 0 0 0⎥ . ⎢0 1 0 0 0 1 0 1 0 1 0⎥ ⎥ ⎢ ⎢0 0 0 0 0 0 0 0 0 0 0⎥ ⎥ ⎢ ⎣0 1 0 0 0 1 0 1 1 1 0⎦ 01000001110 Since detected facade objects are often not properly aligned, the number of cells and thus the size of the matrix is significantly greater than necessary and might exceed an acceptable input size of the RNN networks. Therefore, we reduce the number of lattice lines of the initial IRL and the size of the matrix by merging neighboring lines. To this end, we iteratively determine neighboring lines of the same orientation within a threshold distance of eight pixels. These lines are replaced with a single line by computing a mean position. Figure 3 (b) shows how an IRL and its matrix change by merging close lines in comparison to Fig. 3 (a). Figures 3 (c) and 3 (d) show the corresponding bounding boxes whereas Fig. 4 visualizes the corresponding matrix representations. It can be seen that the facade layout does not change significantly but also that merging lines can lead to patterns and symmetry. Alternatively to the chosen merging approach, a pre-processing step could be applied that aligns rectangles by solving an optimization problem, see [13] or [15], cf. Sect. 2. Whereas the RNN networks work on matrix representations, the spatial coordinates of rectangles representing windows and doors are required to generate 3D models based on the network output. Therefore, the simplified IRL is stored in addition to the matrix representation. For the considered datasets, matrices had sizes between 10 and 100 rows and columns. Due to hardware restrictions, we choose a maximum network input size of 25 × 25 labels. Smaller matrices were fitted using zero padding. We experimented with scaling to also fit larger matrices, but that did not lead to acceptable results. Therefore, we only worked with facades that result in matrices with at most 25 rows and columns. This leaves 359 facades of the CMP and 46 facades of the Graz50 data for training and testing. Whereas the MD LSTM and the RMD LSTM networks are able to process 2D matrix input, matrices have to be serialized to be processed by the QRNN.  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  29  
   
  Fig. 4. Impact of merging vertical and horizontal lattice lines in the IRL of Fig. 3(a). Note that axes are of a different scale, such that the resulting matrix of the simplified IRL is much smaller [14].  
   
  Fig. 5. Rotated Multi-Dimensional Long Short-term Memory Network [14].  
   
  4 Recurrent Neural Networks for Pattern Completion and Inpainting Mask Generation We consider the ability of RNNs to utilize predictions from the past (previously processed spatial regions) to learn the patterns and symmetries of facade layouts. To this end, we introduce the RMD LSTM for facade completion and object recommendation, and present a comparison of outcomes from QRNN, RMD LSTM (and MD LSTM). In addition, we show how the network output can also be used to create inpainting masks for facade images. 4.1 Multi-Dimensional Long Short-Term Memory Network MD LSTM [9] is an RNN architecture that allows for multi-dimensional input by utilizing separate memory connections for each dimension. In a two-dimensional setting,  
   
  30  
   
  S. Hensel et al.  
   
  the MD LSTM network computes a value for a spatial cell (x, y) based on values of preceding cells (x − 1, y) and (x, y − 1). Thus, a directed spatial context is established in the network. The proposed RMD LSTM is composed of four MD LSTM networks. 4.2  
   
  Quasi Recurrent Neural Network  
   
  The QRNN is not an RNN but a CNN that emulates memory connections with an embedded pooling layer. For time-dependent inputs x1 to xT , the convolutional layers compute an output for time step t that consist of three vectors: a candidate vector zt , a forget vector ft and an output vector ot : zt = tanh(convWz (xt , . . . , xt−k+1 )) ft = σ(convWf (xt , . . . , xt−k+1 )) ot = σ(convWo (xt , . . . , xt−k+1 )) . In the convolution conv, a filter kernel size of k is used and the weight vectors are represented by Wz , Wf and Wo . The immediate outputs zt , ft , and ot are then used to compute the hidden states ct in the pooling layers. Let c0 := h0 := 0. Then the network output ht for time step t is computed via ct = ft  ct−1 + (1 − ft )  zt ht = ot  ct , where ht is the network output for time step t. The operator  denotes element-wise multiplication of the vectors. With the exception of the pooling layer, the QRNN can be easily parallelized, while regular recurrent networks compute intermediate outputs sequentially. Due to efficient convolution and pooling operations, the QRNN is fast and memory efficient. 4.3  
   
  Rotated Multi-Dimensional Long Short-Term Memory Network  
   
  RNNs were developed to solve one-dimensional, time-dependent problems in which information of previous network outputs have to be taken into account for subsequent computations. When dealing with two- or three-dimensional spatial problems, often data of a spatial surrounding has to be considered. But when MD LSTM for twodimensional input deals with cell (x, y), information of “future” cells (x + 1, y) and (x, y + 1) have not been computed yet. Especially when MD LSTM is applied to facade completion, missing facade objects in the upper left image region are impossible to add. To overcome this problem, we combine four MD LSTM networks. Each network operates on the input matrix rotated by k × 90◦ , k ∈ {0, 1, 2, 3}, respectively. Thus, each single MD LSTM starts its iterative processing at a different corner of the facade layout. The outputs of the four networks are rotated back to the original orientation. Then they are combined by fully connected layers with sigmoid activation, see Fig. 5. We also experimented with maximum pooling layers instead of fully connected layers to combine results. This led to final results which were not significantly different.  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  31  
   
  Based on the previously described neural networks, we set up a reconstruction workflow. In principle, the networks are applied on matrices obtained from IRL simplification, see Sect. 3.3. Their output is a matrix that contains probability values for cells being labeled as windows or doors, cf. Fig. 1. If a probability exceeds a threshold value of 0.5, the cell is interpreted as a recommended object. The object is equipped with a bounding box by using the cell coordinates of the IRL. However, this could lead to multiple boxes that cover a single object. Such boxes are merged in a post-processing step. It turned out that the results could be significantly improved by helping the networks with additional layout information, see Sect. 6. To this end, we did not use the binary matrices as inputs but replaced label values 1 that indicate facade objects with cluster numbers representing similar facade objects belonging to the same floor. To this end, we first group neighboring entries of 1 labels. If some groups are positioned in exactly the same rows and possess the same number of columns, then they belong to the same cluster. We enumerate all clusters and use the numbers as new matrix entries. Clustering is only applied to input data, ground truth and network output still consist of probability values. With minor adjustments, this workflow is also able to automatically generate inpainting masks. By subtracting the binary version of the input matrix, we filter out the added windows and doors. The regions covered by these objects are likely to be occluded in the given facade image. Thus, a mask is generated by drawing filled circles at the positions of these objects. Positions and diameters of circles are obtained from the IRL coordinates. The radius r of each circle is calculated by d r = 1.5 · , 2 where d is the diagonal of the IRL cell. The factor 1.5 is necessary because occlusions typically do not end at cell boundaries.  
   
  5 Network Training and Testing To implement, train and test the networks, we used Tensorflow [1]. Our source code is freely available2 . We trained the networks with a batch size of 16 and over 20,000 batches. Training was performed with randomly removed window and door rectangles from facade layouts of the CMP dataset whereas the original layouts served as the ground truth. This led to 320,000 distorted input matrices. To avoid overfitting, we added low-amplitude noise to background entries of the input, i.e., to the zeros. Training on an NVIDIA P6000 GPU lasted about a full day for MD LSTM and RMD LSTM whereas the QRNN could be trained within 3 h. The networks were trained with an equal number of iterations by using the Adam optimizer and a Mean Squared Error loss function. We tested the trained networks on matrices derived from the Graz50 dataset [24], see Sect. 3.1. 2  
   
  https://github.com/SimonHensel/LSTM-Facade-Completion.  
   
  32  
   
  S. Hensel et al. Table 1. Testing scores.  
   
  (a) Comparison of network results on binary input matrices (without cluster labels). Acc.  
   
  Prec.  
   
  Rec.  
   
  (b) Comparison of QRNN and RMD LSTM on input data labeled by clustering.  
   
  IoU  
   
  START  
   
  0.938 1.000 0.682  
   
  0.682  
   
  MD LSTM  
   
  0.980 0.913 0.787  
   
  0.732  
   
  QRNN  
   
  0.973 0.888 0.720  
   
  0.664  
   
  RMD LSTM 0.979 0.907 0.785  
   
  0.726  
   
  Acc.  
   
  Prec.  
   
  Rec.  
   
  IoU  
   
  START  
   
  0.938 1.000 0.682  
   
  QRNN  
   
  0.969 0.901 0.641  
   
  0.604  
   
  RMD LSTM 0.984 0.925 0.832  
   
  0.779  
   
  0.682  
   
  6 Results In this Section, we describe results qualitatively and quantitatively. In addition, we show inpainting examples based on masks that were generated with the RMD LSTM  
   
  Fig. 6. Network output consisting of 25 × 25 confidence values (0 = blue, 1 = red) for three facades of the Graz50 dataset: images (a) to (c) represent the ground truth, images (d) to (f) show the input, images (g) to (i) present the QRNN output, and images (j) to (l) show the RMD LSTM results [14] (Color figure online).  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  33  
   
  as described in Sect. 4. We also describe the limitations of our approach with some examples.  
   
  Fig. 7. Detection results of the Scaled YOLOv4 network are shown in red. Objects added by the RMD LSTM are annotated with green boxes. (Color figure online)  
   
  6.1 Layout Completion All experiments were performed on the Graz50 dataset. To be able to compare results, we computed a fixed test dataset with 10,000 facade layouts by removing 20% of facade objects, see Sect. 3.1. We begin with a description of quantitative results and discuss qualitative results later. Tables 1a and 1b compare results with and without pre-clustering, cf. Sect. 4. The output of the neural networks consists of a probability map of confidence values. Object probabilities below a threshold of 0.5 were considered as background. For calculating evaluation scores, we counted every matrix entry in one of four sums: true positive (tp), false positive (fp), true negative (tn) or false negative (fn). Tables 1a and 1b show the average values of  
   
  34  
   
  S. Hensel et al.  
   
  tp + tn tp + tn + fp + fn tp Precision (Prec.) = tp + fp Accuracy (Acc.) =  
   
  Fig. 8. Examples of generated inpainting masks and inpainting results.  
   
  tp tp + fn tp IoU = . tp + fp + fn  
   
  Recall (Rec.) =  
   
  The Intersection over Union (IoU) does not consider the correct classifications of the large background class and thus is especially meaningful. All scores were computed separately for each input and output pair. Afterwards, the arithmetic mean was computed.  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  35  
   
  To determine whether the networks were able to improve the facade layouts, we also calculated the scores directly on the network input without applying the networks. The outcomes are listed in the rows in Table 1 denoted by “START”. The precision is equal to 1 because no false positives are present in the randomly generated input data. As it can be seen in Table 1a, the RMD LSTM produces better results than the QRNN in terms of accuracy, recall and IoU. Figure 6 illustrates how probability values of recommendations look like. Although we trained the networks on the segregated CMP dataset, RMD LSTM was able to also complete Graz50 layouts. QRNN was not able to achieve similar results. Figure 7 shows examples of RMD LSTM network results on the Scaled YOLOv4 data. Whereas the examples (a), (b), (h), and (i) in Fig. 10 were completed within a small margin of error, there were some difficulties with objects that are composed of more than one IRL cell. The network also adds door-shaped objects at wrong positions. Besides testing on Graz50 data, we also tested on training data. Here, QRNN outperformed the RMD LSTM network. Compared with test data, the IoU of QRNN increased by 0.18. If this is not an effect of overfitting and if the style of facades is known and if the size of training data for this given style is sufficient, then QRNN might be superior to RMD LSTM. Furthermore, experiments were also conducted with the GridLSTM network [17]. This network showed better results for language and text translation tasks than certain LSTM networks due to a grid of multi-way interactions. However, this grid increases memory requirements such that we were not able to apply the network with the same hyperparameters as the other networks. We had to reduce both the batch size and the number of hidden units. But under these restrictions, the network wrongly classified everything as background. 6.2 Inpainting Besides improving facade object detection, the presented network architecture can also be used to automatically generate inpainting masks, see Sect. 4. We used these masks to replace occluded or distorted image regions with fitting textures. We detected windows and doors with the Scaled YOLOv4 network. Then detection results were completed with the RMD LSTM network such that inpainting masks could be generated based on the added objects. The regions defined by the masks were filled with the DeepFillv2 network [34]. Figure 8 shows two examples. There is no occlusion in the second example image so that no correction is required. Additional information is required to exclude such images from automatic procession.  
   
  36  
   
  S. Hensel et al.  
   
  Fig. 9. Examples that showcase the limitations of our workflow presented in Fig. 1. Figures (a)– (h) show the effects of an insufficient object detection. Figures (i)–(l) show the effects of separating facade structure knowledge and image information.  
   
  6.3  
   
  Limitations  
   
  Large regions without detected windows and doors make it difficult to find an IRL that represents the entire facade structure. Two examples of this problem are shown in Fig. 9 (a)–(d) and (e)–(h). Another consequence of the sole reliance on object detection is that the LSTM networks do not take pixel information into account. For example, the missing window in Fig. 9 (i)-(l) has a different size and shape than the other windows. Results shown in Figs. 9 and 10 indicate that improvements are still possible. The main cause for detection errors are objects that are represented by more than one matrix entry. This occurs in some rare cases and is therefore troublesome for learning. Most often, doors are incomplete or mistaken for a window, see examples in Fig. 10 (a), (d) and (j). A major limitation in the application of LSTMs is their high memory usage. MD LSTM and RMD LSTM allocate up to 23 GB of GPU memory, depending on the number of hidden units used.  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  37  
   
  Fig. 10. Objects were randomly removed from the ground truth of the Graz50 dataset, see also Sect. 3.1. Remaining objects are annotated with red boxes. The RMD LSTM network added the missing objects within a small margin of error. Added objects are marked with green boxes [14]. (Color figure online)  
   
  38  
   
  S. Hensel et al.  
   
  7 Conclusion and Future Work Experiments with the RMD LSTM showed that LSTMs are a suitable means to fill gaps in facade layouts. RMD LSTM outperformed the original MD LSTM and QRNN. It increases the IoU by 14%. An advantage of such deep learning methods over grammarbased algorithms is that no rules have to be defined explicitly. So far, the training of neural networks was limited to windows and doors. However, other facade objects could be treated in a similar way. Future improvements should also target the reduction of memory consumption and the other problems listed in Sect. 6.3. The current workflow works best if the detected objects are distributed over the entire image. An incomplete IRL resulting from insufficient detection results could be improved by detecting patterns present in the IRL.  
   
  References 1. Abadi, M., et al.: TensorFlow: a system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265–283. USENIX Association, Savannah (2016) 2. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 70, pp. 214–223. PMLR (2017) 3. Bradbury, J., Merity, S., Xiong, C., Socher, R.: Quasi-recurrent neural networks. arXiv arXiv:1611.01576 (2016) 4. Chen, J., Yi, J.S.K., Kahoush, M., Cho, E.S., Cho, Y.K.: Point cloud scene completion of obstructed building facades with generative adversarial inpainting. Sensors 20(18), 5029 (2020) 5. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp. 833–851. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 49 6. Dai, D., Riemenschneider, H., Schmitt, G., Van Gool, L.: Example-based facade texture synthesis. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1065–1072 (2013) 7. Dehbi, Y., Staat, C., Mandtler, L., Pl, L., et al.: Incremental refinement of facade models with attribute grammar from 3D point clouds. ISPRS Ann. Photogrammetry Remote Sens. Spat. Inf. Sci. 3, 311 (2016) 8. Goodfellow, I., et al.: Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems 27, pp. 2672–2680. Curran Associates, Inc. (2014) 9. Graves, A., Fern´andez, S., Schmidhuber, J.: Multi-dimensional recurrent neural networks. CoRR (2007) 10. Gr¨oger, G., Kolbe, T.H., Czerwinski, A.: OpenGIS CityGML Implementation Specification (City Geography Markup Language). Open Geospatial Consortium Inc., OGC (2007) 11. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask R-CNN. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2961–2969 (2017) 12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778 (2016)  
   
  Facade Layout Completion with Long Short-Term Memory Networks  
   
  39  
   
  13. Hensel, S., Goebbels, S., Kada, M.: Facade reconstruction for textured LoD2 CityGML models based on deep learning and mixed integer linear programming. ISPRS Ann. Photogrammetry Remote Sens. Spat. Inf. Sci., IV-2/W5, 37–44 (2019). https://doi.org/10.5194/isprsannals-IV-2-W5-37-2019 14. Hensel, S., Goebbels, S., Kada, M.: LSTM architectures for facade structure completion. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 1: GRAPP, pp. 15–24. INSTICC, SciTePress (2021). https://doi.org/10.5220/0010194400150024 15. Hu, H., Wang, L., Zhang, M., Ding, Y., Zhu, Q.: Fast and regularized reconstruction of building facades from street-view images using binary integer programming. ISPRS Ann. Photogrammetry Remote Sens. Spat. Inf. Sci. V-2-2020, 365–371 (2020). https://doi.org/10. 5194/isprs-annals-V-2-2020-365-2020 16. Huang, J.B., Kang, S.B., Ahuja, N., Kopf, J.: Image completion using planar structure guidance. ACM Trans. Graph. (TOG) 33(4), 1–10 (2014) 17. Kalchbrenner, N., Danihelka, I., Graves, A.: Grid long short-term memory. arXiv:1507.01526 (2015) 18. Kottler, B., Bulatov, D., Zhang, X.: Context-aware patch-based method for fac¸ade inpainting. In: Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 1: GRAPP, pp. 210–218 (2020) 19. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P.: Focal loss for dense object detection. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2980– 2988 (2017) 20. Mehra, S., Dogra, A., Goyal, B., Sharma, A.M., Chandra, R.: From textural inpainting to deep generative models: an extensive survey of image inpainting techniques. J. Comput. Sci. 16(1), 35–49 (2020) 21. Mtibaa, F., Nguyen, K.K., Azam, M., Papachristou, A., Venne, J.S., Cheriet, M.: LSTMbased indoor air temperature prediction framework for HVAC systems in smart buildings. Neural Comput. Appl. 32, 1–17 (2020) 22. Nazeri, K., Ng, E., Joseph, T., Qureshi, F.Z., Ebrahimi, M.: EdgeConnect: generative image inpainting with adversarial edge learning. arXiv:1901.00212 (2019) 23. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 24. Riemenschneider, H., et al.: Irregular lattices for complex shape grammar facade parsing. In: Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1640–1647 (2012) 25. Salehinejad, H., Sankar, S., Barfett, J., Colak, E., Valaee, S.: Recent advances in recurrent neural networks. arXiv:1801.01078 (2017) 26. Sherstinsky, A.: Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D 404, 132306 (2020) 27. Tan, M., Pang, R., Le, Q.V.: EfficientDet: scalable and efficient object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10781–10790 (2020) 28. Teboul, O., Kokkinos, I., Simon, L., Koutsourakis, P., Paragios, N.: Shape grammar parsing via reinforcement learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2273–2280. IEEE (2011) 29. Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P.: Domain randomization for transferring deep neural networks from simulation to the real world. In: Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 23–30 (2017)  
   
  40  
   
  S. Hensel et al.  
   
  ˇ ara, R.: Spatial pattern templates for recognition of objects with regular struc30. Tyleˇcek, R., S´ ture. In: Weickert, J., Hein, M., Schiele, B. (eds.) GCPR 2013. LNCS, vol. 8142, pp. 364– 374. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-40602-7 39 31. Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M.: Scaled-YOLOv4: scaling cross stage partial network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13029–13038 (2021) 32. Wonka, P., Wimmer, M., Sillion, F., Ribarsky, W.: Instant architecture. ACM Trans. Graph. (TOG) 22(3), 669–677 (2003) 33. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image inpainting with contextual attention. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5505–5514 (2018) 34. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form image inpainting with gated convolution. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 35. Yu, T., et al.: Region normalization for image inpainting. Proc. AAAI Conf. Artif. Intell. 34(07), 12733–12740 (2020). https://doi.org/10.1609/aaai.v34i07.6967 36. Zhang, D., Wang, D.: Relation classification: CNN or RNN? In: Lin, C.-Y., Xue, N., Zhao, D., Huang, X., Feng, Y. (eds.) ICCPOL/NLPCC -2016. LNCS (LNAI), vol. 10102, pp. 665– 675. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-50496-4 60  
   
  Human Computer Interaction Theory and Applications  
   
  Generating Haptic Sensations over Spherical Surface Patrick Coe1,2(B) , Grigori Evreinov1,2 , Mounia Ziat1,2 , and Roope Raisamo1,2 1  
   
  The Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland {patrick.coe,grigori.evreinov,roope.raisamo}@tuni.fi 2 Department of Information Design and Corporate Communication, Bentley University, Waltham, MA, USA [email protected]  Abstract. Haptic imagery, the imagining of haptic sensations in the mind, makes use of and extends human vision. Thus, enabling a better understanding of multidimensional sensorimotor information by strengthening space exploration with “seeing by touch.” Testing this concept was performed on a spherical surface to optimize the way of generating localized haptic signals and their propagation across the curved surface to generate dynamic movements of perceivable peak vibrations. Through testing of several spherical structure prototypes, it was found that offset actuations can dynamically amplify vibrations at specific locations. A pilot study was followed to understand the impact of haptic stimulation on viewers of video content in a passive VR environment. Results showed a correlation between heart rate and the presented content; complimenting the technical data recorded. Keywords: High-definition haptics · Tangible mental images · Virtual tactile actuation · Interference maximum · Actuation plate  
   
  1 Introduction Human intellectual and creative potential, as well as the development of perceptual and motor abilities, are all influenced by our visual-based culture [68]. As technology progresses, more advanced features become available for computer users. Great achievements have been made in processing visual information and communication technology. Different types and formats of digital video are becoming more common; emotional components and precise patterns in video, pictures, and audio messages may readily enhance the perceiver’s experience. When emotionally rich information is unavailable, blind and visually impaired children often experience severe emotional distress, which can lead to depression and an inhibition in cognitive development. [3, 7]. Immersion, interactivity, and imagination have been the focus of Virtual Reality (VR) since 1965, when Ivan Sutherland first proposed the technology [73]. Progress This work was supported by project Adaptive Multimodal In-Car Interaction (AMICI), funded by Business Finland (grant 1316/31/2021). c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 43–68, 2023. https://doi.org/10.1007/978-3-031-25477-2_3  
   
  44  
   
  P. Coe et al.  
   
  in computer graphics and sound synthesis over the last 50 years has enabled VR systems to achieve fairly realistic rendering and stimulation of the human imagination. That said, the natural empathic ties of humans to other humans and to the world also contributed to their imaginative powers [70]. Moreover, sighted people are eager to live richer interactive experiences through other senses; visual exploration being the easiest way to achieve “theoretical imagination” (for example, Neo learning Kung Fu (haptic imagery) in the Matrix (through vision) [76, 77]). Despite these advances in the visual realm, haptic imagery falls far short of what users expect. Most VR systems’ haptic sensations pale in comparison to the vast array of haptic qualities that humans can truly detect in the real world [15, 19]. From desktop haptics, surface haptics, and wearable haptics to more powerful haptic devices, haptic technology will continue to advance to the point that they are able to simulate physical properties in a natural way and in more details allowing the integration of spatially and temporally discrete sensory inputs [82]. Despite the apparent relevance of haptics in human perception development, spatial visual representations of distance, size, shape, and motion often prevail over haptic perception [62]. As a result, there is a great challenge to propose new ways to induce tactile sensations of spatial objects through dynamical haptic stimulation or modify the visual experience of the user through the use of new haptic technology and materials. Multiple tech companies such as Apple [59] focused on improving of the haptic feedback in their products attesting in an increasing demand for improved tactile feedback. From curved edges to flexible displays, more innovative and interesting device shape factors are continuously explored [34]. As interfaces and screens become more sophisticated and nonstandard in design, adaptive tactile output will be required for further haptics improvements in consumer products. Tactile click buttons, which could formerly be felt, were quickly phased out in favor of capacitive touchscreen displays, which were also featured in the prototypes detailed in this paper. Our present study focuses on localized haptics on a spherical surface, with the goal of developing haptics with various geometries. Understanding how enhanced haptic signals may be used to introduce high quality haptics is equally important as form factors develop and we move away from standard flat displays. We utilized a simpler method over previous studies to produce localized points of actuation in our study. Although the employment of a large number of actuators is useful, it is impractical due to the added complexity and cost. Furthermore, any system that requires constant surface monitoring may be challenging to execute outside of a laboratory context. We want to address the aforementioned concerns by lowering the number of actuators necessary to generate a localized point of vibration while determining the offsets required for a specific material. When engaging with graphical objects via physical actions, humans mix the visual information prompted by the tangible interface, such as keystrokes on a keyboard, mouse buttons, or another haptic device. Force feedback in a personal space, in direct contact with the surface [14], is often used to verify and predict visual inputs, to prevent a collision, or present more specific physical information about the external object subject to the interaction. Nonetheless, force feedback parameters can only change within a restricted range of magnitude gradient and duration (length of tactile stimuli). Furthermore, force feedback is referred to as shared forces in most haptic interfaces that are based on direct finger contact (those tangential to the skin). When skin travels laterally across a sensitive surface, the pressure generated (65–100g) provides a contact  
   
  Generating Haptic Sensations over Spherical Surface  
   
  45  
   
  force that causes orthogonal skin deformation (normal to the surface). The human sense of touch, on the other hand, is a more complex analyser of processing dynamic arrays of force vectors (e.g., when distinguishing the concave and convex components of surfaces). This is evident when haptic textures and objects are reproduced using 3D haptic instruments [13], but it is not yet commonly applicable to surface haptics on touchscreens [41], when ordinary haptic exciters are employed. As a result, dynamically actuated virtual vibration, sources of vector force traveling across the display surface, can be used to convey a higher bandwidth of information to the user in order to display more complex vector graphic haptic images than primitive down sampling based reliefs [21, 22, 40, 44, 47, 48, 56, 69]. Actively explored touch surfaces provide a rich haptic experience to users by giving kinesthetic, proprioceptive, and cutaneous information. To control each “tactile pixel” (or taxel) spread out in a two-dimensional array, the approach often employed for tactile modeling of objects and their surfaces was adopted [75]. Taxels have been utilized for sparse low-resolution approximation of interactive surfaces and virtual stages rather than high-definition haptic simulation of items [13, 44, 48]. However, using visual perception concepts [69, 71] to replicate the most sophisticated tactile display technology [79] may not work for haptic visualization since a surface can be defined by multiple physical qualities. These must be perceived, identified, and understood as a static, dynamic, or virtual (cross-section) array of identical pieces using haptic imagination. When investigating and engaging directly or indirectly with virtual surfaces, a range of technical techniques have been investigated for surface modeling and control of attributes (mechanical and acoustic) [20, 22], simulating shapes [20, 21, 27], texture [69] properties such as stiffness, curvedness [27, 35], friction [55], and compliance/elasticity [51]. It has already been shown that without curvature, the properties of vibrotactile interference can be achieved on a flat surface [42]. Taking advantage of the properties of wave interference it may be possible to create feelable precise high definition tactile points traveling across a surface with variable curvature leading to an apparent tactile motion [6, 60]. This indicates that a high definition vibrotactile display would require fewer actuators (Fig. 2). This might be accomplished by accurately offsetting any number of provided actuations in sync from exciters attached to the actuation surface of contact, of which can also be curved. At the exact point of contact, the ensuing point of constructive wave interference would considerably magnify the amount of vibration signal over the ambient noise (Fig. 4). If the actuation offsets required to dynamically produce a point of maximum constructive wave interference at every point traveling over the surface are known, a matrix of values may be recorded and utilized to stimulate apparent tactile motion through inducing haptic imagination. A music instructor encouraging a student to play the piano on their desk is an example of haptic imagination. Without manipulating the piano keys, the learner may picture and feel the music composition to be played. “The hands-on tactile exploration is the gateway to haptic imagination,” as said by R. Schwaen. [66]. We may augment this notion, for example, by enabling virtual moveable haptic vibrations that can be felt traveling over the surface by consecutively activating the appropriate offsets that produce a feelable moving point of maximum interference. This vibration interference position might be dynamically positioned in order to show  
   
  46  
   
  P. Coe et al.  
   
  information to a user in an unusual manner. Tactile data may be moved around a user’s hand, or the user could be told to focus on or follow a moving virtual actuator. Previous work in this area of haptic research has demonstrated a similar strategy of induce locations of virtual actuation across a given surface by utilizing wave characteristics. Enferad and others [17], for example, who worked on establishing a controlled localized point of stimulation utilizing voltage modulated signals to activate piezoelectric patches over an aluminum beam. They accomplished superposition mostly through voltage phase modulation. Charles Hudin and his colleagues used time-reversal wave focusing to solve a similar challenge [33]. During the focusing step, a vibrometer calibrated the time-reversal wave, which was then followed by an actuation signal from an array of 32 actuators glued to the bottom perimeter of a glass plate. This worked well in terms of producing a precise, localized point of haptic stimulation. However, the usage of a closed loop control system creates substantial issues since it limits its practical application when a touch point in a consumer product is masked or repressed by a finger.  
   
  2 Concept and Design of the Spherical Haptic Display To put the previously described design concept to the test (Fig. 1 [11]), we created a mockup of a spherical haptic display (Figs. 2–6 [11]). This preliminary design is designed to investigate the viability of virtual force actuation, as well as various methods of optimizing the configuration of actuator assembly in relation to these forces. The architecture and quantity of actuators, the characteristics of the virtual sources of vector force, and the arrangement of elementary haptic signals may all be changed to optimize the system. The prototype will also be used to assess the propagation of constructive wave interference across the curved display surface. Measurements from similar research have shown that by controlling the offset of several signals, it is feasible to obtain precise localization of enhanced vibration at a chosen point of contact [9, 12]. We employed a special combination of strong unidirectional voice coil actuators to produce a virtual vibration source at a spot on the curved touch display surface (Tectonics and Lofelt). Figure 1 [11] depicts a concept known as the Volumetric Tactile Display (VTD). It is made up of constructive wave interference that propagates sequentially to the places of contact with the skin. By combining geographically and temporally distinct sensory inputs, the resultant point of localized vibration is capable of correctly mediating haptic signals. We created two dome-shaped prototypes to collect preliminary data. The goal of each prototype was to study several ways of localisation that all aimed at the same goal. The first sphere was concerned with wave interference, while the second was concerned with vector force concepts. Before attempting to merge both approaches, each prototype was successfully tested independently. Both prototypes were made within a 116 mm diameter polycarbonate dome. Wires were routed from the inside of both domes to an external motor controller (L298). The motor controller uses an Arduino DUE, which was chosen for its high speed of 84 Mhz, allowing for high accuracy outputs and data collecting at 5.3 µm intervals. A copper strip was utilized for exact calibration of vector forces propagating over the Spherical Haptic Surface (SHS) from the configuration of  
   
  Generating Haptic Sensations over Spherical Surface  
   
  47  
   
  Fig. 1. The variants of haptic actuators assembly affixed to the actuation plane. 1-5 - Lofelt L5 (14) actuators and Tectonic exciter TEAX25C10-8HS (5). Red arrows indicate linear motion, while green arrows indicate angular motion. Black arrows indicate actuator movement. 8 - Represents a targeted point of increased magnitude or vibration; 6 - an actuation plate; 7 - a spherical haptic surface [11]. (Color figure online)  
   
  unidirectional actuators (where micro-displacements over touch surface are sensed with the MicroSense sensor). The first dome was used to investigate the possibility of wave interference between seismic signals generated by actuators directly attached to a spherical haptic surface. It was made up of four Tectonic actuators (TEAX1402-8) that were joined from the inside and put at the vertices of the tetrahedron (Figs. 3 and 2 [11]). The controlled offset of several actuation impulses was to be used to localize the vector force at the appropriate  
   
  48  
   
  P. Coe et al.  
   
  Fig. 2. Top view of the first spherical prototype, with centimeter lines on copper tape for sensor placement [11].  
   
  Fig. 3. A side view of the first spherical prototype with centimeter lines on copper tape for sensor placement [11].  
   
  Generating Haptic Sensations over Spherical Surface  
   
  49  
   
  point of contact over SHS. The controlled offset actuation intended to move the point at which constructive wave interference occurred over the hemisphere’s surface.  
   
  Fig. 4. Top view of the second spherical prototype, showing centimeter markings for sensor location. This structure is made up of four Lofelt L5 actuators and a single Tectonic exciter TEAX25C10-8HS attached to a Haptic Actuation Plate (HAP) that transfers seismic waves over a spherical surface [11]. (Color figure online)  
   
  In the second dome (Figs. 5 and 4 [11]), we concentrated on putting the ideas of a shifting magnitude to the test. These effects were achieved by altering the magnitudes of lateral and vertical motions. The installed more powerful next-generation Lofelt technology L5 actuators were attached to the actuation plate along the X and Y axes, while a strong Tectonic exciter was utilized to actuate vertically in the Z-axis direction. We intended to use this design to increase the resultant force of seismic signals that initially interfered in orthogonal directions across from an actuation plate by applying various  
   
  50  
   
  P. Coe et al.  
   
  Fig. 5. View of the second spherical prototype from the side, with red markings every two cm for measurement placement [11].  
   
  magnitudes of actuation in the vertical and horizontal axes. Nonetheless, as shown in Fig. 1 [11], unidirectional haptic actuators may be combined in a variety of ways to provide both linear (red) and angular (green) force momentum (torques). It was discovered during the creation of the actuation plate that hydrophobic materials (such as Gorilla-glass, Teflon, and silicone) might impact the perception of convexity vs. concavity at the point of finger contact. The thickness of a material, such as glass, has an effect on the vibration that is perceived [80]. The outcome is promising for further validation via a user study. This paves the way for new methods of modeling volumetric forms in virtual and augmented reality. As a result of the combination of novel material characteristics and actuation technologies, we can produce complex haptic sensations that are required for developing haptic imagination in both healthy persons and those with perceptual difficulties. Aside from physical dimensions, personal exploratory behavioral characteristics will have an influence on interpreting numerous tactile information obtained while interacting with SHS during the perception of mental representations of the items shown. As a result, a user-centered approach will be employed to illustrate the issues and limits of the suggested interaction strategies (Fig. 6 [11]). The spherical surface, as depicted, compliments the hand’s form. Tactile feedback may now spread over the palm and fingers. We concentrated on the impact of varied magnitudes of lateral and vertical motions while designing the second dome. Wave interference existed and happened moving across the surface due to its form, however the entire structure was moved by the vertical and horizontal movement created by connected actuators. As a result, distinct magnitudes of actuation in the vertical and horizontal axes might be used to amplify a point  
   
  Generating Haptic Sensations over Spherical Surface  
   
  51  
   
  of maximal vibration. These magnitude maxima might potentially be used with wave interference maximum to enhance tactile signals and focus given vibration received on the surface.  
   
  Fig. 6. Top: A relaxed hand is put over a spherical prototype to demonstrate a comfortable position. Middle: Exploratory behavior displayed by just touching the finger pads. Bottom: Four fingers extended straight ahead, as if attempting to feel the smooth edge of a surface [11].  
   
  3 Methods We investigated two ways for determining the ideal offset in establishing a point of peak magnitude vibration approximately five centimeters from the hemisphere’s base. This was determined by going vertically over the sphere’s surface from the first actuator (A, Figs. 2 and 3 [11]). The first approach involved evaluating a variety of offset vibrations on the sphere between a starting pair of actuators (AB). After determining the offset necessary to achieve maximum vibration interference between these two actuators, we tested a third actuator by offsetting it against the existing offset pulse of the first two actuators (ABC). This procedure was repeated for the fourth actuator, offsetting it against the prior three actuators’ offset pulses (ABCD). MicroSense sensors were used to collect data (Model 5622-LR Probe, with 0.5 mm x 2.5 mm sensor). Because it is a capacitive sensor, a copper strip was needed to be put across the surface of the sphere in order for measurements to be taken. The sensor has an accuracy of 0–200 µm and noise of 3.44 µm-rms at 5 kHz, and it has been amplified using Gauging Electronics up to 10 V and attenuated to a range of 0 to 5 V to be compatible with the Arduino’s analogue input. The sensor was positioned to track the curvature of the sphere.  
   
  4 Results 4.1 Constructive Wave Interference Figure 7 indicates that the addition of the third actuator (C) greatly increased displacement while the addition of the fourth actuator (D) introduced just a little rise. A probable constructive wave interference occurred between the first two actuators A and B, with actuator A triggering 2 ms before actuator B, resulting in a displacement of 183 µm. The vibration was raised to 257 µm by triggering the third actuator 3 ms after actuator  
   
  52  
   
  P. Coe et al.  
   
  B. However, activating the fourth actuator resulted in a very little increase in peak vibration. Before actuator A raised the vibration to 276 µm, the fourth actuator (D) engaged for 24 ms. Nonetheless, actuator (D) can have a good effect on the total vibration. The vibration was decreased to 203 µm by delivering a deconstructive pulse 19 ms before actuator A.  
   
  Fig. 7. Measured maximum displacement using offsets for two (AB), three (ABC) and four (ABCD) actuators.  
   
  We conducted more extensive testing because we were unsure whether the actuators in a spherical setup would interact with each other in the same way as those in a flat actuation plane. We tried out every possible combination of offsets between each of the four actuators for 15 ms before and after each other. This process’s optimum offset resulted in a maximum displacement of 276 µm, which is equal to the prior result when all four actuators were triggered. Figure 8 displays the maximum displacement when the full range offset sweep test is used to determine the offsets needed to achieve a maximum displacement. The discovered offsets varied, showing that there are numerous ways to obtain a peak vibration maximum. We also discovered that the offset is the consequence of actuators (B) and (C) being triggered 5 ms after actuator (A), and actuator (D) being triggered 9 ms after actuator (C). Although we consider the data gained by cycling through every conceivable combination offers highly precise offsets, the approach is hampered by the length of time necessary to measure all combinations as well as the volume of data that must be collected. Based on this information, we predict that, while some waves will most likely pass over the surface, the semi-flexible connection to the base implies that actuators would most likely pull the entire object. We must consider not just the delay of wave propagation, but also the movement of the entire dome. We must test the optimal magnitudes  
   
  Generating Haptic Sensations over Spherical Surface  
   
  53  
   
  Fig. 8. Measured maximum displacement using offsets when scanning through all four actuators simultaneously.  
   
  and phases of each signal applied to each actuator in addition to determining the needed offset delays. 4.2 Combination of Peak Displacement Magnitudes To remedy the earlier issue with wave propagation, we conducted additional testing by mixing different actuation magnitudes with offset triggering. We utilized the second prototype for this test (Figs. 4 and 5) featuring Lofelt L5 actuators for X and Y vibrations and a central Tectonic actuator for Z axis movement. In particular, we activated the Lofelt L5 actuators across the X-axis for 10 ms and the center actuator for 1 ms. This arrangement should minimize the magnitude of the central actuator’s actuation in comparison to the Lofelt L5 actuators. We experimented with various offsets to calculate the optimal vibration offset (Fig. 10). The data displayed in Fig. 9 exhibits a pattern until roughly the third point, when the sphere’s angle begins to become more horizontal. The practical effect of this tendency is that we are not only feeling forces attributable to wave interference from vertically positioned actuators, but also vertical displacement of the whole hemisphere caused by horizontally placed actuators. We would need to measure distinct magnitudes for a fixed offset rather than a changing offset in the future. Magnitude may be changed by modifying the size of the pulse or the voltage applied to a certain actuator. Additional testing should be performed to determine the range of these modifications and their impact on the resultant vibration.  
   
  54  
   
  P. Coe et al.  
   
  Fig. 9. Offset required for maximum peak vibrations for Lofelt L5 actuators with and without Tectonic actuator (center coil).  
   
  Fig. 10. Time offset required for maximum peak vibrations while activating Lofelt L5 actuator (1) for 10 ms and tectonic actuator (5) for 1 ms.  
   
  Generating Haptic Sensations over Spherical Surface  
   
  55  
   
  5 Providing Enhanced Immersion to Video Content Application Pilot Study An initial pilot study has been implemented to understand the initial effectiveness of the spherical display. Haptic sequences were induced over a tactile spherical surface which supported the hands. Participants watched a 1-minute video tested under two conditions: with and in the absence of accompanying haptic signals. Their heart rate was measured during the experiment. Results revealed a significant difference in heart rate data between the two conditions. Moreover, a heart rate response has allowed to reveal significant difference between viewers with respect to visual content and haptification (t(119) = 31.4 vs t(119) = 11.7 (p < .0001)). This leads us to believe that an enhanced immersion and affective experience can be achieved through the addition of a haptic channel to audio-visual content that already are shared over video streaming platforms and social media. 5.1 Pilot Study Background Haptic theater enhancements, such as D-Box Motion Effect chairs [49] are already in use in multiple locations worldwide to enhance the viewing experience by adding haptic signals to 3D visuals. Being synchronized by the action depicted in the film, the chair’s effects range from soft vibration to a hard jolt backwards if, for example, a character is hit. But what about home viewers that stream videos online through platforms such as YouTube or Vimeo? People share their life experience through these tools; yet rich emotional personal experiences cannot be shared in full as other senses such as smell, taste, and touch are missing. Our focus is on the haptic channel, which has been in use already for nearly 200 years [24, 57] to share, communicate, and enhance human sensations. Haptic signals directly connected with kinesthetic sense and motor imagination are strong enough to provoke premotor or ideomotor actions (the cognitive representation of an action) [32]. Additionally, to induce or initiate haptic apprehension in observed visual actions, haptic information has to be personally and emotionally significant and linked to previous human experience. Human vision is not only limited to visual feedback but also helps with navigation and locomotion, contributing to human proprioception. As in hunter-gatherers, human vision has been developed to predict behavior of any items, including motion, through the total control of the personal space [28]. Proprioception is the main component of the afferent flow that integrate information from different modalities to support adequate human response and behavior. Cross-modal information transfer help to more efficiently perceive and interact with an external space surrounding the body [23]. In this work, we would like to explore haptification of visual content supposed to enhance emotional effects in relation to the activity of an actor in the dramatic situation of an activity that is able to elicit fear, anxiety, or general sympathetic activation [50]. More specifically, we asked participants to watch a video clip of down-hill biking as the dramatic competition scenario while interacting with a spherical haptic surface (SHS) [11].  
   
  56  
   
  P. Coe et al.  
   
  There are different systems for haptic effects based on haptification model [63], plethora of video clips have not been yet investigated sufficiently with respect to such a way of affective visualization. In particular, haptics does not yet integrate with dynamic visualization in multimedia, even though both are naturally and tightly connected in the ontogenesis of perception [39] and development of intermodal perception and imagination [74]. The scenario that we are interested in is users watching video clips, movies, and other dynamic art media. The viewer is a relatively passive observer and cannot directly impact to the digital content. The viewer’s state of mind and video content, which are closely intertwined in human imagination, contribute to the immersion and transition of the passive observer into active participant of the visual scene, while the sensory motor activity manifests in the form of emotions and physiological reactions. To induce human imagination, video content is often accompanying with background audio (soundtrack). 5.2  
   
  Experimental Design  
   
  The experiment combined visual content presented through VR headset enhanced with a new haptic concept that combine spherical and planar sensations [10–12]. We also expect that the haptification [63] of visual content is a natural way to support affective visualization [82]. Participants. Six people participated in this study. None of the participants reported skin or cardiovascular issues. Apparatus. The experiment is designed around a Microsoft Surface Go tablet (Fig. 11) that was used as a Haptic Actuation Plate (HAP) and was chosen for its ease of software and hardware implementation. The Spherical Haptic Surface (SHS) consists of a 116 mm diameter polycarbonate dome (Fig. 11) attached to the surface of the tablet. Parameters for local interference maximum (LIM) of seismic signals over the touchscreen tablet and propagating across SHS were determined in previous studies [10, 11]. The TEAX1402-8 actuators attached to the top of the tablet display, as well as TEAX25C10-8/HS affixed to the base of the SHS [11], are designed to strike the surface at a predetermined offset from each other. The offset creates a point of increased vibration where the seismic shear waves interfere. This allows us to create discernable, feelable, and dynamic haptic LIM signals moving over either touchscreen or other surfaces properly affixed to and being in mechanical contact with HAP. The method described can be explored in further detail in previous works. [12]. An Arduino Due was used to store the predetermined offset locations as well as the output sequence created for this experiment. Output signals were sent to an external MX1508 motor drive module driven at 7.5 V. Silicone molded legs were attached to the bottom of the tablet to provide vibration isolation. A Huawei Band 6 watch was used to track heart rate data with ECG comparable accuracy [67] that are logged in 5-second intervals.  
   
  Generating Haptic Sensations over Spherical Surface  
   
  57  
   
  Fig. 11. Left: An arrangement of Tectonic actuators and SHS over tablet touchscreen. Blue arrows indicate virtual haptic scanpaths Right: A participant interacting with Spherical haptic surface affixed to MS Surface Go tablet during the experiment. (Color figure online)  
   
  Video Content and Haptic Actuation. A Samsung head-mounted display (HMD) Odyssey VR headset [64] was used to display the visual content that consisted of one-minute video, created from publicly available online footage and was converted to match the requirements of the display. The output signals of the actuators that generate the haptic cues were synchronized with the actions in the video footage resulting in haptic motion sequences matching the motion presented in the visual scenes. The haptification (vibration sequences) has been manually designed using video footage timings where each second of footage was visually analyzed to associate tactile sense with objects that are in proximity or visual periphery.  
   
  6 Experimental Procedure Participants were instructed to first put the Huawei Band 6 watch on their left wrist. They were then asked to put the VR headset on, wear the headphones, and adjust the volume to a comfortable level. To start exploring the video, they were asked to rest both hands on the SHS. A one-minute clip of down-hill biking was used (Fig. 12). This clip was played 20 times. Ten of these playbacks were with the haptic (WH) signals synchronized with the video content, while the remaining ten playbacks were with no haptification (NH), resulting in two experimental conditions. After the experiment, participants were asked to complete the NASA-Task Load Index (TLX) questionnaire [31] along with additional questions to help us understand the effectiveness of device. These questions are shown in Table 1.  
   
  7 Results Objective Results. We can assume participants likely had different experiences biking, ways of thinking, temperaments and possibly even moods during the test. Therefore, we cannot expect the same response from viewers to specific visual content and haptic stimulation. Nevertheless, heart rate scores have been measured 12 times and grouped  
   
  58  
   
  P. Coe et al.  
   
  Fig. 12. Key dramatic footage at seconds 5, 24, 36, 40, 41, 43, 45, 50, 52. Table 1. Questionnaire questions. Did you perceive a local and/or moving vibration? On a scale from 1–5 how well did you perceive the Localized vibration? Are you prone to motion Sickness in VR? If yes, did the vibration Worsen or alleviate your symptoms? In general, which did you Prefer. The experience with or without the vibration? In what devices do you think This would be best suited? Why? What benefit could Localized haptification provide to an existing device?  
   
  based on the Pearson correlation coefficient r > 0.83 (p < 0.001) and r < 0.8 (p < 0.001) over all (12) presented footage. The heart rate response has revealed significant difference between viewers with respect to visual content and haptification. In all situations observed over footage, with all participants, we see a significant consistent difference between instances when the haptic signals were enabled (WH) vs. when the haptic signals were disabled (NH) (Fig. 13). Paired Samples 2-tailed t-test revealed significant difference within the group r = .9023, t(59) = –2.98, p = .02, 95% CI(WH/NH) = [82/84, 75/77]. When haptic signals were enabled, we have observed a marked decrease in heart rate for the entire video sequence. When looking at heartrate data with haptic signals enabled, we see a gradual increase from the start of the sequence up until a peak at the 30 s mark. If we compare this with the data, we have collected when haptics was disabled (NH), we see a far less pronounced synchronization r = 0.688 (p < .05) in average with the events on screen. That is to say that the data shows an increased correlation when the haptic display is in use (WH) r = 0.90 (p < .0001) in average. Subjective Results The NASA-TLX Questionnaire. The NASA-TLX questionnaire allows the evaluation of six sub-scales: the mental demand, physical demand, temporal demand, own performance, effort, and frustration the results of the TLX are summarized in Fig. 14. Participants responded that the workload demand was of concern, with an average response of 50 on the NASA-TLX scale. For several of our participants this was their  
   
  Generating Haptic Sensations over Spherical Surface  
   
  Fig. 13. A cardiovascular response in participants.  
   
  Fig. 14. Radar chart of all TLX responses.  
   
  59  
   
  60  
   
  P. Coe et al.  
   
  first experience wearing a VR headset. Otherwise, with an average score of 76, participants felt that they were able to successfully complete the task they were given. Post-experiment Questionnaire. When asked if they could feel the presence of localized moving vibrations, all participants confirmed that they could. Their ability to perceive localized vibrations obtained an average score of 3.125 out of 5 (5 being the best). 62.5% of the participants reported that they preferred watching the VR video using the SHS. Half of our participants answered that they had felt motion sickness during the experiment. Of those who felt motion sickness, half claimed that the use of the SHS alleviated their symptoms. Open-ended questions revealed that participants think that the SHS can be used for Gamepads to improved games interactivity, for professional training for drivers and pilots, and in-car displays to eliminate distractions. 7.1  
   
  Pilot Study Discussion  
   
  Artists have been developing the expression of motion in drawings for thousands of years. To illustrate motion, we can refer to the artists of the Lascaux caves who portrayed animals upon the walls with multiple heads, legs, and tails. The superimposition and matching successive images (e.g., juxtaposition of colors) have been used by many artists in a different ways to achieve a similar result. The visualization of dynamic motion is followed by a long tradition of motion visualization throughout the history of visual art [53, 57]. The impression of movement in a still image can bring about an emotional experience. As mentioned by Barry Ackroyd “It can bring you to tears, and take you to places unimaginable” [1]. In modern cinema, it might be thought that we can only assess the meaning of audio in movies when a soundtrack has been interrupted. On the contrary, thanks to imagination, the soundtrack is often able to fully compensate the lack of visual peripheral immersion in scenes. The collected data showed to us a significant difference between two conditions of the presence or absence the haptic information concerning signals accompanying in sync with visual content viewing with the VR headset. Furthermore, the relative accuracy at which heart rates increased to match the viewed activity on the screen was prominent with the haptic enhancement relying on SHS. This falls in-line with existing research on heart rate variability in virtual reality immersion [50]. The assumption would be that the participants are greater immersed when receiving physical sense of contact in connection with the audio-visual content. Seeing that the use of the haptic signals subjectively may have reduced symptoms of motion sickness, we can assume that the experience may have been more immersive. Giving participants accurate haptic enhancement may have decreased the cognitive load of what would be an audio-visual only experience. Similarly Liu et al. have also found that the introduction of haptics can be used to reduce virtual reality sickness [46]. A SHS with localized dynamic haptic signals moving in sync with visual events of scenes may be the key to better immersion. While our current study was focused on the use of the SHS to improve the VR experience, the technology involved could be implemented in a wide variety of devices to enhance the user contact experience.  
   
  Generating Haptic Sensations over Spherical Surface  
   
  61  
   
  7.2 Pilot Study Conclusion The hypothesis that we set to explore was shown to be valid. Yes, localized dynamic haptic signals can support affective visualization. Participants were eager and excited to try out the Spherical Haptic Surface. Even better, they were able to see and understand possible use cases for this technology in modern devices. The data we collected contained other captured sensor information, including SPO2 levels and stress levels. In the future we would like to further parse this data out so that we can get a better understanding of how this data might support our results. The next step would be to see how a SHS might aid in the creation of visual experiences without visual data, by inducing imagination by haptic signals. How would the dynamic haptic signals affect our audio only experiences, or haptic symbolic patterns (tactons) only experiences? The use of emerging haptic technologies needs to be studied in depth to understand how they should be implemented to improve our daily interaction through computing devices. Our contributions are as follows: We investigated the subjective perception of the video content enhanced with haptic signals dynamically presented in motion over spherical haptic surface to the palms; We found that haptic signals moving in sync with video content had a significant effect on the participant’s heart rate.  
   
  8 Discussion and Future Work A wide spectrum of users would be able to engage with the high-fidelity spherical display. The sphere has a natural shape on which a user may rest their hands for lengthy periods of time. As we continue to have access to a wider range of interactive technology, we will need to investigate new intuitive techniques of feedback and engagement. Our visual culture has a significant influence on human intellectual and creative capacity, as well as the development of perceptual and motor skills [38]. Despite the significance of haptics in the evolution of human perception, visual information tends to prevail over haptic perception after spatial visual representations of distance, size, shape, and motion have been formed [6, 43]. Much of the visual content currently available is often inaccessible to blind and visually impaired people [37]. Learning is frequently found to increase with the help of visual feedback, leaving persons with visual impairment suffering in courses. Fortunately, it has been demonstrated that the use of haptic feedback can help to bridge the gap between visual and tactile learning. The use of haptics in education may be broadened to help all students who are compelled and linked to a subject, for example, by constructing a bridge between the sciences and physical reality. David Grow’s work on educational robots has yielded successful outcomes in this area [29] as well as by Michael Pantelios [58] with input gloves and force-feedback devices. Much research available [8, 25, 30, 54] would imply that incorporating haptics into the educational environment at all levels can boost student learning. A spherical surface, such as the one we’re testing, can give a long-lasting polycarbonate surface that can sustain heavy, frequent use. It also gives kids with a unique surface to explore. It is feasible to combine it with a spherical projection [26, 81, 83] over the surface, displaying an interactive picture or video that may be explored via tactile feedback.  
   
  62  
   
  P. Coe et al.  
   
  Because we do not limit our notion to any size and hope to be able to repeat our findings in larger and smaller spherical shapes, we open up the concept of spherical haptics to a wide range of applications. In place of an analog control stick on a gaming controller, we propose a haptic hemisphere. Feedback, in addition to giving accurate input, may be modified to produce a number of effects. For example, the texture of a game may vary as you go through uneven terrain, or the localization of feedback could reveal the location of an adversary. We may picture a bigger sphere being used to precisely manage heavy machinery in 3D space, such as a crane lifting a concrete slab. A spherical display in the center of a round table might be utilized as an interactive map to assist a team in collaborating with localized feedback offering extra information to prevent visual overload. Localized input might alert a taskforce to the presence of subsurface structures or other areas of interest. Manufacturing technology are always evolving. We are moving away from the strict constraints of consumer electronics design, as the continued trend of miniaturization, as well as the development of flexible displays and innovative molded integrated circuits, means that products may now assume any imagined shape or form. Many user interfaces, such as a mouse, gamepad, or even a car steering wheel, already include significant portions with substantial curvature. We should be able to enhance the bandwidth accessible to the user by providing vibration that can be localized at any point across these surfaces, allowing us to offer new, more natural engagement cues. To go further, we are aware that existing virtual reality headsets provide credible visual input but have yet to provide high-fidelity haptic feedback to a large number of consumers. Incorporating this improved realism into present controllers might provide a new degree of immersion to current technology [2]. There is also the possibility of using such a spherical gadget in public places. The extra benefit of accurate tactile feedback might not only make an information kiosk more generally accessible, but also assist users in navigating the device in a loud setting such as a mall [18]. Surprisingly, our recent research discovers that the offsets required to produce localized vibration locations occur within milliseconds of each other. This means that it may be feasible to swiftly and progressively trigger various offsets to generate several focus points that appear to be synchronous. This would open up a new channel for the development of haptic patterns, as well as the development of haptic imagination. The ultimate objective of this study is to create a perceivable moveable actuation that can be mediated to any position on the spherical surface. What we’d call a virtual haptic actuator. We would need to look into this more to find the best combinations that provide the most effective (and immediately recognizable) numerous afferent flows, increasing in intensity to a specific spot or along edges around the sphere’s surface. We would also need to investigate how wave interference can be combined with different magnitude combinations to increase the precision and force of a given vibration across the surface of the sphere, as well as how perceptual interference of other receptive fields can affect fingertip tactile sensation. [45]. Intermediate materials have been shown to improve an object’s sensation of touch. Auto body shops, for example, have utilized cellophane film to evaluate polishing on cars [65]. Additionally, the Touch Enhancing Pad [61], a patented instrument consisting  
   
  Generating Haptic Sensations over Spherical Surface  
   
  63  
   
  of lubricant placed between two thin plastic sheets can help identify malignancies in breast tissue. As a consequence, it would be worthwhile to experiment with various materials in order to improve the perceptible localized input in our haptic environments. As this study progresses, we will have a greater knowledge of the use cases that this developing approach may bring to users.  
   
  9 Future Applications Spherical interfaces do exist [4, 5, 16, 72, 78], nonetheless, they are not widely used. The suggested technology for high-fidelity haptic feedback offers up several options for future touch interaction. Because the sphere’s shape adapts to the hand in its natural resting posture [36, 52] it may be utilized as a generic computing interface with rich tangible information that can be used for long periods of time. Hidden or veiled entities and deeper structures of a palpated substance, whether biological (tumor) or physical (defect inspection), can be increased with localized haptic input while investigating medical images. Similarly, the basic user interface might be improved, such as allowing a user to feel and pick icons on a desktop that are beneath a document they are working on without having to minimize or switch windows. The spherical surface needs a significantly reduced range of motion to engage with, which might be advantageous for anyone suffering from a movement-impairing injury or sickness. High-fidelity feedback can also assist persons with little or no eyesight in navigating an operating system by employing detailed haptic images. We don’t consider the spherical interface as being restricted in size. A bigger childsized spherical interface might enable children to study instructional content in a more engaging manner. An adult-sized spherical display may serve as a kiosk at a mall, presenting information that, on a visual-only flat display, can be confounding, such as orientation or direction. A big sphere might serve as an interface in the center of a circular meeting table, allowing people to collaborate. Meetings are frequently stopped to address minor matters, such as informing a coworker that a file has been sent or that they need to slip out. This information might be transmitted utilizing high-definition haptics, which would eliminate unwanted interruptions. A haptic spherical interface also opens up intriguing new possibilities for enabling secure entrance into a gadget. A passcode, for example, predicated on recognizing localized light-pressure patterns, may be actively moved around the surface while still being identified by localization. Although the input pattern would stay constant, the constant change of the physical location would make capturing the passcode by a third party challenging. From the outside, each physical entry of the passcode would appear to be unique. Overall, we envision a broad range of applications that can benefit from the usage of a high-fidelity spherical haptic interface. Unlike many consumer interfaces already on the market, we see the spherical haptic interface as extremely customizable and open to a wide range of design use cases.  
   
  64  
   
  P. Coe et al.  
   
  10  
   
  Conclusions  
   
  We discovered that offset actuations may be employed to enhance vibrations at specified spots on a spherical surface based on objective measurements of constructive interference of spherical structure prototypes. These magnifications can be achieved through a combination of two methods: first, through wave interference, in which we can use the properties of constructive wave interference to create an amplified point on the surface, and second, through the combination of peak displacement magnitudes, in which different forces are applied to an object’s X, Y, and Z axes to increase forces felt at a specific point across the surface. The current study shows that a localized vibration effect may be reproduced over a curved surface. Second, using magnitude combinations, we acquired preliminary data indicating that increased amplitude has an impact at a given position over the surface. In this study, we discovered that once offsets are located and set, the subsequent output is very constant. Localization offsets should only need to be collected once for a particular actuator setup, which is critical for sustainability. It has also been established that the usage of numerous installed actuators compensates for observed losses owing to attenuation of individual actuators. When compared to existing global non-localized vibrations that generate a muddled sensation, the amount of localization exhibited has the potential to boost consumers’ immersion in XR settings. This field of dispersed haptic resolution can be likened to the fields of optical and auditory propagation. Improvements in haptic fidelity, like improvements in other sensory modalities, aim to improve the user experience. The utilization of a virtual vibration point over three-dimensional curved constructions is demonstrated in this paper. When producing high-fidelity haptics, this may enable the use of fewer actuators in a variety of feedback interfaces.  
   
  References 1. Ackroyd, B.: But, is it art? (June 2021). www.britishcinematographer.co.uk/but-is-it-art/ 2. Al-Sada, M., Jiang, K., Ranade, S., Piao, X., H¨oglund, T., Nakajima, T.: Hapticserpent: A wearable haptic feedback robot for VR, pp. 1–6 (April 2018). https://doi.org/10.1145/ 3170427.3188518 3. Barresi, J., Moore, C.: Intentional relations and social understanding. Behav. Brain Sci. 19, 107–122 (1996). https://doi.org/10.1017/S0140525X00041790 4. Benko, H., Wilson, A., Balakrishnan, R.: Sphere: Multi-touch interactions on a spherical display, pp. 77–86 (January 2008). https://doi.org/10.1145/1449715.1449729 5. Bolton, J., Kim, K., Vertegaal, R.: Snowglobe: A spherical fish-tank VR display, pp. 1159– 1164 (January 2011). https://doi.org/10.1145/1979742.1979719 6. Burtt, H.E.: Tactual illusions of movement. J. Exp. Psychol. 2(5), 371–385 (1917). https:// doi.org/10.1037/h0074614 7. Chartrand, T.L., Bargh, J.A.: The chameleon effect: the perception-behavior link and social interaction. J. Pers. Soc. Psychol. 76(6), 893 (1999) 8. Christodoulou, S., Garyfallidou, D., Gavala, M., Ioannidis, G., Papatheodorou, T., Stathi, E.: Haptic devices in virtual reality used for education: Designing and educational testing of an innovative system (September 2005)  
   
  Generating Haptic Sensations over Spherical Surface  
   
  65  
   
  9. Coe, P., Evreinov, G., Raisamo, R.: Gel-Based Haptic Mediator For High-definition Tactile Communication, pp. 7–9 (October 2019) https://doi.org/10.1145/3332167.3357097 10. Coe, P., Evreinov, G., Sinivaara, H., Hippula, A., Raisamo, R.: Haptic actuation plate for multi-layered in-vehicle control panel. Multimodal Technol. Interact. 5, 25 (2021). https:// doi.org/10.3390/mti5050025 11. Coe, P., Evreinov, G., Ziat, M., Raisamo, R.: Generating Localized Haptic Feedback over a Spherical Surface, pp. 15–24 (Janury 2021). https://doi.org/10.5220/0010189800150024 12. Coe, P., Farooq, A., Evreinov, G., Raisamo, R.: Generating Virtual Tactile Exciter For Hd Haptics : A Tectonic Actuators’ Case Study, pp. 1–4 (October 2019). https://doi.org/10.1109/ SENSORS43011.2019.8956569 13. Culbertson, H., Schorr, S., Okamura, A.: Haptics: The present and future of artificial touch sensation. Annu. Rev. Control Robot. Autonom. Syst. 1 (2018). https://doi.org/10.1146/ annurev-control-060117-105043 14. Cutting, J., Vishton, P.: Perceiving Layout and Knowing Distances: The Interaction, Relative Potency, And Contextual Use Of Different Information About Depth, vol. 5, pp. 69–177 (January 1995) 15. Dangxiao, W., Yuan, G., Shiyi, L., Zhang, Y., Weiliang, X., Jing, X.: Haptic display for virtual reality: progress and challenges. Virt. Reali. Intell. Hardware 1(2), 136–162 (2019) 16. Daniel, S., Wright, C., Welland, S.: Spherical display and control device (13 July 2010), uS Patent 7,755,605 17. Enferad, E., giraud audine, C., Fr´ed´eric, G., Amberg, M., Semail, B.: Generating controlled localized stimulations on haptic displays by modal superimposition. J. Sound Vibr. 449 (2019). https://doi.org/10.1016/j.jsv.2019.02.039 18. Evreinov, G., Raisamo, R.: Information kiosks for all: issues of tactile access. In: Proceedings of WWDU 2002 (January 2002) 19. Evreinova, T., Evreinov, G., Raisamo, R.: From kinesthetic sense to new interaction concepts: Feasibility and constraints. Int. J. Adv. Comput. Technol. 3(4), 1–33 (2014) 20. Evreinova, T., Evreinov, G., Raisamo, R.: Virtual sectioning and haptic exploration of volumetric shapes in the absence of visual feedback. In: Advances in Human-Computer Interaction 2013 (July 2013). https://doi.org/10.1155/2013/740324 21. Evreinova, T., Evreinov, G., Raisamo, R.: An exploration of volumetric data in auditory space. J. Audio Eng. Soc. 62, 172–187 (2014). https://doi.org/10.17743/jaes.2014.0008 22. Evreinova, T., Evreinov, G., Raisamo, R.: Evaluation of effectiveness of the stickgrip device for detecting the topographic heights on digital maps. Int. J. Comput. Sci. Appli. 9, 61–76 (2012) 23. Evrienova, T.G., Evreinov, G., Raisamo, R.: Cross-Modal Assessment of Perceptual Strength of Communication Signals Presented in Auditory and Tactile Modalities (2009) 24. Farrell, G.: Fingers for Eyes. Harvard University Press (1969) 25. Fern´andez, C., Esteban, G., Conde-Gonz´alez, M., Garc´ıa-Pe˜nalvo, F.: Improving motivation in a haptic teaching/learning framework. Int. J. Eng. Educ. 32, 553–562 (2016) 26. Ferreira, F., et al.: Spheree: A 3d perspective-corrected interactive spherical scalable display (August 2014). https://doi.org/10.1145/2614066.2614091 27. Follmer, S., Leithinger, D., Olwal, A., Hogge, A., Ishii, H.: inform: Dynamic physical Affordances and Constraints Through Shape And Object Actuation, pp. 417–426 (October 2013). https://doi.org/10.1145/2501988.2502032 28. Goldstein, E.B., Brockmole, J.R.: Sensation and perception. Cengage Learning (2017) 29. Grow, D., Verner, L., Okamura, A.: Educational Haptics, pp. 53–58 (January 2007) 30. Hamza Lup, F., Stefan, I.: The haptic paradigm in education: Challenges and case studies (November 2018)  
   
  66  
   
  P. Coe et al.  
   
  31. Hart, S.G.: Nasa-task load index (nasa-tlx); 20 years later. In: Proceedings of The Human Factors And Ergonomics Society Annual Meeting,vol. 50, pp. 904–908. Sage publications Sage CA, Los Angeles, CA (2006) 32. Hommel, B.: Ideomotor action control: On the perceptual grounding of voluntary actions and agents. In: Action Science Foundations of an Emerging Discipline (2013). https://doi. org/10.7551/mitpress/9780262018555.003.0005 33. Hudin, C., Lozada, J., Hayward, V.: Localized tactile feedback on a transparent surface through time-reversal wave focusing. IEEE Trans. Haptics 8 (2015). https://doi.org/10.1109/ TOH.2015.2411267 34. Huitema, E.: The future of displays is foldable. Inf. Display 28, 6–10 (2012). https://doi.org/ 10.1002/j.2637-496X.2012.tb00470.x 35. Jang, S., Kim, L., Tanner, K., Ishii, H., Follmer, S.: Haptic Edge Display For Mobile Tactile Interaction, pp. 3706–3716 (May 2016). https://doi.org/10.1145/2858036.2858264 36. Jeannerod, M.: The timing of natural prehension movements. J. Motor Behav. 16(3), 235– 254 (1984) 37. Jones, G., Minogue, J., Oppewal, T., Cook, M., Broadwell, B.: Visualizing without vision at the microscale: Students with visual impairments explore cells with touch. J. Sci. Educ. Technol. 15, 345–351 (2006). https://doi.org/10.1007/s10956-006-9022-6 38. Kantner, L.A., Segall, M.H., Campbell, D.T., Herskovits, M.J.: The influence of culture on visual perception. Stud. Art Educ. 10(1), 68 (1968). https://doi.org/10.2307/1319670 39. Kellman, P.J.: Chapter 9 - ontogenesis of space and motion perception. In: Epstein, W., Rogers, S. (eds.) Perception of Space and Motion, pp. 327–364. Handbook of Perception and Cognition, Academic Press, San Diego (1995). https://doi.org/10.1016/B978-012240530-3/ 50011-0, www.sciencedirect.com/science/article/pii/B9780122405303500110 40. Kim, S.C., Han, B.K., Kwon, D.S.: Haptic rendering of 3d geometry on 2d touch surface based on mechanical rotation. IEEE Trans. Haptics 1 (2017). https://doi.org/10.1109/TOH. 2017.2768523 41. Kim, S., Park, G., Kim, S.C., Jung, J.: Surface haptics, pp. 421–425 (November 2019). https://doi.org/10.1145/3343055.3361925 42. Klare, S., Peer, A.: The formable object: A 24-degree-of-freedom shape-rendering interface. IEEE/ASME Trans. Mechatron. 20(3), 1360–1371 (2014) 43. Klevberg, G., Anderson, D.: Visual and haptic perception of postural affordances in children and adults. Hum. Movement Sci. 21, 169–86 (2002). https://doi.org/10.1016/S01679457(02)00100-8 44. Krufka, S., Barner, K., Aysal, T.: Visual to tactile conversion of vector graphics. IEEE Trans. Neural Syst. Rehabilit. Eng. Publicat. IEEE Eng. Med. Biol. Soc. 15, 310–21 (2007). https:// doi.org/10.1109/TNSRE.2007.897029 45. Lakshminarayanan, K., Lauer, A., Ramakrishnan, V., Webster, J., Seo, N.J.: Application of vibration to wrist and hand skin affects fingertip tactile sensation. Physiol. Reports 3 (2015). https://doi.org/10.14814/phy2.12465 46. Liu, S.H., Yu, N.H., Chan, L., Peng, Y.H., Sun, W.Z., Chen, M.: Phantomlegs: Reducing Virtual Reality Sickness Using Head-worn Haptic Devices, pp. 817–826 (March 2019). https:// doi.org/10.1109/VR.2019.8798158 47. Loomis, J.M.: Tactile pattern perception. Perception 10(1), 5–27 (1981). https://doi.org/10. 1068/p100005 48. Loomis, J.M., Lederman, S.J.: Handbook of Perception and Human Performance Volume 1: Sensory processes and perceptiong, 2nd. edn., vol. 1. Wiley-Interscience, New York (1986) 49. Loria, D.: A moving experience: D-box celebrates 10 years in the cinema business (July 2019). www.boxofficepro.com/d-box-immersive-seating-10-year-anniversary/  
   
  Generating Haptic Sensations over Spherical Surface  
   
  67  
   
  50. Mali´nska, M., Zu˙zewicz, K., Bugajska, J., Grabowski, A.: Heart rate variability (hrv) during virtual reality immersion. Int. J. Occupat. Safety Ergonom. 21, 47–54 (2015). https://doi.org/ 10.1080/10803548.2015.1017964 51. Mansour, N., Fath El Bab, A., Assal, S.: A Novel Sma-based Micro Tactile Display Device For Elasticity Range Of Human Soft Tissues: Design And Simulation (August 2015). https:// doi.org/10.1109/AIM.2015.7222574 52. McRae, L.T., McRae, B.J.: Implements usable by persons afflicted with arthritis (19 July 1977), uS Patent 4,035,865 53. Michaud, P.A.: Aby Warburg and the image in Motion. Zone Books (2007) 54. Minogue, J., Jones, M.: Haptics in education: Exploring an untapped sensory modality. Rev. Educ. Res. - REV EDUC RES 76, 317–348 (2006). https://doi.org/10.3102/ 00346543076003317 55. M¨uller-Rakow, A., Hemmert, F., Wintergerst, G., Jagodzinski, R.: Reflective haptics: Resistive force feedback for musical performances with stylus-controlled instruments (May 2020) 56. Oakley, I., Brewster, S., Gray, P.: Communicating with Feeling, pp. 61–68 (January 2001). https://doi.org/10.1007/3-540-44589-7 7 57. Olstrom, C.: Undaunted by Blindness, 2nd edn. Ebookit.com (2012). www.books.google.fi/ books?id=k9K77s1IRgoC 58. Pantelios, M., Tsiknas, L., Christodoulou, S., Papatheodorou, T.: Haptics technology in educational applications, a case study. JDIM 2, 171–178 (2004) 59. Parisi, D., Farman, J.: Tactile temporalities: The impossible promise of increasing efficiency and eliminating delay through haptic media. Conver. Int. J. Res. New Media Technol. 135485651881468 (2018). https://doi.org/10.1177/1354856518814681 60. Park, J., Kim, J., Oh, Y., Tan, H.Z.: Rendering moving tactile stroke on the palm using a sparse 2D array. In: Bello, F., Kajimoto, H., Visell, Y. (eds.) EuroHaptics 2016. LNCS, vol. 9774, pp. 47–56. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-42321-0 5 61. Perry, D., Wright, H.: Touch enhancing pad (2009), patent No. 4,657,021, Filed April 13th., 1989, Issued Aug. 24th., 1993 62. Rock, I., Victor, J.: Vision and touch: An experimentally created conflict between the two senses. Science 143(3606), 594–596 (1964) 63. Saboune, J., Cruz-Hernandez, J.M.: Haptic effect authoring tool based on a haptification model (3 May 2016), uS Patent 9,330,547 64. Samsung: Hmd odyssey (mixed reality) (Mar 2021). www.samsung.com/us/support/ computing/hmd/hmd-odyssey/hmd-odyssey-mixed-reality/ 65. Sano, A., Mochiyama, H., Takesue, N., Kikuuwe, R., Fujimoto, H.: Touchlens: Touch enhancing tool, pp. 71–72 (December 2004). https://doi.org/10.1109/TEXCRA.2004. 1425003 66. Schwaen, R., Arlt, R.: Effective assignments and haptic teaching methods in architectural structure. In: Structures and Architecture, pp. 838–845. CRC Press (2016) 67. Scientist TQ: Huawei band 6 complete scientific review (Jun 2021). www.youtube.com/ watch?v=QDQzzyQFYQs 68. Segall, M.H.: The influence of culture on visual perception (1966) 69. Shin, S., Choi, S.: Geometry-based haptic texture modeling and rendering using photometric stereo, pp. 262–269 (March 2018). https://doi.org/10.1109/HAPTICS.2018.8357186 70. Slote, M.: The ethics of care and empathy. Routledge (2007) 71. Sofia, K., Jones, L.: Mechanical and psychophysical studies of surface wave propagation during vibrotactile stimulation. IEEE Trans. Haptics 6, 320–329 (2013). https://doi.org/10. 1109/TOH.2013.1 72. SSI: Screen solutions international: Spherical projection displays (May 2020). www. ssidisplays.com/projection-sphere/  
   
  68  
   
  P. Coe et al.  
   
  73. Sutherland, I.: The ultimate display. multimedia: From wagner to virutal reality (1965) 74. Turvey, M., Carello, C.: Chapter 11 - dynamic touch. In: Epstein, W., Rogers, S. (eds.) Perception of Space and Motion, pp. 401–490. Handbook of Perception and Cognition, Academic Press, San Diego (1995). https://doi.org/10.1016/B978-012240530-3/50013-4, www. sciencedirect.com/science/article/pii/B9780122405303500134 75. Vechev, V., Zarate, J., Lindlbauer, D., Hinchet, R., Shea, H., Hilliges, O.: Tactiles: Dual-mode low-power electromagnetic actuators for rendering continuous contact and spatial haptic patterns in vr, pp. 312–320 (March 2019). https://doi.org/10.1109/VR.2019.8797921 76. Wachowski, A., Wachowski, L.: The art of the matrix. Newmarket Press (2000) 77. Wachowski, L., Wachowski, L.: The matrix: The shooting script. Titan (2002) 78. Williamson, J., Sund´en, D., Bradley, J.: Globalfestival: evaluating real world interaction on a spherical display. pp. 1251–1261 (September 2015). https://doi.org/10.1145/2750858. 2807518 79. Xie, X., et al.: A review of smart materials in tactile actuators for information delivery, vol. 3, p. 38 (December 2017). https://doi.org/10.3390/c3040038 80. Xu, H., Peshkin, M., Colgate, J.: How the mechanical properties and thickness of glass affect tpad performance (December 2019) 81. Zhou, Q., Hagemann, G., Fafard, D., Stavness, I., Fels, S.: An evaluation of depth and size perception on a spherical fish tank virtual reality display. IEEE Trans. Visualiz. Comput. Graphics 1 (2019). https://doi.org/10.1109/TVCG.2019.2898742 82. Ziat, M., Chin, K., Raisamo, R.: Effects of visual locomotion and tactile stimuli duration on the emotional dimensions of the cutaneous rabbit illusion. pp. 117–124 (October 2020). https://doi.org/10.1145/3382507.3418835 83. Zuffo, M., et al.: Spheree: An interactive perspective-corrected spherical 3d display (July 2014). https://doi.org/10.1109/3DTV.2014.6874768  
   
  Effects of Emotion-Induction Words on Memory and Pupillary Reactions While Viewing Visual Stimuli with Audio Guide Mashiho Murakami1 , Motoki Shino1 , Munenori Harada2 , Katsuko T. Nakahira2(B) , and Muneo Kitajima2 1  
   
  The University of Tokyo, Tokyo, Japan [email protected]  , [email protected]  2 Nagaoka University of Technology, Nagaoka, Niigata, Japan [email protected]  , [email protected]  , [email protected]   
   
  Abstract. This study aimed to examine the possibility of using emotioninduction words in audio guides for education via visual content. This was performed based on the findings of a previous study that focused on the provision timings of visual and auditory information [6]. Thirty emotion-induction words were extracted from the database and categorized into positive, negative, and neutral words, and three experiments were performed. The first experiment was conducted to confirm the reliability of emotional values. The results revealed a strong consistency between the values in the database and the ratings given by the participants. The second experiment assessed whether consistency was maintained if the words appeared in the sentences. The results confirmed that a certain degree of consistency was maintained, as expected, but showed larger individual differences compared with the first experiment. The third experiment was conducted to probe the effect of emotion-induction words used in the audio guide to explain the visual content of memory. Our results revealed that participants who were exposed to positive and negative emotion-induction words remembered the content better than those who were presented with neutral words. Per the three experiments, the emotion value of the neutral words was found to be sensitive to the context in which they were embedded, which was confirmed by observing the changes in pupillary reactions. Suggestions for designing audio and visual content using emotion-induction words for better memory are provided. Keywords: Memory · Audio guide · Emotion · Omnidirectional watching · Information acquisition · Cognitive model  
   
  1 Introduction This paper is based on the previous work originally presented in [10]. It extends the analysis of pupillary reactions in Sect. 5.2 and appendices. The effect of an excess of information on human beings’ cognitive processes has been pointed out in a variety of contexts. Simon suggested that an excess of information could result in a lack of attention [15]. Bitgood focused on how museum visitors were not able to learn because of the amount of information they were presented c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 69–89, 2023. https://doi.org/10.1007/978-3-031-25477-2_4  
   
  70  
   
  M. Murakami et al.  
   
  with, as follows: “During museum visits, learners may fail to understand the exhibits deeply because of the abundance of exhibits and time limitations leading to information overload” [2]. With the development of Information and Communication Technology (ICT), the amount of information that can be transmitted from artifacts to human beings continues to increase. In contrast, human beings, who receive the information, are equipped with limited perceptual and cognitive capabilities for processing rich information. As described above, an excess of information may result in undesirable effects, such as lack of attention, fatigue, and ineffective learning. To balance the excess of information with human perceptual and cognitive capabilities, Pierdicca et al. suggested several methodologies that might enable the use of both novel Internet of Things (IoT) architectures and suitable algorithms to derive indicators concerning visitor attention with a significant degree of confidence in the concept of a “ubiquitous museum” [13]. Lifelong learning has become an important part of our daily lives, and visiting museums is considered a typical method supporting lifelong learning. The development of ICT has changed the style of exhibitions in that they now tend to contain an excessive amount of information. However, more information transmitted does not necessarily translate to more learning due to the challenges inherent to processing excessive amounts of information. Given excess information, learning methods should be designed considering human perceptual and cognitive capabilities. In the context of lifelong learning, the knowledge acquired while appreciating exhibits in museum visits needs to be considered. Hirabayashi et al. [6] used omnidirectional movies as an example of an exhibition that utilizes modern advanced technologies and assessed the effect of auditory information presentation timings on memory. They suggested that if auditory information for a particular object is provided after viewing the object as projected on the surface of the dome, it would result in greater retention. To direct the viewer’s attention to the object in question, they incorporated the appearance of the object into the audio guide (appearance information). The information available for explaining the object (content information), which is not directly accessible from the appearance of the object, was provided as an audio guide following the appearance information using a variety of intervals. They found experimentally that 2–3 s intervals were the most effective for creating a memory of the object. They argued that, in the best presentation interval condition, the visual and auditory processes that are carried out to comprehend the object should jointly activate part of long-term memory to generate the most richly connected network. This study furthers Hirabayashi et al.’s study [6] by shifting the focus of research from the richness of the network connections to the contents of the richly connected network. The idea is to strengthen the constituent nodes by manipulating the words used in the audio guide while maintaining the topology of the network generated by processing visual and auditory information using the best 2–3 s interval between the timing of the appearance of information and the content information. Assuming that the generation of a richly connected memory network is assured by the best interval condition, this study investigates the possibility of making the network stronger in terms of the total amount of activation the network holds by manipulating the concrete words used in the content information.  
   
  Effects of Emotion-InductionWords on Memory  
   
  71  
   
  Fig. 1. A cognitive model on memory formation.  
   
  To this end, this study utilized the finding that emotion enhances episodic memory by strengthening constituent nodes. Deborah et al. [16] proposed an extension of the context maintenance and retrieval model (CMR) [14], eCMR, to explain the way people may represent and process emotional information. The eCMR model assumes that a word associated with an emotion, such as spider, is encoded with its emotional state in working memory (they called it “context layer”) and that the presented emotional word establishes a stronger link than neutral words. This study operationally implements the same effect by attaching a greater weight to the emotional node in the network and assesses the effect of emotion-induction words used in audio guides on the memory of the movie viewing experience. It is likely that viewers’ reactions to emotion-induction words may reflect their personal experiences or knowledge. Therefore, this study also incorporated the finding that pupil dilation reflects the time course of emotion recognition [5, 11, 12] to gather evidence that the manipulation of emotion induction has been successful. The remainder of this paper is organized as follows. Section 2 describes the cognitive framework that shows the effect of timings and contents of the audio guide on memory. Section 3 describes three experiments that investigate the effects of emotioninduction words on memory. The first one probes participants’ responses to emotioninduction words, the second one focuses on sentences with emotion-induction words, and the last one measures memory for movies with positive, negative, and neutral emotion-induction words. Sections 4 and 5 provide the results of the experiments and discuss them from the viewpoint of pupil diameter changes.  
   
  2 Integration of Visual Information and Auditory Information Hirabayashi et al. [6] studied the importance of the timing of providing auditory information while watching movies to make the experience memorable. This study extends their findings regarding the effective provision timing of auditory information for memory formation by focusing on the effect of emotion-induction words in the context of  
   
  72  
   
  M. Murakami et al.  
   
  Fig. 2. Timeline of memory activation.  
   
  auditory information. This section outlines Hirabayashi et al.’s model [6] that explains the effective timing auditory information while processing visual information along with necessary modifications to deal with the effect of emotion-induction words in the context of auditory information. Starting from the introduction of the cognitive model of memory formation, this section discusses why it is essential to take timings into account. 2.1  
   
  Memory Formation by Integrating Visual and Auditory Information  
   
  Figure 1 represents the perceptual and cognitive processes involved in acquiring visual information with the support of an audio guide, incorporating the finding related to the effective timing of providing an audio guide [6]. In Fig. 1, visual and auditory information are represented as “movie exhibit” and “audio guide narration”, respectively. A situation is considered whereby visual and auditory stimuli that have the amount of information of IV and IA are fed into sensory memory. Part of the information stored in the sensory memory, IV and IA , is passed to working memory (WM) via the sensory information filter as I1 and I2 at times t1 and t2 , respectively. We assume that Δt = t2 − t1 ≈ 2 ∼ 3 seconds following Hirabayashi et al.’s finding [6] that their participants showed the best memory performance for the movie exhibit when auditory information was provided 2 ∼ 3 seconds after the corresponding visual information I1 (t1 ) and auditory information I2 (t2 ) are present in the WM for a duration time of τ1 and τ2 , respectively. The visual and auditory information in WM activate part of the long-term memory (LTM) via a resonance mechanism [9]. The activated portion of LTM is incorporated in WM, which thereafter serves as the next source of activation as long as it exists in WM. In this study , WM is considered the activated portion of LTM, which is called long-term working memory [3]. This process is expressed as follows: Only Visual Information is Available (t1 ≤ t < t2 ) 1. Visual information I1 is stored in WM at t1 , which is present in WM for the duration of τ1 . 2. At t = ti (> t1 ), a chunk in LTM Ci is activated by the current WM.  
   
  Effects of Emotion-InductionWords on Memory  
   
  73  
   
  3. The activated chunk Ci is incorporated into WM. The information thus incorporated in WM at ti is denoted h(ti ). 4. h(ti ) is present in WM for the duration of τi . 5. During the overlapping period of (t1 , t1 + τ1 ) and (ti , ti + τi ), I1 and h(ti ) serve as the WM contents to further activate the LTM. In Fig. 1, chunk Cj is activated at tj and incorporated into WM as h(tj ) with at lifetime of τj . Auditory Information is Available (t ≥ t2 ) 1. Auditory information I2 is stored in WM at t2 , which is present in WM for the duration time of τ2 . 2. At t = tk (> t2 ), a chunk in LTM Ck is activated by the current contents of WM. 3. The activated chunk Ck is incorporated into WM as h(tk ) with the lifetime of τk . 4. During the overlapping period of (t2 , t1 + τ2 ) and (tk , tk + τk ), I2 and h(tk ) serve as the WM contents to activate LTM further. The chunk Cl is activated at tl and incorporated in WM as h(tl ) with the lifetime of τl . As these processes proceed, the visual information I1 at t1 and the auditory information I2 at t2 are elaborated through the cascade of activation of chunks in LTM using the dynamically updated contents of WM. The activated chunks are then integrated to make sense of the visual information and the auditory information. In text comprehension research in cognitive psychology, these processes are modeled as the Construction-Integration process [7, 8]. Figure 2 schematically shows the process how the visual information provided at t1 collects information in LTM by activating relevant chunks at ti and tj . The vertical axis represents the number of chunks incorporated in WM. At t = t2 , where t1 < t2 < t1 + τ1 , auditory information is incorporated in WM, and collects information in LTM by activating relevant chunks at tk and tl . The information originated from the visual information and the auditory information are represented as red rectangles and blue rectangles, respectively. The number of information is the largest between the time tk < t < t1 + τ1 = ti + τi . It is five, three and two originated from visual and auditory information, respectively. Assuming that the auditory information provided after the provision of visual information should be used for helping the viewers comprehend the movie better, the overlapping part of the diagram should direct to the common areas of the memory network in LTM. Hirabatashi et al. [6] examined how the intervals between the provision of visual information and auditory information should affect the number of links that could be established through these processes. Through these processes, the activated chunks establish links with the existing memory networks and as a result, it is memorized. Figure 2 is the best timing for creating a richly connected network for better memory. If t2 − t1 gets longer or shorter, the area of overlap of the red area and the blue area becomes smaller than the case shown by Fig. 2. Therefore, not only the input information itself but also the other pieces of information that is available at the same time plays an important role in how memorable the input information is. Even if the same pieces of information are presented, if the pieces of information are perceived in different timings, it directly affects the quantity of information available for integration and organization of the acquired information.  
   
  74  
   
  M. Murakami et al.  
   
  Fig. 3. The outline of three experiments flow.  
   
  2.2  
   
  Emotion-Induction Words for Better Memory  
   
  This study examines the possibility of the effect of the overlapping part of Fig. 2 by using emotion-induction words in the auditory information. In Fig. 2, the blue part that corresponds to the auditory information is represented by different color values which corresponds to the strength of the activated chunks. According to the theory of cognition, Adaptive Control of Thought - Rational (ACTR) [1], the stronger the chunk becomes, the more probable the chunk is retrieved. This study adopts a slightly modified version of activation level equation defined in the ACTR theory. The activation level of the i-th chunk, Ai , is defined by the following equation: Ai = Bi +  
   
  N   
   
  Wji × Aj .  
   
  j=1  
   
  In this equation, Bi , Wji , and N denote the base level activation of the chunk Ci , the strength of the link between Cj and Ci (from j to i), and the number of chunks that are connected to the chunk Ci . The base level activation decays overtime. But it gets larger when the chunk is used, or activated. This study assumes that emotion-induction words should activate stronger chunks than emotionally neutral words. Figure 2 depicts the situation where the auditory information contains emotion-induction words and stronger chunks are activated and incorporated in WM. This should cause stronger memory trace than neutral words are used in the auditory information.  
   
  3 Three Experiments for Investigating Effects of EmotionInduction Words Three experiments were carried out to dissect the relationships between emotioninduction words and memory when watching a movie. Figure 3 outlines the relationship  
   
  Effects of Emotion-InductionWords on Memory  
   
  75  
   
  between the different experiments. The first experiment (Word Impression Evaluation Experiment, Exp-W) was conducted to confirm the reliability of the emotional values. The second experiment (Sentence Impression Evaluation Experiment, Exp-S) was used to examine whether consistency was maintained if the words appeared in the context of sentences. The third experiment (Video Appreciation Experiment, Exp-V) was conducted to examine the effect of emotion-induction words used in the audio guide to explain visual contents on memory. Thirty participants participated in all experiments. They first participated in Exp-S and Exp-V consecutively scheduled on a single day in the laboratory. Following an interval of approximately 10 days, they participated in Exp-W on their own computers. Different sets of induction words were used for Exp-S and Exp-V. Exp-W was conducted after Exp-S and Exp-V to probe the appropriateness of the emotion-induction words used in the sentences and audio guides presented in Exp-S and Exp-V. 3.1 Measurement Data Impression Rating: In this study, emotion-induction words were used to assess the influence of emotion on memory. All were taken from the database [4], having been created in the following way: 618 participants were asked to rate their impressions to the presented words by using a 7-point Likert scale as follows: “very positive”, “positive”, “somewhat positive”, “neither”, “somewhat negative”, “negative”, and “very negative.” After the rating experiment, a database of 389 words consisting of 122 negative, 146 positive, and 121 neutral valence words were constructed. In this study, impression ratings were collected for words in Exp-W, sentences in Exp-S, and videos in Exp-V. The options for rating included the ones used for constructing the above-mentioned database and a new option, “I cannot catch the meaning of this word/sentence/video.” The last one was added considering the possibility of not knowing the word or not being able to get the meaning of the sentence. Pupil Diameter: The pupil diameter dilates when emotions change [5, 11, 12]. Therefore, the participants’ pupil diameters were measured in Exp-S and Exp-V. Tobii Pro Glass 2, with a sampling rate 50 Hz, was used in the experiment. Memory: Memory was measured in the Exp-V. A questionnaire was administered to investigate the information that the participants memorized when viewing movies. Since the questionnaire was conducted soon after viewing the movie, a recall test was chosen. To quantitatively assess memory, participant responses were broken down into meaningful units using a morphological analysis technique. They were then scored from the quantitative and qualitative perspectives by providing special points to those units related to the targets and the narrative contents spoken in the audio guides (one point was given to a noun or a verb, two points to a pronoun). Those points were summed up to define “Memory Score.” Table 1 represents an example of the responses from Exp-V and the calculated memory score. 3.2 Word Impression Evaluation Experiment (Exp-W) The purpose of Exp-W was to confirm that there was no discrepancy between the emotional value of two-character words in the Japanese language in the database and participants’ evaluations. Thirty participants assessed their impressions of the words  
   
  76  
   
  M. Murakami et al.  
   
  Table 1. Examples of Memory Score (n/v:noun or verb, pn: pronoun, MS: Memory Score (points)). Response  
   
  n/v pn MS  
   
  Statue Statue of messenger Replica of statue of messenger Statue of messenger was Given as a proof of friendship  
   
  1  
   
  0  
   
  1  
   
  0  
   
  1  
   
  2  
   
  1  
   
  1  
   
  3  
   
  2  
   
  1  
   
  4  
   
  displayed on their own PCs using a 7- point Likert scale. There was a total of 30 words. Considering the order effect on evaluation, multiple patterns were randomly created, and experiments were conducted on five patterns for a total of 30 words. 3.3  
   
  Sentence Impression Evaluation Experiment (Exp-S)  
   
  The purpose of Exp-S was to confirm that emotion-induction words affect emotion even when they are presented in a sentence. The procedure was carried out in the exact same method as for Exp-W, except that the stimuli consisted of sentences instead of words and were presented in audio format. There was a total of 30 sentences randomly presented as Exp-W in the laboratory using a laptop computer with a 13-inch display. Participants operated the touch pad of a laptop computer. Ten negative words, 10 positive words, and 10 neutral words were extracted from the database. Each sentence consisted of a single sentence containing one of the extracted words. Each sentence consisted of a single sentence lasting about 3 ∼ 5 s, and was broadcast twice, read aloud by the same person in succession. The break between the two was clear. Participants listened to 30 sentences and assessed their impressions. The sentences were not visually presented on the display. 3.4  
   
  Video Appreciation Experiment (Exp-V)  
   
  Exp-V was carried out in an appreciation environment similar to the learning environment of a social education institution. Thirty participants took part in the experiments. None had visual or health problems during the experiment. They watched three movies consecutively and evaluated the impression of each movie using a 7-point Likert scale. Each movie was provided with an audio guide that contained a few positive, negative, or neutral words extracted from the database. The memory test was conducted immediately after viewing the movies by having the participants write down anything they remembered. In Exp-V, biological information was collected as a potential objective indicator that should reflect the effect of emotion-induction words on the psychological states of the participants. Exp-V experiments were conducted in the laboratory. Participants operated the touchpad of a laptop computer. Participants evaluated their impressions of the sentences in the same way as in Exp-W after watching three movies. Three patterns were considered for the presentation order of the positive, negative, and neutral audio guides, as represented in Table 2.  
   
  Effects of Emotion-InductionWords on Memory  
   
  77  
   
  Table 2. Three presentation patterns for audio guides of three movies that contain positive, negative, and neutral emotion-induction words. Pattern Movie 1 Movie 2 Movie 3 1  
   
  Neutral  
   
  Positive  
   
  2  
   
  Negative Neutral  
   
  3  
   
  Positive  
   
  Negative Positive  
   
  Negative Neutral  
   
  Fig. 4. Upper three graphs: frequency of ratings for “word” and “sentence.” The left, middle and right sections side shows the negative, positive, and neutral modes, respectively. Lower three graphs: probability distributions for “word” versus “sentence.” Word − stands for negative, Word + stands for positive, and WordN stands for neutral.  
   
  Visual and Auditory Stimuli. Three movies were created. Each movie had a respective audio guide with one of the three attributes of emotion-induction words. The movies showed the landscape taken from a slow-paced boat going down the Sumida River in Tokyo. A movie taken from a slow-paced boat was chosen as a stimulus for this experiment because it is likely to contain scenes or targets that satisfy the following conditions: – The target in the movie should move at a slow pace. This condition was needed to make the target appear and stay in the field of view long enough for a viewer to take the required visual information of the target object. – The target should not be easily noticed without guidance. This condition was needed to prevent viewers from paying attention to the target beforehand and to clearly see the effect of the audio guide. – Scenes contain many objects throughout the movie. This condition was needed to simulate situations in which an audio guide is needed.  
   
  78  
   
  M. Murakami et al.  
   
  Experiment System. Viewing behavior, including pupil diameter, was recorded using a wearable eye tracker (Tobii Pro Glasses 2) at a sampling rate 50 Hz. The experiment was conducted with one participant at a time. Participants were seated on a chair and their head positions were located approximately 0.8 m from the display. Procedure. Prior to viewing the movie, the participants were told to make themselves comfortable and to view the movie freely to simulate the actual viewing behavior. Each movie lasted approximately 1 min, and intervals were inserted between the movies. Considering the order effect, each participant was presented with a randomly chosen pattern from the three represented in Table 2. After participants finished viewing the movies, they were asked to complete the questionnaire.  
   
  4 Results This section begins by laying forth the results of Exp-W and Exp-S concerning the impression evaluation of words that appeared in isolation and in context. This is followed by the results of Exp-V concerning memory scores for the movies that used positive, negative, and neutral emotion-induction words. 4.1  
   
  Impression of Emotion-Induction Words  
   
  The normalized frequencies in Exp-W and Exp-S are represented in the top half of Fig. 4. At a single glance, the ratings for emotion-induction words that appeared in isolation or in the context of a sentence were consistent, irrespective of the nature of the words, that is, positive, negative, or neutral. The mode of the rating for the negative words in isolation and in a sentence was 6, that for the positive words was 2, and that for the neutral words was 4. The results represented in the histograms are further decomposed by focusing on the ratings for individual words. Each word was rated in isolation and in the context of a sentence. The bottom half of Fig. 4 shows the normalized frequency of the data points of two-dimensional space, that is, the rating of word in isolation vs. the rating of word in a sentence. There were 300 data points (10 words by 30 participants) for negative, positive, and neutral conditions. As shown in the figures, the ratings for the emotion-induction words were consistent, whether they were rated in isolation or in sentences in general. However, the neutral emotion-inducing words showed a significant difference in terms of the degree of variance in the sentence rating, as shown in the bottom-right plot in Fig. 4. This indicates that neutral emotion-induction words were rated neutral when they were presented as single words in isolation, but that the rating of the emotion value fluctuated when they were presented in a sentence. This suggests that neutral emotion-induction words cannot be emotionally neutral when they appear in a sentence. Box plots of the ratings for the emotion-induction words, the sentences in which the emotion-induction words are embedded, and the visual content with audio that includes the emotion-induction words are represented in Fig. 5(a), (b), and (c). The ratings were significantly different across conditions, that is, positive, negative, and  
   
  Effects of Emotion-InductionWords on Memory  
   
  79  
   
  Fig. 5. Box-plots of ratings for (a) emotion-induction words in Exp-W, (b) sentences that emotioninduction words are embedded in Exp-S, and (c) visual contents with audio that includes emotioninduction words in Exp-V. The signs ‘+’, ‘-’, and ‘N’ stand for ‘positive’, ‘negative’, and ‘neutral’, respectively.  
   
  neutral, except for the ratings between neutral and positive emotion-induction words in Exp-V. More specifically, by comparing the results of Exp-V shown in Fig. 5(c) with the results of Exp-W and Exp-S, it is found that the impression ratings were consistent with the attributes of the emotion-induction words only in the negative condition, and that the impression ratings for the positive and neutral conditions were not consistent with the attributes of the emotion-induction words. The ratings for the positive condition in Exp-V shifted to the region of neutral impression, and those for the neutral condition shifted to the region of positive impression. This observation is consistent with the result demonstrated in the bottom-right plot in Fig. 4. It is likely that the ratings of emotion value fluctuated when emotion-induction words were presented in context. 4.2 Memory Score A standardized summary of the memory score for each video is represented in Fig. 6. Using the t-test, a statistically significant difference was observed between C– and C+ conditions and between C– and C-N conditions. However, there was no statistically significant difference between the C+ and C-N conditions. This result is consistent with the result denoted in Fig. 5(c) for Exp-V described in Sect. 4.1. In the condition in which the emotion-induction words appeared in the audio guide for providing information about the movies, it is likely that positive and neutral words were more strongly affected by the context in which they appeared, and that they could change their context-free emotion values, which were maintained even if they appeared in sentences.  
   
  80  
   
  M. Murakami et al.  
   
  Fig. 6. Memory scores for the three movie categories; the movies in the C–, C+, and C– N categories consisted of the negative, positive, and neutral emotion-induction words, respectively.  
   
  5 Discussion This section starts by analyzing the results of Exp-V with regard to the relationship between emotion-induction words included in movies and memory score. It follows pupil diameter dilation analysis as an objective indicator of emotional changes. Finally, this section discusses the possibility of implementing a design method based on the two aforementioned points. 5.1  
   
  Memory Score Analysis  
   
  Section 4 demonstrated that 1) the memory scores were high in the negative condition, and 2) the attributes of emotion-induction words in the impression evaluation were only maintained in the negative condition. As shown in Fig. 4, emotion-induction words should also work in sentences, regardless of whether they are positive or negative. When they were added as an audio guide to the video, it is likely that for some reason, the positive emotion-induction words did not manifest their expected effect when incorporated into the video. Therefore, it is not possible to discuss the relationship between positive emotion-induction words and memory. Further analyses are required in the future. However, these results suggest that negative emotion-induction words also influenced the evaluation of impressions and contributed to the improvement of memory in the movies. Next, the relationship between impression evaluation and memory scores is discussed. As demonstrated in Fig. 1 and 2, the presence of emotion-induction words in a sentence should enhance memory by overlapping visual information as objects with auditory information at appropriate times. In addition, as shown in Fig. 4, the influence of neutral words on emotion varied more when they were presented in sentences than when they appeared in isolation. This suggests that this tendency is more strongly pronounced when words are presented in the context.  
   
  Effects of Emotion-InductionWords on Memory  
   
  81  
   
  Fig. 7. Relation between context (movies) rating for emotion and memory scores. C+ represents the movies which were constructed with the positive emotive-induction words. C– represents the movies which were constructed with the negative emotive-induction words. The dotted lines represent the linear regression for each movie. The blue dotted line represents C– with an R2 value of 0.79. The red dotted line represents C+ with an R2 value of 0.33. (Color figure online)  
   
  The relationship between ratings and the memory scores for each of the three videos obtained from the participants is represented as a scatter plot in Fig. 7. The dotted lines represent the linear regressions for each movie. The blue dotted line represents C–, with an R2 value of 0.79. The blue dotted line increases as the ratings increase. This indicated that the memory score increases when the movie in the C– condition was rated with the negative emotion-induction words that appeared in the audio guide. The red dotted line represents C+, with an R2 value of 0.33. The red dotted line decreases as the ratings increase. This indicates that the memory score decreases when the movie in the C+ condition was rated with the positive emotion-induction words that appeared in the audio guide. When emotion-induction words used to provide an explanation of the movies in the audio guide function as expected, memory scores should increase. In contrast, when they do not function as they should, memory scores should not increase. 5.2 Pupil Diameter Analysis Figure 8 represents the pupillary reactions from the two participants, ut33 and ut19, characterized as the consistent and inconsistent participant where the degree of con-  
   
  82  
   
  M. Murakami et al.  
   
  Fig. 8. Box-plots of ratings for (a) emotion-induction words in Exp-W, (b) sentences that emotioninduction words are embedded in Exp-S, and (c) visual contents with audio that includes emotioninduction words in Exp-V. The signs “+,” “–,” and “N” stand for “positive,” “negative,” and “neutral,” respectively. Exp-V, Video Appreciation Experiment; Exp-W, Word Impression Evaluation Experiment; Exp-S, Sentence Impression Evaluation Experiment  
   
  sistency is defined by the degree of matching between their ratings to the emotioninduction words and the values in the database. This result indicates that ut33 has a difference in the effect of emotions between emotion-induction words and neutral words, and ut19 has a small difference between emotion-induction words and neutral words. This result corresponds to the characteristic of correlation with the database. In other words, the pupil diameter variability was greater for those who rated impressions according to the database when they heard emotion-induction words, while those who rated impressions not according to the database showed no difference in pupil diameter variability when they heard emotion-induction words compared to when they heard neutral words. The results suggest that there are individual differences in the emotional changes caused by emotion-induction words, which can be captured by the pupil diameter analysis. Further pupil diameter analysis will support this finding. To understand the effect of emotion-induction words included in audio guide on memory, pupil diameter changes in Exp-S was analyzed as an objective evaluation index. To analyze the relationship between the attributes of emotion-induction words which each participant heard and pupil diameter changes, the participants were screened according to the following criteria. 1. There is no problem in the ratings in Exp-W and Exp-S. 2. There is no problem in the pupil diameter data. Of those who satisfied these criteria, two participants were selected for a preliminary analysis of pupil data considering the degree of accordance with their ratings for the sentences used in Exp-S with the attributes of the sentences, i.e., S+, S-, and S-N. One participant, ut33, showed the highest correlation of 0.92 (consistent-participant) and the other, ut19, showed the lowest correlation of 0.18 (inconsistent-participant). Since the  
   
  Effects of Emotion-InductionWords on Memory  
   
  83  
   
  Fig. 9. A sample of D(t) and E(t).  
   
  pupil diameter dilation to emotional changes is smaller than the change to light changes, the dilation to emotional changes is considered to be captured by lower envelope for pupil diameter changes. Figure 9 is a graph showing the approximate lines of the lower envelopes. The difference between the value of pupil diameter curve and the value of the lower envelope was added by the time when the sentence was played, and the average was calculated by using the following procedure: The pupil dilation at the time t represents D(t), where t defines as the measurement time for pupil diameter. Here, let the pupil diameter measurement time be Ts for the starting measurement time and Te for the ending measurement time. At this time, the measurement time is expressed by the next closed interval [Ts , Te ]. The local minima in this interval are obtained as follows (consult the APPENDIX 2 for the pseudo code for this procedure). {T1 , T2 , · · · , TN }, where Ts ≤ T1 ≤ TN ≤ Te . Here, let Lk be the line segment connecting the two points, D(Ti ) and D(Ti+j ), which generates m line segments. If Lk does not intersect with D(t) in that interval, Tj , which is the end point, is shifted by one as j. If Lk intersects with D(t), Lk is determined to be the lower envelope of k, the next start point Ti is set to Tj , and the end point is set to Tj+1 to determine again whether to draw a line segment. Here, E(t) in Fig. 9 is a connection of discontinuous line segments Lk . Let Sk be the area surrounded by Lk and D(t). The area surrounded by Lk and D(t) is Sk , and C(t) are named as cumulative pupillary reaction measure. C(t) =  
   
  m  k=1  
   
  Sk  
   
  84  
   
  M. Murakami et al.  
   
  Fig. 10. An example of the cumulative pupillary reaction measure C(t). The lines are drawn by using the pupillary reactions of the participant who showed (a) negative emotion, (b) positive emotion, and (c) neutral emotion to the stimuli.  
   
  Once defined C(t), we can quantify the reaction characteristics between stimulus type and participants. Figure 10 represents a sample of C(t). Figure 10(a) is for the negative stimulus, (b) is for the neutral stimulus, and (c) is for the positive stimulus. This participant’s behavior shows a larger pupillary response to the negative stimulus, but a similar pupillary response to the positive stimulus and neutral stimulus, with no significant changes compared to the negative stimulus. These findings suggest a possibility that we could predict the audiences’ emotion while viewing visual stimuli with audio guide by using their pupillary reactions. 5.3  
   
  Design Implications  
   
  This subsection discusses the design of multimodal information for better memory. First, as shown in Fig. 4, it was found that impression ratings corresponded to the attributes of emotion-induction words. Second, it is well-known that emotional changes are reflected in pupil diameter dilation. As shown in Fig. 8, for the same stimuli, the consistent participant and inconsistent participant showed significantly different reactions in terms of pupil diameter reactions. This indicates that the pupil diameter could be used to monitor how the participants might react to the stimuli. As discussed in this paper, ratings of impression, which could be subject to individual differences, as evidenced by the existence of inconsistent participants, should correlate with memory  
   
  Effects of Emotion-InductionWords on Memory  
   
  85  
   
  score. Monitoring participants’ emotional state could represent a promising method for designing auditory information to help construct better memories. In addition, as shown in Fig. 6, memory scores should be higher when negative emotion induction words are acquired. This finding is also applicable to the design of auditory information.  
   
  6 Conclusion and Future Work This study probed the effect of emotion-induction words in an audio guide on memory. Based on integration of visual information and auditory information, we designed three experiments which included the following probes: responses to emotion-induction words, sentences with emotion-induction words, and movies with emotion-induction words. Our findings revealed that auditory information with negative emotion-induction words was easy to remember. It was also suggested that emotional changes during appreciation behavior might be reflected in changes in pupil diameter. We proposed a method for analyzing the relation between pupil dilation and emotion. It used the cumulative pupillary reaction measure C(t), which was obtained by summing Sk . This measure corresponded to the area of closed curves between envelope and a convex line of pupillary dilation profile. We showed that the plot of C(t) on the plane with the duration time for the horizontal axis and the total dilation for the vertical axis was useful for grasping the relationship between the multiple categories of emotion-induction words. We further suggested that the shape of C(t) might represent the characteristics of the participant’s reaction to the presented words, and could be used to construct effective indices for categorizing his/her emotional reaction types, i.e., positive, neutral, or negative, for the particular words. For now, this study only focused on two-dimensional movie viewing behaviors to simulate a real environment. However, to facilitate its application to everyday life, such as in the context of museums, galleries, and guided tours, among others, it is important to apply and examine what this study found in omnidirectional situations, such as in the context of a dome theater. In addition, a pupil diameter analysis was performed for only two participants to examine the feasibility of the research direction. These results are promising. We plan to continue this approach and try to find a way to objectively estimate the viewer’s cognitive state that would enhance the learning of visual content accompanied by auditory information.  
   
  86  
   
  M. Murakami et al.  
   
  Appendix 1: Emotion-Induction Words and Sentences The emotion induction words and the sentences used for the experiment. No. Word  
   
  Score Sentence including the word  
   
  Positive 1  
   
  [murder]  
   
  6.51  
   
  Murder is one of the worst sins a person commit  
   
  2  
   
  [tragedy]  
   
  6.38  
   
  A tragedy happened in the baseball tournament final  
   
  3  
   
  [assassinate]  
   
  6.36  
   
  Ryoma Sakamoto was assassinated on his birthday  
   
  4  
   
  [banish]  
   
  6.33  
   
  Unable to understand the situation, he was banished from the dinner party  
   
  5  
   
  [prejudice]  
   
  6.15  
   
  6  
   
  [transfer school] 6.14  
   
  The first day of transfer school is full of anxiety  
   
  7  
   
  [worst]  
   
  6.13  
   
  Enter the site assuming the worst situation  
   
  8  
   
  [dismiss]  
   
  6.13  
   
  One of the college students working part-time must be dismissed  
   
  9  
   
  [penalty]  
   
  6.10  
   
  Nothing is as boring as paying a penalty  
   
  10  
   
  [fall]  
   
  6.06  
   
  I have fallen since I became a college student  
   
  It is difficult to be aware of unconscious prejudice  
   
  Neutral 1  
   
  [free]  
   
  1.93 Rice is often included free of charge at Iekei ramen shops  
   
  2  
   
  [contribute]  
   
  1.93 How much do I contribute to the sales of the nearest convenience store?  
   
  3  
   
  [love]  
   
  1.90 I cook curry with plenty of love  
   
  4  
   
  [experienced]  
   
  1.87 Rookie is good, but experienced veteran is also good  
   
  5  
   
  [courage]  
   
  1.80 I gave up my seat with courage  
   
  6  
   
  [holiday]  
   
  1.77 When I was in elementary school, I loved holidays  
   
  7  
   
  [plenty]  
   
  1.53 There are plenty of drink bars here, so I’ll follow you  
   
  8  
   
  [fortunate]  
   
  1.50 I was fortunate to meet you  
   
  9  
   
  [achieve]  
   
  10 [clear day]  
   
  1.47 I am good at achieving goals one by one 1.47 Laundry progresses on a clear day.  
   
  Negative 1  
   
  [field]  
   
  2  
   
  [loading platform] 4.00 I’m watching the cardboard boxes pile up on the loading platform  
   
  3  
   
  [railway route]  
   
  4.00 There are so many railway routes in Tokyo that you can’t compare to the countryside  
   
  4  
   
  [job seeker]  
   
  4.07 Job seekers were in line  
   
  5  
   
  [address]  
   
  3.97 When I write my address, I’m wondering whether to write it from the prefecture  
   
  6  
   
  [jacket]  
   
  3.93 It is difficult to choose a jacket because the temperature difference between day and night is large  
   
  7  
   
  [seal]  
   
  4.11 I always carry my seal  
   
  8  
   
  [budget]  
   
  4.07 You can’t decide anything else unless you decide on a budget  
   
  9  
   
  [next time]  
   
  4.1  
   
  10 [clerk]  
   
  4.0  
   
  I think I know more about this field than most people  
   
  Next time I will try to order a different menu  
   
  3.98 Turn right at the end and a clerk is standing.  
   
  Effects of Emotion-InductionWords on Memory  
   
  Appendix 2: Algorithm for Pupil Analysis Algorithm 1. Envelope. 1: function ENVLOPE LINE(T ,t,D) 2: delta ← t[1] − t[0] 3: sp, j old, slope old, itr old ← 0 4: ep ← 1 5: slopes, itrs, LP ← [] 6: while True do 7: i ← T [sp], j ← T [ep] 8: if i > j then 9: ep ← ep + 1 10: continue 11: end if 12: slp ← (D[j] − D[i])/(t[j] − t[i]) 13: itr ← D[i] − slp ∗ t[i] 14: D temp ← D[i : j + 1] 15: L ← t[i : j + 1] ∗ slp + itr 16: dif f ← D temp − L 17: if min(dif f ) >= −1.5 ∗ delta ∗ |slp|) then 18: f lag ← True 19: if j >= t.length − 1 then 20: LP.append(j) 21: slopes.append(slp) 22: itrs.append(itr) 23: break 24: end if 25: slp old ← slp, itr old ← itr, j old ← j 26: else 27: if f lag is True then 28: ep ← ep − 1 29: LP.append(j) 30: slopes.append(slp) 31: itrs.append(itr) 32: sp ← ep 33: i ← T [sp] 34: end if 35: f lag ←False 36: end if 37: ep ← ep + 1 38: if ep >= T.length − 1 then 39: env point ← SEARCH EN V LOP E P OIN T (j old, t, D, delta) 40: T.insert(sp + 1, env point) 41: ep ← sp + 1 42: end if 43: end while 44: return LP, slopes, itrs 45: end function  
   
  87  
   
  88  
   
  M. Murakami et al.  
   
  Algorithm 2. Search Envelope. 1: function SEARCH ENVLOPE POINT(sp,t,D,delta) 2: ep, env point ← sp + 1 3: env point ← sp 4: while ep < t.length do 5: slp ← (D[j] − D[i])/(t[j] − t[i]) 6: itr ← D[i] − slp ∗ t[i] 7: D temp ← D[i : j + 1] 8: L ← t[i : j + 1] ∗ slp + itr 9: dif f ← D temp − L 10: if min(dif f ) >= −1.5 ∗ delta ∗ |slp|) then 11: env point ← ep 12: end if 13: ep ← ep + 1 14: end while 15: return env point 16: end function  
   
  References 1. Anderson, J.R., Lebiere, C.: The Atomic Components of Thought. Lawrence Erlbaum Associates, Mahwah (1998) 2. Bitgood, S.: Environmental psychology in museums, zoos, and other exhibition centers. In: In Handbook of Environmental, pp. 461–480 (2002) 3. Ericsson, A.K., Kintsch, W.: Long-term working memory. Psychol. Rev. 102, 221–245 (1995) 4. Gotoh, F., Ohta, N.: Affective valence of two-compound kanji words. Tsukuba Psychol. Res. 23(23), 45–52 (2001). http://ci.nii.ac.jp/naid/110000258875/en/, in Japanese 5. Henderson, R.R., Bradley, M.M., Lang, P.J.: Emotional imagery and pupil diameter. Psychophysiology 55(6), e13050 (2018). https://doi.org/10.1111/psyp.13050 6. Hirabayashi, R., Shino, M., Nakahira, K.T., Kitajima, M.: How auditory information presentation timings affect memory when watching omnidirectional movie with audio guide. In: Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, vol. 2: HUCAPP, pp. 162–169. INSTICC, SciTePress (2020). https://doi.org/10.5220/0008966201620169 7. Kintsch, W.: The use of knowledge in discourse processing: a construction-integration model. Psychol. Rev. 95, 163–182 (1988) 8. Kintsch, W.: Comprehension: A Paradigm for Cognition. Cambridge University Press, Cambridge (1998) 9. Kitajima, M., Toyota, M.: Decision-making and action selection in Two minds: an analysis based on model human processor with realtime constraints (MHP/RT). Biol. Insp. Cogn. Arch. 5, 82–93 (2013). https://doi.org/10.1016/j.bica.2013.05.003 10. Murakami, M., Shino, M., Nakahira, K., Kitajima, M.: Effects of emotion-induction words on memory of viewing visual stimuli with audio guide. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - HUCAPP, pp. 89–100. INSTICC, SciTePress (2021). https://doi.org/10.5220/ 0010348800890100  
   
  Effects of Emotion-InductionWords on Memory  
   
  89  
   
  11. Oliva, M., A., Pupil, A.: dilation reflects the time course of emotion recognition in human vocalizations. Sci. Rep. 8, 4871 (2018). https://doi.org/10.1038/s41598-018-23265-x 12. Partala, T., Surakka, V.: Pupil size variation as an indication of affective processing. Int. J. Human-Comput. Stud. 59, 185–198 (2003). https://doi.org/10.1016/S1071-5819(03)00017X 13. Pierdicca, R., Marques-Pita, M., Paolanti, M., Malinverni, E.S.: Iot and engagement in the ubiquitous museum. Sensors 19(6), 1387 (2019). https://doi.org/10.3390/s19061387 14. Polyn, S.M., Norman, K.A., Kahana, M.J.: A context maintenance and retrieval model of organizational processes in free recall. Psychol. Rev. 116, 129–156 (2009) 15. Simon, H.A.: Designing organizations for an information rich world. In: Greenberger, M. (ed.) Computers, Communications, and the Public Interest, pp. 37–72. Johns Hopkins University Press, Baltimore (1971). https://opacplus.bsb-muenchen.de/search?isbn=0-80181135-X 16. Talmi, D., Lohnas, L.J., Daw, N.D.: A retrieved context model of the emotional modulation of memory. Psychol. Rev. 126, 455–485 (2019)  
   
  A Bimanual Flick-Based Japanese Software Keyboard Using Direct Kanji Input Yuya Nakamura1 and Hiroshi Hosobe2(B) 1  
   
  Graduate Shchool of Computer and Information Sciences, Hosei University, Tokyo, Japan 2 Faculty of Computer and Information Sciences, Hosei University,Tokyo, Japan [email protected]   
   
  Abstract. Direct kanji input is a Japanese text input method that is totally different from kana-kanji conversion commonly used in Japan. Direct kanji input is said to enable the user to efficiently input kanji characters after mastering it. In this paper, we propose a bimanual flick-based Japanese software keyboard for a tablet that uses direct kanji input. Once the user masters it, the user can efficiently input kanji characters while holding a tablet with both hands. We present three kanji layouts that we designed for this software keyboard. We show the results of the three experiments that we conducted to evaluate the performance of this keyboard. In the first experiment, we compared it with exiting software keyboards. In the second experiment, we evaluated how much the user can learn it by using its learning support functions. In the third experiment, one of the authors continuously used it for 15 months. Keywords: Text input · Touch panel · Software keyboard  
   
  1 Introduction Most people in Japan use “kana-kanji” conversion to input Japanese text with computers including tablets and smartphones. Kana and kanji are characters that are commonly used in Japan: while the Chinese-originated kanji characters have meanings, the Japanese own kana characters do not have meanings but are associated with speech sounds instead. A kana-kanji conversion method allows the user to first input kana characters and then to select necessary kanji characters from the candidates suggested by the conversion method. Direct kanji input is a Japanese text input method that is totally different from kanakanji conversion. It allows the user to directly select kanji characters, without the prior input of kana characters. Direct kanji input is said to enable the user to efficiently input kanji characters after mastering it, because the user does not need to find necessary kanji characters from the suggested candidates. For hardware keyboards, there are several direct kanji input methods such as T-Code [16]. However, there are no such methods commonly used for touch-panel devices including tablets and smartphones. In this paper, we propose a Japanese software keyboard for a tablet that uses direct kanji input. For this purpose, we extend the bimanual flick-based tablet keyboard that c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 90–111, 2023. https://doi.org/10.1007/978-3-031-25477-2_5  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  91  
   
  we previously proposed [9]. Once the user masters our new keyboard, the user can efficiently input kanji characters while holding a tablet with both hands. Also, we present three kanji layouts that we designed for this software keyboard, one based on elements of kanji called bushu, one based on other elements of kanji called on’yomi, and one that is a revision of the first bushu-based layout. Our keyboard supports the direct input of the 2136 kanji characters called joyo-kanji. We show the results of the three experiments that we conducted to evaluate the performance of our software keyboard. In the first experiment, we compared the bushubased layout and the on’yomi-based layout with two types of a QWERTY software keyboard and with a flick-based software keyboard. In the second experiment, we evaluated how much the user can learn our keyboard by using the learning support functions that we developed for the keyboard. In the third experiment, to evaluate whether the longterm use of the keyboard enables its mastery, one of the authors continuously used the bushu-based layout for 15 months. This paper is a revised and extended version of the paper that we previously published as [10]. Especially, the following parts of this paper are newly added or updated: – We propose a new layout called the revised bushu-based layout, which we describe in the last paragraph of Subsect. 4.2; – We propose three support function for the user’s learning our software keyboard, which we describe in Subsect. 4.5; – We present the result of an experiment on the performance of the revised bushubased layout and the learning support functions in Sect. 7 and discuss it in Subsection 9.2; – We update the result of the long-term experiment in Sect. 8 by extending the term of the experiment from 12 months to 15 months; – We include a discussion on direct kanji input in Subsect. 9.4.  
   
  2 Related Work We extend our previous bimanual flick software keyboard for a tablet [9]. It improved screen space efficiency by splitting a flick keyboard into the left and the right. However, it was not suitable for kana-kanji conversion because the conversion space generally extends from the left to the right edge of the screen. In this paper, we solve this problem by introducing direct kanji input. Many Japanese input method uses predictive conversion, which presents conversion suggestions based on previously used words. For example, Ichimura et al. proposed a predictive kana-kanji conversion system [5]. It used the current mainstream predictive conversion method that had been proposed several years before, and reduced users’ keystrokes to 78 %. Unlike predictive conversion that is used in many current Japanese input methods, direct kanji input lets the user select a kanji character. There are two types of direct kanji input: associative and non-associative. Associative direct kanji input has clear relationships between keystrokes and kanji characters. This method has the advantage of being more intuitive and easier to use [16]. On the other hand, non-associative direct  
   
  92  
   
  Y. Nakamura and H. Hosobe  
   
  kanji input does not have such clear relationships between keystrokes and kanji characters. From the user’s point of view, it is a random key placement. For this reason, it is more difficult to use than the associative method. T-Code [16] is one of the most famous methods of non-associative direct kanji input. T-Code uses a combination of two keystrokes on the QWERTY keyboard to enter a kanji character. There is no regularity in such key combinations, and the user needs to first learn them. The non-associative method takes longer time to learn than the associative method, but allows faster input [16]. This is because the associative method requires the user to associate kanji characters with keystrokes, but the non-associative method does not. However, whichever method is used, direct kanji input requires the user to more practice than kana-kanji conversion-based input. In this paper, direct kanji input keyboard was not included in the comparative experiment. The reason is that we were not able to find any available software keyboards for tablets that used direct kanji input. Research and development of input methods for Chinese characters are not limited to the Japanese language. Pinyin input is a widely used Chinese character input method that uses Chinese readings of characters [7]. Cangjie is a direct Chinese character input method used in Hong Kong. In this method, users think of a Chinese character as a combination of parts. A keystroke corresponds to such a part, and a combination of keystrokes is used to input a Chinese character. Liu and Lin [8] proposed an extension of Cangjie to classify similar Chinese characters. Niu et al. [11] proposed Stroke++, a Chinese character input method for mobile phones, in which an input is made by combining bushu elements. Various research has been done on keyboards for tablets. Sax et al. proposed an ergonomic QWERTY tablet keyboard [14]. Bi et al. proposed a bimanual gesture keyboard to reduce display space and to shorten finger movement [1]. Hasegawa et al. studied input of a software keyboard, with a focus on aging effects and differences between dominant and non-dominant hands [4]. Odell studied feedbacks of software keyboards [12]. Takei and Hosobe proposed a Japanese kana input keyboard that input one character with two strokes by using 2 × 6 keys [18]. Yajima and Hosobe proposed a Japanese software keyboard for tablets that reduced user fatigue [20]. In Japan, much research on flick keyboards has been done. Sakurai and Masui proposed a QWERTY flick keyboard [13]. This keyboard enabled input of Japanese kana characters and English letters without mode changes. Fukatsu et al. proposed an eyesfree Japanese kana input method called no-look flick [2]. This method enabled flick input for vowels and consonants in two keystrokes. Hakoda et al. proposed a kana input method using two fingers for touch-panel devices [3]. This method was also an eyes-free Japanese input method, but enabled gesture input by two fingers.  
   
  3 Japanese Characters and Keyboards 3.1  
   
  Kana Characters  
   
  Japanese text is composed of Chinese-originated kanji characters and Japanese kana characters. While a kanji character typically has a meaning, a kana character is associated with a speech sound. There are two kinds of kana characters called hiragana and katakana. Although they are used for different purposes, they correspond to each other;  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  93  
   
  for each hiragana character, there is a corresponding katakana character, and vice versa. There are approximately 50 basic kana characters, which are further divided into 10 groups that are ordered, each of which typically consists of 5 characters. The first group is special because its 5 characters indicate 5 vowels that are pronounced “a,” “i”, “u”, “e”, and “o”. The other 9 groups are associated with the basic consonants, “k”, “s”, “t”, “n”, “h”, “m”, “y”, “r”, and “w”. A kana character in these 9 groups forms the sound that combines a consonant and a vowel. For example, the 5 characters of the “k” group are pronounced “ka”, “ki”, “ku”, “ke”, and “ko”. This grouping of kana characters is basic knowledge of the Japanese language. 3.2 Elements of Kanji Characters We explain bushu and on’yomi elements of kanji characters that we use in our software keyboard. Bushu. Bushu, also called a radical, indicates an element of kanji characters. Kanji characters are generally made up of dots and lines. A bushu element is a common collection of such dots and lines. For example, Fig. 1 shows kanji characters for a pine and cherry blossoms. The red boxes in the figure indicate their bushu elements. This type of bushu is called “kihen” and is typically used in kanji characters related to trees. Other kanji characters that use kihen correspond to, for example, a small forest and a bridge. The total number of bushu elements used in Japan is 214.  
   
  Fig. 1. Examples of bushu elements.  
   
  On’yomi. Kanji characters typically have two kinds of readings, on’yomi and kun’yomi. The on’yomi of a kanji character indicates its old Chinese reading, while the kun’yomi indicates its Japanese reading. In general, the on’yomi of a kanji character does not make sense in Japanese while the kun’yomi does. For example, the kun’yomi of the kanji character for cherry blossoms in the Fig. 1 is “sakura”, which means cherry blossoms by itself. By contrast, its on’yomi, which is “ou”, has no meaning in Japanese. 3.3 Japanese Keyboards Figure 2-a shows a typical Japanese flick keyboard. The main key layout is composed of 4 × 3 keys. If a user flicks a key to the left, upward, to the right, or downward with  
   
  94  
   
  Y. Nakamura and H. Hosobe  
   
  a thumb, the keyboard inputs a character corresponding to the direction (Figs. 2-b and 2-c). A conversion space is located at the top of the keyboard. When a user touches a word, kana characters are converted to kanji or other characters. If a user touches the upward arrow, it will show other kanji candidates.  
   
  Fig. 2. Typical Japanese flick keyboard (available on iOS).  
   
  4 Proposed Method 4.1  
   
  Bimanual Flick  
   
  We propose a software keyboard for tablets that splits a flick keyboard into the left and right sides. It is based on our previous bimanual split flick keyboard [9] that uses normal kana-kanji conversion. Instead of using such normal kana-kanji conversion, our new software keyboard introduces direct kanji input. Since the user can use the keyboard while holding the tablet with both hands, the advantages of split keyboards are not lost. If the user wants to input a kana character, the user only needs to flick a key on one side as with other splits keyboards. A primary reason for developing a new direct kanji input method is that previous methods were designed for desktop computers with hardware keyboards. Although these previous methods could be adapted to newer devices such as tablets and smartphones, there has not been much research, and their effectiveness is unclear. Also, the proposed method might be applicable to the Chinese language. This is because Cangjie is similar to our bushu-based method and Pinyin input is similar to our on’yomi-based method (although Japanese and Chinese use different sets of standard Chinese characters, which would require extra efforts.) A typical input flow for a kanji character is shown in Fig. 3. This example inputs the kanji character meaning “cherry blossoms” previously shown in Fig. 1. Figure 3-a shows a state in which no input is made. This kanji character uses the bushu element called “kihen”, which belongs to the “ki” group. The “ki” group further belongs to the “k” group. Therefore, the user first touches the “ka” key that represents the “k” group (Fig. 3-b). Then the user flicks the finger to the left to show the “ki” group (Fig. 3-c). The user looks for the kanji character for cherry blossoms, finding that it belongs to the top left key. Therefore, the user touches the top left key (Fig. 3-d). Then the user flicks the finger to the right (Fig. 3 -e), which completes the input of the kanji character.  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  95  
   
  Fig. 3. Typical input flow for a kanji character.  
   
  The number of kanji characters that can be input with this method is 2136. These kanji characters, called joyo-kanji, are indicated as the standard for using kanji characters in social life in Japan. According to a survey conducted by the Japanese Agency for Cultural Affairs, more than 96 % of the total number of kanji characters regularly used in Japanese society are joyo-kanji characters [17]. It was possible to cover more kanji characters in the proposed method. However, we decided that no more kanji characters were needed. Other kanji and kana characters can be input using the conversion function of the previous bimanual flick keyboard. It also should be noted that our method covers more characters than T-Code [16], which covers 1600 characters. 4.2 Kanji Layouts We propose three kanji layouts: the bushu-based layout, the on’yomi-based layout, and the revised bushu-based layout. We used the first two methods in the comparative experiment, and used the revised bushu-based layout in the experiment on learning. Bushu-based Layout. The bushu-based layout uses bushu elements of kanji characters. The layout is shown in Figs. 4 and 5. Each cross in Fig. 4 indicates a key that can be flicked upward, downward, to the left, and to the right, and the positions of the crosses correspond to the shape of the keyboard. The bushu elements that appear more than once in the figure are those that have many kanji characters. On the contrary, the bushu elements shown in Fig. 5 have a few kanji characters. They are grouped together in the parts labeled with the numbers in Fig. 4. In the case of kanji characters with the same bushu element, they are arranged by on’yomi from the top left. Also, since there is a limit on the size of the keyboard, four bushu elements with large numbers of characters are divided into two keys. In this case, they are located symmetrically. On’yomi-based Layout. The on’yomi-based layout arranges kanji characters by using their readings. Since dakuon is also present in on’yomi, the “k,” “s,” “t,” and “h” groups  
   
  96  
   
  Y. Nakamura and H. Hosobe  
   
  Fig. 4. Bushu-based layout.  
   
  can be replaced with the dakuon keyboard. Figure 6 shows the process of inputting the kanji characters of the “ka” group. If the user presses the top left key on the kanji keyboard, it is replaced with the dakuon keyboard. If the user presses the top left key on the dakuon keyboard, it is replaced with the original keyboard. Revised Bushu-based Layout. The third layout is a revision of the bushu-based layout. We designed this layout, motivated by the requests that we obtained from the participants in the comparative experiment that we present in Sect. 6. This revised bushu-based layout places the bushu elements according to the numbers of their components (Fig. 7), where the key sizes are not limited even for the bushu elements with many kanji characters. Since bushu elements with many kanji characters correspond to single keys, there is perfect correspondence between bushu elements and kana characters. 4.3  
   
  Direct Kanji Input  
   
  Our keyboard is based on direct kanji input. Previous direct kanji input methods were categorized into associative input and non-associative input. We regard the bushu-based layout as being semi-associative because it is neither associative nor non-associative in the original senses. It basically arranges kanji characters by bushu elements, but there is no perfect correspondence between kana characters and bushu elements. By contrast, the on’yomi layout is associative since there is correspondence between kanji characters and kana characters. 4.4  
   
  Key Arrangement  
   
  Our software keyboard is of 240-px height and 180-px width before conversion. This layout reduces the display space by 73 % in the portrait mode and by 83 % in the  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  97  
   
  Fig. 5. Bushu elements with a few kanji characters.  
   
  Fig. 6. On’yomi-based layout (for inputting a kanji characters in the “ka” group).  
   
  landscape mode, compared with the QWERTY keyboard with the maximum display width. The size and the position of the bushu-based layout are based on the heat map used in the design of the Windows 8 touch keyboard [6]. This limited the bushu-based layout to 4 × 4. By contrast, the on’yomi layout does not have this limit. Therefore, in the case of the on’yomi-based layout, there may be much more kanji characters for a kana character than in the case of the bushu-based layout. In the case of the bushu-based layout, the maximum number of kanji characters for a kana character is 80. Since there are 50 kana characters in total, the total number of kanji characters that can be placed is 4000. We adopted 2136 joyo-kanji characters. Kana keys with a few kanji characters are composed of a 2 × 2 kanji layout. The keyboard in the bushu-based layout is of the maximum size of 320-px height and 320-px width on one side. The keyboard in the on’yomi-based layout is of the maximum size of 480-px height and 400-px width.  
   
  98  
   
  Y. Nakamura and H. Hosobe  
   
  Fig. 7. Revised bushu-based layout.  
   
  4.5  
   
  Learning Support Functions  
   
  Our software keyboard provides the following three learning support functions: 1. Support function for learning the type of the bushu element of a kanji character (Fig. 8); 2. Support function for leaning the location of the bushu element of a kanji character (Fig. 9); 3. Support function for learning the location of a kanji character on the keyboard (Fig. 10). We developed these functions, motivated by the result of the comparative experiment that we present in Sect. 6. In the development of these functions, we reflected the participants’ comments that we obtained from the comparative experiment, as well as our own experience in the long-term experiment that we present in Sect. 8.  
   
  Fig. 8. Support function 1 for learning the type of the bushu element of a kanji character.  
   
  Support function 1 displays Fig. 8 while the user is pressing the hint button. Support function 2 displays the bushu element of a kanji character in the target text, which allows  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  99  
   
  Fig. 9. Support function 2 for learning the location of the bushu of a kanji character.  
   
  the user to switch to the bushu element of the next kanji character by pressing the hint button. Support function 3 colors the kanji characters with red whose readings match with the kana characters that are being input.  
   
  5 Implementation We implemented the proposed software keyboard on an ASUS ZenPad 10 tablet (Android OS 7.0, 1920 × 1200-px screen) as shown in Fig. 11. The keys are of 60 × 60 px, and the keyboard is placed symmetrically at the lower ends of the screen. In the proposed keyboard, its position was adjustable with a bar at the bottom of the screen. The red part in the figure indicates the conversion space that is used to enter non-joyokanji and katakana characters. The user can display different characters by swiping the conversion space to the left or to the right. The radio button in the center of the figure allows the user to change the key layout.  
   
  6 Comparative Experiments To evaluate the proposed software keyboard, we conducted an experiment on its comparison with existing software keyboards. We compared the bushu-based layout, the on’yomi-based layout, two QWERTY keyboards, and a flick keyboard. There are two types of QWERTY keyboards, one with the learning of predictive conversion enabled and one with the learning disabled. The reason why the direct input method was not compared is that we were not able to find any available software keyboards for tablets that used direct kanji input. The learning of the predictive conversion is reset for each participant. The comparative experiments treated the landscape mode of each keyboard layout. In the proposed keyboard, its position was adjustable with a bar at the bottom of the screen. The position of the keyboard was set by each participant.  
   
  100  
   
  Y. Nakamura and H. Hosobe  
   
  Fig. 10. Support function 3 for learning the location of a kanji character on the keyboard.  
   
  We recruited 8 participants who were Japanese university students and workers. Their ages ranged from 23 to 24, and all the participants were male. They were seated on a chair and held a tablet in the landscape mode with both hands. If participants were not able to reach the center of the keyboard in using the QWERTY, they were allowed to release their hands. The comparative experiments were composed of two input experiments and subjective evaluation. After the input experiments, we investigated subjective evaluation for each method. In addition to the UEQ, free descriptions were also collected. We measured the input speed and the error rate. The input speed is measured by the number of characters per minute CPM, which is calculated as follows: CPM =  
   
  T−E × 60 S  
   
  (1)  
   
  where T is the length of the input string, S is the input time, and E is the number of characters that were wrong. This means the number of characters typed correctly per minute. On the other hand, the error rate ER is calculated as follows: ER =  
   
  IF C + IF + INF  
   
  (2)  
   
  where C is the total number of correct words, IF is the number of incorrect but fixed (backspaced) words, and INF is the number of incorrect (but not fixed) words. These equations are based on Bi et al.’s research [1], and we adjust them to Japanese character input. The comparative experiments treated two kinds of input: sentence input and kanji conversion-required word input. In the following, we describe the details and the results of the two experiments.  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  101  
   
  Fig. 11. Implementation of the proposed software keyboard.  
   
  6.1 Sentence Input In the sentence input experiment, five input sentences were selected for each method from a book [19] about learning joyo-kanji with example sentences. One of the QWERTY keyboards and the flick keyboard enabled learning in predictive conversion, but the sentences were specific to each method, and therefore the learning was limited to the inside of a method. Before the experiment, participants warmed up with a few sentences for each method. They started the experiment by pushing the start button on the upper left corner of the screen, and moved to the next sentence by pushing the enter key. A target sentence was displayed on the text field at the top of the screen. The sentences included in the list were of about 20-character length and mixed kana and kanji characters. Since the bushu-based layout generally takes long time for users to learn, and also since the short warm-up before the experiment was not sufficient for the participants’ learning, a support function was provided. It consisted of two hints: a hint for the bushu element of a kanji character and a hint for the position of a kanji character. The hint for bushu was displayed by pressing the hint button in the upper right corner of the screen (Fig. 11). When the button was pressed, the bushu elements of kanji characters in the target sentence were displayed one by one. The hint for a kanji position was shown in Figs. 4 and 5. The participants were able to look at this diagram if they did not know the positions of kanji characters. 6.2 Result of the Sentence Input Experiment The results of the input speeds and the error rates are shown in Figs. 12-a and 12-b respectively. In the charts, “QWERTY ON” and “Flick” indicate the QWERTY keyboard and the flick keyboard with learning in predictive conversion respectively. A higher input speed is better, and a high error rate is worse. The results show large differences between the proposed methods and the existing methods (shown as the three  
   
  102  
   
  Y. Nakamura and H. Hosobe  
   
  bars on the right sides of the charts). The ANOVA on the two proposed methods also showed a significant difference in the input speeds (p < 0.05), but not in the error rates. Among the existing methods, the flick keyboard was the fastest, and the two QWERTY keyboards were of about the same speeds. Also, the flick keyboard was the highest in the error rates, and the two QWERTY keyboards showed about the same error rates.  
   
  Fig. 12. Result of the sentence input experiment: (a) input speeds and (b) error rates.  
   
  6.3  
   
  Kanji Conversion-Required Word Input  
   
  The experiment on the kanji conversion-required word input treated the same layouts as the sentence input experiment. Its procedure was almost the same as that of the sentence input experiment, and the only difference was what the participants input. In this experiment, the participants input words written in kana characters at the top of the screen and converted them into kanji characters. The experiment was conducted after the sentence input experiment and there was no warm-up time. Five words were used for each method, and when a participant pressed the enter key, the next word was displayed. The target word was the word that appeared in the sentence input experiment. For this reason, the hints used in the sentence input experiment were not used in the word input. 6.4  
   
  Result of the Kanji Conversion-Required Word Input Experiment  
   
  The results of the input speeds and the error rates are shown in Figs. 13-a and 13b respectively. The results show large differences between the proposed methods and the existing methods. The ANOVA on the two proposed methods showed a significant difference in the input speeds (p < 0.05), but not in the error rates. Among the existing methods, the QWERTY keyboard with learning in predictive conversion and the flick keyboard were the fastest and almost comparable. The three existing methods showed similar low error rates. 6.5  
   
  Subjective Evaluation and Participants’ Comments  
   
  The subjective evaluation was performed by using the UEQ [15] to investigate the two proposed methods and the QWERTY keyboard with learning. Figures 14-a and 14b show the results of the subjective evaluation. In the bushu-based layout, the UEQ showed an excellent rating for novelty although the ratings of the other items were low.  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  103  
   
  Fig. 13. Result of the kanji conversion-required word input experiment: (a) input speeds and (b) error rates.  
   
  In particular, ratings for perspicuity and efficiency were very low. The on’yomi-based layout was rated higher than the bushu-based layout except novelty. When the on’yomibased layout and the existing methods were compared, the existing methods were better in perspicuity, efficiency, and dependability, and the on’yomi-based layout was better in novelty.  
   
  Fig. 14. Results of the UEQ on (a) the bushu-based layout and (b) the on’yomi-based layout.  
   
  The comparison of the on’yomi-based layout and the existing methods is shown in Fig. 15. Both the QWERTY and the flick keyboard obtained higher ratings in perspicuity, efficiency, and dependability. The proposed method was rated higher in novelty. The participants’ comments mainly indicated the difficulty of inputting with the proposed methods. The comments included “It is like a mental exercise”, “I’m tired”, and “It is not for me”. The on’yomi-based layout had more positive comments than the bushu-based layout.  
   
  104  
   
  Y. Nakamura and H. Hosobe  
   
  Fig. 15. Result of the comparison of the on’yomi-based layout, the QWERTY keyboard, and the flick keyboard in the UEQ.  
   
  7 Experiment on Learning After the comparative experiment, we revised the bushu-based layout and added three learning support functions. To evaluate the revised layout and the support functions, we conducted an experiment. 7.1  
   
  Method  
   
  We recruited 8 participants who were Japanese university students and company employees, 21 to 24 years old, and all male. The experiment consisted of two sessions: the practice session in which the participants input 100 sentences, and the main session in which they input 30 sentences. Each sentence for the practice session contained one kanji character, and 5 kanji characters were selected from 20 bushu elements with many kanji characters. The sentences for the main session consisted of 10 sentences used in the practice session, 10 sentences with kanji characters whose bushu elements were the same as the ones used in the practice session, and 10 sentences with kanji characters whose bushu elements were not used in the practice session. The experiment were divided to sections, each of which consisted of 10 sentences; there were pauses of several seconds between the sections. After the main session, we conducted a subjective evaluation using the UEQ and collected comments from the participants. 7.2  
   
  Result  
   
  Figures 16-a and 16-b show the results of the input speeds and the error rates in the practice session. The horizontal axes correspond to the every 10 inputs. Figure 16-a suggests that the input speeds generally increased according to the numbers of the inputs. Around the 100th input, The input speed became nearly the double of the first 10 inputs. By contrast, according to Fig. 16-b, the error rate for the first 10 inputs were significantly high, and those after that indicated almost the average result.  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  105  
   
  Figures 17-a and 17-b show the results of the input speeds and the error rates in the main session. The horizontal axes correspond to (1) the 10 sentences used in the practice session, (2) the 10 sentences with kanji characters whose bushu elements were the same as the ones used in the practice session, and (3) the 10 sentences with kanji characters whose bushu elements were not used in the practice session. The mean input speeds were the fastest for (1), the next for (2), and the slowest for (3). The error rates were the best for (3), the next for (1), and the worst for (2).  
   
  Fig. 16. Result of the practice session in the experiment on learning: (a) input speeds and (b) error rates.  
   
  Fig. 17. Result of the main session in the experiment on learning: (a) input speeds and (b) error rates.  
   
  7.3 Subjective Evaluation and Participants’ Comments We conducted subjective evaluation by using the UEQ. Figure 18 shows the result. It obtained high ratings for stimulation and novelty, a nearly average rating for attractiveness, and low ratings for perspicuity, efficiency, and dependability. Compared with the evaluation of the bushu-based layout and the on’yomi-based layout, the evaluation of the revised bushu-based layout is similar but appears superior. However, it is still largely inferior to the existing methods especially in perspicuity, efficiency, and dependability.  
   
  106  
   
  Y. Nakamura and H. Hosobe  
   
  Fig. 18. Result of the UEQ on the revised bushu-based layout.  
   
  The positive comments of the participants included “Kanji flick input is rare”, “Inputting kanji characters became easy after I memorized their locations”. The negative comments included “The method was difficult because there were many kanji characters for single keys”, “The method was confusing when similar looking kanji characters were near”. Also, there was an opinion about the unclear display of the learning support functions.  
   
  8 Long-Term Experiment Direct kanji input is a method that takes time to learn. Therefore, one of the authors by himself conducted an experiment to confirm its mastery by using the proposed method for 15 months. 8.1  
   
  Method  
   
  The experiment consisted of the input of sentences. Every day the author entered five sentences chosen at random from 602 sentences in the literature [19]. The 602 sentences contain 68 % of the joyo-kanji characters. The author is a 23-year-old male graduate student in the field of computer science. In the experiment, the author was seated on a chair and held a tablet with both hands. The experiment was conducted with the bushubased layout, and a search function was used when the location of a kanji character was not known. 8.2  
   
  Result  
   
  Figure 19-a and 19-b show the results of the input speeds and the error rates respectively. In both charts, the main line indicates the average value for a day. The green bars represent the maximum and the minimum values for the day. For comparison, the author entered the same sentences by using the QWERTY keyboard on the ASUS ZenPad 10 (red line) and our previous bimanual flick keyboard [9] (purple line). The QWERTY keyboard on the ASUS ZenPad 10 also enabled learning in predictive conversion. According to the charts, both the input speeds and the error rates gradually improved day by day. The reason why the bimanual flick keyboard had a high error rate was because it used the backspace key to make adjustments during the conversion.  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  107  
   
  Fig. 19. Result of the long-term experiment (15 months): (a) input speeds and (b) error rates.  
   
  9 Discussion 9.1 Comparative Experiment Sentence Input. The result of the sentence input experiment showed that the proposed methods were inferior to existing methods in terms of the input speeds. There was also a significant difference in the input speeds between the proposed methods. We think that this is because the bushu-based layout is a semi-associative direct kanji input method and the on’yomi-based layout is associative. The semi-associative method takes longer time for users to learn than the associative method, but can be expected to enable faster input. The experiment tried to compensate for the difference by providing hints, but it was not successful. The result also showed that the proposed methods were inferior to the existing methods in terms of the error rates. However, there was no significant difference between the proposed methods in terms of the error rates. We think that the error rates of the on’yomi-based layout in the sentence input experiment were higher than its error rates in the kanji conversion-required word input experiment because the given sentences were written in kanji and kana characters. In this case, if a participant could not read kanji, he would have trouble in input.  
   
  108  
   
  Y. Nakamura and H. Hosobe  
   
  Therefore, to compare input speeds, users who used the proposed method for a long time are needed. One solution might be to distribute the software of the proposed method. We could implement a function to measure the input speed and evaluate it. However, it would be difficult to have many users of the proposed method. Another solution might be to have a small number of participants who would use the proposed method for a long term. Either way, it would be difficult to measure the performance for one year. Therefore, we think that the immediate solution is to increase the efficiency of the learning of the proposed method. Specifically, better key layouts and learning support software are needed. Kanji Conversion-Required Word Input. The results of the experiments showed that the input speeds of the proposed methods in the kanji conversion-required word input were not very different from those in the sentence input. However, there was a difference between the existing methods. The QWERTY keyboard and the flick keyboard with learning in predictive conversion showed high input speeds. As in the sentence input experiment, a significant difference in the input speeds was shown between the bushubased layout and the on’yomi-based layout. Overall, the error rates in the kanji conversion-required word input were lower than those in the sentence input. We think that this is because of the small number of characters entered. However, there was a difference in the error rates between the bushu-based layout and the on’yomi-based layout. We think that the reason is that the participants using the on’yomi-based layout were able to easily associate kanji characters with kana characters, which was not applicable to the bushu-based layout. However, there was still no significant difference between the bushu-based and the on’yomi-based layout. We think that this is because the on’yomi-based layout is not always associative. Since the bushu-based layout is semi-associative and the on’yomi-based layout is associative, the input speeds and the error rates might be reversed depending on the degree of learning. For accurate assessment, we need to find out what is the ratio of the two degrees of learnability. There was no large difference in the error rates between the on’yomi-based layout and any existing method. According to this result, although it is difficult to compare the input speeds without the long-term use of the methods, we think that the error rates of the methods could be compared even without the long-term use. To reduce the error rate, another associative layout should be proposed and compared with the on’yomi-based layout. Subjective Evaluation. The subjective evaluation showed that the bushu-based layout was of high novelty. However, all the other ratings were low, and in particular, those of perspicuity and efficiency were very low. We think that this is because of the complicated layout that is difficult to use without learning. We also think that this is because the user evaluated the bushu-based and the on’yomi-based layout by comparing them. The on’yomi-based layout was higher than the bushu-based layout in all the items except novelty. Still, other than stimulation and novelty, it was rated low. One major factor is the short amount of time that the participants spent using the proposed methods. However, in order for users to use it for a long time, the proposed methods should give  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  109  
   
  a good impression at the beginning. The goal is to improve each item to the point that cause no large difference between the proposed methods and the existing methods. 9.2 Experiment on Learning The input speeds in the practice session in the experiment on learning generally increased according to the numbers of inputs although there was some variation in the increase. We think that this variation was due to the bushu elements of the kanji characters in the sentences. In fact, their order was not systematic since we did not place any priorities on the kanji characters for the practice session. The error rates in the practice session showed significantly high for the first 10 sentences and became almost average after the first 10 sentences. We observed that many participants made significantly high error rates especially for the first 5 sentences. We think that this was because they were not used to the proposed method. After the first 10 sentences, the error rates remained almost the same and did not improve. The input speeds in the main session were the fastest for the sentences used in the practice session, the next for the sentences with kanji characters whose bushu elements were the same, and the slowest for the sentences with kanji characters whose bushu elements were not used in the practice session. The results were expected because the speeds for the already practiced kanji characters and bushu elements were faster. We think that this shows the effectiveness of our learning support functions. However, in this experiment, we asked all the participants to use the learning support functions. Therefore, we were not able to measure the input speeds for the case that these functions were not used. The error rates were the best for the unpracticed bushu elements, the next for the practiced kanji characters, and the worst for the practiced bushu elements. The results were unexpected because the error rate for the practiced kanji characters was not the best. We think that this was because the participants became careful in inputting the sentences with the unpracticed bushu elements but became careless otherwise. Also, we think that the error rate for the practiced kanji characters were not much worse because the participants memorized their locations, and that the error rate for the practiced bushu elements were worse because they did not know the locations of the kanji characters. We think that our learning support functions were not effective from the viewpoint of the error rates. 9.3 Long-Term Experiment According to the result of the long-term experiment, the input speed of the bushu-based layout after 15 months was about the same as that of the QWERTY keyboard. The long-term experiment was conducted by one of the authors alone, and therefore it is not objective. However, it shows that the input speed of the bushu-based layout could improve with a long-term use. Based on the author’s experience, we think that there are three stages of growth in the input speeds. The first is to learn the placement of bushu elements. In other words, the user can remember the locations of bushu elements without referring to Fig. 4. The next stage is to identify bushu elements of kanji characters whose bushu is confusing.  
   
  110  
   
  Y. Nakamura and H. Hosobe  
   
  For example, a kanji character is confusing if it has more than one bushu-like element. The final stage is to learn the location of a kanji character. After this, the author knows where frequently used kanji characters are located. We expect that the number of such kanji characters will increase as the author further continues it. The error rate of the bushu-based layout after 15 months is still higher than that of the QWERTY keyboard. There are several reasons. The first is due to a mistake in the bushu element of a kanji character. Because of the specification of the input, if the user makes a mistake in the bushu, a kana character will be input. The error rate also increases if the user makes a mistake in the kanji character itself. Since some kanji characters have similar shapes, users may make inputting errors. These problems might be solved after a longer-term use. We have not yet compared the bushu-based layout with other methods with a longterm use. Especially, it would be desirable to include the on’yomi-based layout in the experiment. 9.4  
   
  Direct Kanji Input  
   
  Direct kanji input may enable the users to efficiently input kanji characters once they master after the long time for learning it. The results of our research also show that it is true to a certain degree. However, there are currently no popular kanji direct input methods for touch-panel devices such as smartphones and tablets. We believe that direct kanji input has potential for a more efficient and more learnable kanji input method for such devices. From the observation of our experimental results, we think that kanji direct input typically shows large variation in the input speed before it is mastered. This is because it requires learning for individual kanji characters and causes different input speeds based on the mastery of individual kanji characters. We think that such variation in the input speed should be smaller.  
   
  10  
   
  Conclusions and Future Work  
   
  We proposed a bimanual flick-based Japanese software keyboard using direct kanji input. We also presented three layouts for this keyboard, the bushu-based, the on’yomibased, and the revised bushu-based layout. We evaluated the performance of our method by conducting a comparative experiment, an experiment on learning, and a long-term experiment. Our future work is to allow the users to learn our software keyboard during its daily use by enabling the learning support functions to work whenever necessary. Other future directions include developing a learning support function for reducing the error rate, determining the kanji layout by comparing the changes of the input speeds in the long-term use of the on’yomi-based layout and the revised bushu-based layout, and enabling the users to add necessary non-joyo-kanji characters.  
   
  A Bimanual Flick-Based Japanese Software Keyboard  
   
  111  
   
  References 1. Bi, X., Chelba, C., Ouyang, T., Partridge, K., Zhai, S.: Bimanual gesture keyboard. In: Proceedings of UIST, pp. 137–146 (2012) 2. Fukatsu, Y., Shizuki, B., Tanaka, J.: No-look flick: Single-handed and eyes-free Japanese text input system on touch screens of mobile devices. In: Proceedings of MobileHCI, pp. 161–170 (2013) 3. Hakoda, H., Fukatsu, Y., Shizuki, B., Tanaka, J.: An eyes-free kana input method using two fingers for touch-panel devices. IPSJ SIG Tech. Rep. 154(6), 1–8 (2013). In Japanese 4. Hasegawa, A., Hasegawa, S., Miyao, M.: Characteristics of the input on software keyboard of tablet devices: Aging effects and differences between the dominant and non-dominant hands for input. J. Mobile Interact. 2(1), 23–28 (2012) 5. Ichimura, Y., Saito, Y., Kimura, K., Hirakawa, H.: Kana-kanji conversion system with input support based on prediction. In: Proceedings of COLING, vol. 1, pp. 341–347 (2000) 6. Knox, K.: Designing the Windows 8 touch keyboard (2012). https://learn.microsoft.com/enus/archive/blogs/b8/designing-the-windows-8-touch-keyboard 7. Li, G., Li, Y.: Chinese pinyin input method in smartphone era: a literature review study. In: Yamamoto, S., Mori, H. (eds.) HCII 2019. LNCS, vol. 11569, pp. 34–43. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-22660-2 3 8. Liu, C.-L., Lin, J.-H.: Using structural information for identifying similar Chinese characters. In: Proceedings of ACL HLT, pp. 93–96 (2008) 9. Nakamura, Y., Hosobe, H.: A Japanese bimanual flick keyboard for tablets that improves display space efficiency. In: Proceedings of VISIGRAPP, vol. 2, pp. 170–177 (2020) 10. Nakamura, Y., Hosobe, H.: A flick-based Japanese tablet keyboard using direct kanji input. In: Proceedings of VISIGRAPP, vol. 2, pp. 49–59 (2021) 11. Niu, J., Zhu, L., Yan, Q., Liu, Y., Wang, K.: Stroke++: a hybrid Chinese input method for touch screen mobile phones. In: Proceedings of MobileHCI, pp. 381–382 (2010) 12. Odell, D.: On-screen keyboard: Does the presence of feedback or tactile landmarks improve typing performance? In: Proceedings of MobileHCI, pp. 131–136 (2015) 13. Sakurai, Y., Masui, T.: A flick-based Japanese input system for a QWERTY software keyboard. IPSJ SIG Tech. Rep. 154(5), 1–4 (2013). In Japanese 14. Sax, C., Lau, H., Lawrence, E.: LiquidKeyboard: an ergonomic, adaptive QWERTY keyboard for touchscreens and surfaces. In: Proceedings of ICDS, pp. 117–122 (2011) 15. Schrepp, M., Hinderks, A., Thomaschewski, J.: Construction of a benchmark for the user experience questionnaire (UEQ). Int. J. Interact. Multimedia Artif. Intell. 4(4), 40–44 (2017) 16. T-Code Project. T-Code laboratory (2003). https://openlab.ring.gr.jp/tcode/index.html. In Japanese 17. Takeda, Y.: A survey of the occurrence of kanji characters. In: 4th Meeting of the Working Group on the Standard of Japanese Education. Japanese Agency for Cultural Affairs (2019). In Japanese 18. Takei, K., Hosobe, H.: A 2-by-6 button Japanese software keyboard for tablets. In: Proceedings of VISIGRAPP, vol. 2, pp. 147–154 (2018) 19. Waragai, H.: Learning Joyo-Kanji by Reading One Sentence. Goto Shoin (2008). In Japanese 20. Yajima, T., Hosobe, H.: A Japanese software keyboard for tablets that reduces user fatigue. In: Proceedings of COMPSAC, pp. 339–346 (2018)  
   
  Comparison of Cardiac Activity and Subjective Measures During Virtual Reality and Real Aircraft Flight Patrice Labedan(B) , Fr´ed´eric Dehais , and Vsevolod Peysakhovich ISAE-SUPAERO, Universit´e de Toulouse, Toulouse, France [email protected]  Abstract. Pilot training requires significant resources, both material and human. Immersive virtual reality is a good way to reduce costs and get around the lack of resources availability. However, the effectiveness of virtual flight simulation has not yet been fully assessed, in particular, using physiological measures. In this study, 10 pilots performed standard traffic patterns on both real aircraft (DR400) and its virtual simulation (in head-mounted device and motion platform). We used subjective measures through questionnaires of immersion, presence, and ability to control the aircraft, and objective measures using heart rate, and heart rate variability. The results showed that the pilots were able to fully control the aircraft. Points to improve include updating the hardware (better display resolution and hand tracking) and the simulator dynamics for modelling ground effect. During the real experience, the overall heart rate (HR) was higher (+20 bpm on average), and the heart rate variability (HRV) was lower compared to the virtual experience. The flight phases in both virtual and real flights induced similar cardiac responses with more mental efforts during take-off and landing compared to the downwind phase. Overall, our findings indicate that virtual flight reproduces real flight and can be used for pilot training. However, replacing pilot training with exclusively virtual flight hours seems utopian at this point. Keywords: Virtual reality · Flight simulation · Heart rate · Heart rate variability · Piloting  
   
  1 Introduction Despite a significant passenger traffic decrease in aviation due to the COVID-19 pandemic, the global air transport sector is recovering. The post-COVID-19 aviation industry will inevitably face challenges of pilot and instructors shortage. Pilot training requires significant resources, both material and human, such as flight simulators, flight instructors, and, of course, aircraft. It represents a high cost and generates constraints on the availability of these means. Safety is also a paramount aspect of pilot training, particularly, while flying on real aircraft (breakdowns, weather phenomena, etc.). Immersive virtual reality (VR) seems to be an adequate alternative to reduce costs and get around the lack of availability of resources (aircraft, simulators, instructors) for skill acquisition [19, 44]. The recent development makes the design of virtual environments such as cockpit or flight simulators more flexible [1]. The VR is already used c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 112–131, 2023. https://doi.org/10.1007/978-3-031-25477-2_6  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  113  
   
  professionally or evaluated for training in various fields such as UAV piloting [36], fire fighting [6, 7], maritime domain [29], first responsers training [24], evacuation training [12], or mining industry training [45]. The medical field is a forerunner in the field of VR adoption [5, 20, 22, 28, 34]. VR now allows to deliver, in some cases, cost-effective, repeatable, standardized clinical training on demands. It is a powerful educational tool and implementation is growing worldwide [33]. In surgery, the use of virtual reality has been successfully tested for many years [38]. Surgeons in training can acquire skills without threatening the lives of patients, especially in laparoscopic surgery [16], with positive results in terms of feelings of presence and the ability to transfer the training to the real operation. This is also the case in cataract surgery [40], in oral and maxillofacial surgery [3], etc. These promising results in the field of surgery and its similarities to flight, including high levels of stress, accuracy, and risk-taking [35], make VR worthy of consideration for pilot training. However, the use of VR as an operational learning tool still presents challenges [13] in terms of immersion, sense of presence [42], fatigue, and motion sickness [26]. Indeed, it is recognized that simulators do not reproduce the level of engagement that pilots may experience in real-world conditions [14, 31]. Studies comparing VR and simulator training [2, 27, 31] or simulator and real flight training [17], have already been conducted. A recent study [26] with pilot instructors showed that the strong feeling of immersion, combined with good controllability of the aircraft, generates high presence levels. Another study [32] showed that VR is an efficient tool for learning checklists in the early stages of pilot training. But recently it was shown that virtual reality flight simulations induce higher workload, physical demand, and effort, exceeding acceptable levels [2]. To date, to our best knowledge, no research except one has been found directly comparing VR and real flights. This study [25], with 4 participants, showed preliminary results with a higher heart rate and a lower heart rate variability in real flight compared with virtual reality. An interesting perspective for such a comparison is to measure subjective and objective indicators of the mental effort of pilots in both virtual and real flight situations. Cardiac activity, in particular, is a possible indicator for cognitive load [30], even in operational conditions [37]. A similar approach had already been carried out to compare simulators to virtual reality [27] and disclosed a slightly higher heart rate in virtual reality than in a flight simulator. The present study focuses on the data acquisition and analysis of heart activity parameters, in real and virtual flight conditions, with student pilots in training. We also report subjective measures of pilots’ experience in virtual reality, particularly the feeling of presence and the difficulty perceived by the pilots to perform different actions during the flight of the chosen scenario. We build up upon a previous preliminary experiment with a limited participant number (N = 4) [25]. The present work includes 10 pilots, thus, allowing to perform a statistical analysis between different conditions and flight phases. Previous insights can be thus statistically verified. We also added other subjective questionnaires such as the Presence Questionnaire, the Immersive Tendencies Questionnaire, and the Flight Difficulty Questionnaire to evaluate the difficulties to perform the virtual reality flight.  
   
  114  
   
  P. Labedan et al.  
   
  Fig. 1. The three flight phases: take-off, downwind and landing, each lasting 60 s.  
   
  2 Materials and Methods 2.1  
   
  Participants  
   
  Ten student pilots of the ISAE-SUPAERO, Toulouse, France, holding a private pilot’s license (PPL) or in training to obtain it, participated in the experiment (all males, mean age 22.4 ± 4.4 years; mean flight experience 154 h). All were in good health, as evidenced by their flight medical certificates. No participant had a history of cardiac or neurological disease and, as required by aviation regulations, no participant was taking psychoactive substances or medications. Participants signed a consent form prior to the experiment. The virtual reality experience was approved by the Ethics and Research Committee of Toulouse (n2019-193). The flight experience was approved by the European Aviation Safety Agency with the permit to fly N◦ 0147/21/NO/NAV (the flight conditions are defined in the document Monitoring Pilot’s brain activity in real flights dated 26/01/2021, approved by EASA 60077217 on 24/03/2021). 2.2  
   
  Real Aircraft  
   
  The aircraft used during the experiments was the ISAE-SUPAERO experimental fourseater airplane Robin DR400 (160 HP). It is the same aircraft that is used to train our volunteers who participated in the study (Fig. 2). The experiment took place at Toulouse-Lasbordes airfield, in France (OACI code: LFCL). 2.3  
   
  Virtual Aircraft  
   
  We used the VRtigo platform (Fig. 4) [25, 26], a VR flight simulator, that realistically reproduces the DR400 cockpit and Lasbordes Airfield environment to allow comparison with the real flight settings. This platform is composed of:  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  115  
   
  Fig. 2. The aircraft used for the experiment (Robin DR400).  
   
  Fig. 3. DR400 cockpit panel identically reproduced in virtual reality.  
   
  – Aerofly FS2 flight simulation software (IPACS); – Runway 33 of Toulouse-Lasbordes airfield (OACI code: LFCL), identically reproduced, with some buildings commonly used for visual cues; – DR400 panel, also identically reproduced to the aircraft used during real flights (Fig. 3); – The 6-axis motion platform MotionSystems PS-6TM-150 with the following characteristics: heave –106.9 mm +117.1 mm, pitch –25◦ +25.6◦ , roll ±26◦ , yaw ±22.5◦ , surge –100 mm +121 mm, sway –99.5 mm +121mm; – Simple and conventional controls: stick, rudder, throttle, and flap lever; – A cockpit, including the controls and a pilot’s seat; – A virtual reality headset (HTC Vive); – An Alienware “VR ready” Laptop computer.  
   
  116  
   
  P. Labedan et al.  
   
  Fig. 4. VRtigo: the virtual flight simulator at ISAE-SUPAERO.  
   
  2.4  
   
  Flight Scenario  
   
  The scenario consisted of three consecutive standard traffic patterns. This exercise is highly formalized [8, 37] in terms of flight procedures and flight path which makes it a relevant candidate for comparing pilot’s behavior in the two experimental conditions (i.e. VR vs real flight). The scenario was identical in both virtual and real flights, and consisted of three traffic patterns, with a touch-and-go between each of them. After the last pattern, the pilots made a final landing and complete stop of the aircraft. In this study, we focused our analyses on three specific phases of the standard traffic pattern (Fig. 1): – Take-off (from maximum power setting); – Downwind (the center of the return path); – Landing (before touchdown). The separation into 60-s phases allowed to compare with the previous study performed in similar conditions [37], and to improve our previous study [25].  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  117  
   
  2.5 Measures Cardiac Activity. The cardiac data were acquired with a Faros 360 eMotion electrocardiogram (ECG) device. It provides the raw ECG signal at a sampling rate 500 Hz and R-R intervals data using a built-in R-detection algorithm. To improve the signal quality, we applied a conductive gel to each of the three electrodes connected to the Faros system. Physiological measures were synchronized with the flight parameters with the Lab Streaming Layer (LSL) [23]. Flight Parameters. In the VR flights, the following Aerofly FS2 simulator flight parameters were streamed, recorded, and stored via the LabRecorder: – – – – – – – – –  
   
  Altitude (feet) Pitch and bank angle (degrees) Groundspeed (knots) Indicated airspeed (knots) Longitude/Lattiude (degrees) Mach number (feet) Magnetic and true heading (degrees) Throttle (%) Vertical airspeed (feet/min)  
   
  For the real flights, an ILevil 3-10-AW acquisition unit was used to collect the trajectory (via GPS), accelerations, altitude (in feet), speed (in knots), and yaw/pitch/roll information. Similarly, these data were recorded and stored via the LabRecorder. This acquisition unit had to be mounted in the aircraft’s cargo area at a specific location that guaranteed the accuracy of the attitude data (roll, pitch, and yaw). These parameters were then used to automatically identify the three flight phases of interest. Questionnaires. Regarding the ten pilots, four participated in the experiment without filling out the questionnaires (2019). The other six (2021) participated in the experiment with three subjective questionnaires about the virtual environment. All the questionnaires used a visual analog scale from 1 to 7 for each answer, with different significations according to the question. The Presence Questionnaire (PQ). This questionnaire measures the feeling of presence in a virtual environment. There are many versions of it and the one we used is composed of 17 questions. It is inspired by the 2002 french version from The Cyberpsychology Lab of UQO (Universit´e du Quebec en Outaouais) [4]. The signification of the answer on the visual scale from 1 to 7 is different for each question (please refer to the 2002 Cyberpsychology Lab version for more details); The Immersive Tendencies Questionnaire (ITQ). This questionnaire measures differences in the tendencies of individuals to experience presence. We selected the 2002 french version from The Cyberpsychology Lab of UQO (Universit´e du Quebec en Outaouais) [4]. This version is inspired by Witmer and Singer’s 1998 original version  
   
  118  
   
  P. Labedan et al.  
   
  [43]. It is composed of 18 questions that measure the level to which the individual can cut off from external distractions to concentrate on different tasks. The participant must answer each question on a 7-point scale. A score from 1 to 7 is associated with each answer and the global score is calculated. These 18 questions are also divided into 4 subscales, which measure different aspects of immersion propensity (Focus, Involvement, Emotion, and Game). The “Focus” subscale measures the sustained attention generated by an activity (5 items). The “Emotion” subscale deals with the individual’s ease of feeling intense emotions evoked by the activity (4 items). The “Game” subscale refers to the individual’s ability to project him/herself into a playful context (video game, etc.) (3 items). The “Involvement” subscale measures the tendency of an individual to identify with characters or to feel completely absorbed by an activity (4 items). A score is calculated for each subscale. The Cyberpsychology Lab has established certain norms (minimum scores for the overall and per subscale). Like the presence questionnaire, the answer on the visual scale had different signification according to the question (more details in the 2002 Cyberpsychology Lab version [4]); A Flight Difficulty Questionnaire (FDQ). A set of 14 questions, specific to aircraft piloting in our flight scenario, was used to assess and compare the level of difficulty of the different flight segments across conditions. This questionnaire was created in 2019 in our lab and was already used for a previous study [26]. For questions 1 to 4, the answer on the visual scale could be from 1 (not at all) to 7 (completely). For questions 5 to 14, the answer could be from 1 (not similar) to 7 (very similar). See Fig. 8 for details about the 14 questions. 2.6  
   
  Experimental Protocol  
   
  Real Flight. Three people were present on the aircraft: the left seated pilot-participant, the right seated flight instructor (FI) acting as a safety pilot (right seated), and the experimenter (backseater). Before getting on the plane, the participants received a briefing about the experiment and completed their pre-flight inspection. The experiment then placed the ECG electrodes on their torso. During the flight, an LSL’s Viewer application displayed ECG data and flight parameters in real-time, which was necessary for the experimenter to ensure data consistency over time. The first data check was performed between engine start and taxi. The experimenter started the ECG and flight parameters data recordings before the aircraft’s first take-off and stopped after the last landing. The meteorological conditions were compatible with the flights. In most cases, the Ceiling and Visibility were OK (CAVOK), and the wind was calm. For some subjects, the wind was stronger, with 15 knots gusting 25 knots, with a 30 angle from the runway’s axis. Temperatures sometimes rose to 36 ◦ C in the cockpit on the ground. Virtual Flight. The virtual reality flights were conducted in a temperature-controlled room. Three people were inside the room: the participant (on the VRtigo platform), the experimenter (monitoring ECG data and aircraft configuration), and the safety technician (controls the correct functioning of the moving platform, ready to interrupt the  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  119  
   
  Fig. 5. The three flight phases on each of the three traffic patterns, over the R-R data for a complete flight (a 25-min. duration in this example). The green line schematically depicts the aircraft’s altitude. (Color figure online)  
   
  simulation at any time by pressing an emergency stop button). The weather conditions were CAVOK, and no wind was programmed. The recordings were switched on and off at the same time as the real flights. 2.7 Data Analyses Electrocardiogram. The R-R intervals of the raw ECG signal were detected using the built-in QRS detection algorithm of Kubios HRV software [39]. All the recordings were visually inspected to correct for potentially missed or false positive R-peak detection. We then computed the mean values of heart rate (HR; in beats per minute) and heart rate variability (HRV; assessed as Root Mean Square of the Successive Differences of the R-R intervals – RMSSD in ms, and NN50 and pNN50 – number and percentage, respectively, of R-R intervals that differ from each other by more than 50 ms) within the 60-s window of the three phases (take-off, downwind, landing) of the three traffic patterns (Fig. 5). Usually, studies report a decrease in HR and an increase in HRV (i.e. higher variability) as task demand gets lower [10, 37, 41]. The Kubios software also computes other metrics such as LF, HF, LF/HF in the frequency domain, but their computations on short-term signals are not recommended [18], therefore, given the 60-s window length, we only considered HR, RMSSD, NN50, and pNN50 metrics. Statistical Analyses. The statistical analyses were carried out with JASP 0.16.1 software. Two-way (3 flight phases × 2 settings) repeated analyses of variances (ANOVAs) were computed over the HR and HRV metrics. The Greenhouse-Gessier sphericity correction was applied and the Holm correction was used for all post hoc comparisons. The significance level was set at p < .05 for all analyses.  
   
  120  
   
  P. Labedan et al.  
   
  Fig. 6. Cumulative trajectories in virtual reality (blue) and real flights (orange). (Color figure online)  
   
  3 Results 3.1  
   
  Flight Parameters  
   
  The flight parameters have not been fully exploited for the moment. They were used in this study for the extraction of the three flight phases by the analysis of the following parameters: longitude, latitude, altitude, heading, and power. We then only visually checked the trajectories to verify their coherence between real and virtual reality flights (Fig. 6). 3.2  
   
  Subjective Measures  
   
  The Presence Questionnaire. Figure 7 represents the results of the Presence Questionnaire. The min score is 2.14 for question 10 (“How much of a delay did you feel between your actions and their consequences?”). The response could be from 1 (“no delay”) to 7 (“long delay”). The max score is 6.57 for question 9 (“How involved were you in the experience in the virtual environment?”). The response could be from 1 (“no at all”) to 7 (“completely”). The Immersive Tendencies Questionnaire. Participants showed a global mean score of 79.00 (SD = 11.98), and mean scores of 24.67 (SD = 4.37) for the Focus subscale, 21.42 (SD = 5.12) for the Involvement subscale, 15.75 (SD = 4.05) for the Emotion subscale, and 11.67 (SD = 2.94) for the Games subscale.  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  121  
   
  Fig. 7. Results of the presence questionnaire.  
   
  Means and standard deviations for the ITQ global score and subscales are presented in Table 1, along with norms from The Cyberpsychology Lab of UQO [4];  
   
  Table 1. ITQ results score (global and sub-items). Score  
   
  Norms  
   
  Global  
   
  79.00 ± 11.98 64.11 ± 13.11  
   
  Focus  
   
  24.67 ± 4.37  
   
  24.81 ± 7.54  
   
  Involvement 21.42 ± 5.12  
   
  15.33 ± 8.67  
   
  Emotion  
   
  15.75 ± 4.05  
   
  14.25 ± 6.70  
   
  Games  
   
  11.67 ± 2.94  
   
  6.56 ± 4.95  
   
  The Flight Difficulty Questionnaire. Figure 8 shows the results of the Flight Difficulty Questionnaire. The min score is 3.43 for question 14 (“How similar were your experiences during the landing phase in the virtual environment to those in the real environment?”). The response could be from 1 (“not similar”) to 7 (“very similar”). The max score is 6.57 for question 4 (“How well were you able to control the thrust of the aircraft?”). The response could be from 1 (“not at all”) to 7 (“completely”).  
   
  122  
   
  P. Labedan et al.  
   
  Fig. 8. Results of the flight difficulty questionnaire.  
   
  3.3  
   
  Physiological Measures  
   
  Heart Rate. A first two-way repeated ANOVA disclosed a main effect of the flight setting on HR, F (1, 9) = 14.0, p = .005, ηp2 = 0.610, and a main effect of the flight phase on HR, F (2, 18) = 19.1, p < .001, ηp2 = 0.680, as well as a significant flight setting × phase interaction, F (2, 18) = 7.6, p < .01, ηp2 = 0.461, see Fig. 9A. Post-hoc analyses revealed that all the three flight phases in real flight led to higher HR than their counterpart in VR (p = 0.02). In real flight setting, the take-off and the landing led to significantly higher HR than during downwind (p < .001). In VR setting, only the take-off induced significantly higher HR than during the downwind (p = .02). Heart Rate Variability. A second two-way repeated ANOVA disclosed a main effect of the flight setting on RMSSD, F (1, 9) = 17.2, p = .002, ηp2 = 0.657, as well as a main effect of the flight phase on heart rate, F (2, 18) = 13.9, p = .001, ηp2 = 0.608, but no significant flight setting × phase interaction, see Fig. 9B. Posthoc analyses revealed that all the RMSSD was lower during real flight than during VR flight (p < 0.01) and that HRV was lower during the landing than the two other phases (p < 0.03) and also lower during the landing compared to the take-off (p < 0.03). Similar to RMSSD, a two-way repeated ANOVA on NN50 (Fig. 9C) revealed a significant main effect of flight settings, F (1, 9) = 14.0, p = .005, ηp2 = 0.608, as well as a main effect of the flight phase, F (2, 18) = 4.7, p = .042, ηp2 = 0.345. No significant interaction was found. Post-doc analyses showed lower NN50 values for the landing phase compared to the downwind (p = .021).  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  123  
   
  Fig. 9. A) mean HR (in bps); B) mean HRV (RMSSD in ms); C) mean NN50; D) mean pNN50 (%).  
   
  Finally, a two-way repeated ANOVA on pNN50 (Fig. 9D) revealed the same results as for the NN50 measure: a signiifcant main effect of flight settings, F (1, 9) = 14.5, p = .004, ηp2 = 0.617, as well as a main effect of the flight phase, F (2, 18) = 4.5, p = .049, ηp2 = 0.332. No significant interaction was found. Post-doc analyses showed lower NN50 values for the landing phase compared to the downwind (p = .026).  
   
  4 Discussion In this study, we compared pilots’ behavior through subjective questionnaires and cardiac activity during virtual reality and real flight. This study builds upon a previous conference paper [25] with a preliminary report of data from 4 subjects. In the present work, we analyze the data from 10 pilots who performed the same traffic pattern in a  
   
  124  
   
  P. Labedan et al.  
   
  DR400 light aircraft in a VR flight simulator and in actual flight condition. We also computed additional metrics of heart rate variability, i.e. NN50 and pNN50, as compared to only RMSSD in the previous study. Finally, using additional questionnaires allowed us to better understand what parts of virtual flight need to be considered in more detail to increase the realism of the simulation. 4.1  
   
  Subjective Measures  
   
  Sens of Presence. The analysis of the answers to the presence questionnaire revealed several positive aspects thus showing that the VR simulator was immersive. However, some negative points also appeared in these results such as a low definition of the graphics and interactions related issues. Immersion. First of all, the feeling of immersion came out quite positively in the responses. Several factors contributed to this result: the quality of the global graphic environment, the similarity with a known real environment, and the realism of the audio environment of the simulation. The interactions with the aircraft also allowed the pilots to quickly feel in control. The pilots, while performing this flight in an identically reproduced virtual environment, immediately felt at ease by judging themselves capable of controlling the events (question 1 = 5.17 ± 0.98). The score of question 9 shown a strong involvement of the pilots in the virtual environment experience (question 9 = 6.67 ± 0.52), this is even the highest score of the questionnaire. The other two highest scores were for questions 16 and 17 about the audio environment. Pilots were perfectly able to recognize the sounds (engine), which strongly invited them to be involved in the experiment (question 16 = 5.67 ± 1.03; question 17 = 5.67 ± 1.86). They also felt involved because of some visual aspects of the environment (question 8 = 5.33 ± 1.75) and felt quite capable of anticipating the consequences of their actions (question 6 = 5.50 ± 1.38). In addition, the low score of question 10 represents a positive evaluation here, as this question allowed for judging the delay between actions and consequences (question 10 = 2.33 ± 1.03). Thus, the pilots did not feel any delay, which also contributed to their immersion in the experiment. Finally, the score on the sensitivity of the environment following the pilots’ actions (question 2 = 5.17 ± 0.98), and on the speed with which the pilots adapted to the virtual environment (question 1 = 5.17 ± 0.75) are also elements that suggest a good level of immersion. All of these factors (interface quality, realism, and interactions) allowed the pilots to experience a strong sense of immersion, which fostered a high level of self-confidence, and allowed them to feel a strong sense of presence. This immersion did not generate any particular distraction since the maneuvers were perfectly performed by the pilots. Points to Improve. Given the relatively low scores for some questions, several points of the virtual simulation can be improved. Question 3 disclosed that the interactions with the virtual environment are not necessarily well or poorly rated since the score is perfectly neutral on this scale from 1 to 7 (question 3 = 4 ± 1.26). The sensitive point was related to the absence of tracking of the hands in the virtual environment. While it  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  125  
   
  was not an issue to operate the stick and the rudder since our participants could sense them with their hands and feet, it remained challenging to find and interact with the physical throttle and the flap levers. However, the technologies of motion capture and the synthesis of the virtual hands evolve quickly [21] including using own hands in mixed reality [11] for better seamless interactions, and while the current version of the VRtigo simulator did not include the hands’ synthesis, the further studies will include these aspects. Question 7 on the ability to visually explore the environment (question 7 = 4.17 ± 1.33) also highlighted a weakness noticed in a previous study [26]. The pilots also pointed out the difficulty to read accurately some critical cockpit instruments such as the anemometer due to relatively low graphics definition. This made the virtual flight more complicated than the real ones in this respect. However, todays’ virtual reality headsets improved greatly since the beginning of this study, and recently released headsets such as Varjo XR-3 provide a higher resolution that could increase the readability of the instruments and parameter values. Immersive Tendencies. Concerning the overall score of the ITQ, we know that the higher this score is, the more the participants will have a propensity to be immersed in the virtual reality experience. Here, the results show that this score is 79, which greatly exceeds the norms established by the Cyberpsychology laboratory at UQO (+15 points, i.e. +23%). Our group of participants, therefore, had a high propensity for immersion and the experiment could proceed. The results by subscale show us first of all that the Focus subscale, even if slightly below the norm (24.67 vs. 24.81, i.e. –0.5%), corresponds well to it. Then, the Emotion subscale is slightly above the norm (15.75 vs 14.25, i.e. +10%). On the other hand, we note that the scores of the other two subscales are well above the norms (Involvement +39% and Game +77%). This could be explained by the relatively young age of the participants (22.4 ± 4.43 years), a generation more in contact with new technologies. Flight Control in Virtual Reality. Keeping in the mind that in terms of virtual reality hardware on the VRtigo simulator, we only used the virtual reality headset and not the two classical stick controllers associated. We decided to provide the pilot with more intuitive aircraft control elements than virtual reality controllers (a rudder, a control stick, a flap lever, and a throttle). The answers to the questionnaire on the difficulties in accomplishing certain tasks or parts of the flight are therefore to be taken into consideration in relation to these control elements that are more typical of aircraft and not virtual reality controllers. Questions 1 to 4 were related to the ability to control the aircraft (attitude, altitude, bank angle, and thrust), while questions 5 to 14 were related to the evaluation of the similarity of certain parts of the flight in real vs. virtual reality. Control of the Aircraft. Regarding the first part of the questions about the controllability of the aircraft, our participants answered quite positively and felt comfortable with flying the virtual aircraft. Question 4, on the ease of controlling the thrust of the aircraft, done by the throttle lever, obtained the highest score on the questionnaire (score  
   
  126  
   
  P. Labedan et al.  
   
  = 6.5 ± 0.55). The other three scores, for attitude, altitude, and bank angle, were also highly rated with respective values of 5.83 ± 0.98, 5.67 ± 1.38, and 5.5 ± 1.21. These 4 scores are among the 5 highest on the questionnaire. This indicated that the pilots were comfortable with the different aircraft controls such as stick, rudder, and throttle. This observation on the controllability of the aircraft was important in our case of light aircraft piloting in virtual reality. This allowed pilots to feel involved and successful, as evidenced by the score of question 9 on involvement in the presence questionnaire (6.67 ± 0.52). These initial results corroborated from our previous 2018 study [26]. Similarity of the Virtual Experience. Responses to questions about the similarity of different parts of the traffic pattern between the virtual and real flights were more nuanced overall. Question 10, about the descending turn (score = 5.83 ± 0.75), and question 11 about the descent phase (5.50 ± 0.84) are the two best scores in this second part of the questions. The descending part of the traffic pattern is thus highly rated by the participants. However, we can also note that some phases get lower scores. Question 14 in particular, about the landing phase (score = 3 ± 1.55) as well as question 5 about the take-off phase (score = 4.33 ± 2.07). This shows us that the phases close to the ground, take-off, and landing, the most stressful and dangerous phases, in reality, were the least representative, which is also consistent with previous study [26]. One of the reasons could be related to some limitations of the virtual reality simulator already noted in the same study [26], in particular, the low resolution of the virtual reality headset, which negatively impacted the pilots’ ability to correctly read the speed on the anemometer. We also found, during post-flight discussions with the pilots, that the physical sensations during the phases closed to the ground, mainly landing, were not completely realistic, even with the 6-axis mobile platform. The ground effects were not simulated realistically enough, which may have disturbed the pilots during the landing to execute flare phase. Another reason could be a lower experience level when using the VR simulator than the real aircraft. 4.2  
   
  Cardiac Activity  
   
  Heart Rate. As far as the real flights were concerned, the HR analyses showed that the closer the plane was to the ground (take-off and landing), the higher the HR was (Fig. 9A), with a maximum for landing (close to the take-off). The mean HR in real flight was 9.5% higher (take-off) and 10.2% higher (landing) than during the downwind phase. These results were consistent with previous findings reported during a traffic pattern experiment in real flight conditions [25, 37]. Regarding the virtual reality flights, it was interesting to note that the comparison of these three phases was similar to real flights with respectively an increase of 6.1% (take-off) and 4.1% (landing) compared to the downwind phase, with a maximum for takeoff. In both environments (virtual and real), the HR of the downwind phase was, therefore, lower than that of the other two phases. We also noted, both in VR and in real flight, that the evolution of the HR following the three flight phases tended to be similar. The HR analysis also revealed a gap between virtual reality and real flights (Fig. 9A). The  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  127  
   
  HR was higher in real flight by about 22% (take-off), 18% (downwind), and 25% (landing) compared to virtual reality. This result could be interpreted as a lack of feeling of immersion experienced by participants in the VR condition. However, it is important to mention that the real flights were performed with crosswinds, especially for some pilots (16 G 26 kt at 30◦ from the runway axis). The crosswind induced in return higher mental demand (constant correction of trajectories). These aerological differences conditions could thus explain this difference between VR and reality findings. Hear Rate Variability The RMSSD Parameter. The analysis of the mean RMSSD during the real fight condition (Fig. 9B) disclosed higher values in downwind than during the take-off and landing phases, with a minimum for landing. These results for real flights were similar to previous studies [25, 37]. However, the difference of the RMSSD values between take-off and downwind was more pronounced in the 2021 study [25]. The results in virtual reality (Fig. 9B), followed a similar shape than in real flight with a higher average RMSSD in downwind than during the take-off and landing phases, and a minimum for landing too. Note however that the difference between takeoff and downwind was much more marked than in virtual reality (–5 ms vs. –0.5 ms). Again, these results seem to suggest that the real flight condition induced higher mental demand and psychological stress than the simulated condition [10]. The analysis of the RMSSD also revealed a gap between real flights and virtual reality. The RMSSD values were on average 25% (take-off), 36% (downwind), and 35% (landing) lower in reality than in VR. This is also in line with the results of the HR analysis, but with reversed lag. The NN50 and pNN50 Parameters. The same analyses from the RMSSD were obtained for these two parameters describing the HRV (Figs. 9C and 9D). We have also a higher value for the downwind phase, and lower for the landing phase, for both virtual reality and real flights. Values for the three phases are also lower in real flights than in VR. The NN50 values were on average 35% (take-off), 60% (downwind), and 65% (landing) lower in reality than in VR. The pNN50 values were on average 46% (take-off), 62% (downwind), and 72% (landing) lower in reality than in VR. 4.3 Motion Sickness The large majority of pilots did not report motion sickness after using virtual reality during the experiments. Only one pilot was slightly bothered. It must be said that the virtual flight lasted about 25 min, which can seem relatively long in terms of immersion for this kind of activity (important mental load for the management of a flight). This pilot immediately felt better once the experiment was over. This information this time confirms our previous study [25] for which we had too few participants.  
   
  128  
   
  4.4  
   
  P. Labedan et al.  
   
  Conclusion  
   
  Virtual reality technology continues to gain terrain in simulation and training with advanced state-of-the-art headsets with improved display resolution, embedded eye and hand tracking for natural interactions, and improved comfort. Recently, the first virtual flight simulator for rotary pilot training was qualified by European Aviation Safety Agengy. This first step of larger VR technology integration into the pilot training domain requires an in-depth understanding of how virtual flight differs from flying a real aircraft. While seeking the identical render of the flight instruments, as well as a motion platform for providing the pilot the vestibular cues, are paramount, neuroergonomic studies are also required [9, 15]. Such neuroergonomics evaluations allow to go beyond observable behaviors and compare different flight settings using objective neuro- and psycho-physiological measures such as electroencephalography, functional near-red spectroscopy, gaze tracking, or electrocardiography. In this work, we compared standard flight patterns on a DR400 light aircraft in virtual reality (head-mounted device and motion platform) and real flight. We used subjective measures through different questionnaires of immersion, presence, ability to control the aircraft, but, most importantly, objective measures via cardiac activity. To the best of our knowledge, it is the first study comparing real and virtual flight (with a repeated measures analysis) using physiological measures. The results showed that virtual flight is a decent resource for pilot training and pilots were able to fully perform the required actions for controlling the aircraft. Points to improve include updating the hardware (for better display resolution and hand tracking) but also to better model near-ground effects for the motion platform as these vestibular cues are important for the piloting experience. Heart rate and heart rate variability measures pointed out that real and virtual flights induce different stress and workload levels. The same pattern followed different flight phases in both virtual and real flights, i.e. more complicated and stressful operations during take-off and landing compared to the downwind phase yielding higher heart rates and lower HRV values. However, the overall heart rates were, generally, higher during the real experience (+20 bpm on average), and lower HRV values during the real flight compared to the virtual experience. Overall, these results indicated that virtual flight represents real flight and can be used for pilot training. It reproduced the changes in stress/workload throughout the flight but a real flight is required to experience the level of cardiac activity associated with real operations. Further studies with simulators using more advanced hardware can further reduce these differences between the real and virtual flight experiences. However, replacing pilot training with exclusively virtual flight hours remains utopian at this point. Acknowledgements. The authors would like to thank St´ephane Juaneda, the safety pilot, for his availability to perform flights and his precious know-how in-flight experimentation. Special thanks to Fabrice Bazelot and Benoˆıt Momier, LFCL mechanics, for their help during the configuration of the experiments. A special thanks to Boris Jost and Alexandre Iche, ISAE-SUPAERO students, for their involvement in this project. Thanks to Guillaume Garrouste for the 3D development of the LFCL environment, and J´erˆome Dartigues for building the mechanical part of the VRtigo platform.  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  129  
   
  References 1. Ahmed, S., Irshad, L., Demirel, H.O., Tumer, I.Y.: A comparison between virtual reality and digital human modeling for proactive ergonomic design. In: Duffy, V.G. (ed.) HCII 2019. LNCS, vol. 11581, pp. 3–21. Springer, Cham (2019). https://doi.org/10.1007/978-3-03022216-1 1 2. Auer, S., Gerken, J., Reiterer, H., Jetter, H.C.: Comparison between virtual reality and physical flight simulators for cockpit familiarization. In: Mensch und Computer 2021, pp. 378–392 (2021) 3. Ayoub, A., Pulijala, Y.: The application of virtual reality and augmented reality in oral & maxillofacial surgery. BMC Oral Health 19(1), 1–8 (2019) 4. Bouchard, S., Robillard, G., Renaud, P.: Questionnaire sur la propension a` l’immersion. Lab Cyberpsychologie L’UQO (2002) 5. Bric, J.D., Lumbard, D.C., Frelich, M.J., Gould, J.C.: Current state of virtual reality simulation in robotic surgery training: a review. Surg. Endosc. 30(6), 2169–2178 (2016) 6. Chae, J.: Study on firefighting education and training applying virtual reality. Fire Sci. Eng. 32(1), 108–115 (2018) 7. Clifford, R.M., Jung, S., Hoermann, S., Billinghurst, M., Lindeman, R.W.: Creating a stressful decision making environment for aerial firefighter training in virtual reality. In: 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 181–189. IEEE (2019) 8. Dehais, F., et al.: Monitoring pilot’s mental workload using erps and spectral power with a six-dry-electrode eeg system in real flight conditions. Sensors 19(6), 1324 (2019) 9. Dehais, F., Karwowski, W., Ayaz, H.: Brain at work and in everyday life as the next frontier: grand field challenges for neuroergonomics. Front. Neuroergon. 1 (2020) 10. Durantin, G., Gagnon, J.F., Tremblay, S., Dehais, F.: Using near infrared spectroscopy and heart rate variability to detect mental overload. Behav. Brain Res. 259, 16–23 (2014) 11. Feng, Q., Shum, H.P., Morishima, S.: Resolving occlusion for 3d object manipulation with hands in mixed reality. In: Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology, pp. 1–2 (2018) 12. Feng, Z., Gonz´alez, V.A., Amor, R., Lovreglio, R., Cabrera-Guerrero, G.: Immersive virtual reality serious games for evacuation training and research: a systematic literature review. Comput. Educ. 127, 252–266 (2018) 13. Fussell, S.G., Truong, D.: Preliminary results of a study investigating aviation student’s intentions to use virtual reality for flight training. Int. J. Aviat. Aeron. Aeros. 7(3), 2 (2020) 14. Gateau, T., Ayaz, H., Dehais, F.: In silico vs. over the clouds: on-the-fly mental state estimation of aircraft pilots, using a functional near infrared spectroscopy based passive-bci. Front. Human Neurosci. 12, 187 (2018) 15. Gramann, K., et al.: Grand field challenges for cognitive neuroergonomics in the coming decade. Front. Neuroergon. 2 (2021) 16. Grantcharov, T.P., Kristiansen, V.B., Bendix, J., Bardram, L., Rosenberg, J., Funch-Jensen, P.: Randomized clinical trial of virtual reality simulation for laparoscopic skills training. Brit. J. Surg. 91(2), 146–150 (2004). https://doi.org/10.1002/bjs.4407 17. Hays, R.T., Jacobs, J.W., Prince, C., Salas, E.: Flight simulator training effectiveness: a metaanalysis. Milit. Psychol. 4(2), 63–74 (1992) 18. Heathers, J.A.: Everything hertz: methodological issues in short-term frequency-domain hrv. Front. Physiol. 5, 177 (2014) 19. Jensen, L., Konradsen, F.: A review of the use of virtual reality head-mounted displays in education and training. Educ. Inf. Technol. 23(4), 1515–1529 (2018) 20. Joda, T., Gallucci, G., Wismeijer, D., Zitzmann, N.: Augmented and virtual reality in dental medicine: a systematic review. Comput. Biol. Med. 108, 93–100 (2019)  
   
  130  
   
  P. Labedan et al.  
   
  21. J¨org, S., Ye, Y., Mueller, F., Neff, M., Zordan, V.: Virtual hands in vr: motion capture, synthesis, and perception. In: SIGGRAPH Asia 2020 Courses, pp. 1–32 (2020) 22. Kim, Y., Kim, H., Kim, Y.O.: Virtual reality and augmented reality in plastic surgery: a review. Arch. Plast. Surg. 44(3), 179 (2017) 23. Kothe, C., Medine, D., Boulay, C., Grivich, M., Stenner, T.: Lab streaming layer (2014). https://github.com/sccn/labstreaminglayer 24. Koutitas, G., Smith, S., Lawrence, G.: Performance evaluation of ar/vr training technologies for ems first responders. Virt. Reality 25(1), 83–94 (2021) 25. Labedan, P., Darodes-De-Tailly, N., Dehais, F., Peysakhovich, V.: Virtual reality for pilot training: study of cardiac activity. In: VISIGRAPP (2: HUCAPP), pp. 81–88 (2021) 26. Labedan, P., Dehais, F., Peysakhovich, V.: Evaluation de l’exp´erience de pilotage d’un avion l´eger en r´ealit´e virtuelle. In: ERGO’IA (2018) 27. Lawrynczyk, A.: Exploring Virtual Reality Flight Training as a Viable Alternative to Traditional Simulator Flight Training. Ph.D. thesis, Carleton University (2018). https://doi.org/10. 22215/etd/2018-13301 28. Li, L., et al.: Application of virtual reality technology in clinical medicine. Am. J. Transl. Res. 9(9), 3867 (2017) 29. Markopoulos, E., et al.: Neural network driven eye tracking metrics and data visualization in metaverse and virtual reality maritime safety training (2021) 30. Meshkati, N.: Heart rate variability and mental workload assessment. In: Hancock, P.A., Meshkati, N. (eds.) Human Mental Workload, Advances in Psychology, vol. 52, pp. 101– 115. North-Holland (1988). https://doi.org/10.1016/S0166-4115(08)62384-5, http://www. sciencedirect.com/science/article/pii/S0166411508623845 31. Oberhauser, M., Dreyer, D., Braunstingl, R., Koglbauer, I.: What’s real about virtual reality flight simulation? Aviation Psychology and Applied Human Factors (2018) 32. Peysakhovich, V., Monnier, L., Gornet, M., Juaneda, S.: Virtual reality versus real-life training to learn checklists for light aircraft. In: 1st International Workshop on Eye-Tracking in Aviation (2020) 33. Pottle, J.: Virtual reality and the transformation of medical education. Fut. Healthcare J. 6(3), 181 (2019) 34. Pourmand, A., Davis, S., Lee, D., Barber, S., Sikka, N.: Emerging utility of virtual reality as a multidisciplinary tool in clinical medicine. Games Health J. 6(5), 263–270 (2017) 35. Galasko, C.S.: Competencies required to be a competent surgeon. Ann. Roy. Coll. Surg. Engl. 82, 89–90 (2000) 36. Sakib, M.N., Chaspari, T., Behzadan, A.H.: Physiological data models to understand the effectiveness of drone operation training in immersive virtual reality. J. Comput. Civil Eng. 35(1), 04020053 (2021) 37. Scannella, S., Peysakhovich, V., Ehrig, F., Lepron, E., Dehais, F.: Assessment of ocular and physiological metrics to discriminate flight phases in real light aircraft. Hum. Fact.: J. Hum. Fact. Ergon. Soc. 60(7), 922–935 (2018). https://doi.org/10.1177/0018720818787135 38. Silverstein, J.C., Dech, F., Edison, M., Jurek, P., Helton, W., Espat, N.: Virtual reality: immersive hepatic surgery educational environment. Surgery 132(2), 274–277 (2002). https://doi.org/10.1067/msy.2002.125723, http://www.sciencedirect.com/science/article/pii/ S0039606002000843 39. Tarvainen, M.P., Niskanen, J.P., Lipponen, J.A., Ranta-Aho, P.O., Karjalainen, P.A.: Kubios hrv-heart rate variability analysis software. Comput. Methods Progr. Biomed. 113(1), 210– 220 (2014) 40. Thomsen, A.S.S., et al.: Operating room performance improves after proficiency-based virtual reality cataract surgery training. Ophthalmology 124(4), 524–531 (2017) 41. Togo, F., Takahashi, M.: Heart rate variability in occupational health a systematic review. Ind. Health 47(6), 589–602 (2009). https://doi.org/10.2486/indhealth.47.589  
   
  Comparison of Cardiac Activity and Subjective Measures  
   
  131  
   
  42. Walters, W.T., Walton, J.: Efficacy of virtual reality training for pilots: a review of links between user presence, search task performance, and collaboration within virtual reality. In: Proceedings of the Human Factors and Ergonomics Society Annual Meeting, vol. 65, pp. 919–922. SAGE Publications, Los Angeles (2021) 43. Witmer, B.G., Singer, M.J.: Measuring presence in virtual environments: a presence questionnaire. Presence 7(3), 225–240 (1998) 44. Xie, B., et al.: A review on virtual reality skill training applications. Front. Virt. Reality 2, 49 (2021) 45. Zhang, H.: Head-mounted display-based intuitive virtual reality training system for the mining industry. Int. J. Min. Sci. Technol. 27(4), 717–722 (2017)  
   
  Information Visualization Theory and Applications  
   
  Improving Self-supervised Dimensionality Reduction: Exploring Hyperparameters and Pseudo-Labeling Strategies Artur Andr´e A. M. Oliveira1 , Mateus Espadoto1(B) , Roberto Hirata Jr.1 , Nina S. T. Hirata1 , and Alexandru C. Telea2 1  
   
  Institute of Mathematics and Statistics, University of S˜ao Paulo, S˜ao Paulo, Brazil {arturao,mespadot,hirata,nina}@ime.usp.br 2 Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands [email protected]   
   
  Abstract. Dimensionality reduction (DR) is an essential tool for the visualization of high-dimensional data. The recently proposed Self-Supervised Network Projection (SSNP) method addresses DR with a number of attractive features, such as high computational scalability, genericity, stability and out-of-sample support, computation of an inverse mapping, and the ability of data clustering. Yet, SSNP has an involved computational pipeline using self-supervision based on labels produced by clustering methods and two separate deep learning networks with multiple hyperparameters. In this paper we explore the SSNP method in detail by studying its hyperparameter space and pseudo-labeling strategies. We show how these affect SSNP’s quality and how to set them to optimal values based on extensive evaluations involving multiple datasets, DR methods, and clustering algorithms. Keywords: Dimensionality reduction · Machine learning · Deep learning · Neural networks · Autoencoders  
   
  1 Introduction Visualization of high-dimensional data to find patterns, trends, and overall understand the data structure has become an essential ingredient of the data scientist’s toolkit [24, 29]. Within the palette of such visualization methods, dimensionality reduction (DR) techniques, also called projections, have gained an established position due to their high scalability both in the number of samples and number of dimensions thereof. In the last decades, tens of DR techniques have emerged [12, 38], with PCA [22], t-SNE [33], and UMAP [36] having become particularly popular. Neural-network-based techniques have been used to support DR, early examples of such approaches being self-organizing maps [26] and autoencoders [19]. More recently, the NNP technique [10] was proposed to mimic any DR technique. In parallel, the ReNDA method [3] was proposed to improve the projection quality offered by autoencoders. c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 135–161, 2023. https://doi.org/10.1007/978-3-031-25477-2_7  
   
  136  
   
  A. A. A. M. Oliveira et al.  
   
  Deep learning based DR methods are very fast, simple to implement, generically work for any type of quantitative high-dimensional data, are parametric, thus stable to small-scale data variations and offering out-of-sample capability, and – in the case of autoencoders – also provide the inverse mapping from the low-dimensional projection space to the high-dimensional data space. However, such methods also have some limitations. Such methods cannot typically offer the same projection quality, measured e.g. in terms of neighborhood preservation or cluster delineation, as classical methods like t-SNE and UMAP [9, 10, 37]. Inverse projection typically requires training a separate network [13]. NNP-class methods offer a higher quality than autoencoders, but require supervision in terms of using a classical DR method to project a subset of the data [10]. Recently, the Self-Supervised Neural Projection (SSNP [11]) method was proposed to alleviate the above limitations of deep learned projections. SSNP uses a single neural network trained with two objectives – reconstructing the projected data (as an autoencoder does) and classifying the same data (based on pseudo-labels created by a clustering algorithm). In more detail, SSNP aims to provide the following characteristics: Quality (C1): Better cluster separation than standard autoencoders, and close to stateof-the-art DR methods, measured by well-known metrics in DR literature; Scalability (C2): Linear complexity in the number of samples and dimensions, allowing the projection of datasets of a million samples and hundreds of dimensions in a few seconds on consumer-grade GPU platforms; Ease of Use (C3): Minimal or no hyperparameter tuning required; Genericity (C4): Projects any dataset whose samples are real-valued vectors; Stability and Out-of-Sample Support (C5): The trained SSNP model can project new samples along existing ones in a parametric fashion; Inverse Mapping (C6): Ability to infer the high-dimensional point corresponding to a low-dimensional point in the projection space; Clustering (C7): Ability to label (cluster) unseen data. This feature of SSNP also supports requirement C1: Intuitively, clustering aggregates low-level distance information between sample points to a higher level, telling how groups of samples relate to each other. Next, this information is used by SSNP to produce projections which preserve such data clusters well in the low dimensional space. In our original paper [11], we show how SSNP achieves the above requirements by evaluating it on four synthetic and four real-world datasets, using two clustering algorithms to produce pseudo-labels, and compare its results with four existing DR techniques. However, this leaves the ‘design space’ of SSNP insufficiently explored. Similarly to [9], where the authors explored in detail the design space of NNP [10], in this paper we aim to provide more insights on how SSNP’s results depend on its technical components and their hyperparameter settings. For this, we extend the evaluation in [11] by considering two additional projection techniques (MDS and Isomap) and four additional clustering algorithms (affinity propagation, DBSCAN, Gaussian mixture models, and spectral clustering). Separately, we study how SSNP’s performance is influenced by the setting of the hyperparameters of both the clustering algorithms and the underlying neural network. All in all, our extended evaluation proves that SSNP  
   
  Improving Self-supervised Dimensionality Reduction  
   
  137  
   
  indeed complies well with requirements C1-C7, being a serious contender in the class of deep-learning-based DR techniques. We structure this chapter as follows: Sect. 2 introduces notations and discusses related work. Section 3 details the SSNP method. Section 4 describes our experimental setup. Section 5 presents the results of SSNP, including the additional experiments outlined above. Section 6 discusses the obtained findings. Section 7 concludes the paper.  
   
  2 Background Notations: Let x = (x1 , . . . , xn ), xi ∈ R, 1 ≤ i ≤ n be a n-dimensional (nD) sample (also called a data point or observation). Let D = {xi }, 1 ≤ i ≤ N be a dataset of N such samples, e.g., a table with N rows (samples) and n columns (dimensions). All datasets D used in this paper have class labels. Let C be the number of classes (or labels) in a dataset D. A DR, or projection, technique is a function P : Rn → Rq ,  
   
  (1)  
   
  where q  n, and typically q = 2. The projection p = P(x) of a sample x ∈ D is a point p ∈ Rq . Projecting an entire dataset D yields a q-dimensional scatterplot, denoted next as P(D). The inverse of P, denoted x = P−1 (p), maps a q-dimensional point p to the high-dimensional space Rn , so that, ideally, P(x) = p, or in practice, P(x) is close to p. Dimensionality Reduction: Many DR methods have been proposed in the last decades [5, 8, 12, 20, 29, 34, 38, 48, 56]. We next outline how a few representative ones comply with the requirements mentioned in Sect. 1, supporting our point that no DR method fully covers all those requirements. For further evidence for this statement, we refer to the above mentioned surveys. Principal Component Analysis [22] (PCA) is very popular due to its simplicity, speed (C2), stability and out-of-sample (OOS) support (C5), and ease of use (C3) and interpretation. PCA is also used as pre-processing step for other DR techniques that require not-too-high-dimensional data [38]. Yet, due to its linear and global nature, PCA lacks on quality (C1), especially for data of high intrinsic dimensionality. Methods of the Manifold Learning family (MDS [51], Isomap [49], and LLE [45] and its variations [7, 57, 58]) aim to map to 2D the high-dimensional manifold on which data lives. Such methods generally yield higher quality (C1) than PCA. Yet, such methods can be hard to tune (C3), do not have OOS capability (C5), do not work well for data that is not restricted to a 2D manifold, and generally scale poorly (C2) with dataset size. Force-directed methods (LAMP [21] and LSP [40]) can yield reasonably high visual quality (C1), good scalability (C2), and are simple to use (C3). However, they generally cannot do OOS (C5). For LAMP, a related inverse projection (C6) technique iLAMP [1] exists. Yet, LAMP and iLAMP are two different algorithms. Clustering-based methods, such as PBC [39], share many characteristics of force-directed methods, such as good quality (C1) and lack of OOS (C5). SNE (Stochastic Neighborhood Embedding) methods, of which t-SNE [33] is the most popular, have the key ability to visually segregate similar samples, thus being very  
   
  138  
   
  A. A. A. M. Oliveira et al.  
   
  good for cluster analysis. While having high visual quality (C1), t-SNE has a high complexity of O(N 2 ) in sample count (C2), is very sensitive to small data changes (C5), is hard to tune (C3) [54], and has no OOS capability (C5). Tree-accelerated t-SNE [32], hierarchical SNE [42], approximated t-SNE [43], and various GPU accelerations of tSNE [4, 44] improve computation time (C2). Yet, these methods require quite complex algorithms, and still largely suffer from the aforementioned sensitivity, tuning, and OOS issues. Uniform Manifold Approximation and Projection (UMAP) [36] generates projections with comparable quality to t-SNE (C1) but is faster (C2) and has OOS (C5). Yet, UMAP shares some disadvantages with t-SNE, namely the sensitivity to small data changes (C5) and parameter tuning difficulty (C3). Deep Learning: Autoencoders (AE) [19, 25] create a low-dimensional data representation in their bottleneck layers by training a neural network to reproduce its highdimensional inputs on its outputs. They produce results of comparable quality (C1) to PCA. However, they are easy to set up, train, and use (C3), are easily parallelizable (C2), and have OOS (C5) and inverse mapping (C6) abilities. ReNDA [3] is a deep learning approach that uses two neural networks, improving on earlier work from the same authors. One network implements a nonlinear generalization of Fisher’s Linear Discriminant Analysis [15]; the other network is an autoencoder used as a regularizer. ReNDA scores well on quality (C1) and has OOS (C5). However, it requires pre-training of each individual network and has low scalability (C2). Neural Network Projections (NNP) [10] select a training subset Ds ⊂ D to project by any user-chosen DR method to create a so-called training projection P(Ds ) ⊂ R2 . Next, a neural network is trained to approximate P(Ds ) having Ds as input. The trained network then projects unseen data by means of 2-dimensional non-linear regression. NNP is very fast (C2), simple to use (C3), and stable and with OOS ability (C5). However, the projection quality (C1) is lower than the learned projection. The NNInv technique [13], proposed by the same authors as NNP, adds inverse projection ability (C6). However, this requires setting up, training, and using a separate network. Table 1 summarizes how the above DR techniques fare with respect to each characteristic of interest. The last row highlights SSNP which we describe separately in Sect. 3. Table 1. Summary of DR techniques and their characteristics. Names in italic are techniques we compare with SSNP. Technique  
   
  Characteristic Quality Scalability Ease of use Genericity Out-of-sample Inverse mapping Clustering  
   
  PCA  
   
  Low  
   
  High  
   
  High  
   
  High  
   
  Yes  
   
  Yes  
   
  No  
   
  MDS  
   
  Mid  
   
  Low  
   
  Low  
   
  Low  
   
  No  
   
  No  
   
  No  
   
  Isomap  
   
  Mid  
   
  Low  
   
  Low  
   
  Low  
   
  No  
   
  No  
   
  No  
   
  LLE  
   
  Mid  
   
  Low  
   
  Low  
   
  Low  
   
  No  
   
  No  
   
  No  
   
  LAMP  
   
  Mid  
   
  Mid  
   
  Mid  
   
  High  
   
  No  
   
  No  
   
  No  
   
  LSP  
   
  Mid  
   
  Mid  
   
  Mid  
   
  High  
   
  No  
   
  No  
   
  No  
   
  t-SNE  
   
  High  
   
  Low  
   
  Low  
   
  High  
   
  No  
   
  No  
   
  No  
   
  UMAP  
   
  High  
   
  High  
   
  Low  
   
  High  
   
  Yes  
   
  No  
   
  No  
   
  Autoencoder Low  
   
  High  
   
  High  
   
  Low  
   
  Yes  
   
  Yes  
   
  No  
   
  ReNDA  
   
  Mid  
   
  Low  
   
  Low  
   
  Mid  
   
  Yes  
   
  No  
   
  No  
   
  NNP  
   
  High  
   
  High  
   
  High  
   
  High  
   
  Yes  
   
  No  
   
  No  
   
  SSNP  
   
  High  
   
  High  
   
  High  
   
  High  
   
  Yes  
   
  Yes  
   
  Yes  
   
  Improving Self-supervised Dimensionality Reduction  
   
  139  
   
  Clustering: As for DR, clustering is a field that goes back decades, with many techniques proposed over the years. Despite using different approaches, all techniques use some form of similarity measure to determine whether a sample belongs to a cluster or not. Centroid-based techniques, such as K-means [30], compute cluster centers and assign cluster membership based on closeness to a center. Connectivity-based techniques, such as Agglomerative clustering [23], group samples based their relative distances rather than distances to cluster centers. Distribution-based techniques, such as Gaussian Mixture Models [6], fit Gaussian distributions to the dataset and then assign samples to each distribution. Density-based techniques, such as DBSCAN [14], define clusters as dense areas in the data space. More recent techniques use more specialized approaches, such as Affinity Propagation [16], which uses message passing between samples, and Spectral Clustering [47], which uses the eigenvalues of the data similarity matrix to reduce the dimensionality of the data to be clustered.  
   
  3 SSNP Technique As stated in Sect. 2, autoencoders have desirable DR properties (simplicity, speed, OOS, and inverse mapping abilities), but create projections of lower quality than, e.g., t-SNE and UMAP. A likely cause for this is that autoencoders do not use neighborhood information during training, while t-SNE and UMAP (obviously) do that. Hence, we propose to create an autoencoder architecture with a dual optimization target that explicitly uses neighborhood information. First, we have a reconstruction target, as in standard autoencoders; next, we use a classification target based on labels associated with the samples. These can be “true” ground-truth labels if available for a given dataset. If not, these are pseudo-labels created by running a clustering algorithm on the input dataset. The key idea behind this is that (pseudo)labels are a compact and high-level way to encode neighborhood information, i.e., same-label data are more similar than different-label data. Since classifiers learn a representation that separates input data based on labels, adding an extra classifier target to an autoencoder learns how to project data with better cluster separation than standard autoencoders. We call our technique Self-Supervised Neural Projection (SSNP). SSNP first takes a training set Dtr ⊂ D and assigns to it pseudo-labels Ytr ∈ N by using some clustering technique. We then take samples (x ∈ Dtr , y ∈ Ytr ) to train a neural network with a reconstruction and a classification function, added to form a joint loss. This network (Fig. 1a) contains a two-unit bottleneck layer, same as an autoencoder, used to generate the 2D projection when in inference mode. After training, we ‘split’ the layers of the network to create three new networks for inference (Fig. 1b): a projector NP (x), an inverse projector NI (p), and a classifier NC (x), which mimics the clustering algorithm used to create Ytr . The entire training-and-inference way of working of SSNP is summarized in Fig. 2.  
   
  4 Experimental Setup In this section we detail the experimental setup we used to evaluate SSNP’s performance. The obtained results are discussed next in Sect. 5.  
   
  140  
   
  A. A. A. M. Oliveira et al.  
   
  Fig. 1. SSNP network architectures used during training (a) and inference (b). training set Dtr labels Ytr clustering algorithm training set Dtr  
   
  trained network N  
   
  labels Ytr  
   
  projector NP  
   
  trained network N  
   
  inverse projector NI classifier NC  
   
  dataset D scatterplot P(D) dataset D  
   
  NP NI NC  
   
  scatterplot P(D) dataset D labels Y  
   
  Fig. 2. SSNP training-and-inference pipeline.  
   
  4.1  
   
  Datasets  
   
  We first evaluate SSNP on synthetic datasets consisting of blobs sampled from a Gaussian distribution of different dimensionalities (100 and 700), number of clusters (5 and 10), and standard deviation σ, yielding datasets with cluster separation varying from very sharp to fuzzy clusters. All synthetic datasets have 5K samples. Next, we evaluate SSNP on four public real-world datasets that are high-dimensional, reasonably large  
   
  Improving Self-supervised Dimensionality Reduction  
   
  141  
   
  (thousands of samples), and have a non-trivial data structure (same datasets as used in the original SSNP paper [11]): MNIST [28]: 70K samples of handwritten digits from 0 to 9, rendered as 28x28-pixel gray scale images, flattened to 784-element vectors; Fashion MNIST [55]: 70K samples of 10 types of pieces of clothing, rendered as 28x28-pixel gray scale images, flattened to 784-element vectors; Human Activity Recognition (HAR) [2]: 10299 samples from 30 subjects performing activities of daily living used for human activity recognition grouped in 6 classes and described with 561 dimensions. Reuters Newswire Dataset [50]: 8432 samples of news report documents, from which 5000 attributes are extracted using TF-IDF [46], a standard method in text processing. All datasets had their attributes rescaled to the range [0, 1], to conform with the sigmoid activation function used by the reconstruction layer (see Fig. 1a). 4.2 Projection Quality Metrics We measure projection quality by four metrics widely used in the projection literature (see Table 2 for their definitions). All metrics range in [0, 1] with 0 indicating poorest, and 1 indicating best, values: Table 2. Projection quality metrics used in evaluating SSNP. Metric  
   
  Definition  
   
  Trustworthiness (T )  
   
  2 1 − NK(2n−3K−1) ∑N i=1 ∑ j∈U (K) (r(i, j) − K)  
   
  Continuity (C)  
   
  2 1 − NK(2n−3K−1) r(i, j) − K) ∑N i=1 ∑ j∈V (K) (ˆ  
   
  i  
   
  Neighborhood hit (NH)  
   
  1 N  
   
  yl  
   
  i  
   
  ∑y∈P(D) ykk  
   
  Shepard diagram correlation (R) Spearman’s ρ of (xi − x j , P(xi ) − P(x j )), 1 ≤ i ≤ N, i = j  
   
  Trustworthiness T [53]: is the fraction of close points in D that are also close in P(D). T tells how much one can trust that local patterns in a projection, e.g. clusters, represent (K) actual data patterns. In the definition (Table 2), Ui is the set of points that are among the K nearest neighbors of point i in the 2D space but not among the K nearest neighbors of point i in Rn ; and r(i, j) is the rank of the 2D point j in the ordered-set of nearest neighbors of i in 2D. We choose K = 7 following [34, 35]; Continuity C [53]: is the fraction of close points in P(D) that are also close in D. In the (K) definition (Table 2), Vi is the set of points that are among the K nearest neighbors of n point i in R but not among the K nearest neighbors in 2D; and rˆ(i, j) is the rank of the Rn point j in the ordered set of nearest neighbors of i in Rn . As for T , we use K = 7; Neighborhood Hit NH [40]: measures how well-separable labeled data is in a projection P(D), in a rotation-invariant fashion, from perfect separation (NH = 1) to no separation (NH = 0). NH is the number ylK of the K nearest neighbors of a point y ∈ P(D),  
   
  142  
   
  A. A. A. M. Oliveira et al.  
   
  denoted by yK , that have the same label as y, averaged over P(D). In this paper, we use K = 3; Shepard Diagram Correlation R [21]: The Shepard diagram is a scatter plot of the pairwise distances between all points in P(D) vs the corresponding distances in D. The closer the plot is to the main diagonal, the better overall distance preservation is. Plot areas below, respectively above, the diagonal show distance ranges for which false neighbors, respectively missing neighbors, occur. We measure how close a Shepard diagram is to the diagonal by computing its Spearman rank correlation R. A value of R = 1 indicates a perfect (positive) correlation of distances. 4.3  
   
  Dimensionality Reduction Techniques Compared Against  
   
  We compared SSNP against six DR techniques, namely t-SNE, UMAP, MDS, Isomap, autoencoders (AE), and NNP (see also Table 1). We selected these techniques based on popularity (t-SNE, UMAP, MDS, Isomap) or on similar operation (AE and NNP are also deep learning based, like SSNP) and also on having desirable properties to compare against. For instance, t-SNE and UMAP are known to produce strong visual cluster separation by evaluating local neighborhoods. MDS, on the other hand, tries to preserve global distances between samples. Isomap can be seen as an extension of MDS that uses local neighborhood information to infer geodesic distances. AE produce results similar to PCA, which preserves global distances. Finally, NNP does not have specific built-in heuristics but rather aims to mimic and accelerate other DR techniques. For all these DR techniques, we used default values for their hyperparameters. 4.4  
   
  Clustering Techniques for Pseudo-Labeling  
   
  In addition to using ground-truth labels in SSNP, we also used six clustering algorithms to generate the pseudo-labels for using during SSNP training (Sect. 3). Table 3 lists all clustering algorithms used, as well as the hyperparameters used in all experiments, except when noted otherwise. Hyperparameters not listed in Table 3 used default values. We used these algorithms since they employ quite different approaches to clustering, which could produce different results for SSNP. We selected two of these clustering algorithms alongside two datasets — K-means and DBSCAN, HAR and MNIST — to further explore the effect of their main hyperparameters on the quality of the SSNP projection. For K-means, we studied the n clusters parameter by choosing values well below and above the known number of clusters C in the data — n clusters = {5, 10, 15, 20, 30} for MNIST (C = 10), n clusters = {3, 6, 9, 12, 18} for HAR (C = 6). For DBSCAN, we explored the eps parameter, which determines the maximum distance between samples for them to be considered as neighbors. We used eps = {6.1, 6.3, ..., 6.9} for MNIST and eps = {1.9, 2.1, ..., 2.7} for HAR.  
   
  Improving Self-supervised Dimensionality Reduction  
   
  143  
   
  Table 3. Clustering algorithms used as for pseudo-label creation and their hyperparameters used during testing. Ground-truth is listed here as another labeling strategy. Algorithm  
   
  Acronym  
   
  Hyperparameters  
   
  Ground truth labels  
   
  SSNP(GT)  
   
  None  
   
  Affinity propagation  
   
  SSNP(AP)  
   
  None  
   
  Agglomerative clustering SSNP(Agg)  
   
  n clusters = 2 ×C  
   
  DBSCAN  
   
  SSNP(DB)  
   
  eps = 5  
   
  Gaussian mixture model  
   
  SSNP(GMM) n components = 2 ×C  
   
  K-means  
   
  SSNP(Km)  
   
  n clusters = 2 ×C  
   
  Spectral clustering  
   
  SSNP(SC)  
   
  n clusters = 2 ×C  
   
  4.5 Neural Network Hyperparameter Settings We further evaluated SSNP by using several hyperparameter settings for its neural network. To avoid a huge hyperparameter space, for each parameter explored, we kept the other parameters set to their defaults, similarly to the strategy used to explore NNP [9]. The explored hyperparameters are described next (see also Table 4). L2 Regularization [27]: decreases layer weights to small but non-null values, leading to every weight only slightly contributing to the model. It works by adding a penalization term λw2 to the cost function, where w are the weights of a selected network layer. The parameter λ ∈ [0, 1] controls the amount of regularization; Embedding Layer Activation: The embedding (bottleneck) layer creates the 2D projection after training (Fig. 1). Changing the activation function of this layer affects the projection’s overall shape. We used four activation functions for this layer (see Table 4); Weight Initialization: A neural network has thousands of parameters whose initialization can affect the training outcome. We used three common initialization types: random uniformly distributed in the range [−0.05, 0.05], Glorot uniform [17] with the range  [−b, b] for b = 6/(lin + lout ), where lin and lout are the number of input and output units in the layer, and He uniform [18], which uses the range [−b, b] with b = 6/lin ; Training Epochs: We explored SSNP’s performance for different numbers of epochs η ranging from 1 to 20. Table 4. SSNP neural network parameters explored with default values in bold. Dimension  
   
  Values  
   
  L2 regularization  
   
  λ = {0, 0.1, 0.5, 1.0}  
   
  Embedding layer activation α = { ReLU, sigmoid, tanh, Leaky RELU } Weight initialization  
   
  φ = {Glorot uniform, He uniform, Random uniform }  
   
  Training epochs  
   
  η = {1, 2, 3, 5, 10, 20}  
   
  144  
   
  A. A. A. M. Oliveira et al.  
   
  5 Results We next present the results for all experiments conducted to demonstrate SSNP’s quality and robustness to hyperparameter selection. 5.1  
   
  Quality on Synthetic Datasets  
   
  Figure 3 shows the SSNP projection of the synthetic blob datasets with SSNP(Km) with K-means set to use the correct (ground-truth) number of clusters alongside AE, tSNE, and UMAP. In most cases SSNP(Km) shows better visual cluster separation than autoencoders. The t-SNE and UMAP projections look almost the same regardless of the standard deviation σ of the blobs, while SSNP(Km) shows more spread clusters for larger σ, which is the desired effect. We omit the plots and measurements for NNP for space reasons and since these are very close to the ones created by the learned technique [10]. Table 5 shows the quality metrics for this experiment for datasets using 5 and 10 clusters. For all configurations, SSNP performs very similarly quality-wise to AE, tSNE, and UMAP. Section 5.2, which studies more challenging, real-world, datasets will bring more insight in this comparison. Table 5. Quality metrics, synthetic blobs experiment with 100 and 700 dimensions, 5 and 10 clusters, and σ ∈ [1.3, 11.2]. Projection  
   
  σ  
   
  100 dimensions 5 clusters T C R  
   
  700 dimensions 5 clusters T C R  
   
  NH  
   
  10 clusters T C  
   
  R  
   
  NH  
   
  1.000 1.6 1.000 1.000 1.000  
   
  0.909 0.917 0.906 0.904  
   
  0.914 0.951 0.933 0.908  
   
  0.739 0.362 0.878 0.568  
   
  1.000 1.000 1.000 1.000  
   
  0.953 0.960 0.954 0.953  
   
  0.955 0.976 0.965 0.955  
   
  0.254 0.346 0.471 0.399  
   
  1.000 1.000 1.000 1.000  
   
  0.484 0.227 0.537 0.549  
   
  1.000 4.8 1.000 1.000 1.000  
   
  0.910 0.914 0.906 0.905  
   
  0.914 0.950 0.931 0.907  
   
  0.615 0.608 0.697 0.612  
   
  1.000 1.000 1.000 1.000  
   
  0.953 0.960 0.954 0.953  
   
  0.954 0.977 0.965 0.954  
   
  0.354 0.331 0.390 0.296  
   
  1.000 1.000 1.000 1.000  
   
  0.328 0.254 0.342 0.437  
   
  0.999 11.2 0.911 0.906 0.600 1.000 0.914 0.950 0.492 1.000 0.905 0.931 0.557 0.995 0.904 0.906 0.557  
   
  1.000 1.000 1.000 1.000  
   
  0.955 0.959 0.953 0.950  
   
  0.954 0.977 0.965 0.945  
   
  0.382 0.296 0.336 0.314  
   
  1.000 1.000 1.000 0.998  
   
  NH  
   
  10 clusters T C  
   
  R  
   
  NH  
   
  AE 1.3 0.923 0.938 0.547 0.937 0.955 0.818 T-SNE 0.921 0.949 0.868 UMAP 0.910 0.919 0.687 SSNP(Km)  
   
  1.000 1.000 1.000 1.000  
   
  0.958 0.967 0.957 0.956  
   
  0.963 0.977 0.970 0.959  
   
  0.692 0.192 0.721 0.602  
   
  AE 3.9 0.919 0.926 0.750 0.931 0.953 0.707 t-SNE 0.911 0.940 0.741 UMAP 0.910 0.918 0.622 SSNP(Km)  
   
  1.000 1.000 1.000 1.000  
   
  0.959 0.966 0.956 0.955  
   
  0.963 0.978 0.969 0.958  
   
  AE 9.1 0.905 0.901 0.569 0.913 0.951 0.533 t-SNE 0.888 0.939 0.535 UMAP 0.888 0.917 0.595 SSNP(Km)  
   
  1.000 1.000 1.000 0.998  
   
  0.938 0.948 0.929 0.927  
   
  0.945 0.974 0.966 0.952  
   
  σ  
   
  Improving Self-supervised Dimensionality Reduction  
   
  145  
   
  Fig. 3. Projection of synthetic blobs datasets with SSNP(Km) and other techniques, with different number of dimensions and clusters. In each quadrant, rows show datasets having increasing standard deviation σ.  
   
  5.2 Quality on Real-World Datasets Figure 4 shows the projections of real-world datasets by SSNP with ground-truth labels (SSNP(GT)), SSNP with pseudo-labels created by the six clustering algorithms in Table 3, and projections created by AE, t-SNE, UMAP, MDS, and Isomap. We omit again the results for NNP since they are very close to the ones created by t-SNE and UMAP. SSNP and AE were trained for 10 epochs in all cases. SSNP used twice the number of classes as the target number of clusters for the clustering algorithms used for pseudo-labeling.  
   
  146  
   
  A. A. A. M. Oliveira et al.  
   
  Fig. 4. Projection of real-world datasets with SSNP (ground-truth labels and pseudo-labels computed by six clustering methods) compared to Autoencoders, t-SNE, UMAP, MDS, and Isomap.  
   
  SSNP with pseudo-labels shows better cluster separation than AE but slightly worse than SSNP(GT). For the more challenging HAR and Reuters datasets, SSNP(GT) looks better than t-SNE and UMAP. In almost all cases, SSNP yields a better visual cluster separation than MDS and Isomap. We see also that, for almost all clustering algorithmdataset combinations, SSNP creates elongated clusters in a star-like pattern. We believe this is so since one of the network’s targets is a classifier (Sect. 3) which is trained to partition the space based on the data. This results in placing samples that are near a  
   
  Improving Self-supervised Dimensionality Reduction  
   
  147  
   
  decision boundary between classes closer to the center of the star; samples that are far away from a decision boundary are placed near the tips of the star, according to its class. Table 6 shows the four quality metrics (Sect. 4.2) for this experiment. SSNP with pseudo-labels consistently shows better cluster separation (higher NH) than AE as well as better distance preservation (higher R). For the harder HAR and Reuters datasets, SSNP(GT) shows NH results that are similar to and even higher than those for tSNE and UMAP. Also, SSNP(GT) scores consistently higher than MDS and Isomap on all quality metrics, which correlates with these two projection techniques having been found as of moderate quality in earlier studies [12]. For the T and C metrics, SSNP(GT) outperforms again AE in most cases; for FashionMNIST and HAR, SSNP yields T and C values close to the ones for NNP, t-SNE, and UMAP. Separately, we see that the clustering algorithm choice influences the four quality metrics in several ways. DBSCAN (DB) yields in nearly all cases the lowest quality values while K-means (Km) and Agglomerative (AG) yield overall the best quality values. Spectral clustering (SC) is also a quite good option if one is mainly interested in cluster separation (high NH values). Finally, Affinity Propagation (AP) and Gaussian Mixture Models (GMM) Table 6. Quality measurements for the real-world datasets (Sect. 5.2). Dataset  
   
  Method  
   
  T  
   
  C  
   
  R  
   
  NH  
   
  Method  
   
  T  
   
  C  
   
  R  
   
  NH  
   
  MNIST  
   
  SSNP(Km) SSNP(AG) AE SSNP(GT) NNP TSNE UMAP  
   
  0.882 0.859 0.887 0.774 0.948 0.985 0.958  
   
  0.903 0.925 0.920 0.920 0.969 0.972 0.974  
   
  0.264 0.262 0.009 0.398 0.397 0.412 0.389  
   
  0.767 0.800 0.726 0.986 0.891 0.944 0.913  
   
  SSNP(AP) SSNP(DB) SSNP(GMM) SSNP(SC) MDS Isomap  
   
  0.827 0.689 0.880 0.849 0.754 0.759  
   
  0.940 0.802 0.895 0.925 0.862 0.958  
   
  0.094 0.032 0.257 0.164 0.618 0.528  
   
  0.729 0.588 0.755 0.831 0.580 0.618  
   
  FashionMNIST SSNP(Km) SSNP(AG) AE SSNP(GT) NNP TSNE UMAP  
   
  0.958 0.950 0.961 0.863 0.963 0.990 0.982  
   
  0.982 0.978 0.977 0.944 0.986 0.987 0.988  
   
  0.757 0.707 0.538 0.466 0.679 0.664 0.633  
   
  0.739 0.753 0.725 0.884 0.765 0.843 0.805  
   
  SSNP(AP) SSNP(DB) SSNP(GMM) SSNP(SC) MDS Isomap  
   
  0.947 0.890 0.952 0.957 0.923 0.920  
   
  0.986 0.921 0.982 0.981 0.957 0.976  
   
  0.750 0.431 0.689 0.706 0.903 0.749  
   
  0.728 0.665 0.737 0.756 0.652 0.685  
   
  HAR  
   
  SSNP(Km) SSNP(AG) AE SSNP(GT) NNP TSNE UMAP  
   
  0.932 0.926 0.937 0.876 0.961 0.992 0.980  
   
  0.969 0.964 0.970 0.946 0.984 0.985 0.989  
   
  0.761 0.724 0.805 0.746 0.592 0.578 0.737  
   
  0.811 0.846 0.786 0.985 0.903 0.969 0.933  
   
  SSNP(AP) SSNP(DB) SSNP(GMM) SSNP(SC) MDS Isomap  
   
  0.929 0.852 0.924 0.893 0.911 0.925  
   
  0.972 0.909 0.966 0.952 0.890 0.971  
   
  0.736 0.759 0.768 0.811 0.941 0.896  
   
  0.787 0.690 0.796 0.805 0.765 0.861  
   
  Reuters  
   
  SSNP(Km) SSNP(AG) AE SSNP(GT) NNP TSNE UMAP  
   
  0.794 0.771 0.747 0.720 0.904 0.955 0.930  
   
  0.859 0.824 0.731 0.810 0.957 0.959 0.963  
   
  0.605 0.507 0.420 0.426 0.594 0.588 0.674  
   
  0.738 0.736 0.685 0.977 0.860 0.887 0.884  
   
  SSNP(AP) SSNP(DB) SSNP(GMM) SSNP(SC) MDS Isomap  
   
  0.631 0.574 0.622 0.607 0.575 0.634  
   
  0.768 0.650 0.788 0.758 0.757 0.785  
   
  0.039 0.360 0.460 0.027 0.551 0.150  
   
  0.742 0.705 0.793 0.730 0.699 0.765  
   
  148  
   
  A. A. A. M. Oliveira et al.  
   
  score in between Km and AG (best overall) and DB (worst overall). From the above, we conclude that Km and AG are good default clustering methods that SSNP can use in practice. 5.3  
   
  Quality vs Clustering Hyperparameters  
   
  Figure 5 shows projections of the HAR and MNIST datasets created by SSNP with pseudo-labels assigned by DBSCAN and K-means and using the various clustering hyperparameter settings described in Sect. 4.4. For DBSCAN, we see that as the value of eps increases, the SSNP projection seems to vary between global- and local-distance preservation. This effect is more pronounced for the HAR dataset, where we see the number of clusters in the data varying from two (eps = 1.9) and three (eps = 2.7). For the MNIST dataset, the increase in eps only makes the entire projection take a sharper shape, with no improvement in cluster separation. Overall, SSNP with DBSCAN having low eps values produces results similar to an autoencoder, which defeats the purpose of using SSNP. This correlates to the earlier findings in Sect. 5.2 that showed that DBSCAN is not a good clustering companion for SSNP. The quality metrics in Table 7 strengthen this hypothesis – we do not see any clear trend of these metrics being improved by varying eps in a specific direction. Table 7. Quality measurements for the cluster hyperparameter experiment (Sect. 5.3). Dataset Technique Parameter  
   
  T  
   
  C  
   
  R  
   
  NH  
   
  MNIST DBSCAN eps=6.1 eps=6.3 eps=6.5 eps=6.7 eps=6.9 K-means n clusters=5 n clusters=10 n clusters=15 n clusters=20 n clusters=30  
   
  0.685 0.679 0.722 0.698 0.729 0.782 0.834 0.867 0.880 0.899  
   
  0.821 0.798 0.812 0.801 0.825 0.905 0.916 0.927 0.909 0.932  
   
  0.097 0.012 0.044 0.022 0.011 0.408 0.379 0.410 0.047 0.358  
   
  0.555 0.570 0.614 0.576 0.605 0.641 0.697 0.760 0.755 0.790  
   
  HAR  
   
  0.854 0.848 0.875 0.896 0.898 0.887 0.921 0.920 0.930 0.937  
   
  0.928 0.920 0.914 0.924 0.933 0.939 0.959 0.965 0.968 0.972  
   
  0.917 0.841 0.717 0.844 0.887 0.932 0.749 0.877 0.854 0.840  
   
  0.696 0.650 0.685 0.725 0.749 0.693 0.767 0.812 0.815 0.812  
   
  DBSCAN eps=1.9 eps=2.1 eps=2.3 eps=2.5 eps=2.7 K-means n clusters=3 n clusters=6 n clusters=9 n clusters=12 n clusters=18  
   
  Improving Self-supervised Dimensionality Reduction  
   
  149  
   
  For K-means, we see that the value of n clusters has a great effect on the overall shape of the SSNP projection. Particularly, when n clusters is higher than the true number of classes in the data (10 for MNIST, 6 for HAR), we see that the cluster separation gets sharper. This suggests that, when the true number of clusters is not known, starting with a reasonably high number of clusters will produce better results for SSNP with K-means. This is confirmed by the quality metrics in Table 7 which show higher values for higher n clusters settings. 5.4 Quality vs Neural Network Settings We next show how the different neural network hyperparameter settings affect the SSNP results following the sampling of these parameters discussed in Sect. 4.5. We also use this analysis to derive good default values for these parameters. L2 Regularization: Figure 6 shows projections created with different amounts of L2 regularization during SSNP’s training. We see that regularization has a detrimental effect to the visual quality of the projection. For values of λ ≥ 0.5, the projection points collapse to a single point, marked by the red circles in the figure. Table 8 shows the metric values for this experiment confirming that all quality values decrease with λ. We conclude that SSNP obtains optimal results without regularization. Activation Functions: Figure 7 shows the effect of using different activation functions α in the embedding layer. We see that the ReLU and LeakyReLU activations produce  
   
  Fig. 5. Projections of MNIST and HAR datasets using different hyperparameters for the DBSCAN and K-means clustering methods (see Sect. 5.3 and Table 7).  
   
  150  
   
  A. A. A. M. Oliveira et al.  
   
  Fig. 6. Projections created with SSNP(GT) and SSNP(Km) for the MNIST dataset varying the amount of L2 regularization λ (Sect. 5.4).  
   
  similarly good results. Both produce visual cluster separation comparable to t-SNE and UMAP (see Fig. 4), albeit with a distinct star or radial shape. The sigmoid activation collapses all data points into a single diagonal, making it a poor choice for the embedding layer. Finally, the tanh activation produced the best cluster separation of all, with results that look very close to the ones by t-SNE and UMAP for this dataset (see again Fig. 4). We conclude that the tanh activation function is the best option for SSNP. Initialization: Figure 8 shows how weight initialization affects projection quality. We see that both Glorot and He uniform initializations produce good and comparable Table 8. Quality measurements for SSNP for different training hyperparameters. NA indicates that the measurement failed for the respective experiment (Sect. 5.4). Method  
   
  Parameter Value  
   
  SSNP(GT) α  
   
  η  
   
  φ  
   
  λ  
   
  T  
   
  C  
   
  R  
   
  NH  
   
  LeakyReLU 0.780 0.930 0.429 0.971 ReLU  
   
  0.789 0.921 0.402 0.983  
   
  sigmoid  
   
  0.703 0.891 0.088 0.746  
   
  tanh  
   
  0.784 0.929 0.190 0.983  
   
  2  
   
  0.781 0.924 0.428 0.903  
   
  3  
   
  0.787 0.926 0.428 0.940  
   
  5  
   
  0.786 0.925 0.419 0.966  
   
  10  
   
  0.789 0.921 0.402 0.983  
   
  20  
   
  0.797 0.920 0.391 0.989  
   
  Glorot  
   
  0.789 0.921 0.402 0.983  
   
  He  
   
  0.789 0.928 0.328 0.982  
   
  Random  
   
  0.758 0.905 0.071 0.927  
   
  0  
   
  0.789 0.921 0.402 0.983  
   
  0.1  
   
  0.757 0.909 0.360 0.870  
   
  0.5  
   
  0.538 0.502 NA  
   
  1  
   
  0.538 0.502 NA 0.101 (continued)  
   
  0.101  
   
  Improving Self-supervised Dimensionality Reduction  
   
  151  
   
  Table 8. (continued) Method  
   
  Parameter Value  
   
  SSNP(Km) α  
   
  η  
   
  φ  
   
  λ  
   
  LeakyReLU ReLU sigmoid tanh 2 3 5 10 20 Glorot He Random 0 0.1 0.5 1  
   
  T  
   
  C  
   
  R  
   
  NH  
   
  0.863 0.888 0.678 0.884 0.847 0.827 0.854 0.881 0.886 0.884 0.874 0.741 0.888 0.872 0.538 0.538  
   
  0.919 0.916 0.872 0.928 0.927 0.926 0.915 0.908 0.911 0.915 0.903 0.869 0.924 0.910 0.502 0.502  
   
  0.177 0.119 0.196 0.265 0.267 0.244 0.323 0.188 0.128 0.333 0.267 0.115 0.351 0.352 NA NA  
   
  0.748 0.768 0.568 0.774 0.726 0.714 0.775 0.770 0.766 0.784 0.753 0.640 0.763 0.753 0.101 0.101  
   
  Fig. 7. Projections of the MNIST dataset using SSNP(GT) and SSNP(Km) varying the activation function α (Sect. 5.4).  
   
  results, whereas random initialization yields very poor results. We opt for using He uniform as the default initialization, which correlates with the same choice (obtained by an independent investigation) for NNP [9]. Training Epochs: Finally, Fig. 9 shows projections created with SSNP trained for different numbers η of epochs. With as little as η = 3 training epochs, SSNP already produces good cluster separation. As η increases, the created visual clusters become sharper. However, there seems to be little improvement when going from η = 10 to η = 20. As such, we conclude that a good default is η = 10 training epochs. Interestingly, this is significantly less than the 50 epochs needed by NNP to achieve good  
   
  152  
   
  A. A. A. M. Oliveira et al.  
   
  Fig. 8. Projections of the MNIST dataset using SSNP(GT) and SSNP(Km) varying the weight initialization strategy φ (Sect. 5.4).  
   
  Fig. 9. Projections of MNIST dataset using SSNP(GT) and SSNP(Km) varying the number of training epochs η (Sect. 5.4).  
   
  projection quality [9], especially if we consider that SSNP has to train a more complex, dual-objective, network. 5.5  
   
  Computational Scalability  
   
  Using SSNP means (a) training the network and next (b) using the trained network in inference mode (see also Fig. 1). We analyze these two times next. Setup Time: Table 9 shows the time needed to set up SSNP and three other projection techniques. For SSNP, NP, and AE, this is the training time of the respective neural networks using 10 training epochs. Note that we used 10K training samples, which is largely sufficient to train SSNP to obtain good results. In practice, SSNP obtains good results (quality-wise) with as few as 1K samples. For UMAP and t-SNE, this is the time needed to actually project the data since these techniques do not have a training phase. We see that the SSNP variants using clustering take about the same time as tSNE and UMAP and less than NNP. SSNP(GT), which does not need clustering, is far faster than these competitors, with the exception of AE which is about twice faster. This is explainable since SSNP uses a dual-objective network (Sect. 3), one of these being essentially the same as AE.  
   
  Improving Self-supervised Dimensionality Reduction  
   
  153  
   
  Table 9. Setup time for different projection methods for 10K training samples, MNIST dataset. Method  
   
  Setup time (s)  
   
  SSNP(GT) SSNP(Km) SSNP(Agg)  
   
  6.029 20.478 31.954  
   
  AE 3.734 25.143 UMAP 33.620 t-SNE NNP(t-SNE) 51.181  
   
  Fig. 10. Inference time for SSNP and other techniques (log scale). Techniques using training use 10K samples from the MNIST dataset. Inference is done on MNIST upsampled up to 1M samples.  
   
  Inference Time: Figure 10 shows the time needed to project up to 1M samples using SSNP and the other compared projection techniques. For SSNP, AE, and NNP, this is the inference time using the respective trained networks. For t-SNE and UMAP, this is the actual projection time, as described earlier in this section. Being GPU-accelerated neural networks, SSNP, AE, and NNP perform very fast, all being able to project up to 1M samples in a few seconds – an order of magnitude faster than UMAP, and over three orders of magnitude faster than t-SNE. We also see that SSNP, AE, and NNP have practically the same speed. This is expected since they have comparably large and similar-architecture neural networks which, after training, take the same time to execute their inference. 5.6 Inverse Projection Recalling from Sect. 2, an inverse projection P−1 (p) aims to create a data point x so that its projection P(x) is as close as possible to p. Hence, we can test how well a method computes P−1 for a given direct projection function P by evaluating how close  
   
  154  
   
  A. A. A. M. Oliveira et al.  
   
  P−1 (P(x)) is to the data point x itself. To test this, we consider points x being images in the MNIST dataset and P and P−1 being computed by SSNP as described in Sect. 3). Figure 11 shows a set of digits from the MNIST dataset – both the actual images x and the ones obtained by P−1 (P(x)). We see that SSNP(Km) yields results very similar to AE, both of these being visually quite close images to the actual images x, modulo a limited amount of fuzziness. Hence, SSNP’s dual-optimization target succeeds in learning a good inverse mapping based on the direct mapping given by the pseudo-labels (Sect. 3). Table 10 strengthens this insight by showing the values of the Mean Squared Error (MSE) between the original and inversely-projected images 1 −1 2 |D| ∑x∈D x − P (P(x)) for SSNP(Km) and AE for both the training and test sets. These errors, again, are very similar. Furthermore, the SSNP MSE errors are of the same order of magnitude – that is, very small – as those obtained by the recent NNInv technique [13] and the older iLAMP [1] technique that also compute inverse projections – compare Table 10 with Fig. 2 in [13] (not included here for space reasons). Summarizing the above, we conclude that SSNP achieves a quality of inverse projections on par with existing state-of-the-art techniques. Table 10. Inverse projection Mean Square Error (MSE) for SSNP(Km) and AE, trained with 5K samples and tested with 1K samples, different datasets. Dataset  
   
  SSNP(Km) Train Test  
   
  Autoencoder Train Test  
   
  MNIST  
   
  0.0474 0.0480 0.0424 0.0440  
   
  FashionMNIST 0.0309 0.0326 0.0291 0.0305 HAR  
   
  0.0072 0.0074 0.0066 0.0067  
   
  Reuters  
   
  0.0002 0.0002 0.0002 0.0002  
   
  Fig. 11. Sample images from MNIST inversely projected by SSNP and AE, both trained with 10 epochs and 5K samples, MNIST dataset. Bright images show the original images that the inverse projection should be able to reproduce.  
   
  5.7  
   
  Data Clustering  
   
  Table 11 shows how SSNP performs when doing classification or clustering, which corresponds respectively to its usage of pseudo-labels or ground-truth labels. We see that  
   
  Improving Self-supervised Dimensionality Reduction  
   
  155  
   
  SSNP generates good results in both cases when compared to the ground-truth (GT) labels and, respectively, the underlying clustering algorithm K-means (Km), which emerged as one of the best clustering companions for SSNP (Sect. 5.2). However, we should stress that classification or clustering is only a side result of SSNP, needed for computing the dual-objective cost that the network uses (Sect. 3). While one gets this by-product for free, SSNP only mimics the underlying clustering algorithm that it learns, rather than doing data clustering from scratch. As such, we do not advocate using SSNP as a potential replacement for clustering algorithms. Table 11. Classification/clustering accuracy of SSNP when compared to ground truth (GT) and clustering labels (Km), trained with 5K samples, tested with 1K samples. Dataset MNIST  
   
  SSNP(GT)  
   
  SSNP(Km)  
   
  Train Test  
   
  Train Test  
   
  0.984 0.942 0.947 0.817  
   
  FashionMNIST 0.866 0.815 0.902 0.831 HAR  
   
  0.974 0.974 0.931 0.919  
   
  Reuters  
   
  0.974 0.837 0.998 0.948  
   
  5.8 Implementation Details All experiments discussed in this section were run on a 4-core Intel Xeon E3-1240 v6 at 3.7 GHz with 64 GB RAM and an NVidia GeForce GTX 1070 GPU with 8 GB VRAM. Table 12 lists all open-source software libraries used to build SSNP and the other tested techniques. Our neural network implementations leverages the GPU power by using the Tensorflow Keras framework. The t-SNE implementation used is a parallel version of Barnes-Hut t-SNE [31, 52], run on all four available CPU cores for all tests. The UMAP reference implementation is not parallel, but is quite fast (compared to t-SNE) and welloptimized. The implementation of MDS, Isomap, and all clustering techniques comes from Scikit-Learn [41]. Our implementation, plus all code used in this experiment, are publicly available at https://github.com/mespadoto/ssnp. Table 12. Software used for the SSNP implementation and evaluation. Technique  
   
  Software used publicly available at  
   
  SSNP (our technique)  
   
  keras.io (TensorFlow backend)  
   
  Autoencoders t-SNE  
   
  github.com/DmitryUlyanov/Multicore-t-SNE  
   
  UMAP  
   
  github.com/lmcinnes/umap  
   
  Affinity propagation  
   
  scikit-learn.org  
   
  Agglomerative clustering DBSCAN Gaussian mixture model K-means Spectral clustering  
   
  156  
   
  A. A. A. M. Oliveira et al.  
   
  6 Discussion We discuss next how the available hyperparameter settings influence the performance of SSNP with respect to the seven criteria laid out in Sect. 1. Quality (C1): As shown in Figs. 3 and 4, SSNP provides better cluster separation than Autoencoders, MDS, and Isomap, and comparable quality to t-SNE and UMAP, as measured by the selected metrics (Tables 5 and 6). Interestingly, using ground-truth labels (SSNP(GT)) does not always yield the highest quality metrics as compared to using pseudo-labels produced by clustering. Related to the latter, K-means (Km) and Agglomerative clustering (AG) yield, overall, higher quality metrics for most tested datasets as compared to DBSCAN, Gaussian mixture models, Spectral clustering, and Affinity propagation. When we consider the neighborhood hit (NH) metric, which models the closest from all studied metrics the ability of a projection to segregate similar samples into visually distinct clusters, SSNP(GT) performs better than all tested methods, t-SNE and UMAP included. Importantly, note that SSNP uses labels only during training and not during inference, so it can be fairly compared with such other projection methods. Scalability (C2): SSNP(GT) is roughly half the speed of Autoencoders during training which is expected given its dual-optimization target. Training SSNP with pseudo-labels is slower, roughly the speed of t-SNE or UMAP, which is explained by the time taken by the underlying clustering algorithm which dominates the actual training time. In our experiments, K-means seems to be faster than Agglomerative clustering, being thus more suitable when training SSNP with very large datasets. Inference time for SSNP practically identical to Autoencoders and NNP, and one order of magnitude faster than UMAP and three orders faster than t-SNE, being also linear in the sample and dimension counts. This shows SSNP’s suitability to situations where one needs to project large amounts of data, such as streaming applications; Ease of Use (C3): SSNP yielded good projection results with little training (10 epochs), little training data (5K samples) and a simple heuristic of setting the number of clusters for the clustering step to twice the number of expected clusters in the data. Furthermore, we examined several hyperparameters of SSNP and found good default values (in terms of obtaining high quality metrics) as follows: no L2 regularization, tanh activation function for the embedding layer, and He uniform weight initialization. The clustering algorithm default is K-means or Agglomerative, with K-means slightly preferred for speed reasons. As such, SSNP can be used with no parameter tweaking efforts needed. Genericity (C4): We show results for SSNP with different types of high-dimensional data, namely tabular (HAR), images (MNIST, FashionMNIST), and text (Reuters). As these datasets come from quite different sources and as the SSNP method itself makes no assumption on the nature or structure of the data, we believe that SSNP is generically applicable to any high-dimensional real-valued dataset. Stability and Out-of-Sample Support (C5): All measurements we show for SSNP are based on inference, i.e., we pass the data through the trained network to compute them. This is evidence of the out-of-sample capability, which allows one to project new  
   
  Improving Self-supervised Dimensionality Reduction  
   
  157  
   
  data without recomputing the projection, in contrast to t-SNE and other non-parametric methods. Inverse Mapping (C6): SSNP shows inverse mapping results which are, quality-wise, very close to results from Autoencoders, NNInv and iLAMP, these being state-of-the-art methods for computing inverse projections. Additionally, SSNP computes the inverse projection at no extra cost or need for a separate implementation, in contrast to NNInv and iLAMP. Clustering (C7): SSNP is able to mimic the behavior of the clustering algorithm used as its input, as a byproduct of its training with labeled data. We show that SSNP produces competitive results when compared to pseudo- or ground truth labels. Although SSNP is not a clustering algorithm, it provides this for free (with no additional execution cost), which can be useful in cases where one wants to do both clustering and DR. However, we stress that SSNP should not be considered as a replacement for state-ofthe-art clustering algorithms, since it only learns to mimic the actual clustering. This is similar to the distinction between a classifier and an actual clustering technique. In addition to the good performance shown for the aforementioned criteria, a key strength of SSNP is its ability to performing all its operations after a single training phase. This saves effort and time in cases where all or a subset of those results (e.g., direct projection, inverse projection, clustering) are needed. Limitations: While scoring high on several criteria, SSNP also has several limitations. Quality-wise, its operation in pseudo-labeling mode cannot reach the high quality values for all metrics that are delivered by t-SNE or UMAP for challenging datasets (Table 6). We believe that this is affected by the number of clusters used during training, which is related to the neighborhood size that t-SNE and UMAP use. More involved strategies in setting this number of clusters can be explored to further increase SSNP’s quality. Visually, while we argue for the reason of the star-shaped cluster structures produced by SSNP (Sect. 5.2), such patterns can be less suitable for visual exploration than the blob-like patterns produced typically by t-SNE. Using a tanh activation function partially alleviates this issue (Sect. 5.4). However, more studies are needed to explore other activation functions that allow even better control of the visual cluster shapes. Most importantly however, SSNP is a learning method. As with any such method, its quality will decrease when inferring on (that is, projecting) datasets which are too far away from the ones used during training, an issue also present for NNP and autoencoders. In contrast, methods that do not use training can obtain similar quality for any input dataset. Yet, the price to pay for such methods is that they cannot guarantee stability and out-of-sample behavior, which come with SSNP by default.  
   
  7 Conclusion We presented an in-depth analysis of a dimensionality reduction (DR) method called Self-Supervised Neural Projection (SSNP) recently proposed by us. SSNP uses a neural network with a dual objective – reconstruction of the high-dimensional input data and classification of the data – to achieve several desirable characteristics of a general-purpose DR method. SSNP is, to our knowledge, the only technique that jointly  
   
  158  
   
  A. A. A. M. Oliveira et al.  
   
  addresses all characteristics listed in Sect. 1 of this paper, namely producing projections that exhibit a good visual separation of similar samples, handling datasets of millions of elements in seconds, being easy to use (no complex parameters to set), handling generically any type of high-dimensional data, providing out-of-sample support, and providing an inverse projection function. Our evaluation added two additional dimensionality reduction methods, four clustering algorithms, and also explored the hyperparameter space of both the clustering algorithms and neural network training to gauge SSNP’s behavior. The evaluation results led to establishing default values for all these hyperparameters which obtain high quality values and also turn SSNP into a parameter-free method. Additionally, the obtained results show that SSNP with ground-truth labels yields higher quality in terms of visual cluster separation than all tested projections including the state-of-the-art tSNE and UMAP methods. When pseudo-labels are used due to the lack of true labels, SSNP achieves lower but still competitive results with t-SNE and UMAP, slightly to significantly higher quality than autoencoders, and significantly higher quality than MDS and Isomap. As future work, we consider studying better heuristics for controlling the clustering process which we believe are a low hanging fruit towards improving SSNP’s quality. Another interesting direction is to explore other activation function designs that can offer control to the end users on the shape of the visual clusters that the projection creates, which would be, to our knowledge, an unique feature in the family of projection techniques. A more ambitious, but realizable, goal is to have SSNP learn its pseudolabeling during training and therefore remove the need for using a separate clustering algorithm. Acknowledgments. This study was financed in part by FAPESP grants 2015/22308-2, 2017/25835-9 and 2020/13275-1, and the Coordenac¸a˜ o de Aperfeic¸oamento de Pessoal de N´ıvel Superior - Brasil (CAPES) - Finance Code 001.  
   
  References 1. Amorim, E., Brazil, E.V., Daniels, J., Joia, P., Nonato, L.G., Sousa, M.C.: iLAMP: exploring high-dimensional spacing through backward multidimensional projection. In: Proceedings of IEEE VAST, pp. 53–62 (2012) 2. Anguita, D., Ghio, A., Oneto, L., Parra, X., Reyes-Ortiz, J.L.: Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. In: Bravo, J., Herv´as, R., Rodr´ıguez, M. (eds.) IWAAL 2012. LNCS, vol. 7657, pp. 216–223. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-35395-6 30 3. Becker, M., Lippel, J., Stuhlsatz, A., Zielke, T.: Robust dimensionality reduction for data visualization with deep neural networks. Graph. Models 108, 101060 (2020) 4. Chan, D., Rao, R., Huang, F., Canny, J.: T-SNE-CUDA: GPU-accelerated t-SNE and its applications to modern data. In: Proceedings of SBAC-PAD, pp. 330–338 (2018) 5. Cunningham, J., Ghahramani, Z.: Linear dimensionality reduction: survey, insights, and generalizations. JMLR 16, 2859–2900 (2015) 6. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the EM algorithm. J. Roy. Stat. Soc. Ser. B (Methodological) 39(1), 1–22 (1977)  
   
  Improving Self-supervised Dimensionality Reduction  
   
  159  
   
  7. Donoho, D.L., Grimes, C.: Hessian eigenmaps: locally linear embedding techniques for highdimensional data. Proc. Natl. Acad. Sci. 100(10), 5591–5596 (2003) 8. Engel, D., Hattenberger, L., Hamann, B.: A survey of dimension reduction methods for highdimensional data analysis and visualization. In: Proceedings of IRTG Workshop, vol. 27, pp. 135–149. Schloss Dagstuhl (2012) 9. Espadoto, M., Falcao, A., Hirata, N., Telea, A.: Improving neural network-based multidimensional projections. In: Proceedings of IVAPP (2020) 10. Espadoto, M., Hirata, N., Telea, A.: Deep learning multidimensional projections. J. Inf. Vis. (2020). https://doi.org/10.1177/1473871620909485 11. Espadoto, M., Hirata, N.S., Telea, A.C.: Self-supervised dimensionality reduction with neural networks and pseudo-labeling. In: Proceedings of IVAPP, pp. 27–37. SCITEPRESS (2021) 12. Espadoto, M., Martins, R.M., Kerren, A., Hirata, N.S., Telea, A.C.: Towards a quantitative survey of dimension reduction techniques. IEEE TVCG 27(3), 2153–2173 (2019) 13. Espadoto, M., Rodrigues, F.C.M., Hirata, N.S.T., Hirata Jr., R., Telea, A.C.: Deep learning inverse multidimensional projections. In: Proceedings of EuroVA, Eurographics (2019) 14. Ester, M., Kriegel, H.P., Sander, J., Xu, X., et al.: A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proceedings of KDD, vol. 96, pp. 226–231 (1996) 15. Fisher, R.A.: The use of multiple measurements in taxonomic problems. Ann. Eugenics 7(2), 179–188 (1936) 16. Frey, B.J., Dueck, D.: Clustering by passing messages between data points. Science 315(5814), 972–976 (2007) 17. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of AISTATS, pp. 249–256 (2010) 18. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In: Proceedings of IEEE ICCV, pp. 1026–1034 (2015) 19. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks. Science 313(5786), 504–507 (2006) 20. Hoffman, P., Grinstein, G.: A survey of visualizations for high-dimensional data mining. Inf. Vis. Data Min. Knowl. Disc. 104, 47–82 (2002) 21. Joia, P., Coimbra, D., Cuminato, J.A., Paulovich, F.V., Nonato, L.G.: Local affine multidimensional projection. IEEE TVCG 17(12), 2563–2571 (2011) 22. Jolliffe, I.T.: Principal component analysis and factor analysis. In: Principal Component Analysis, pp. 115–128. Springer, New York (1986). https://doi.org/10.1007/978-1-47571904-8 7 23. Kaufman, L., Rousseeuw, P.: Finding Groups in Data: An Introduction to Cluster Analysis. Wiley, Hoboken (2005) 24. Kehrer, J., Hauser, H.: Visualization and visual analysis of multifaceted scientific data: a survey. IEEE TVCG 19(3), 495–513 (2013) 25. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. CoRR abs/1312.6114 (2013), eprint: 1312.6114 26. Kohonen, T.: Self-organizing Maps. Springer, Berlin (1997). https://doi.org/10.1007/978-3642-97966-8 27. Krogh, A., Hertz, J.A.: A simple weight decay can improve generalization. In: Proceedings of NIPS, pp. 950–957 (1992) 28. LeCun, Y., Cortes, C.: MNIST handwritten digits dataset (2010). http://yann.lecun.com/ exdb/mnist 29. Liu, S., Maljovec, D., Wang, B., Bremer, P.T., Pascucci, V.: Visualizing high-dimensional data: advances in the past decade. IEEE TVCG 23(3), 1249–1268 (2015)  
   
  160  
   
  A. A. A. M. Oliveira et al.  
   
  30. Lloyd, S.: Least squares quantization in PCM. IEEE Trans Inf. Theor. 28(2), 129–137 (1982) 31. Maaten, L.V.D.: Barnes-hut-SNE. arXiv preprint arXiv:1301.3342 (2013) 32. Accelerating t-SNE using tree-based algorithms: Maaten, L.V.d. JMLR 15, 3221–3245 (2014) 33. Maaten, L.V.D., Hinton, G.: Visualizing data using t-SNE. JMLR 9, 2579–2605 (2008) 34. Maaten, L.V.d., Postma, E.: Dimensionality reduction: a comparative review. Technical Report, Tilburg University, Netherlands (2009) 35. Martins, R.M., Minghim, R., Telea, A.C., et al.: Explaining neighborhood preservation for multidimensional projections. In: CGVC, pp. 7–14 (2015) 36. McInnes, L., Healy, J.: UMAP: uniform manifold approximation and projection for dimension reduction. arXiv:1802.03426v1 [stat.ML] (2018) 37. Modrakowski, T.S., Espadoto, M., Falc˜ao, A.X., Hirata, N.S.T., Telea, A.: Improving deep learning projections by neighborhood analysis. In: Bouatouch, K., et al. (eds.) VISIGRAPP 2020. CCIS, vol. 1474, pp. 127–152. Springer, Cham (2022). https://doi.org/10.1007/978-3030-94893-1 6 38. Nonato, L., Aupetit, M.: Multidimensional projection for visual analytics: linking techniques with distortions, tasks, and layout enrichment. IEEE TVCG (2018). https://doi.org/10.1109/ TVCG.2018.2846735 39. Paulovich, F.V., Minghim, R.: Text map explorer: a tool to create and explore document maps. In: Proceedings of International Conference on Information Visualisation (IV), pp. 245–251. IEEE (2006) 40. Paulovich, F.V., Nonato, L.G., Minghim, R., Levkowitz, H.: Least square projection: a fast high-precision multidimensional projection technique and its application to document mapping. IEEE TVCG 14(3), 564–575 (2008) 41. Pedregosa, F., et al.: Scikit-learn: machine learning in python. J. Mach. Learn. Res. (JMLR) 12, 2825–2830 (2011) 42. Pezzotti, N., H¨ollt, T., Lelieveldt, B., Eisemann, E., Vilanova, A.: Hierarchical stochastic neighbor embedding. Comput. Graph. Forum 35(3), 21–30 (2016) 43. Pezzotti, N., Lelieveldt, B., Maaten, L.V.d., H¨ollt, T., Eisemann, E., Vilanova, A.: Approximated and user steerable t-SNE for progressive visual analytics. IEEE TVCG 23, 1739–1752 (2017) 44. Pezzotti, N., et al.: GPGPU linear complexity t-SNE optimization. IEEE TVCG 26(1), 1172– 1181 (2020) 45. Roweis, S.T., Saul, L.L.K.: Nonlinear dimensionality reduction by locally linear embedding. Science 290(5500), 2323–2326 (2000) 46. Salton, G., McGill, M.J.: Introduction to Modern Information Retrieval. McGraw-Hill, New York (1986) 47. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE TPAMI 22(8), 888–905 (2000) 48. Sorzano, C., Vargas, J., Pascual-Montano, A.: A survey of dimensionality reduction techniques (2014). arXiv:1403.2877 [stat.ML] 49. Tenenbaum, J.B., Silva, V.D., Langford, J.C.: A global geometric framework for nonlinear dimensionality reduction. Science 290(5500), 2319–2323 (2000) 50. Thoma, M.: The Reuters dataset, July 2017. https://martin-thoma.com/nlp-reuters 51. Torgerson, W.S.: Theory and Methods of Scaling. Wiley, Hoboken (1958) 52. Ulyanov, D.: Multicore-TSNE (2016). https://github.com/DmitryUlyanov/Multicore-TSNE 53. Venna, J., Kaski, S.: Visualizing gene interaction graphs with local multidimensional scaling. In: Proceedings of ESANN, pp. 557–562 (2006) 54. Wattenberg, M.: How to use t-SNE effectively (2016). https://distill.pub/2016/misread-tsne 55. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms (2017). arXiv:1708.07747  
   
  Improving Self-supervised Dimensionality Reduction  
   
  161  
   
  56. Xie, H., Li, J., Xue, H.: A survey of dimensionality reduction techniques based on random projection (2017). arXiv:1706.04371 [cs.LG] 57. Zhang, Z., Wang, J.: MLLE: modified locally linear embedding using multiple weights. In: Advances in Neural Information Processing Systems (NIPS), pp. 1593–1600 (2007) 58. Zhang, Z., Zha, H.: Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. SIAM J. Sci. Comput. 26(1), 313–338 (2004)  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps Daniel Atzberger(B) , Tim Cech, Willy Scheibel, Daniel Limberger, and J¨urgen D¨ollner Hasso Plattner Institute, Digital Engineering Faculty, University of Potsdam, Potsdam, Germany [email protected]  Abstract. For various program comprehension tasks, software visualization techniques can be beneficial by displaying aspects related to the behavior, structure, or evolution of software. In many cases, the question is related to the semantics of the source code files, e.g., the localization of files that implement specific features or the detection of files with similar semantics. This work presents a general software visualization technique for source code documents, which uses 3D glyphs placed on a two-dimensional reference plane. The relative positions of the glyphs captures their semantic relatedness. Our layout originates from applying Latent Dirichlet Allocation and Multidimensional Scaling on the comments and identifier names found in the source code files. Though different variants for 3D glyphs can be applied, we focus on cylinders, trees, and avatars. We discuss various mappings of data associated with source code documents to the visual variables of 3D glyphs for selected use cases and provide details on our visualization system. Keywords: Source code mining · Software visualization · Glyph visualization  
   
  1 Introduction About 90 % of the entire costs of a software project are related to the maintenance phase [14], i.e., to prevent problems before they occur (preventive maintenance), correct faults (corrective maintenance), improve the functionality or performance (perfective maintenance), or adapt to a changing environment (adaptive maintenance) [23]. There are various visualization techniques to represent aspects related to the structure, the behavior, or the evolution of the underlying software, to assist users in program comprehension tasks during the maintenance phase. Nevertheless, since software has no intrinsic gestalt, software visualization uses suitable abstractions and metaphors to depict aspects of and relations within software data to support and, at best, align users in their mental representation of selected software aspects. Interactive visualizations allow users to analyze a software project in an exploratory way and thus support finding information and gaining knowledge. Examples for well-established, interactive software visualization techniques are: – Icicle Plots for representations of trace executions [10, 36], – Treemaps depicting the hierarchical structure of software projects [32, 41], c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 162–182, 2023. https://doi.org/10.1007/978-3-031-25477-2_8  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  163  
   
  – Circular Bundle Views illustrating relations, e.g., include dependencies [11], – Software Cities that reflect the development history of software [45, 46], and – similar approaches based on cartographic metaphors [21, 28]. Many specific questions in maintenance are related to the semantic structure of software projects. For example, in the case of perfective maintenance, source code files implementing a specific functionality or concept need to be identified. It is helpful to be aware of other files that share semantics in this context. Such tasks can become intensively time-consuming with long-lasting software systems and with an increasing number of different developers. In order to support such tasks, various layouts exist that can reflect semantic similarities between files, i.e., by placing files with a similar semantic closer to one another [2, 4, 27, 28]. Using 2D or 3D glyphs to represent files with a semantic positioning and additional data mapping, e.g., software metrics mapped to the glyphs’ visual variables, facilitates the comprehension of the semantic structure of a software project. For the remainder of this work, we refer to the term glyphs as defined by Ward et al.; “In the context of data and information visualization, a glyph is a visual representation of a piece of data or information where a graphical entity and its attributes are controlled by one or more data attributes” [51]. In this work, we present a general approach for placing custom 3D glyphs in a 2D reference space for software visualization tasks, in order to (1) capture the semantic structure of source code files and (2) allow for an additional, inherent visual display of related data, e.g., software metrics. For our layout technique, we assume developer comments and deliberately chosen identifiers to not only provide instructions for compilers but to simultaneously document and communicate intent, function, and context to developers. This assumption motivates the use of techniques from the Natural Language Processing (NLP) domain for mining the semantic structure of source code documents. We apply Latent Dirichlet Allocation (LDA), a probabilistic topic model, to capture the semantic structure of a software structure, which leads to a mathematical description of source code files. By applying Multidimensional Scaling (MDS) as a dimension reduction technique, we generate a two-dimensional layout that reflects the semantic relatedness between the source code files. We represent every source code unit or file as a single 3D glyph. Though plenty of glyphs and metaphors have been applied to software visualization tasks, we focus our discussion on three examples we considered valuable: Cylinders with their extent, height, and color as visual variables. Trees with a variety of visual variables, e.g., size, type, leaf color, health, age, and season. Avatars which can be easily distinguished from each other and clearly identified, e.g., for depicting software developers or teams. We describe fitting use cases for every glyph and provide examples using popular Open Source projects data. Figure 1 shows one exemplary result of our visualization approach. The remainder of this work is structured as follows: In Sect. 2 we review existing work related to our approach. We provide an overview of possible layouts for visualizing source code and glyphs and natural metaphors in the software visualization domain.  
   
  164  
   
  D. Atzberger et al.  
   
  Fig. 1. Example of a Software Forest using handcrafted tree models from SketchFab (sketchfab.com) as 3D glyphs. Each tree represents a source code file. Quantitative and qualitative data associated to the files can be mapped to age, type, and health of a tree.  
   
  In Sect. 3 we detail the layout approach, which is based on LDA and MDS and applied to comments and identifiers in source code. Section 4 describes use case scenarios and shows how data related to the semantics of source code files can be represented. We further present a detailed explanation of our system and its implementation in Sect. 5 and, finally, conclude this paper in Sect. 6 and present directions for future work.  
   
  2 Related Work Our visualizations are created in two distinct steps. First, we generate a semantic layout that is then used for placing 3D glyphs (representing source code files). Second, we map quantitative and qualitative data of source code files to the available visual variables of the 3D glyphs. With respect to the prior art, we, therefore, focus on these two aspects. We describe existing approaches for placing documents in a reference space in order to reflect their semantic similarity and also describe existing 2.5D approaches based on treemaps for software visualization tasks. We then present relevant work on three widely used visualization metaphors, namely the island metaphor, the tree metaphor, and the city metaphor. Selected glyphs are presented at the end of this section. Semantic Layouts for Software Visualization. When designing visualizations, one has to consider the placement of data items in the reference space. In the case of document visualization, we call a layout whose goal is to reflect the semantic relatedness between the data items a semantic layout. In a semantic layout, documents that share a common similarity are placed nearby each other. As documents are mostly viewed as Bag-ofWords (BOW), i.e., the order of words within a document is neglected, and only their frequency is taken into account, dimension reduction techniques are used to project the high-dimensional points to a two-dimensional plane or a three-dimensional space. Skupin et al.proposed an approach for generating two-dimensional visualizations for text documents using cartographic metaphors [44]. The authors applied SelfOrganizing Map (SOM) on the BOW [26], as dimension reduction technique, to place  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  165  
   
  abstracts of publications about geography on the plane. Furthermore, dominant terms were displayed, thus showing the semantic content of the region in the visualization. Kuhn et al.were the first to propose a semantic layout for software visualization tasks [27, 28]. First, each source code file is considered as a single document and several preprocessing tasks are undertaken to remove noise from the vocabulary. Then, the high-dimensional BOW is reduced in their dimension in two steps. The topic model Latent Semantic Indexing (LSI) [13] is applied, which describes each document through its expression in the latent topics within a software project, which can already be seen as a dimension reduction of the BOW. After this, MDS [12] is applied on the dissimilarity matrix that captures the pairwise dissimilarities of the documents using the cosinesimilarity. The resulting two-dimensional scatterplot is then equipped with height lines, resulting in a cartographic visualization. In addition, two-dimensional glyphs are placed for displaying coding activities, e.g., test tubes. Linstead et al. [34, 35] were the first to propose a semantic software layout based on LDA and its variant, the Author-Topic Model (ATM), which additionally takes information about authorship into account [39]. By applying the topic models on the source code of the Eclipse project, both source code files and authors are described as distributions over latent topics. The final layout is computed by applying MDS on the dissimilarity matrix, which contains the pairwise symmetrized Kullback-Leibler divergence of the authors or files. Another approach that models the semantic structure of source code files using LDA for visualization tasks was presented by Atzberger et al. [2]. In their approach, the authors first apply MDS on the topic-word distributions to compute two-dimensional vertices, representing the topics, as presented in [43]. The position of a document is then computed as a convex linear combination according to its document-topic distribution. Using this layout, the authors introduced the tree metaphor for software visualization, resulting in the so-called Software Forest. In a later work, Atzberger et al.discussed the use of pawns and chess figures as 3D glyphs for visualizing the knowledge distribution across software development teams [3]. In this case, the layout reflects the semantic similarity between developers, additional information about the expertise of each developer can then be mapped on the visual variables of the representing glyph. In another work, Atzberger et al.applied their layout approach to a 3D reference space, creating a stylized scatter plot for the depiction of software projects [4]. Inspired by a metaphor introduced by Lanza et al. [29], the authors displayed each source code file as a star, thereby creating a Software Galaxy. The authors also introduced transparent volumetric nebulae to make use of the metaphor of galactic star clusters or nebulae. Attributes such as cluster density or distribution can subsequently be mapped to the nebulaes’ intensities and colors. The Island Metaphor. The 2.5D approach used by Atzberger allows for the integration of a terrain (based on a dynamically generated heightfield), resulting in visualizations resembling islands. Indeed the island metaphor is a widely used visualization metaphor ˘ ep´anek developed Helveg, a framework for in the Software Visualization domain. St˘ visualizing C# code as islands, based on a graph-drawing algorithm layout [47]. Their approach also uses 3D glyphs, e.g., bridges representing dependencies and trees depicting classes, for visualizing the structure of a project. CodeSurveyor is another approach  
   
  166  
   
  D. Atzberger et al.  
   
  that makes use on the cartographic metaphor [21]. Based on a hierarchical graph layout algorithm, files are positioned in a 2D reference plane and are aggregated to states, countries, or continents according to the architectural structure of the software project. CodeSurveyor shares characterisitcs of treemaps that use non-rectangular shapes [40]. Schreiber et al.proposed ISLANDVIZ, another approach using the island metaphor. It enables users to interactively explore a software system in virtual reality and augmented reality alike [42]. Treemap Layouts. Another widely used class of layout algorithms in the software visualization domain are Treemaps. Treemaps are inherently capable of reflecting the typically hierarchical structure of software projects [40, 41]. Given their 2D layout, they can be extended into the third dimension, thus resulting in a 2.5D visualization. Besides height, color, and texture 2.5D treemaps offer additional visual variables for additional information display [31–33]. An approach that refer to natural phenomena, e.g., fire or rain, for visualizing software evolution in a 2.5D treemap was proposed by W¨urfel et al. [54]. It is worth mentioning that the class of treemap algorithms includes a large number of shapes other than just rectangles or Voronoi cells [41]. The Tree Metaphor. In our considerations, we use the tree metaphor since trees offer a variety of visual variables. Kleiner and Hartigan were the first to propose a mapping of multivariate data to a 2D tree [25]. Based on hierarchical clustering of variables, for each data point, the geometry of each tree, i.e., the thickness of a branch, the angle between branches, and their orientation, is derived from the data attributes. Erra presented an approach to visualize object-oriented systems using the tree metaphor, thus resulting in a forest [16, 17]. For every revision each file is depicted as a tree, whose visual variables reflect properties of the source code, i.e., software metrics, in a predefined way. Later Atzberger et al.applied the tree metaphor for software visualization tasks [2]. The main difference between Atzberger et al.and Erra et al.is the placement of the trees in the reference plane. The approach of Atzberger et al.is not restricted to the case of object-oriented programming languages, as it only uses the natural language in source code. Furthermore, the system of Atzberger et al.allows users to specify custom mappings of data and visual attributes. The authors do not focus on rendering realistic trees but rather apply handcrafted models for their approach. Kleiberg et al.use the tree metaphor for visualizing an entire set of hierarchically structured data. This approach differs from most other approaches because each tree does not represent a single document [24]. The City Metaphor. This metaphor is probably the most popular use of 3D glyphs in the software visualization domain. In their approach CodeCity, Wettel and Lanza applied a city metaphor for exploring object-oriented software projects using a 2.5D visualization, referring to real-world cities [52, 53]. Each class is represented by a building and packages are grouped into districts that are placed according to a modified treemap algorithm. By mapping software metrics onto the visual variables of the cuboids, e.g., its height and the size of the base, a user can get an overview of the structure of a project. Steinbr¨uckner adopted the idea of the city metaphor and introduced a novel layout approach, based on a hierarchical street system, that captures a project development over time [45, 46].  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  167  
   
  Other Approaches. Beck proposed a mapping between software metrics of objectoriented software projects and geometric properties of figurative feathers, e.g., its size, shape, and texture [5]. Their approach Software Feathers is intended to support developers in getting a first overview of a software project and to deticting interesting code entities. Fernandez et al.extended an approach by Lewis et al. [30], that generates 2D glyphs in order to identify classes with the same dependencies and similar set of methods [19]. Chuah and Eick proposed the three glyph visualizations InfoBUG, Time-wheel, and 3D-wheel for the task of visualizing project-oriented software data [9].  
   
  3 Glyph Placement in a 2D Reference Space According to Ward et al.there are three general strategies for placing glyphs [51]: 1. Uniform. All glyphs are placed in equidistant positions. 2. Structure-Driven. The positions of the glyphs arise from the structure of the data set, e.g., a hierarchy or graph structure within the data. 3. Data-Driven. The positions of the glyphs are determined by a set of data attributes. In this section, we present the layout approach presented by Atzberger et al. [2]. In a semantic layout, the relative position between two points on the 2D reference plane should reflect the semantic relatedness between the corresponding data points, i.e., source code files of a software project. For this, the assumption is made that the semantic similarity between source code files is reflected in a shared vocabulary and can therefore be captured using techniques from the NLP domain. The approach for placing 3D glyphs on a plane has three stages. First, the source code files of a software project are preprocessed to get rid of words that carry no semantic information. In the second step LDA is applied to the corpus of preprocessed documents to model each source code file as a high-dimensional vector. Lastly, in the third step, MDS is applied to reduce the vectors in their dimensionality. 3.1 Data Preprocessing In our considerations each source code file of a project is viewed as a single document, the set of all documents is called the corpus, and the set of all words in the corpus forms the vocabulary. We neglect the ordering of the words within a document and only store their frequencies in the so-called term-document-matrix. In order to remove words from the vocabulary that carry no semantic information, e.g., stopwords of the natural language, it is necessary to perform several preprocessing steps before applying topic models. Moreover, source code often follows naming conventions, e.g., the Camel Case convention, thus requiring additional preprocessing steps [7]. In our experiments, the following sequence of preprocessing steps has turned out to produce a usable vocabulary [2]. 1. Removal of Non-text Symbols: All special characters such as dots and semicolons are replaced with white spaces to avoid accidental connection of words not meant to be combined. This includes the splitting of identifier names, e.g., the word foo.bar gets split into foo and bar.  
   
  168  
   
  D. Atzberger et al.  
   
  2. Split of Words: Identifiers are split according to delimiters and the Camel Case convention, e.g., FooBar is split into foo and bar, and stripped from redundant white space subsequently. 3. Removal of Stop Words: Stop words based on natural language and programming language keywords are removed as they carry no semantic content. Additionally, we filter the input based on a hand-crafted list comprising domain-specific stop words, e.g., data types and type abstractions. 4. Lemmatization: To avoid grammatical diversions, all words are reduced to their basic form, e.g., said and saying are reduced to say. After applying the four preprocessing steps, we store each document as a BOW. For the remainder of this paper, we refer to a documents’ BOW after preprocessing as a document. 3.2  
   
  Latent Dirichlet Allocation on Source Code  
   
  Topic models are a widely used class of techniques for investigating collections of documents, e.g., for knowledge comprehension or classification tasks [1]. For software engineering tasks, LDA proposed by Blei et al. [6], is the most common technique [7]. Assuming a set of documents D = {d1 , . . . , dm }, the so-called corpus, LDA extracts latent topics ϕ1 , . . . , ϕK , underlying the corpus, where the number of topics K is a hyperparameter of the model. As topics are given as multinomial distributions over the vocabulary V, which contains the terms of the corpus D, the “concept” underlying a topic, in most cases can be derived from its most probable words. Table 1 shows an example for three topics with their ten most probable words extracted from the Bitcoin project [2]. From the most probable words, we suggest that topic #1 deals with the internal logic of cryptocurrency. Words like “thread”, “time”, “queue”, and “callback” are related to the general concept of parallel processing in C++, and topic #3 is concerned about the UI. Besides the topics, LDA learns representations θ1 , . . . , θm of the documents as distributions over the topics. The distributions θ1 , . . . , θm therefore capture the semantic structure of the documents and allow a comparison between them on a semantic level. LDA makes the assumption of an underlying generative process, which is given by 1. For each document d in the corpus D choose a distribution over topics θ ∼ Dirichlet(α) 2. For each word w in d (a) Choose a topic z ∼ Multinomial(θ) (b) Choose the word w according to the probability p(w|z, β). The parameter α = (α1 , . . . , αK ), where 0 < αi for all 1 ≤ i ≤ K, is the Dirichlet prior for the document-topic distribution. Its meaning is best understood, when written as the product α = ac · m of its concentration parameter ac ∈ R and its base measure m = (m1 , . . . , mk ), whose components sum up to 1. The case of a base measure m = (1/K , . . . , 1/K ) is denoted as symmetrical Dirichlet prior. For small values of ac , the Dirichlet distribution would favor points in the simplex that are close to one edge, i.e., LDA would try to describe a document with a minimum of topics. The larger the value  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  169  
   
  Table 1. Three exemplary topics extracted from Bitcoin Core(Source code taken from github.com/bitcoin/bitcoin) source code with K = 50 and the Dirichlet priors set to their default values. Topic #1  
   
  Topic #2  
   
  Topic #3  
   
  Term  
   
  Prob. Term  
   
  Prob. Term  
   
  Std  
   
  0.070 Thread  
   
  0.132 Address 0.115  
   
  Prob.  
   
  Transaction 0.031 Time  
   
  0.070 Model  
   
  0.108  
   
  Fee  
   
  0.027 Queue  
   
  0.064 Table  
   
  0.065  
   
  Tx  
   
  0.026 Std  
   
  0.054 Label  
   
  0.051  
   
  Ban  
   
  0.024 Callback  
   
  0.040 Qt  
   
  0.033  
   
  Str  
   
  0.023 Run  
   
  0.037 Index  
   
  0.030  
   
  Handler  
   
  0.016 Call  
   
  0.025 Dialog  
   
  0.024  
   
  Output  
   
  0.016 Mutex  
   
  0.021 Column 0.024  
   
  Bitcoin  
   
  0.015 Scheduler 0.020 Ui  
   
  0.021  
   
  Reason  
   
  0.015 Wait  
   
  0.019  
   
  0.018 Role  
   
  of ac the more likely that LDA is to fit all topics a non-zero probability for a document. Analogous, those considerations hold true for the Dirichlet prior β = (β1 , . . . , βN ), 0 < βi for 1 ≤ i ≤ N for the topic-term distribution, where N denotes the size of the vocabulary V. Since inference for LDA is intractable, approximation techniques need to be taken into account [6]. Among the most widely used are Collapsed Gibbs Sampling (CGS) [20], Variational Bayes (VB) [6], and its online version (OVB) [22]. 3.3 Multidimensional Scaling LDA applied on the source code files leads to a description of each document as a highdimensional vector, whose components represent the expression in the respective topic. Therefore using a similarity measure, e.g., the Jensen-Shannon divergence, two documents can be compared on a semantic level, thus forming structures, e.g., clusters and outliers, in the set of all documents. Linstead et al.used this notion of similarity and applied the dimension reduction MDS on the documents to generate a two-dimensional layout. However, this approach implicitly assumes that all extracted topics are “equally different” to each other and neglects the fact that the topics, viewed as distributions over the vocabulary, can be compared among each other themselves. The layout approach by Atzberger et al.addresses this issue and applies the dimension reduction technique MDS on the topics ϕ1 , . . . , ϕK , which can be compared to each other using the JensenShannon distance [2, 43]. This results in points ϕ¯1 , . . . , ϕ¯K ∈ R2 , whose Euclidean distance reflects the Jensen-Shannon-distance of the high-dimensional topics. A document d, given by its document-topic distribution θ = (θ(1) , . . . , θ(K) ), is then represented as ¯ precisely the convex linear combination d, d¯ =  
   
  K  j=1  
   
  θ(j) φ¯j .  
   
  (1)  
   
  170  
   
  D. Atzberger et al.  
   
  A document with a strong expression in a topic is subsequently placed next to that topic, taking the similarity of topics into account.  
   
  4 Visual Attributes of 3D Glyphs and Use Cases In Sect. 2, we summarized popular visualization metaphors based on 3D glyphs in the Software Visualization domain. In this section, we review (1) the city metaphor and (2) the forest metaphor together with (3) the island metaphor. We categorize our visualization as A3 ⊕ R2 , i.e., three-dimensional primitives placed on a two-dimensional reference space pl [2, 15]. We further present a novel idea of placing avatars into the visualization, thus indicating developer activities. 4.1  
   
  City Metaphor  
   
  The city metaphor, as proposed by Wettel et al., owes its name its visual similarity to modern cities, caused by displaying software files as cuboids [52]. The motivation for choosing this metaphor was to support a user navigating through a software system by adopting a well-known metaphor from everyday life. All existing approaches rely on a layout that captures the hierarchical structure of a software project, thus focusing on the architectural aspect of a project. Therefore, the approaches do not support program comprehension tasks related to the underlying semantic concepts of a software project. One advantage of the city metaphor is that they offer various visual variables. Examples proposed in the literature are: – Wettel et al.mapped the number of methods to height and the number of attributes to the cuboids’ horizontal extent [52]. – Steinbr¨uckner et al.used stacked cylinders, where each cylinder displays the coding activity of a single developer made on a file [46]. – Limberger et al.present various advanced visual variables, e.g., sketchiness or transparency, for cuboids that can be applied for the city metaphor [32]. Recently, Limberger et al.investigated the use of animations for displaying the evolution of source code artifacts measured between two revisions [31]. We adapt the idea of the city metaphor in our considerations by representing each file of a software project as a cylinder. Figure 2 shows a simple example for the project globjects, where the height and the color of the cylinders are used as visual variables. The height of the cylinder displays the Lines-of-Code (LOC) of the respective file, thus revealing the impact of underlying concepts for the software project. The color displays the percentage of commented lines in the respective file using a sequential color scheme, which allows drawing conclusions about the code quality in relation to a concept. In most cases, large files are grouped nearby each other, thus indicating their underlying concepts seem to have a large impact on the size of the project in terms of LOC. Furthermore, large files often harbor the risk of non-sufficient documentation in the form of comments. This observation could motivate the project maintainer to focus on that concept, distributed over the individual files, in a future refactoring process.  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  171  
   
  Fig. 2. Example for cylinders placed on a 2D reference plane. Each cylinder represents a single source code file of the project globjects4 . The height displays the LOC of each file, and the color the percentage of commented lines.  
   
  4.2 Forest Metaphor As shown in Sect. 2, the idea of forest islands is two widely used metaphors in the Software Visualization domain, especially as they offer a grouping of files according to some “relatedness”. Furthermore, islands and forests are real-world structures, thus making them suitable for creating a mental map for the user to support program comprehension tasks. Our presentation here follows the preceding work of Atzberger et al.closely [2]. We focus our discussion to the following set of visual variables: – The tree height, e.g., for depicting the size of a file in terms of LOC. – The color of the tree crown, e.g., displaying software metrics related to the quality or complexity of the respective file. – The tree type, e.g., to distinguish the source code files by their file endings. – The health status of a tree, e.g., for displaying failed tests. – Chopped trees, e.g., for visualizing deleted files. Figure 3 shows two sets of tree glyphs and demonstrates the visual attributes they inherit [2]. When visualizing a software project using the tree metaphor, we first compute the position of each file as presented in Sect. 3, i.e., each file is represented as a single tree, thus forming an entire forest. Then for each point, the value of a height field is computed as presented in [28]. This has the effect that dense regions are placed on higher ground than regions with fewer trees, but still assuring that single trees stand on a terrain of 30%−50% of the maximal height. Furthermore, we integrated the possibility to configure the water height, which can be seen as a height filtering technique.  
   
  172  
   
  D. Atzberger et al.  
   
  Fig. 3. Examples of two sets of tree glyphs: the top row inherits the visual variables size, color, and health status. The bottom row shows trees of different types. Both models were purchased on SketchFab: “HandPainted Pine Pack” by ZugZug and “Low Poly Nature Pack” by NONE.  
   
  Fig. 4. Software Forest of the notepad-plus-plus project. The tooltip shows the underlying topic distribution of the file represented by the selected tree (highlighted). All trees with the same dominant topic are highlighted.  
   
  Figure 4 shows the result of applying our approach on the notepad-plus-plus project5 based on the set of pine trees shown in Fig. 3. The underlying visual mapping aimed to represent the document-topic distribution of each file. The tree type is chosen according to the main topic of each file, i.e., the topic that has the highest probability in the file, e.g., documents with the main topic “User Interface Code” are displayed as green pine trees. All trees of the same color are highlighted when hovering over a tree. Here we want to mention that we manually labeled the topics for the project by examining their most probable words. In general, this is a time-consuming task that needs to be done manually [37]. 5  
   
  Source code taken from https://github.com/notepad-plus-plus/notepad-plus-plus.  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  173  
   
  Our next application demonstrates the use of Software Forest for the bitcoin project6 . In Sect. 3 we showed three interesting topics extracted from the source code by applying LDA with K = 50 topics and default values for the Dirichlet priors. We map the topic with the highest impact for a document onto the tree type. As the number of tree types is usually limited, this visualization approach does not scale for a large number of topics. The bitcoin project is mainly written in C++, C, and Python. We ignore the other source code files and map the programming language onto the color of the tree. The height of the tree captures the LOC for the respective file (Fig. 5).  
   
  Fig. 5. Part of the software forest for the bitcoin project.  
   
  4.3 Developer Avatars Our last visualization metaphor uses 3D glyphs displaying people for showing coding activities within a software project. Each developer or team is assigned an avatar whose position shows the source code contributed to within a given timespan. One question that could be addressed with this visualization would be the assignment of suitable developers when a bug related to a concept or a file would occur. Figure 6 shows an 6  
   
  Source code taken from github.com/bitcoin/bitcoin.  
   
  174  
   
  D. Atzberger et al.  
   
  example for placing avatars and cylinders on an island for the example of the globjects project. The color of the cylinders displays a complexity metric, whereas each figure represents a single developer. The avatars are placed nearby the file on which they contributed the most. The large red cylinder indicates a considerable risk in a file, as its complexity is very high. Moreover, we can deduce from the visualization that an avatar next to a cylinder might have the required knowledge to maintain or review the risk. In our example, each team member can choose among a set of given figures, which he would favor as a representation, however the idea of mapping data to glyphs displaying developers has been presented by Atzberger et al.for the task of displaying data related to the skills and expertise of developers [3]. However when using humanlooking glyphs for displaying developer related data, various visual attributes become critical and should be considered very carefully. One idea to overcome this issue, would be the use of abstract forms, which only reminds on human faces, as initially presented as the popular Chernoff faces [8].  
   
  Fig. 6. Examples using avatars positioned in relation to their coding activities.  
   
  5 System Design and Implementation Details In this section, we present implementation details with respect to the layout computation and the rendering of our visualization prototype, i.e., the tools and libraries we choose for generating a 2D layout based on the vocabulary in the source code files and the visualization mapping and rendering, respectively. We further describe the supported interaction techniques for enabling a user to explore a software project. We use a separation of the layout computation and the interactive rendering component, where the layout computation component computes a visualization dataset from a source code repository that is used as an input of the rendering component (see Fig. 7).  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  175  
   
  5.1 Layout Computation Our approach follows the implementation presented by Atzberger et al.in their earlier work about the Software Forest [2]. For preprocessing, the natural language in the source code documents, we apply the nltk7 library for obtaining a list of stopwords for the English language. We further use spacy8 for lemmatization. We used the LDA implementation provided by the library Gensim9 . Gensim offers an LDA implementation based on the original implementation by Blei et al. [6] as well as its online version introduced by Hofman et al. [22]. The implementation of MDS is taken from the Machine learning library scikit-learn10 . The result of the layout computation is a 2D layout that is merged with other static source code metrics into a CSV file that is later used for input to the rendering component.  
   
  Fig. 7. The data processing pipeline to compute the semantic layouts. The rendering component composes the layouts and glyph atlases based on the visual mapping.  
   
  5.2 Rendering The rendering component is an extension to a scatter plot renderer, written in TypeScript and WebGL [49]. The main dependency is the open-source framework webgl-operate11 , which handles canvas and WebGL context management, as well as labeling primitives and glTF scene loading and rendering. The 3D glyph models are integrated into the rendering component by means of a glyph atlas and a configuration file. The basic designs for our more advanced visualization metaphors, i.e., trees and people, are taken from SketchFab12 . The 3D glyph atlases are constructed manually using Blender13 , but 7 8 9 10 11 12 13  
   
  https://www.nltk.org/. https://spacy.io/. https://radimrehurek.com/gensim/. https://scikit-learn.org/stable/. https://webgl-operate.org/. https://sketchfab.com/feed. https://www.blender.org/.  
   
  176  
   
  D. Atzberger et al.  
   
  every other 3D editor with glTF is a feasible alternative. Together with a glyph atlas, we have to specify its objects within a JSON configuration file (an example is given in Listing 1). This rendering component is embedded into a Web page with further GUI elements for the visual mapping and direct interaction techniques on the canvas to support navigation in the semantic software map. 5.3  
   
  User Interaction  
   
  As basic interaction techniques, a user can choose mappings from data to visual variables of the selected glyphs, navigate through the 2.5D visualization, and retrieve details on demand displayed by tooltips by rotating and zooming. Figure 8 shows the user interface of our web-based implementation prototype. Our system supports basic interaction techniques, e.g., rotating and zoom. Furthermore, a tooltip displaying the entire entries of the respective data point contained in the CSV file shows up. As our approach highly depends on LDA, we highlight all trees with the same dominant topic as the selected one when hovering over it. Furthermore, our system allows the user to define a custom mapping between data columns and the visual variables provided by the selected model. For each model, we ensure that at least the type, the height, and the color are available as visual metaphors. By adjusting the effect of the variable tree size, which depends on a data attribute, the user can further interactively explore the effect of data variables for the source code files. In order to enhance the rendering with visual cues and more fidelity, the user can modify rendering details, e.g., by toggling Anti-Aliasing or soft shadows. { "modelFile": "PeopleCylinders2.glb", "attributes": [ "color" ], "modelScale": 1.0, "types": [ { "name": "Cylinder", "baseModel": "Cylinder_Ax.001", "variants": [ { "name": "Cylinder_Ax.001", "color": { "name": "Cylinder_Ax.002", "color": { "name": "Cylinder_Ax.003", "color": { "name": "Cylinder_Ax.004", "color": { "name": "Cylinder_Ax.005", "color": ] }, { "name": "People", "baseModel": "Person0", "variants": [ { "name": "Person0", "color": 1.0 }, { "name": "Person1", "color": 0.5 }, { "name": "Person2", "color": 0.0 } ] } ]  
   
  1.0 }, 0.75 }, 0.5 }, 0.25 }, 0.0 }  
   
  }  
   
  Listing 1: An example JSON configuration of a mapping from attribute values to 3D model. Two types of glyph categories are defined, namely “Cylinder” and “People”. This configuration can be used to display source code files as cylinders and developers as people-looking glyphs in the same semantic software map.  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  177  
   
  6 Conclusion 6.1 Discussion Software Visualization techniques support users in program comprehension tasks by displaying images based on data related to software artifacts. In many cases, the questions are about the semantics of a software project, e.g., for locating concepts or functionalities in the source code. Our previous work used a tree metaphor and a semantic layout to create map-like visualizations to support users in program comprehension tasks related to the semantics. In this extended work, we detailed how the topic model LDA and the dimension reduction technique MDS are applied for generating a layout for capturing the semantic relatedness between source code files. We presented mappings between quantitative and qualitative aspects of source code files, e.g., source code metrics or file types, and visual variables of selected 3D glyphs for concrete program comprehension tasks. We applied the city, the forest, and the island metaphor for our use-cases. Our web-based visualization can visualize large data sets and provides a significant degree of freedom to the user by supporting various interaction techniques.  
   
  Fig. 8. User interface of our web-based implementation. Besides the full mapping configuration, rendering parameters can be adjusted by the user.  
   
  Though LDA has shown great success in modeling the concepts inherent in a software project [38], the possibility of visual indication of misleading or irrelevant relations must not be neglected. For example, the positioning of a document in 2D is not unique as it arises from a convex linear combination of the reduced topic-word distributions. Therefore, two documents with totally different document-topic distributions may be placed next to one another. In practice, however, the choice of the Dirichlet prior α forces LDA to favor document-topic distributions with only a few topics.  
   
  178  
   
  D. Atzberger et al.  
   
  For our experiments, we created visualizations for the two Open Source projects globjects and bitcoin. Both can be seen as representatives for mid-sized software projects. However, a software visualization should also provide insights into large projects as the need for program comprehension increases with project size. Figure 9 shows a 2.5D visualization for the Machine Learning framework TensorFlow14 that comprises a total of 13 154 files, where each file is represented by a cylinder with the same visual mapping as presented in Sect. 4. The data volume makes it difficult to maintain interactive framerates on average machines. Though the island (without glyphs) is a map-like visualization in itself, its capabilities are limited, as its number of visual variables is limited. Our visual mappings were motivated by common questions in a software development process. However, we do not provide empirical measurements, e.g., provided by a user study, that would demonstrate the actual benefit of our visualizations for users. It is yet unclear whether the choice of visual variables and glyphs is appropriate for users in real-world settings. The ideas presented in Sect. 4 so far only provide a starting point for future investigations. Nevertheless, the given examples of map configurations indicate that our 2.5D software visualization is suitable for depicting aspects of and relations within software data, supports finding information and gaining knowledge, and possibly synchronizes the mental representation of selected software aspects with the actual data.  
   
  Fig. 9. Visualization of the tensorflow dataset using cylinders. The dataset contains 13 154 files, which shows the limitation in discernible data items of our technique.  
   
  14  
   
  Source code taken from https://github.com/tensorflow/tensorflow.  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  179  
   
  6.2 Future Work Concerning our visualization approach, various possibilities for future work exist. Most importantly, the effectiveness of our approach and the visual mappings should be evaluated in a systematic user study, e.g., to identify visual mappings best-suited for program comprehension tasks in an industrial setting with developers and project managers. Furthermore, we can imagine including more advanced visual mappings, especially in the case of the city metaphor [32]. Our glyph placement strategy is an example of a data-driven approach [50] for that distortion techniques should be considered. Our examples indicate that an increased glyph height tends to increase visual clutter. Therefore, distortion strategies as presented in [50] seem well-suited for mitigation. A modern approach for removing distortion in 2.5D visualizations was presented in [48] and should be applicable for our case. Furthermore, quality metrics associated with the results of dimension reduction techniques can help measure whether the dimension reduction was able to capture local and global structures within a dataset. As our visualization approach mainly builds upon the semantic layout, we plan to implement a feature in our framework that generates the layout for a given software project automatically by evaluating various dimension reduction techniques with respect to selected quality metrics, as presented in [18]. Acknowledgements. This work is part of the “Software-DNA” project, which is funded by the European Regional Development Fund (ERDF or EFRE in German) and the State of Brandenburg (ILB). This work is part of the KMU project “KnowhowAnalyzer” (F¨orderkennzeichen 01IS20088B), which is funded by the German Ministry for Education and Research (Bundesministerium f¨ur Bildung und Forschung). We further thank the students Maximilian S¨ochting and Merlin de la Haye for their work during their master’s project at the Hasso Plattner Institute during the summer term 2020.  
   
  References 1. Aggarwal, C.C., Zhai, C.: Mining text data. Springer, New York (2012). https://doi.org/10. 1007/978-1-4614-3223-4 2. Atzberger, D., et al.: Software forest: a visualization of semantic similarities in source code using a tree metaphor. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 3 IVAPP, IVAPP 2021, pp. 112–122. INSTICC, SciTePress (2021). https://doi.org/10.5220/ 0010267601120122 3. Atzberger, D., et al.: Visualization of knowledge distribution across development teams using 2.5d semantic software maps. In: Proceedings of 13th International Conference on Information Visualization Theory and Applications, IVAPP 2022, INSTICC, SciTePress (2022) 4. Atzberger, D., Scheibel, W., Limberger, D., D¨ollner, J.: Software galaxies: displaying coding activities using a galaxy metaphor. In: Proceedings of 14th International Symposium on Visual Information Communication and Interaction, VINCI 2021, pp. 18:1–2. ACM (2021). https://doi.org/10.1145/3481549.3481573 5. Beck, F.: Software feathers - figurative visualization of software metrics. In: Proceedings of 5th International Conference on Information Visualization Theory and Applications - Volume 1: IVAPP, IVAPP 2014, pp. 5–16. INSTICC, SciTePress (2014). https://doi.org/10.5220/ 0004650100050016  
   
  180  
   
  D. Atzberger et al.  
   
  6. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn. Res. 3, 993– 1022 (2003). https://doi.org/10.5555/944919.944937 7. Chen, T.-H., Thomas, S.W., Hassan, A.E.: A survey on the use of topic models when mining software repositories. Empirical Softw. Eng. 21(5), 1843–1919 (2015). https://doi.org/10. 1007/s10664-015-9402-8 8. Chernoff, H.: The use of faces to represent points in k-dimensional space graphically. J. Am. Stat. Assoc. 68(342), 361–368 (1973). https://doi.org/10.1080/01621459.1973.10482434 9. Chuah, M., Eick, S.: Glyphs for software visualization. In: Proceedings of 5th International Workshop on Program Comprehension, IWPC 1997, pp. 183–191. IEEE (1997). https://doi. org/10.1109/WPC.1997.601291 10. Cornelissen, B., Zaidman, A., van Deursen, A.: A controlled experiment for program comprehension through trace visualization. IEEE Trans. Softw. Eng. 37(3), 341–355 (2011). https:// doi.org/10.1109/TSE.2010.47 11. Cornelissen, B., Zaidman, A., Holten, D., Moonen, L., van Deursen, A., van Wijk, J.J.: Execution trace analysis through massive sequence and circular bundle views. J. Syst. Softw. 81(12), 2252–2268 (2008). https://doi.org/10.1016/j.jss.2008.02.068 12. Cox, M.A.A., Cox, T.F.: Multidimensional scaling. In: Handbook of Data Visualization, pp. 315–347. Springer, Berlin (2008). https://doi.org/10.1007/978-3-540-33037-0 14 13. Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R.: Indexing by latent semantic analysis. J. Am. Soc. Inf. Sci. 41(6), 391–407 (1990). https://doi.org/10.1002/ (SICI)1097-4571(199009)41:6391::AID-ASI13.0.CO;2-9 14. Dehaghani, S.M.H., Hajrahimi, N.: Which factors affect software projects maintenance cost more? Acta Informatica Medica 21(1), 63–66 (2013). https://doi.org/10.5455/aim.2012.21. 63-66 15. D¨ubel, S., R¨ohlig, M., Schumann, H., Trapp, M.: 2d and 3d presentation of spatial data: a systematic review. In: Proceedings of VIS International Workshop on 3DVis, 3DVis ’14, pp. 11–18. IEEE (2014). https://doi.org/10.1109/3DVis.2014.7160094 16. Erra, U., Scanniello, G.: Towards the visualization of software systems as 3d forests: the CodeTrees environment. In: Proceedings of 27th Annual ACM Symposium on Applied Computing, SAC 2012, pp. 981–988. ACM (2012). https://doi.org/10.1145/2245276.2245467 17. Erra, U., Scanniello, G., Capece, N.: Visualizing the evolution of software systems using the forest metaphor. In: Proceedings of 16th International Conference on Information Visualisation, iV 2012, pp. 87–92 (2012). https://doi.org/10.1109/IV.2012.25 18. Espadoto, M., Martins, R.M., Kerren, A., Hirata, N.S.T., Telea, A.C.: Toward a quantitative survey of dimension reduction techniques. Trans. Vis. Comput. Graph. 27(3), 2153–2173 (2021). https://doi.org/10.1109/TVCG.2019.2944182 19. Fernandez, I., Bergel, A., Alcocer, J.P.S., Infante, A., Gˆırba, T.: Glyph-based software component identification. In: Proceedings of 24th International Conference on Program Comprehension, ICPC 2016, pp. 1–10. IEEE (2016). https://doi.org/10.1109/ICPC.2016.7503713 20. Griffiths, T.L., Steyvers, M.: Finding scientific topics. Proc. Natl. Acad. Sci. 101, 5228–5235 (2004). https://doi.org/10.1073/pnas.0307752101 21. Hawes, N., Marshall, S., Anslow, C.: CodeSurveyor: Mapping large-scale software to aid in code comprehension. In: Proceedings of 3rd Working Conference on Software Visualization, VISSOFT 2015, pp. 96–105. IEEE (2015). https://doi.org/10.1109/VISSOFT.2015.7332419 22. Hoffman, M., Bach, F., Blei, D.: Online learning for latent dirichlet allocation. In: Advances in Neural Information Processing Systems. NIPS 2010, vol. 23, pp. 856–864 (2010) 23. Systems and software engineering-Vocabulary: Standard. International Organization for Standardization (2017). https://doi.org/10.1109/IEEESTD.2017.8016712 24. Kleiberg, E., van de Wetering, H., van Wijk, J.J.: Botanical visualization of huge hierarchies. In: Proceedings of Symposium on Information Visualization, INFOVIS 2001, pp. 87–87. IEEE (2001). https://doi.org/10.1109/INFVIS.2001.963285  
   
  Visualization of Source Code Similarity Using 2.5D Semantic Software Maps  
   
  181  
   
  25. Kleiner, B., Hartigan, J.A.: Representing points in many dimensions by trees and castles. J. Am. Stat. Assoc. 76(374), 260–269 (1981). https://doi.org/10.1080/01621459.1981. 10477638 26. Kohonen, T.: Exploration of very large databases by self-organizing maps. In: Proceedings of International Conference on Neural Networks, ICNN 1997, pp. 1–6. IEEE (1997). https:// doi.org/10.1109/ICNN.1997.611622 27. Kuhn, A., Loretan, P., Nierstrasz, O.: Consistent layout for thematic software maps. In: Proceedings of 15th Working Conference on Reverse Engineering, WCRE 2008, pp. 209–218. IEEE (2008). https://doi.org/10.1109/WCRE.2008.45 28. Kuhn, A., Erni, D., Loretan, P., Nierstrasz, O.: Software cartography: thematic software visualization with consistent layout. J. Softw. Maintenance Evol. Res. Pract. 22(3), 191–210 (2010) 29. Lanza, M.: The evolution matrix: recovering software evolution using software visualization techniques. In: Proceedings of 4th International Workshop on Principles of Software Evolution, IWPSE 2001, pp. 37–42. ACM (2001). https://doi.org/10.1145/602461.602467 30. Lewis, J.P., Rosenholtz, R., Fong, N., Neumann, U.: VisualIDs: automatic distinctive icons for desktop interfaces. Trans. Graph. 23(3), 416–423 (2004). https://doi.org/10.1145/ 1015706.1015739 31. Limberger, D., Scheibel, W., Dieken, J., D¨ollner, J.: Visualization of data changes in 2.5d treemaps using procedural textures and animated transitions. In: Proceedings of 14th International Symposium on Visual Information Communication and Interaction, VINCI 2021, pp. 6:1–5. ACM (2021). https://doi.org/10.1145/3481549.3481570 32. Limberger, D., Scheibel, W., D¨ollner, J., Trapp, M.: Advanced visual metaphors and techniques for software maps. In: Proceedings of 12th International Symposium on Visual Information Communication and Interaction, VINCI 2019, pp. 11:1–8. ACM (2019). https://doi. org/10.1145/3356422.3356444 33. Limberger, D., Trapp, M., D¨ollner, J.: Depicting uncertainty in 2.5d treemaps. In: Proceedings of 13th International Symposium on Visual Information Communication and Interaction, VINCI 2020, pp. 28:1–2. ACM (2020). https://doi.org/10.1145/3430036.3432753 34. Linstead, E., Rigor, P., Bajracharya, S., Lopes, C., Baldi, P.: Mining eclipse developer contributions via author-topic models. In: Proceedings of 4th International Workshop on Mining Software Repositories, MSR 2007, pp. 30:1–4. IEEE (2007). https://doi.org/10.1109/MSR. 2007.20 35. Linstead, E., Bajracharya, S., Ngo, T., Rigor, P., Lopes, C., Baldi, P.: Sourcerer: mining and searching internet-scale software repositories. Data Min. Knowl. Disc. 18(2), 300–336 (2009). https://doi.org/10.1007/s10618-008-0118-x 36. Malony, A., Hammerslag, D., Jablonowski, D.: Traceview: a trace visualization tool. IEEE Softw. 8(5), 19–28 (1991). https://doi.org/10.1109/52.84213 37. Markovtsev, V., Kant, E.: Topic modeling of public repositories at scale using names in source code. arXiv CoRR cs.PL (2017). https://arxiv.org/abs/1704.00135 38. Maskeri, G., Sarkar, S., Heafield, K.: Mining business topics in source code using latent dirichlet allocation. In: Proceedings of 1st India Software Engineering Conference, ISEC 2008, pp. 113–120. ACM (2008). https://doi.org/10.1145/1342211.1342234 39. Rosen-Zvi, M., Griffiths, T., Steyvers, M., Smyth, P.: The author-topic model for authors and documents. In: Proceedings of 20th Conference on Uncertainty in Artificial Intelligence, UAI 2004, pp. 487–494. AUAI Press (2004). https://doi.org/10.5555/1036843.1036902 40. Scheibel, W., Limberger, D., D¨ollner, J.: Survey of treemap layout algorithms. In: Proceedings of 13th International Symposium on Visual Information Communication and Interaction, VINCI 2020, pp. 1:1–9. ACM (2020). https://doi.org/10.1145/3430036.3430041  
   
  182  
   
  D. Atzberger et al.  
   
  41. Scheibel, W., Trapp, M., Limberger, D., D¨ollner, J.: A taxonomy of treemap visualization techniques. In: Proceedings of 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 3: IVAPP, IVAPP 2020, pp. 273–280. INSTICC, SciTePress (2020). https://doi.org/10.5220/0009153902730280 42. Schreiber, A., Misiak, M.: Visualizing software architectures in virtual reality with an island metaphor. In: Chen, J.Y.C., Fragomeni, G. (eds.) VAMR 2018. LNCS, vol. 10909, pp. 168– 182. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-91581-4 13 43. Sievert, C., Shirley, K.: LDAvis: a method for visualizing and interpreting topics. In: Proceedings of Workshop on Interactive Language Learning, Visualization, and Interfaces, pp. 63–70. ACL (2014). https://doi.org/10.3115/v1/W14-3110 44. Skupin, A.: The world of geography: visualizing a knowledge domain with cartographic means. Proc. Natl. Acad. Sci. 101(suppl 1), 5274–5278 (2004). https://doi.org/10.1073/pnas. 0307654100 45. Steinbr¨uckner, F., Lewerentz, C.: Representing development history in software cities. In: Proceedings of 5th International Symposium on Software Visualization, SOFTVIS 2010, pp. 193–202. ACM (2010). https://doi.org/10.1145/1879211.1879239 46. Steinbr¨uckner, F., Lewerentz, C.: Understanding software evolution with software cities. Inf. Visual. 12(2), 200–216 (2013). https://doi.org/10.1177/1473871612438785 ˘ ep´anek, A.: Procedurally generated landscape as a visualization of C# code. Technical 47. St˘ Report, Masaryk University, Faculty of Informatics (2020). bachelor’s Thesis 48. Vollmer, J.O., D¨ollner, J.: 2.5d dust & magnet visualization for large multivariate data. In: Proceedings of 13th International Symposium on Visual Information Communication and Interaction, VINCI 2020, pp. 21:1–8. ACM (2020). https://doi.org/10.1145/3430036. 3430045 49. Wagner, L., Scheibel, W., Limberger, D., Trapp, M., D¨ollner, J.: A framework for interactive exploration of clusters in massive data using 3d scatter plots and webgl. In: Proceedings of 25th International Conference on 3D Web Technology, Web3D 2020, pp. 31:1–2. ACM (2020). https://doi.org/10.1145/3424616.3424730 50. Ward, M.O.: A taxonomy of glyph placement strategies for multidimensional data visualization. Inf. Visual. 1(3–4), 194–210 (2002) 51. Ward, M.O., Grinstein, G., Keim, D.: Interactive Data Visualization: Foundations, Techniques, and Applications. CRC Press, Boca Raton (2010) 52. Wettel, R., Lanza, M.: Visualizing software systems as cities. In: Proceedings of International Workshop on Visualizing Software for Understanding and Analysis, VISSOFT 2007, pp. 92– 99. IEEE (2007). https://doi.org/10.1109/VISSOF.2007.4290706 53. Wettel, R., Lanza, M.: CodeCity: 3d visualization of large-scale software. In: Companion of the 30th International Conference on Software Engineering, ICSE Companion 2008, pp. 921–922. Association for Computing Machinery (2008). https://doi.org/10.1145/1370175. 1370188 54. W¨urfel, H., Trapp, M., Limberger, D., D¨ollner, J.: Natural phenomena as metaphors for visualization of trend data in interactive software maps. In: Proceedings of Conference on Computer Graphics and Visual Computing, CGVC 2015, pp. 69–76. EG (2015). https://doi.org/ 10.2312/cgvc.20151246  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing S¨oren Domr¨os(B) , Daniel Lucas , Reinhard von Hanxleden , and Klaus Jansen Kiel University, 24118 Kiel, Germany {sdo,stu124145,rvh,kj}@informatik.uni-kiel.de  
   
  Abstract. We present an improved 2D rectangle packing heuristic that preserves the initial ordering of the rectangles while maintaining a left-to-right reading direction. We also present an algorithm configuration to fall back to a simpler algorithm that works more reliably for simple packing problems and an option to optimize the result in non-interactive scenarios. This is achieved by checking for stackability, approximating the required width, and using a strip packing algorithm to pack the rectangles with the option to improve the approximated width iteratively. We present still existing Obviously Non-Optimal packings and general problems of packings that preserve the reading direction, and discuss the problem of rectangle packing in hierarchical graphs. Moreover, the algorithm without the width approximation step can solve strip packing problems such that a reading direction is maintained. Keywords: Automatic layout · Model order · Rectangle packing  
   
  1 Introduction Rectangle packing problems remain in most cases hard problems irrespective of whether the rectangles are packed in a specific area, aspect ratio, width, height, or different bins [6]. Variations of bin packing or strip-packing are often used in the transportation industry. Here packing problems have additional constraints and rectangles, which correspond to packages, have to be packed such that they can be removed in a specific order and a stable packing can be achieved without tilted packages [4]. This work extends the work of [5]. They evaluate the box approach, the LR-rectpacking approach that we will use in its scale measure configuration and just call rectpacking, and a constraint based solution via CP optimizer for flat rectangle graphs. As in [5], this work is motivated by the placement of regions in the graphical language SCCharts [8]. SCCharts are modeled textually in the Kiel Integrated Environment for Layout Eclipse Rich Client (KIELER)1 [7, 9] and are automatically synthesized into a diagram using the open source Eclipse Layout Kernel (ELK)2 . This allows to use common version management tools for textual modeling and at the same time has the 1 2  
   
  www.rtsys.informatik.uni-kiel.de/en/research/kieler. https://www.eclipse.org/elk/.  
   
  c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 183–205, 2023. https://doi.org/10.1007/978-3-031-25477-2_9  
   
  184  
   
  S. Domr¨os et al.  
   
  advantage of transient views of the graphical model [7]. While the textual model is created, we assume that the ordering of regions is intended by the developer. Therefore, we want to generate a diagram that places the regions such that their model order, given by the textual input file, is respected in the resulting drawing, while still being able to utilize the screen real estate efficiently. Even though SCCharts do not only consist of rectangles but also node-link diagrams inside the rectangles, we want to focus only on the rectangle packing problem in this paper. SCCharts regions always require a rectanglular area and can be collapsed (see regions A - E, G, I, J, and L in Fig. 1) to make their contents invisible or expanded (see regions F, H, and K). Collapsed regions always have the same height but a width depending on the region name. Expanded regions may have arbitrarily big inner behavior and are usually much bigger than collapsed regions. In the KIELER tool an SCChart is drawn in a specific window. This window has a specific aspect ratio called the desired aspect ration (DAR) defined by its width divided by it height. For simplicity and without loss of generality, we will assume that the DAR is set to 1.3. The generated drawing does not have to fit this aspect ratio exactly but its contents can typically be drawn with a higher zoom level if it is the case, as expressed by the scale measure (SM), formally defined later. A bigger scale measure means that regions can be drawn bigger, which means all components and labels can be read more easily. Regions are placed such that they do not have any “gaps” between them (e. g. compare Fig. 1c to Fig. 1d) since it creates clear rows and is aesthetically more pleasant. Contributions and Outline. The new contributions are the following: – We showcase region packing problems for which the Nothing is Obviously NonOptimal (NONO) principle [11] is difficult to abide, resulting in Obviously NonOptimal (ONO) packings in Sect. 4.1. – Additionally to the box and rectpacking algorithm, we present two improvements to the rectpacking algorithm and propose an improved rectpacking algorithm that can be used to solve packings as a result of badly approximated width in Sect. 5. – We describe the constraints used to solve the region packing problem via an optimization problem that maximizes the scale measure in Sect. 6. – We compare the improved rectpacking algorithm using the generated region packing problems from [5] in Sect. 7.1. – Additionally to the evaluation of box, rectpacking, and the constraint-based solution with generated models, we evaluate them with real SCCharts on a local and global level with the new problem class of only expanded regions in Sect. 7.2. – We evaluate and discuss the problem of local maximum scale measures in hierarchical graphs at the example of SCCharts in Sect. 7.3. We also cover the following contributions from [5]: – The presentation and formalization of the region packing problem (Sect. 2), including placement constraints that facilitate a reading direction. – The box algorithm to solve the region packing problem (Sect. 3, see Fig. 1b).  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing Example +  
   
  A + B + C + D + E - F  
   
  +  
   
  Example  
   
  G - H  
   
  +  
   
  A  
   
  1:  
   
  A  
   
  2: +  
   
  I + J - K int X = 2  
   
  +  
   
  A + B + C + D + E - F  
   
  B  
   
  C  
   
  2: I + J - K int X = 2  
   
  +  
   
  D + E  
   
  -  
   
  Example  
   
  G  
   
  +  
   
  H  
   
  A  
   
  2:  
   
  I + J  
   
  +  
   
  A  
   
  +  
   
  D  
   
  -  
   
  H  
   
  +  
   
  K int X = 2  
   
  +  
   
  +  
   
  B +  
   
  -  
   
  C  
   
  +  
   
  F  
   
  B  
   
  1:  
   
  A  
   
  L  
   
  2:  
   
  D  
   
  G  
   
  A  
   
  E  
   
  +  
   
  -  
   
  B  
   
  C  
   
  D  
   
  (b) Box, after whitespace elimination, SM = 0.00363  
   
  A  
   
  1:  
   
  C  
   
  L  
   
  Example +  
   
  B  
   
  1:  
   
  D  
   
  (a) Box, with whitespace, SM = 0.00363  
   
  +  
   
  G - H  
   
  A  
   
  +  
   
  A + B + C - F  
   
  +  
   
  A  
   
  L  
   
  +  
   
  185  
   
  C  
   
  I  
   
  +  
   
  J  
   
  -  
   
  K int X = 2  
   
  D  
   
  +  
   
  L  
   
  (c) Rectpacking, with whitespace, SM = (d) Rectpacking, after whitespace elimination, 0.00446 SM = 0.00446 Example +  
   
  A + B - H  
   
  +  
   
  C + D  
   
  +  
   
  E  
   
  -  
   
  F  
   
  A  
   
  1: 2:  
   
  B  
   
  C  
   
  D  
   
  A + +  
   
  G  
   
  I + J - K int x = 2  
   
  +  
   
  L  
   
  (e) CP optimizer solution, SM = 0.00508  
   
  Fig. 1. An SCChart with region F, H, and K expanded, desired aspect ratio of 1.6 (red-dashed bounding box), with scale measure SM (larger is better), [5]. (Color figure online)  
   
  – The rectpacking algorithm (Sect. 4, see Fig. 1d). The evaluation of the algorithms can be found in Sect. 7, which is divided into the evaluation of region packing with partly collapsed regions using generated models, and the novel evaluation of the region packing problem with only expanded regions with real SCCharts models. We conclude and present future work in Sect. 8. Related Work. As also described in [5], the general approach is related to several existing works:  
   
  186  
   
  S. Domr¨os et al.  
   
  [6] mention several rectangle and strip packing algorithms that do not consider an ordering of the rectangles or a reading direction. [3] present a strip packing algorithm that considers removal order as an ordering constraint. However, the removal order allows different reading directions, e. g. left-toright and right-to-left, in the same packing, which we do not allow in our problem. [1] only restrict the vertical placement of regions in their strip packing with precedence constraints and strip packing with release times algorithm for FPGA programming, which, again, is not equivalent to a reading direction. However, their algorithm allows to eliminate whitespace between different rectangles, which they do not consider to do in their context. [10] showcase an asymptotic fully polynomial approximation scheme for strip packing that does not consider an ordering. Future work could be to design an approximation scheme for the region packing problem. [2] introduce a treemap visualization algorithm for file systems or other hierarchical structures. They also form rows and subrows to place the rectangles in. However, they do not consider order and only have a fix area for each rectangle instead of minimum height and width. [20] present an algorithm to layout wordclouds that preserves the neighborhood of words. While a neighborhood preserving algorithm is indeed interesting it does not preserve a reading-direction, does not create rows and subrows of words to read them easily based on their order, does not have a clear order of words, and does not allow to eliminate whitespace between them. The second author presented a prototype of the rectpacking algorithm in his thesis that also motivates the need to solve the region packing problem better for SCCharts with one big and several small or collapsed regions [12]. The rectpacking algorithm as it is now still uses the general idea of the width approximation step that was proposed in this thesis.  
   
  2 The Region Packing Problem Let us formally define the packing problem that we henceforth call the region packing problem based on the example of SCCharts regions: The regions describe an ordered sequence R = (r1 , r2 , . . . rn ) with ri = (wi , hi ) for region ri , with the minimal width wi and minimal height hi . To get the final drawing one wants to compute coordinates xi and yi for each region ri and optionally a new width wi and height hi if one wants to eliminate whitespace (see Fig. 1a compared to Fig. 1b). Additional configuration, i. e. spacing between regions, padding, and a minimal width of the parent can be configured, but we will omit these in the following sections for simplicity. Henceforth, we call the regions ri−1 and ri+1 the neighbors of ri . A first intuitive requirement is that the resulting drawing has to be free of overlaps. Moreover, we want that the new width wi and height hi of each region is not smaller than the minimum width wi and height hi . We call these requirements the correctness requirements that have to be respected to produce a valid drawing. These requirements are of course founded in aesthetic criteria since we generally want to prevent overlaps  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  187  
   
  of diagram elements [13]. Additionally, we want the regions to be ordered, which introduces layout stability and helps to maintain a mental map, as well as to produce compact drawings by making the best possible use of the drawing area. Usually one cannot use a mental map during layout creation since no previous depiction of the diagram exist [14]. However, as stated before, we assume that the user generated the textual input file step by step and already knows how regions are ordered in it or at least has a general idea of that. The generated diagram should preserve this order. There might be region placements, such as region A to D in Fig. 1c or Fig. 3a, for which it is unclear whether they are ordered by reading them from left-to-right or top-down. We solve this by introducing a clear left-to-right reading direction to allow the user to trust in this order, which allows to infer the correct order in this case, since it is clear the we read from left to right. Note that this approach is not limited to a left-toright reading direction, it is just a very common one and for the sake of this paper we only discuss this one. Regions should be recognizable or discoverable. In SCCharts this is often possible by the inner behavior and the size of the region, e. g. since we know that the inner state machine of region H in Fig. 1a looks the way it does, we will recognize it as such. This does of course not work if we have several similar regions, the user does not know about the inner behavior, or the region is collapsed. A clear ordering of regions can, therefore, help to identify regions by their neighbors and their placement. 2.1 Scale Measure The scale measure describes how big an element can be drawn compared to the available wd hd , ) expresses how well the drawing area. The original scale measure OSM = min( w a ha drawing uses the given area [16]. E.g., an OSM of 1 means that both the width and the height fit the drawing area, but that the drawing cannot be enlarged anymore without exceeding the drawing area. An OSM of 0.5 means that the drawing has to be shrunk by a factor of 2 to fit the drawing area. Clearly, a larger OSM allows a more readable diagram and is hence better. In the KIELER tool it is not possible to get concrete values for the desired width and height, but we can use the desired aspect ratio instead. By assuming a desired height hd = 1 and a desired width wd = 1, we can define the scale 1 measure based on the desired aspect ratio DAR as SM = min( DAR wa , ha ) [5]. 2.2 Region Alignment If we want to talk about reading direction and placement of regions, we have to introduce proper terminology to describe region placement and alignment. A region packing consist of rows. If regions have different heights, it wastes screen real estate to only align them in rows, as seen in Fig. 1a. It needs to be possible to stack them somehow if one region is much bigger than the other ones. Since we also want to maintain a reading direction in such a stack, we introduce the concept of subrows in rows. Additionally, we want to group regions that have a similar height together in blocks, since they do not waste much space when they are together in a subrow. An example for this grouping can be seen in Fig. 2. The drawing has two rows. The first row is divided into three blocks based on the region height. The second row is divided into two blocks. All regions inside their row or subrow have an upper bound for  
   
  188  
   
  S. Domr¨os et al.  
   
  their y-coordinate that we henceforth call the row level or subrow level. The division of blocks into subrows can be seen in Fig. 2b. Additionally, we can stack blocks on top of each other in so called stacks, which is not depicted here. Example +  
   
  A  
   
  +  
   
  +  
   
  D  
   
  +  
   
  -  
   
  H  
   
  B  
   
  +  
   
  C  
   
  E  
   
  -  
   
  +  
   
  F  
   
  Example +  
   
  G  
   
  A +  
   
  A  
   
  1: 2:  
   
  +  
   
  J  
   
  +  
   
  B  
   
  +  
   
  D  
   
  +  
   
  E  
   
  -  
   
  H  
   
  -  
   
  K int X = 2  
   
  B  
   
  +  
   
  C  
   
  I  
   
  A  
   
  D  
   
  (a) Blocks (red)  
   
  L  
   
  A  
   
  +  
   
  C  
   
  -  
   
  +  
   
  F  
   
  G  
   
  A +  
   
  1: 2:  
   
  +  
   
  J  
   
  K int X = 2  
   
  B  
   
  +  
   
  C  
   
  I  
   
  -  
   
  L  
   
  D  
   
  (b) Subrows (red)  
   
  Fig. 2. Rows, blocks, and subrows (dotted, black) in a rectangle packing, [5]. (Color figure online)  
   
  Whitespace elimination further helps to maintain a reading direction by solving the orientation problem in some cases (see Fig. 1c compared to Fig. 1d). Moreover, the absence of gaps makes it easier to continue reading from left to right. 2.3  
   
  Ordering Requirements  
   
  The most basic ordering constraint is that for regions ri and rj with i < j, region ri is horizontally or vertically before region rj : xi +wi ≤ xj ∨yi +hi ≤ yj . More important, however, is consistency, meaning that subrows and rows are recognizable. E. g. Fig. 6a abides this horizontal and vertical ordering property but has no clear overall ordering. We further restrict possible positions of region ri+1 based on the preceding region ri by the following rules: (1) (2) (3) (4)  
   
  ri+1 ri+1 ri+1 ri+1  
   
  is directly right of its preceding neighbor ri and top-aligned, or is right of its preceding neighbor and top aligned at the current row level, or is in the next subrow in the same stack (or block) of ri , or is in the next row.  
   
  Rule (1) applies to region D and E or F and G in Fig. 1c. This describes two cases to position a node: next the the last node in the row or the subrow. Rule (2) applies to E and F and (3) applies to C and D or K and L. In this example rule (4) only applies to G and H. Note that these constraints do not suffice to get good solutions. The region packing in Fig. 1a only uses rule (1) and (4) and has a clear reading direction but a bad scale measure. Figure 3a does potentially use all rules but does not use (1) often enough. Therefore, we introduce a top-down reading direction inside the rows, as seen in region A to E, that is still ambiguous after whitespace elimination, as seen in Fig. 3b. This kind of drawings can also be produced by the constraints-based packing solution if they yield a better scale measure. Since the rules are prioritized, as described in Sect. 6, this will, however, not be the case in this example, as seen in Fig. 1e.  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing Example  
   
  Example +  
   
  A + C + E - F  
   
  +  
   
  B + D  
   
  -  
   
  +  
   
  H  
   
  A  
   
  +  
   
  G  
   
  A  
   
  1:  
   
  B  
   
  +  
   
  I  
   
  +  
   
  J  
   
  -  
   
  2:  
   
  K int X = 2  
   
  C  
   
  189  
   
  D  
   
  +  
   
  L  
   
  (a) Before whitespace elimination  
   
  A  
   
  +  
   
  B  
   
  -  
   
  H  
   
  A  
   
  + +  
   
  C  
   
  +  
   
  E  
   
  +  
   
  F  
   
  G  
   
  A  
   
  D  
   
  1:  
   
  -  
   
  B  
   
  +  
   
  I  
   
  +  
   
  J  
   
  -  
   
  2:  
   
  C  
   
  D  
   
  K int X = 2 +  
   
  L  
   
  (b) After whitespace elimination  
   
  Fig. 3. No clear reading direction in region packing solution for regions A – E, [5].  
   
  Note that the box algorithm will only use rules (1) and (4) to layout the regions in rows. Using only rule (1) and (3) could replicate the trivial approach of the box layouter by having only subrows inside a single row, which is not distinguishable from multiple rows without subrows. For all non-trivial approaches no rule has a clear priority over the others and one can always construct packing problems for which a specific decision is not optimal.  
   
  3 The Box Algorithm All algorithms that we present in this paper consist of two steps: width approximation, which reduces the region packing problem to a strip packing problem, and region placement. For the box algorithm, the first algorithm we present now, these two steps are shown in Alg. 1 and 2. The box algorithm is a greedy algorithm for layouting regions that only uses the rules (1) and (4), meaning it places regions in a row until they no longer fit and a new row has to be opened, as seen in Alg. 2. The target width is approximated using the total area of all regions and its standard deviation (see Alg. 1). This algorithm fulfills all ordering constraints, has a clear reading direction, and performs reasonably well for regions of similar height, as we will show in Sect. 7. Its inability to stack regions, however, produces ONO drawings such as Fig. 1a that could easily be improved by stacking regions. After whitespace elimination this becomes even more evident since the height increases a lot and abnormally formed regions with an aspect ratio that does not stand in relation to its content are created. This motivates the rectpacking algorithm.  
   
  4 The Rectpacking Algorithm The rectpacking algorithm also consists of a width approximation (see Alg. 3) and a simple placement step (see Alg. 4). After that the regions are compacted (see Alg. 5) by forming stacks, blocks, and subrows, as seen in Fig. 6, Fig. 1c, and Fig. 1d. Instead of doing a static analysis based on the area of the regions involved, we perform a greedy algorithm that optimizes the scale measure, as seen in Alg. 3. We begin by placing the first region top-left and for each new region we place it such that  
   
  190  
   
  S. Domr¨os et al. Algorithm 2: boxPlace.  
   
  1  
   
  Algorithm 1: boxWidthApproximation.  
   
  1 2 3  
   
  Input: Regions rs, DAR Output: Approximated width   
   
  totalArea = area(rs) area = totalArea + |rs| ∗ stddev(totalArea) √ return max(maxWidth(rs), area ∗ DAR)  
   
  2 3  
   
  Input: Regions rs, width w Output: Placed regions rs lineX , lineY , lineHeight = 0 foreach r in rs do if lineX + r.width ≤ w then  
   
  r.x = lineX r.y = lineY lineX += r.width lineHeight = max(lineHeight, r.height)  
   
  4 5 6 7  
   
  8 9 10 11 12 13  
   
  else lineY += lineHeight lineX = 0 lineHeight = r.height r.x = 0 r.y = lineY  
   
  Fig. 4. Box width approximation and placement algorithms, [5].  
   
  the succeeding region is placed based on the preceding region and the scale measure is optimal with area and aspect ratio as a secondary and tertiary criterion, which is calculated by the bestPlacement function that greedily selects the best of the following alternatives: LR Directly right of the preceding region DR Right of the whole drawing, top aligned LB Directly below the preceding region DB Below the whole drawing, left aligned. The placement step (see Alg. 4) places regions in the same manner as the box heuristic. The step is used to group regions with similar height in blocks and assign each block to its own stack, as illustrated in Fig. 6b. Additionally, the minimum and maximum block width and height are calculated, since they speed up certain compaction steps. These structures will be used during compaction to stack compacted blocks. Moreover, the row height of the placement step is used as the maximum row height. Note that this aspect of the heuristic can produce ONO packings, as seen in Sect. 4.1. After placement the drawing is compacted, as described in Alg. 5. For each row, we check for each block, whether the next block (in this row or the next) has regions that could be included in the current block if it is compacted. In the next step, we check whether the next block would fit on top of the current, which would visually create a subrow in the current row. If this is not the case, the next block might be placed right of the current one in a new stack. In this case, the current stack is drawn such that the full row height and minimal row width is used since we are sure that no other block in this row can be placed on top of it without compromising the ordering. Placing two stacks next to each other may result in the inevitable stack alignment ONO-case (see Sect. 4.1) that is usually prevented by adding same height regions to the same block. Else, the next block is in the next row and cannot be added to the current one. Therefore, the stack of  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  Algorithm 3: rpWidthApproximation. Input: Regions rs, DAR Output: Width w 1 Drawing d 2 foreach r in rs do 3 lr = placeLR(d, r) 4 dr = placeDR(d, r) 5 lb = placeLB(d, r) 6 db = placeDB(d, r) 7 d= bestPlacement(lr, dr, lb, db, DAR) 8 return getWidth(d)  
   
  191  
   
  Algorithm 4: rpPlace [5]. Input: Regions rs, width w Output: Placed regions rs 1 Row row = new Row(w) 2 Stack stack = new Stack(row) 3 Block block = new Block(row, stack) 4 foreach r in rs do 5 similar = hasSimilarHeight(block, r) 6 f it = fitRow(block, r) 7 if similar ∧ f it then 8 block.add(r)  
   
  12  
   
  else if f it then stack = new Stack(row) block = new Block(row, stack, block) block.add(r)  
   
  13  
   
  else  
   
  9 10 11  
   
  row = new Row(w, row) stack = new Stack(row) block = new Block(row, stack, block) block.add(r)  
   
  14 15 16 17  
   
  Fig. 5. Rectpacking width approximation and placement. Example  
   
  Example  
   
  +  
   
  A + B + E - F  
   
  +  
   
  I  
   
  +  
   
  C + D  
   
  +  
   
  J  
   
  +  
   
  G - H  
   
  A  
   
  - K int X = 2 +  
   
  A  
   
  1:  
   
  B  
   
  +  
   
  A  
   
  +  
   
  B  
   
  +  
   
  C  
   
  +  
   
  C  
   
  -  
   
  L  
   
  D  
   
  (a) After width approximation, SM = 0.00446  
   
  +  
   
  E  
   
  -  
   
  F  
   
  +  
   
  G  
   
  +  
   
  I  
   
  A H  
   
  A  
   
  -  
   
  K int X = 2  
   
  +  
   
  +  
   
  J  
   
  B  
   
  1: 2:  
   
  2:  
   
  D  
   
  C  
   
  D  
   
  L  
   
  (b) After placement, SM = 0.00371  
   
  Fig. 6. Layout with rectpacking heuristic, DAR = 1.6 (red-dashed). See also Fig. 1c and Fig. 1d for compaction and whitespace elimination steps, [5]. (Color figure online)  
   
  the current block is drawn such that it utilizes the remaining width of the current row. This continues until the last row is compacted. Whitespace can be eliminated by dividing additional space equally to all rows, then to all stacks, blocks, subrows, and finally regions, as seen in Fig. 1c compared to Fig. 1d. Note that the box algorithm allows to only change the width of the last region in each row and only increase the height for the other ones, as seen in Fig. 1b.  
   
  192  
   
  S. Domr¨os et al.  
   
  As seen in [5], this solution work reasonably well for graphs with at least one big node, which are graphs were at least two regions can be stacked instead of drawn next to each other. For the case of same height regions with varying width (SH), which represents only collapsed regions in an SCChart, it is outperformed by the box algorithm. To deal with this and other problems, we propose the improved rectpacking algorithm in Sect. 5. Algorithm 5: rpCompact. Input: Regions rs, width w Output: Placed regions rs 1 rows = getRows(rs) 2 foreach row ∈ rows do 3 foreach block ∈ row.blocks do 4 Block next = block.nextBlock() 5 block.addRegions(next) 6 if fitTop(block, next) then 7 block.stack.add(next)  
   
  10  
   
  else if fitRight(block, next, w) then block.stack.drawInRowHeight() block.placeRight(next)  
   
  11  
   
  else  
   
  8 9  
   
  12  
   
  block.stack.drawInRowWidth(w)  
   
  Fig. 7. Compaction of the rectpacking algorithm, [5].  
   
  4.1  
   
  ONO Cases  
   
  There are several ONO cases that may occur in the region packing problem that are not completely avoidable. Bad Width Approximation. Rectpacking (and also the box algorithm) produces bad scale measures if high and slim region and long and flat regions alternate, as seen in Fig. 8a. The constraint based solution (see Fig. 8b) yields a compact packing with a better scale measure. However, the region ordering is largely lost. It is unclear whether regions n3 to n7 or only regions n3, n4, n5, and n7 form a stack. Whitespace elimination can solve this problem since it expands the regions such that rows and subrows are visible. However, whitespace elimination can also create wrong alignments. If the row and subrow structure is not part of the algorithm, one can eliminate the whitespace by placement alone. To do this the regions are handled beginning with the last placed region. Its width and height are increased such that the drawing bounds do not change and no overlaps are created. This continues with the second last region until all regions are expanded and results in a drawing such as Fig. 8c. If one begins with the first region the formed rows and subrows might compromise the ordering. In Fig. 8d region n8 is expanded to the bottom of the drawing. Therefore, region n9 is visually be before it and also before region n6. Moreover, no clear rows and subrows are formed.  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing n1  
   
  n4n5  
   
  n7  
   
  193  
   
  n2n3  
   
  n6  
   
  n1 n2n3 n4n5  
   
  n8 n6  
   
  n1 n2n3 n4n5  
   
  n8 n6  
   
  n1 n2n3 n4n5  
   
  n8 n6  
   
  n8n9 n7 n9  
   
  n7 n9  
   
  n7 n9  
   
  (a) Rectpacking solu- (b) Optimal packing, (c) Correct whitespace (d) Wrong whitespace tion, [5] [5] elimination elimination  
   
  Fig. 8. Example of bad scale measure resulting from rectpacking. Rows are highlighted in dotted black, blocks are highlighted in red if their bounds may be unclear. (Color figure online)  
   
  Wrong Stack Alignment. Packings, such as region A to D in Fig. 1c, must not only occur such that one has to primarily read from left-to-right but has to read top-down.  
   
  (a) Regions in two different (b) Correct alignment before (c) Wrong alignment after whitespace elimination stacks align whitespace elimination  
   
  Fig. 9. Even though regions are correctly placed in rows, stacks, blocks, and subrows, they may still align with regions and create orderings that do not conform with the model order.  
   
  This can occur if two stacks such as the ones in Fig. 9a are placed next to each other. Since regions n1 and n2 and regions n3 and n4 have a different height, they may not be placed in the same subrow, but are placed in the same stack instead if the row height allows this. This leads to an unintentional alignment of regions n1 and n3 and regions n2 and n4. This can also occur in lesser extent such that only a few subrow between two stacks are wrongfully aligned. Another possible cause is whitespace elimination. The wrong alignment is created by enlarging all regions equally and thus visually creating subrows, as seen in Fig. 9b and 9c. Therefore, a packing such as the one in Fig. 9b is prevented by the compaction algorithm (see Alg. 5), which prefers to stack different blocks on top of each other in the same stack instead of next to each other in different stacks. No Row Height Increase. As mentioned before, the rectpacking heuristic does not increase the height of a row during compaction, which can produce ONO packings such as Fig. 10. The solution shown in Fig. 10b requires the algorithm to backtrack. After adding n5 to the first row, it is also necessary to reevaluate all stacks and blocks in the current  
   
  194  
   
  (a)  
   
  S. Domr¨os et al.  
   
  cannot be added to the same row.  
   
  (b) can be added to the same row, which produces a compact drawing.  
   
  Fig. 10. Allowing to revise the row height after placement can create better drawings at the cost of computation time.  
   
  row and draw them such that they utilize the new height. Refusing to do this creates another ONO packing in which the user can clearly see that the regions could be stacked to improve the drawing. Consider a region packing as the one in Fig. 6b. Even if region H, which is initially placed in the next row, would be in the target width of the first row after compacting region A – G, it would not fit in the height. Since H is much higher than the current highest region F, this would result in a packing which seems unintuitive and like an error of the algorithm. This can be solved by backtracking and stacking regions A – G better to fit the new higher height. Since this potentially increases computation time significantly, the solution is not suitable for interactive scenarios and will not be considered here, but can be implemented as part of future work.  
   
  5 The Improved Rectpacking Algorithm The main problem of the rectpacking algorithm is that its width approximation step is often not correct. Especially for regions that are not stackable, the approximated width is generally too small. In other cases, the width approximation does not clearly underestimate or overestimate the required width, as described in [5]. 5.1  
   
  Checking for Stackability  
   
  We propose a heuristic to check in linear time whether a region packing problem has the potential to stack regions to improve the scale measure. For each three region ri , ri+1 , and ri+2 we check whether two neighboring regions stacked onto each other are smaller than the third region by checking whether hi ≥ hi+1 + hi+2 or hi+2 ≥ hi + hi+1 . If this is not the case, we know that the box heuristic will most likely perform better and execute it instead. Note that this is only a heuristic for stackability. There are still cases left for which regions cannot be stacked, e. g. if the width of one of the smaller regions defines the target width and it is, therefore, in its own row, which also means that it should define its height, which does not allow to place another region on top of it.  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  195  
   
  5.2 Revising the Width Even if the regions are stackable the approximated width might just a little bit too small or too big such that a suboptimal packing is created. Therefore, we propose to revise it. The initial rectpacking run yields an aspect ratio that may be either bigger or smaller than the desired aspect ratio and we can revise the target width as follows: We want to find a new target width based on the previous approximation. Since we do not want to do this blindly, we need the find a minimal decrease or increase of the width that is guaranteed to change the packing. We check for each row how much the width has to be increased to fit the next block of the next row, as seen in Fig. 11a. Here the width of region H has to be added to the width of the first row up to region G to be able to place H in there. The same can be done on subrow level if the region packing consist of only one row. Similarly, we can collect for each row the width of the last block to the current row Fig. 11b. Here the first row must be shortened to not fully include G to decrease the row width. The second row must be shortened to not fully include the block I – L. We get a range of values that is guaranteed to influence the region packing. We choose the minimum value that influenced the packing to not miss a potentially optimal packing and executed rectpacking a second time with a revised target width by skipping the width approximation step. In Fig. 11b the second row width deduction is smaller than the first, we choose the second width to revise the target width. Note that if we are not in an interactive scenario, this approach can be used to configure an iterative algorithm to find the best width to achieve the best scale measure. Example +  
   
  A  
   
  +  
   
  B  
   
  +  
   
  D  
   
  +  
   
  E  
   
  -  
   
  H  
   
  A  
   
  +  
   
  C  
   
  -  
   
  +  
   
  F  
   
  Example  
   
  G w  
   
  A +  
   
  1: 2:  
   
  J  
   
  K int X = 2 +  
   
  (a) Additional width wider row.  
   
  +  
   
  A  
   
  +  
   
  B  
   
  +  
   
  D  
   
  +  
   
  E  
   
  -  
   
  H  
   
  -  
   
  B  
   
  C  
   
  I  
   
  +  
   
  L  
   
  D  
   
  A  
   
  +  
   
  C  
   
  -  
   
  +  
   
  F  
   
  G w  
   
  A  
   
  +  
   
  1: 2:  
   
  +  
   
  J  
   
  K int X = 2  
   
  +  
   
  C  
   
  I  
   
  -  
   
  B  
   
  D  
   
  L w  
   
  per row that results in a (b) Decreasing the row width by would create a narrower row and, therefore, drawing.  
   
  Fig. 11. The width that has to be considered as additional or deducted width is marked in dottedblue and the responsible block in red-dashed lines. (Color figure online)  
   
  6 A Constraint-Based Approach Recall that our goal is to maximize the scale measure and to maintain a reading direction. Formalizing a maximization problem for the scale measure is relatively easy. By adding the correctness requirements for a packing (see Sect. 2), we make sure that the  
   
  196  
   
  S. Domr¨os et al.  
   
  packing is free of overlaps and no rectangles change their size. Whitespace can be eliminated after the layout completed, as explained in Sect. 4.1, therefore, it is not included in the approach itself. A basic ordering constraint such as the one discarded in Sect. 2.3 is not enough to limit the search space to a reasonable size. Therefore, we use the constraints defined as rules (1) to (4) in Sect. 2.3, which we already identified as reasonable positions to facilitate on order and a reading direction. As a result the following variables are introduced based on the drawing after placing region i: – maxHeighti : The current maximum height of the drawing (necessary for rule (1)) – rowLeveli : The current row level (necessary for rule (2)) – startXCurrentStacki : The x-coordinate of the current stack (necessary for rule (3)) – endY CurrentSubrowi : The end y-coordinate of the current subrow (necessary for rule (3)) – endXCurrentStacki : The end x-coordinate of the current stack (necessary to calculate the x-coordinate of the next stack) These allow to specify the possible positions of a region based on their preceding region as follows: Rule (1), right of preceding, in current subrow: xi = xi−1 + wi−1 endXCurrentStacki = max(endXCurrentStacki−1 , xi + wi ) startXCurrentStacki = startXCurrentStacki−1 rowLeveli = rowLeveli−1 endY CurrentSubrowi = max(endY CurrentSubrowi−1 , yi + hi ) maxHeighti = max(maxHeighti−1 , endY CurrentSubrowi ) yi = yi−1 Rule (2), right of preceding region in a new stack: xi = endXCurrentStacki−1 endXCurrentStacki = xi + wi startXCurrentStacki = endXCurrentStacki−1 rowLeveli = rowLeveli−1 endY CurrentSubrowi = rowLeveli + hi maxHeighti = max(maxHeighti−1 , endY CurrentSubrowi ) yi = rowLeveli  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  197  
   
  Rule (3), in a new subrow: xi = startXCurrentStacki−1 endXCurrentStacki = max(endY CurrentStacki−1 , xi + wi ) startXCurrentStacki = startXCurrentStacki−1 rowLeveli = rowLeveli−1 endY CurrentSubrowi = max(currentSubrowEndi−1 + hi ) maxHeighti = max(maxHeighti−1 , endY CurrentSubrowi ) yi = endY CurrentSubrowi−1 Rule (4), in a new row: xi = 0 endXCurrentStacki = wi startXCurrentStacki = 0 rowLeveli = maxHeighti−1 endY CurrentSubrowi = rowLeveli + hi maxHeighti = endY CurrentSubrowi yi = rowLeveli Additionally, maxW idth is defined as max(xi + wi ) for all region ri . This allows DAR 1 to maximize the scale measure by maximizing min( maxW idth , maxHeightn ). Since a fixed prioritization of the rules (1) to (4) does not always produce the best scale measure nor the best drawing regarding reading direction there is nothing to add here and we prioritize the solution by rule number. E. g. if two solutions yield the same scale measure the solution is chosen that used a lower rule number for a first region for which the decisions were different. This allows us to maintain a left-to-right reading direction, as seen in region A – E in Fig. 1e. However, it does not limit the row height by the highest element in it, as seen in Fig. 1e. Note that one can achieve the same result using different rules. For a packing problem with same height regions it might be irrelevant whether one creates only one row with several subrows or several rows without subrows.  
   
  7 Evaluation The box and rectpacking layout algorithm were implemented in the ELK framework3 . The graphs are drawn and evaluated for four different algorithm configurations with a DAR of 1.3 and a spacing of 1 between regions. SCCharts can be hierarchical, we here use a bottom-up layout strategy. This means region H in Fig. 1c knows its minimum size that is defined by its content as described in Sect. 2. 3  
   
  https://www.eclipse.org/elk/reference/algorithms.html.  
   
  198  
   
  S. Domr¨os et al.  
   
  The algorithm configurations are: – B: Using the box layouter with set priorities to enforce region order. – R: The rectpacking heuristic as proposed by [5]. – IR: The improved rectpacking algorithm with both optimizations proposed in Sect. 5 and configured such that the width is revised only once. – C: The solved constraint-based maximization problem proposed in Sect. 6. The run time of the box algorithm is clearly in O(n). Rectpacking solves the placement problem in O(n log(n)), as reported by [5]. IR behaves as box or rectpacking depending on the input graph. In practice the run time of B, R, IR seem linear in the problem size and are in our experiments in millisecond range. 7.1  
   
  Generated Models  
   
  We evaluated the performance of the algorithms using the GrAna tool [15] with 200 graphs for each graph class taken from [5]. The number of regions in the generated graphs is between 20 and 30 to make the instances solvable by CP optimizer. Normal regions have a height of 20 and a width with the mean of 100 and a standard deviation of 20. The big nodes class (BN) has 2 to 5 big regions with a width and height between 300 and 1000. The one big node class (OB) has only one big region. The same height class (SH) has no big regions. The box heuristic (B) handles regions of same height (SH) very good. The main reason for this is that the width approximation (see Alg. 1) is very accurate in this case, as seen in Fig. 12a, since it is near the ideal aspect ratio and produces a good scale measure, as seen in Fig. 12g. The width approximation and the inability to stack regions results in worse results for the one big region (OB) and big regions (BN) cases, as seen in Figure Fig. 12b, 12e, 12h, 12c, 12f, and 12i. In Fig. 12g, it is clear that the constraint-based solution (C) gets a clear advantage from breaking the reading direction and taking time to optimize the scale measure. The rectpacking heuristic is better than the box algorithm and near the optimal solution in all OB and BN cases but its approximation step falls short when regions are not stackable, as it is the case for SH problems since the width is overestimated, as seen in Fig. 12a, 12d, and 12g. The IR heuristic solves this problem by using the box layouter for these cases. For BN problems the box algorithm is not used but the revised width improves the solution significantly, as seen in Fig. 12c, 12f, and 12i. For OB graphs the initially approximated width seems to be good enough to not change after being revised. This means the initial scale measure was good and, therefore, the approximated width was good enough to rarely leave room for improvement after one iteration of width revision, as seen in Fig. 12h and Fig. 12l. For SH graphs IR falls back to the box layouter that improves at most times upon the rectpacking solution, as seen in Fig. 12g and 12l. The whitespace did not have much impact on the quality of the solution, however, less whitespace generally means the area is used more efficiently, which results often in a better scale measure. When we compare the improvement to the rectpacking approach in detail, we see that for most SH and the slightly more of the OB problems, the initial width is overestimated since the resulting aspect ratio is lower, as seen in Fig. 12j. For BN cases an improvement could be made in both directions equally.  
   
  199  
   
  Aspect Ratio 2 3 1  
   
  Aspect Ratio 2 3 1 R IR C Same height  
   
  R IR C One big node  
   
  (b) Aspect ratio (OB) Top−Level Whitespace 0 20 40 60 80  
   
  B  
   
  R IR C Same height  
   
  B  
   
  R IR C Same height  
   
  B  
   
  R IR C One big node  
   
  (h) Scale measure (OB)  
   
  SH OB BN  
   
  (j) Aspect ratio R - IR  
   
  −30  
   
  Whitespace Difference −10 10 30  
   
  Aspect Ratio Difference −0.5 0.0 0.5 1.0  
   
  (g) Scale measure (SH)  
   
  B  
   
  (e) Whitespace (OB)  
   
  0.0005  
   
  Scale Measure 0.0035 0.0045 B  
   
  R IR Big nodes  
   
  C  
   
  (c) Aspect ratio (BN)  
   
  R IR C One big node  
   
  Scale Measure 0.0015 0.0025  
   
  (d) Whitespace (SH)  
   
  B  
   
  SH OB BN  
   
  (k) Whitespace R - IR  
   
  R IR Big nodes  
   
  C  
   
  (f) Whitespace (BN)  
   
  B  
   
  R IR Big nodes  
   
  C  
   
  (i) Scale measure (BN) Scale Measure Difference −0.0005 0.0005 0.0015  
   
  Top−Level Whitespace 0 20 40 60 80  
   
  (a) Aspect ratio (SH)  
   
  B  
   
  Top−Level Whitespace 0 20 40 60 80  
   
  B  
   
  Scale Measure 0.0004 0.0010  
   
  1.0  
   
  Aspect Ratio 1.5 2.0  
   
  4  
   
  4  
   
  2.5  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  SH OB BN  
   
  (l) Scale measure R - IR  
   
  Fig. 12. Aspect ratio, whitespace, scale measure, and comparison of SH, OB, and BN problems.  
   
  7.2 Real World Models (SCCharts) We take 372 real world SCCharts from papers such as [17, 18], as well as student models developed during lectures, and projects such as the Railway Project ’14 and ’17  
   
  200  
   
  S. Domr¨os et al.  
   
  [19], which need 2719 region packing problems to solve. A quantitative analysis of the complexity of these models can be seen below. For real world examples, we limit the computation time to one hour per graph, which was only needed for the biggest three SCCharts from the railway projects. For sake of evaluation all SCCharts are expanded, meaning we introduce a new category next to SH, OB, BN. Moreover, we layout state machines (see the inner behavior of region H in Fig. 1c) from left to right and shorten edge labels to their priority. Table 1 shows how many layout problems exist per hierarchy level. For example Fig. 13 does have 9 states at depth 0, 5 layered problems at depth 1, 4 trivial rectpacking problems at depth 2, 2 layered problems at depth 3, and 1 rectpacking problems at depth 4. Note that rectpacking problems are at an even depth, and layered problems (node link diagrams) have to be solved at odd depth levels. Figure 13 also serves as an example were the width is revised to produce a better drawing. However, since the box algorithm also produces this drawing it is not counted towards the problems improved by width revision in Tab. 1.  
   
  Fig. 13. The SeqAbort SCChart.  
   
  Most SCCharts do not have stackable expanded regions in them. One can see one outlier at depth 10 where most regions are stackable. Of all region fewer than 5% are stackable, which means that most of the SCCharts expanded region packing problems are simple problems that most likely are better solved by the box layouter. The packing problems seem to get more complex in models with more hierarchy, which generally are bigger models, since they are more complex and more likely to reduce this complexity by introducing parallel regions of execution. Only very few models were improved by revising the width via the improved rectpacking algorithm. At depth 2, 16 models increased the approximated width and 16 decreased it. At depth 12, 11 models increased the width. Since most problems are trivial or nearly trivial, this was expected to be the case. The complexity of the state machines in an SCChart peaks at a depth between 5 and 7 with on average over 9 nodes with more than 13 edges  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  201  
   
  between them. The state machines at lower depth seem to be less complex on average, which again might be the case since less complex models seem to be smaller. Since only very few models can be improved by guessing a better target width and since this step is rather expensive and has to be fully evaluated in future work, we propose to not use that per default but only check for stackability. Table 1. Layout problems per depth in the used SCCharts models. Depth Algorithm  
   
  Instances Mean #children Mean #edges Stackable Improved by Sect. 5.2  
   
  0  
   
  13534  
   
  0  
   
  0  
   
  3130  
   
  3.77  
   
  5.71  
   
  1  
   
  Layered  
   
  2  
   
  Rectpacking 2001  
   
  1.5  
   
  0  
   
  3  
   
  Layered  
   
  3.59  
   
  3.5  
   
  4  
   
  Rectpacking 542  
   
  1.36  
   
  0  
   
  5  
   
  Layered  
   
  9.65  
   
  15.45  
   
  6  
   
  Rectpacking 126  
   
  1.08  
   
  0  
   
  7  
   
  Layered  
   
  9.58  
   
  13.39  
   
  8  
   
  Rectpacking 33  
   
  2.27  
   
  0  
   
  9  
   
  Layered  
   
  2.32  
   
  3.4  
   
  10  
   
  Rectpacking 14  
   
  3.42  
   
  0  
   
  11  
   
  Layered  
   
  1  
   
  0  
   
  12  
   
  Rectpacking 2  
   
  6  
   
  0  
   
  13  
   
  Layered  
   
  1  
   
  3  
   
  2  
   
  14  
   
  Rectpacking 1  
   
  4  
   
  0  
   
  675 131 33 25 12  
   
  105  
   
  32  
   
  7  
   
  0  
   
  3  
   
  0  
   
  2  
   
  0  
   
  11  
   
  11  
   
  0  
   
  0  
   
  1  
   
  0  
   
  First, we have a look at the individual region packing problems and not at the whole SCChart. If one only looks at all non-trivial region packing problems, which are region packing problems with at least two children, one sees that all approaches perform more or less the same, as seen in Fig. 14. These problems can also be solved by a minimization problem, however, it performs nearly the same as all other approaches, therefore, we omitted it. The width and height are more or less the same, as seen in Fig. 14a and Fig. 14b. The box algorithm performs better than rectpacking, however, there are still cases were rectpacking is better than the box algorithm, as seen in Fig. 14e. The improved rectpacking algorithm is on average better than rectpacking and the box algorithm, as seen in Fig. 14e. Overall SCCharts with only expanded regions seems to be in most cases a trivial problem and even non-trivial problems can be solved well by the simple box algorithm. 7.3 Hierarchical Graphs An SCChart does not necessarily have only one level of hierarchy, as seen in Fig. 1, but can have multiple, as seen in Fig. 13. The example has a maximum depth of four, the outermost state SeqAbort has three inner regions (potentially it could be arbitrarily many). This region could have arbitrarily many states. The state S in the StrongAbort region also has one inner region, which again has states. Since an SCChart is layouted bottom-up, each region packing problem needs a fixed aspect ratio that cannot be approximated based on the top-level elements and is, therefore, set to the fixed value 1.3. This causes problems, as seen in Fig. 15.  
   
  50 0  
   
  0  
   
  0  
   
  50  
   
  50  
   
  Width 150  
   
  Height 100  
   
  200  
   
  Aspect Ratio 150 250  
   
  S. Domr¨os et al. 250  
   
  202  
   
  B  
   
  R  
   
  IR  
   
  B  
   
  R  
   
  IR  
   
  B  
   
  (b) Height  
   
  R  
   
  IR  
   
  (c) Aspect ratio  
   
  0.00  
   
  Scale Measure 0.05 0.10 0.15  
   
  Scale Measure Difference −0.05 0.00 0.05  
   
  (a) Width  
   
  B  
   
  R  
   
  IR  
   
  R − IR B − R B − IR  
   
  (d) Scale measure  
   
  (e) Scale measure difference  
   
  R  
   
  IR  
   
  (a) Width  
   
  C  
   
  Scale Measure 0.004 0.008  
   
  20 Aspect Ratio 5 10 15 B  
   
  R  
   
  IR  
   
  (b) Height  
   
  C  
   
  B  
   
  R  
   
  IR  
   
  (c) Aspect ratio  
   
  C  
   
  0.000  
   
  B  
   
  0  
   
  0  
   
  0  
   
  20000  
   
  Width  
   
  60000  
   
  Height 10000 20000 30000  
   
  Fig. 14. Width, height aspect ratio, scale measure, and comparison of non-trivial region packing problems.  
   
  B  
   
  R  
   
  IR  
   
  C  
   
  (d) Scale measure  
   
  Fig. 15. Width, height, area, aspect ratio, and scale measure of SCCharts models.  
   
  The constraint based approach does not result in the best aspect ratio. One can see that only a few graphs are actually layouted differently and that the overall width and height of the drawing stays nearly the same for SCCharts with only expanded regions. Solving an inner region packing problem optimally means the local scale measure of each packing problem is optimal. However, this does not necessarily have a positive impact on the overall scale measure, as seen in Fig. 15d. For SCCharts the global scale measure is more important, which must not correlate with the locally optimized scale measure. Nevertheless, it is important to produce compact drawings, even on a local level.  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  203  
   
  Fig. 16. The kh mutex SCChart from the Railway Project ’14.  
   
  8 Conclusion and Future Work The rectpacking algorithm achieves better results than the simple box algorithm for packing problem in which regions are stackable. However, the width approximation step lacks accuracy if this is not the case, for example if all regions have the same or similar height, as it is the case for SCCharts with only collapsed or only expanded regions. This can be improved if one uses the box layouter and regions are not stackable. In hierarchical graphs the local scale measure is not as important as the compactness of the packing. Therefore, all algorithms perform similar and sometimes better than the locally optimal solution (C). The rectpacking and improved rectpacking solutions do, therefore, perform also better for hierarchical graphs if the packing problems allow to stack regions, as it is the case for OB and BN problems.  
   
  204  
   
  S. Domr¨os et al.  
   
  Revising the width may be expensive, since it needs at least double the time of the original approach, however, it can significantly improve the packing for problems with many stackable regions. In real world models this does not occur very often since most real models have few regions that are most times not stackable if all regions are expanded. Checking for stackability is easily done and allows to solve such problems by the much simpler box algorithm, therefore, we suggest to only add the stackability check to the improved rectpacking algorithm and to use it for the SCCharts region packing problem in the future. Even though the improved rectpacking improves upon rectpacking, there are still several ONO-cases it cannot handle. As part of future work a backtracking rectpacking algorithm that solves the problem of a fixed row height (see Sect. 4.1) can be implemented. Moreover, the iterative width revision approach (see Sect. 5.2) would be evaluated with more than one iteration as well as its impact on SCCharts models and the resulting computation time. The problem of hierarchical models remains unsolved for bottom-up layout, which could be solved as follows: First, an initial packing has to be created to see whether the aspect ratio has to be increased or decreased. Second the packing problems that result in a too high width or height must be changed. The height or width critical packing problems can be identified by looking at the highest layer in the node link diagrams and which region packing problems define the width of a layer. The SCChart in Fig. 16 is too high. When looking at the different levels, the regions Right side and Left side are the limiting factors in terms of height. Their height on the other hand is defined by the states in and out in the second layer. Additionally these states define the width of that layer and, therefore, the width of the whole region and the whole packing. Using the width revision improvement proposed in Sect. 5.2 the local drawing is changed and the overall aspect ratio and, therefore, scale measure is changed. This is done by placing region Exiting right of region Entering by increasing the target width, which results in a wider and less high drawing with a better scale measure. Even though one can identify the width or height critical regions, it remains a complex problem that will be dealt with as part of future work. A top-down layout algorithm could change this particular problem but it would be necessary to estimate the width and height of each state machine, which also reduces the size at which elements can be drawn.  
   
  References 1. Augustine, J., Banerjee, S., Irani, S.: Strip packing with precedence constraints and strip packing with release times. In: Proceedings of the Eighteenth Annual Acm Symposium on Parallelism in Algorithms and Architectures (SPAA’06), pp. 180–189. ACM, New York, NY, USA (2006) 2. Bruls, M., Huizing, K., Van Wijk, J.J.: Squarified treemaps. In: de Leeuw, W.C., van Liere, R. (eds.) Data visualization 2000, pp. 33–42. Springer, Vienna (2000). https://doi.org/10.1007/ 978-3-7091-6783-0 4 3. Da Silveira, J.L., Miyazawa, F.K., Xavier, E.C.: Heuristics for the strip packing problem with unloading constraints. Comput. Oper. Res. 40(4), 991–1003 (2013)  
   
  Revisiting Order-Preserving, Gap-Avoiding Rectangle Packing  
   
  205  
   
  4. Da Silveira, J.L., Xavier, E.C., Miyazawa, F.K.: Two-dimensional strip packing with unloading constraints. Discrete Appl. Math. 164, 512–521 (2014) 5. Domr¨os, S., Lucas, D., von Hanxleden, R., Jansen, K.: On order-preserving, gap-avoiding rectangle packing. In: Proceedings of the 13th International Conference on Information Visualization Theory and Applications (IVAPP’21), part of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP’21), pp. 38–49. INSTICC, SciTePress (2021). https://doi.org/10.5220/ 0010186400380049 6. Dowsland, K.A., Dowsland, W.B.: Packing problems. Eur. J. Oper. Res. 56(1), 2–14 (1992). https://doi.org/10.1016/0377-2217(92)90288-K 7. Choppy, C., Sokolsky, O. (eds.): Monterey Workshop 2008. LNCS, vol. 6028. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-12566-9 8. von Hanxleden, R., et al.: SCCharts: sequentially constructive Statecharts for safety-critical applications. In: Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2014), pp. 372–383. ACM, Edinburgh, UK, June 2014 9. von Hanxleden, R., Fuhrmann, H., Sp¨onemann, M.: KIELER–The KIEL integrated environment for layout eclipse rich client. In: Proceedings of the Design, Automation and Test in Europe University Booth (DATE 2011), Grenoble, France, March 2011 10. Kenyon, C., R´emila, E.: A near-optimal solution to a two-dimensional cutting stock problem. Math. Oper. Res. 25(4), 645–656 (2000) 11. Kieffer, S., Dwyer, T., Marriott, K., Wybrow, M.: HOLA: human-like orthogonal network layout. IEEE Trans. Vis. Comput. Graph. 22(1), 349–358 (2016). https://doi.org/10.1109/ TVCG.2015.2467451 12. Lucas, D.: Order- and drawing area-aware packing of rectangles. Bachelor thesis, ChristianAlbrechts-Universit¨at zu Kiel, Faculty of Engineering, September 2018 13. Purchase, H.C.: Metrics for graph drawing aesthetics. J. Vis. Lang. Comput. 13(5), 501–516 (2002) 14. Kaufmann, M., Wagner, D. (eds.): GD 2006. LNCS, vol. 4372. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-70904-6 15. Rieß, M.: A graph editor for algorithm engineering. Bachelor thesis, Kiel University, Department of Computer Science, September 2010 16. R¨uegg, U., von Hanxleden, R.: Wrapping layered graphs. In: Chapman, P., Stapleton, G., Moktefi, A., Perez-Kriz, S., Bellucci, F. (eds.) Diagrams 2018. LNCS (LNAI), vol. 10871, pp. 743–747. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-91376-6 72 17. Schulz-Rosengarten, A., von Hanxleden, R., Mallet, F., de Simone, R., Deantoni, J.: Time in SCCharts. In: Proceedings of Forum on Specification and Design Languages (FDL 2018), Munich, Germany, September 2018 18. Schulz-Rosengarten, A., Smyth, S., Mendler, M.: Towards object-oriented modeling in SCCharts. In: Proceedings of Forum on Specification and Design Languages (FDL 2019), Southampton, UK, September 2019 19. Smyth, S., et al.: SCCharts: the mindstorms report. Technical Report 1904, ChristianAlbrechts-Universit¨at zu Kiel, Department of Computer Science, December 2019. ISSN 2192–6247 20. Wang, Y., et al.: Edwordle: consistency-preserving word cloud editing. IEEE Trans. Vis. Comput. Graph. 24(1), 647–656 (2017)  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data Hamid Mansoor(B) , Walter Gerych, Abdulaziz Alajaji, Luke Buquicchio, Kavin Chandrasekaran, Emmanuel Agu, and Elke Rundensteiner Worcester Polytechnic Institute, MA, USA {hmansoor,wgerych,asalajaji,ljbuquicchio,kchandrasekaran, emmanuel,rundenst}@wpi.edu  
   
  Abstract. Mobile health involves gathering smartphone-sensor data passively from user’s phones, as they live their lives ’In-the-wild”, periodically annotating data with health labels. Such data is used by machine learning models to predict health. Purely Computational approaches generally do not support interpretability of the results produced from such models. In addition, the interpretability of such results may become difficult with larger study cohorts which make populationlevel insights desirable. We propose Population Level Exploration and Analysis of smartphone DEtected Symptoms (PLEADES), an interactive visual analytics framework to present smartphone-sensed data. Our approach uses clustering and dimension reduction to discover similar days based on objective smartphone sensor data, across participants for population level analyses. PLEADES enables analysts to apply various clustering and projection algorithms to several smartphonesensed datasets. PLEADES overlays human-labelled symptom and contextual information from in-the-wild collected smartphone-sensed data, to empower the analyst to interpret findings. Such views enable the contextualization of the symptoms that can manifest in smartphone sensor data. We used PLEADES to visualize two real world in-the-wild collected datasets with objective sensor data and human-provided health labels. We validate our approach through evaluations with data visualization and human context recognition experts. Keywords: Interactive visual analytics · In-the-wild smartphone-sensed data · Exploratory data analysis  
   
  1 Introduction Ubiquitous devices like smartphones are increasingly being used to monitor their users’ health [33]. Smartphones are equipped with several sensors such as accelerometers, gyroscopes, GPS, light, sound and activity detectors. Data from such sensors can be used to infer several health markers such as Circadian Rhythms (sleep-wake cycles) [1, 58], depression [22, 33, 50] and infectious diseases like the flu [35]. To build computational models for such inferences, researchers rely on conducting “In-the-Wild” data collection studies to gather data that mimics real life as closely as possible. Such studies require subjects to install an application on their smartphones that continuously c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 206–231, 2023. https://doi.org/10.1007/978-3-031-25477-2_10  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  207  
   
  and passively gather sensor data as they live their uninterrupted lives “in-the-wild”. The subjects are periodically asked to provide ground truth health status data (quality of sleep, stress levels, activities performed etc.) by answering questionnaires on their devices [57]. This approach results in realistic but imperfect data. Such data often contains missing labels (i.e. objective sensor data with no corresponding human provided labels) and missing periods of data collection [3, 56] due to participants turning off the application, or the application crashing. The data from smartphone sensors is complex, multivariate and is difficult to analyze without pre-processing. It is difficult to meaningful associations between the sensor data and reported health symptoms. Having a way to group data similar in terms of smartphone sensor features values and overlaying human provided symptom and wellness reports may be a useful way to increase explainability. For example, showing the links between poor sleep and late night phone usage [1] may enable an analyst to understand causes behind the symptom/wellness reports. Another example is showing changes in reported sleep duration and quality along with smartphone-sensed sleep location (e.g. main residence [normal] vs. at a workplace [abnormal]) may help contextualize and explain sleep disruptions. As the breadth and scope of such data gathering studies increases to larger populations, it is useful to have visual views that can represent information about multiple participants at the same time. It can also enable analysts to understand, compare and contextualize various sub-populations with the participants. For instance, finding groupings of people based on their location presence across different times of day may indicate important differences in life circumstances such as different shift workers i.e. night vs. day shifts etc. Visualizations over longer time periods may also enable analysts to distinguish between one-off behaviors from longer term patterns. For example visualizing sleep disruptions and poor sleep along with mobility over a significant period of time can differentiate between mentally healthy participants who travel often and have occasional sleep disruptions from mentally ill participants who stay mostly at a single location and report frequent sleep disruptions [43]. This enables the analyst to filter participants who report days with concerning symptoms and use their data to build classification models to assess other participants who may be exhibiting similar manifestations of smartphone sensed data. Unsupervised clustering is effective for sense-making of complex and multi-variate, data [30]. The high-dimension results that are produced by clustering algorithms can be effectively visualized on a 2-D plane using Dimension Reduction. Examples of such dimension reduction methods include t-SNE [34], Multi-dimensional Scaling [41] and Isomap [54]. These approaches work with a large number of features across vast numbers of data points. Clustering and projection along with dimension reduction are often used in Exploratory Data Analysis. Working with large scale multi-dimensional data can become overwhelming due to the availability of several clustering and projection algorithms [30]. Using multiple algorithms and parameter configuration makes it important to keep track of the various outcomes. In addition, analysts may want to assign semantic labels to various data points based on their domain knowledge. Data visualizations can also provide interactive and connected views that show the importance of different smartphone-sensed features for the clustering and projection outcome.  
   
  208  
   
  H. Mansoor et al.  
   
  The study participant living in-the-wild  
   
  Reporting symptoms and context while living unimpeded in-the-wild  
   
  Multiple smartphone sensors  
   
  Interactive Visual Analytics to present data to analysts  
   
  PLEADES Approach  
   
  Fig. 1. Our interactive data visualization approach for exploratory data analysis.  
   
  We researched, designed and implemented Population Level Exploration and Analysis of smartphone DEtected Symptoms (PLEADES), an interactive data visualization framework to analyze smartphone-sensed data. PLEADES enables the analysts to select and define the smartphone-sensed data features that are used for clustering. This lets them use their domain knowledge to find relevant patterns that they may want to focus on for specific categories of smartphone-sensed behavior (e.g. mobility) and reduce the confounding effect of features that are not relevant. PLEADES enables analysts to select various clustering and projection algorithms, along with the features for that are used for clustering. It allows the analyst to filter and select multiple participants and cluster the usable study days from their data. This enables the analysts to compare multiple sub-populations for further intuition. PLEADES visualizes clustering results as color-coded horizontally stacked bars to show the proportion of study across the different participants that fall into each cluster. We use three quality metrics to order the clustering results. PLEADES presents aggregated summaries for different clusters to enable a clearer understanding of the semantic differences that may manifest in the smartphone-sensed data. For example, participants who travel often will have much more variable location data than stay-at-home participants, and clustering and projecting data based on location features allows analysts to contextualize reported symptom/wellness reports. This work is an extension of our previous work on using interactive clustering and projections of smartphone-sensed data for exploratory data analysis [39]. This version is a significant extension as we add – An extended domain analysis of the state of the art in the fields of visual exploratory data analysis of complex data and visualization approaches to analyze and understand data from ubiquitous devices like smartphones – A detailed requirements analysis with experts in data science - specifically in training and classifying machine learning models for health and context detection using smartphone data – Detailed discussions of use cases, along with an additional use case to show the utility of our approach – Insightful and detailed discussion of our findings and future research directions including preliminary sketches of our proposed approach Overall, the specific contributions of our work include:  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  209  
   
  – A detailed domain analysis to present the state-of-the-art in visualizing complex human behavioral data from ubiquitous devices. – A thorough expert-led requirements, goal and task analysis to understand the workflow of analyzing smartphone-sensed data. – PLEADES, an interactive visualization analytics tool for reproducible and flexible population-level exploratory data analysis of smartphone-sensed data along with symptom reports. PLEADES allows analysts to select from multiple clustering and dimension reduction algorithms and provides multiple connected views for insightful analysis. – Demonstration of the utility of PLEADES through intuitive use cases concerning the analysis of in-the-wild collected health and wellness related smartphone data. – Evaluation of PLEADES with four experts in the fields of smartphone sensed health and data visualizations. – Discussion and proposal of future research directions.  
   
  2 Related Work 2.1 Health and Context Detection Using Smartphone-Sensed Data Smartphone-sensed data, along with human provided health and context labels has been shown to accurately detect a diverse variety of ailments. Madan et al. [35] collected symptom data such as coughing and sneezing during a flue season for a group of college students and were able to make models to predict infectious diseases like influenza. Smartphone sensed data also contains semantically important information and can provide insight into abnormal behaviors like mobility patterns that can be indicative of mental illness like depression [22, 33, 44, 50]. GPS sensed mobility has also been linked to general wellness measures such as loneliness, anxiety, affect, stress and energy [45]. Screen interactions to detect disruptions in Circadian Rhythms (sleep-wake patterns) [1], which have health ramifications [58]. Smartphone sensor data has also been shown to be predictive of college students’ academic performance [59] and social functioning in schizophrenia patients [60]. All these approaches relied on well labelled data that the analysts can use to build predictive models. However, data quality becomes an issue on a larger scale and longer term studies. Data visualizations can provide analysts with multiple contextualizing views to help analysts understand their participants better and also the results of their classification models. 2.2 Clustering Multivariate Data for Exploratory Data Analysis Unsupervised clustering, dimension reduction and projection are useful techniques for exploratory analysis of large and complex datasets [63]. Such approaches enable analysts to use their domain knowledge to perform important analytic tasks for flexible understanding of the data. Such tasks can include assigning data points to specific clusters, and using domain knowledge to merge or separate various clusters [62] [4, 61]. No computational models can find perfect solutions to separate complex data into meaningful groupings which is why interactive approaches that utilize domain expertise in  
   
  210  
   
  H. Mansoor et al.  
   
  addition to interactive analysis are valuable. Typically, researchers use such approaches to find groupings that support their research goals or to prove or disprove any conclusions [2, 23]. Interactive clustering, dimension reduction and projection have been used exploratory data analyses in many diverse domains such as health [11, 30], crime [21], parallel and distributed computing usage [31] and clinical data [24, 32]. Exploratory data analysis requires tweaking with several clustering algorithms, projections, parameters and results which can become overwhelming. Clustervision [30] by Kwon et al. is a visualization tool that presents ranked results across multiple dimension reduction and clustering algorithms for intuitive analysis of complex data. It uses multiple context providing linked visualizations across selectable data points. Clustrophile [19] and Clustrophile 2 [10] by Cavallo and Demiralp, are a family of interactive visual analytics tools to perform Exploratory Data Analysis of complex datasets by allowing analysts to fine tune dimension reduction parameters. They introduced a novel concept called the “Clustering Tour”, to present visualizations like feature-average heatmaps and enabled the comparison of different features across different grouping of the data. Chatzimparmpas et al. [12] created t-viSNE, an interactive visual analytics tool for analyzing t-SNE results using multiple linked panes with bar charts and parallel coordinate plots. Their approach was to enable a comprehensive control of the various parameters and inputs to visualize clustering results of t-SNE across multiple linked panes. Li et al. [32] used interactive clustering and projection to help analysts devise accurate classification models of using health records. All these approaches the wide-scale applicability of interactive clustering and projection approaches to analyze, contextualize and understand complex data. Our contribution is to utilize these visual clustering, dimension reduction and projection techniques to the exciting new domain of complex smartphone-sensed data. These approaches are particularly well-suited for analysis of smartphone-sensed data and health and wellness reports due to the many issues with such data including labelling and contextualization. In addition, traditional non-visual approaches become infeasible for exploratory data analysis as the population size of participants grows. Clustering and projection are well-suited to show overlaid human-supplied labels along with important semantic information such as the prevalence of weekdays and weekends against objective smartphone-sensed data. This allows the analysts to find important relationships in such data that can then guide their model building process. Mansoor et al. implemented COMEX [36] and DELFI [37], data visualization framework that present smartphone sensed data across multiple connected panes and used visual encodings of metrics like anomaly scores to guide analysts in finding mislabelled data and apply labels to unlabelled data. 2.3  
   
  Visual Analysis of Data Gathered In-the-Wild Using Ubiquitous Devices  
   
  Data visualizations are particularly well suited to understand smartphone-sensed data, as such data tends to be complex, multivariate and in need to additional analysis before any inferences can be made about health and context [40]. There are several examples of works that have utilized smartphone and smartwatch data to present data to everyday users for personal tracking and goal setting [14–16, 25, 28, 65]. In addition, such data is also used to provide analysts, both healthcare analysts and data scientists, with the  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  211  
   
  tools to make sense of such data [40]. Location data through phone connections with cell towers is one of the most commonly used data streams to analyze human behaviors and is often visualized for urban planning [29, 47, 51]. More granular person level GPS locations is also one of the most commonly collected data stream in in-the-wild studies [42] and can reveal a lot about about potential physical and mental health issues [5, 6]. Shen et al. created MobiVis [52], an interactive visualization analytics tool to understand individual and group behaviors such as movements, communications etc. and helped analysts to visually mine data by semantic filtering for analysis of “socialspacial-temporal” smartphone data, using an intuitive glyph called the “Behavior ring”. Intuitive glyph design can compactly present multiple important aspects of smartphonesensed data. Mansoor et.al. [38] created ARGUS, an interactive visual analytics tool to help analysts not only identify but explain breakages in behavioral rhythms, detected from smartphone-sensed data. They used a version of the z-glyph [8] to highlight presence of levels of behavioral rhythms and breakages therein. Data visualizations can also be used to fix issues with the data quality since automatically sensed data can include mislabelled and unlabelled data, which makes it difficult to improve machine learning classification accuracy. These works show the utility of interactive visual analytics approaches to help analysts at the various stages of smartphone-sensed health model building. Our work adds to this field by utilizing intuitive visual analytics techniques for unsupervised exploratory data analysis to guide model building. Specifically, our work aims to highlight important associations between reports of certain contexts and health symptoms with objective sensor data.  
   
  3 Goal and Task Analysis We conducted internal workshops with four experts in data science, at our department. The experts all had experience in building health and human context classifiers using in-the-wild collected smartphone-sensed data. We asked them about their requirements when working with such in-the-wild data. They stressed the need to have an interactive analysis approach for early exploration of the data, before they started training and testing classification models. The experts were focused on having ways to investigate information of study participants at a Population Level for scalability. From this high level view, they wanted to have the ability to drill down on specific groups of study participants, along with specific participants. They wanted flexible ways to use different statistical features and analyze their relationships with any human reported health and wellness symptoms. We discussed the high-level requirements that any interactive approach would need to fulfill. Here we abstract out the four main requirements: – R1: Ability to see data at a bird’s eye level i.e. at a population level rather than individual analysis. – R2: Design features from objective smartphone-sensed data that have been shown to have health/context predictive capabilities. For e.g. staying at primary location (usually deemed to be home more than usual has been associated with depression [22, 50]).  
   
  212  
   
  H. Mansoor et al.  
   
  – R3: Get an overview of the relationships (or lack thereof) of between objective sensor features and behaviors (detected and reported). – R4: Apply different algorithms and methods to the feature data to find interesting patterns and relationships in data. Interactive visual analytics approaches that let analysts experiment with clustering and projection algorithms fulfill the requirements that were expressed by the experts and have been reported to be very useful for exploratory data analysis [12, 30] in many domains. The experts were aware of clustering and projection and showed a lot of interest in having an interactive visualization interface that would help them fulfill their requirements, discussed above. The specific types of data points that the analysts were interested in clustering and projecting were day level aggregations of data across all the users. The analysts wanted to use a window of 24 h to divide up the data per user as human behaviors are highly influenced by daily cycles like sleep wake patterns [58]. In addition, several studies to collect in-the-wild data often asked participants to provide labels for days such as number of hours slept, quality of sleep and stress levels [6, 59]. After selecting interactive clustering, dimension reduction and projection as the visual approach, we asked the experts to specify the goals that they would have while using interactive clustering and projection. We summarize a list of goals that the experts had and the tasks to accomplish them: Goal 1: Grouping similar days: Segmenting every participant’s data into days and then grouping them in terms of similarity of features of sensor data to show important differences between days such as higher activity clusters vs. sedentary clusters etc. – Task 1: Specifying sensor features for smartphone data for clustering, to understand and analyze different behaviors. – Task 2: Applying clustering and dimension reduction techniques to display similar days on a 2D plane. Clusters will be color-coded. – Task 3: Display multiple possible iterations of the clustering and projection algorithms for interpretation, along with measures of to display the quality of clustering results. Goal 2: Understand the causative factors that lead to the clustering results: Analyze the summary of underlying objective sensor data that comprise the various clusters. – Task 4: Showing the calculated importance of smartphone-sensed features with regard to clustering results to understand cluster separation. For example, a result may assign screen interaction levels higher importance and the clusters may be separated by high screen interaction vs. low screen interaction. – Task 5: Show the variation in feature values between clusters to help analysts to assign semantic meaning to them. Goal 3: Compare individuals to populations and sub-populations: This lets analysts find interesting groupings of users. – Task 6: Show a selectable and filterable list of all participants for clustering analysis. – Task 7: Show the distribution of clusters for each participants’s data to help analysts assign semantic meaning. For instance, showing if a participant has more days in a cluster with lower mobility, activity levels etc.  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  213  
   
  Goal 4: Overlay human-labelled health and wellness symptoms on objective smartphone-sensed data: This helps analysts to assign semantic labels to data. For instance, clusters with days that have higher night screen usage also having poorer reported sleep and higher stress levels etc. – Task 8: Present summaries of human-labelled symptom data such as stress levels, average sleep hours and quality etc. for every cluster, along with enabling filtering and selection of specific days for analysis. – Task 9: Show external day-level factors that may explain the symptoms present (e.g. weekend vs weekdays, academic deadlines etc.) Goal 5: Saving exploration results for reproducible analysis: – Task 10: Storing results on the local machine from analysis sessions to share with other analysts and colleagues for reproducibility and to save time as clustering is computationally intensive.  
   
  4 Our Visual Approach: PLEADES After conducting the requirements, goals and task analysis, we designed and implemented an interactive visual analytics framework called Population Level Exploration and Analysis of smartphone DEtected Symptoms (PLEADES), to enable analysts to perform exploratory data analysis. We based our approach on the “Information Seeking Mantra” by Shneiderman et.al. [53] which states: “Show the important, zoom and filter, details on demand”. Using this principled approach, we designed the various views of PLEADES. Here we describe the main views and the rationale behind our designs. 4.1 Data Description and Clustering and Dimension Reduction Algorithm Selection and Features View The participant data was divided into days and the features were calculated for sensor values across multiple epochs such as day (9am-6pm or 8am-4pm), evening (6pm-12am or 4pm-12am) and night (12am-9am or 12am-8am), similar to [59]. The analyst can select different datasets (we provide access to two datasets: ReadiSens and StudentLife [59]) and can also specify the features that will be used for clustering (G1. T1). The analyst can also filter the participants whose data is used (all participants are included by default). The analyst can select from three dimension reduction (t-SNE, Isomap and multi-dimensional scaling) and three clustering (kMeans, agglomerative and spectral) techniques in (Fig. 2 H). Clicking on the “Features View” (FV) button shows a pane (Fig. 2 I) with all the available sensor values for the chosen dataset. This pane also allows the analyst to choose multiple epochs that the chosen sensor values can be averaged by. These features are then input into the clustering and projection algorithms (G1, T1). Cavallo and Demiralp [10] conducted thorough user studies for exploratory data  
   
  214  
   
  H. Mansoor et al.  
   
  B  
   
  J  
   
  A H  
   
  E  
   
  C D I  
   
  F  
   
  G  
   
  Fig. 2. PLEADES (Image from our earlier paper [39]): A) Every multi-colored bar is a clustering result for the chosen algorithms and number of clusters k. The results are ordered by the average “quality” score, of three different scores. The width of each colored bar in the represents the proportion of days in that cluster. B) Selecting a result projects it with every circle representing one day, color coded by cluster. C) Hovering over any day in the Clusters View shows its cluster’s details in the Cluster Detail View. D) Every study participant is a row in the Users View and the bars represent distribution across the clusters for their days. E) The feature value intensities across all clusters is shown in the Feature Distribution Heatmap. F) The F-Stats View shows the most important features for the selected clustering result, determined by the ANOVA F-Statistic. G) Every polyline is a day with the color representing the cluster. The axes represent features and are brushable i.e. analysts can select ranges of values. H) Analysts can select the clustering and dimension reduction algorithms. I) Selecting features and their epochs for averaging. These features will be calculated for all days which will then be clustered. J) Pre-computed clustering results from previous sessions are displayed to save analysts’ time.  
   
  analysis using clustering and projection and they reported that analysts showed particular focus on feature selection during their analyses as they explained that it was very important for the outcomes. 4.2  
   
  Clustering Results View (CRV)  
   
  The Clustering Results View (CRV) displays the clustering results after the analyst has selected the various parameters and clicked “Compute” in 2 H). Our approach was inspired by Kwon et.al. who designed and implemented Clustervision [30] a visualization framework for exploring complex data through clustering and projection. They also displayed multiple clustering results that were ordered by quality in addition to allowing for specification of clustering and projection algorithms. The CRV displays the results as horizontally stacked bars and the colors within those bars represent the cluster along with the widths representing the proportion of total days (across all participants) that belong to those cluster (Fig. 2 A). We used an 8 color palette from ColorBrewer [27] to assign each cluster a discernible color (G1, T2). The results are ordered by quality  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  215  
   
  (G1, T2, T3). We calculated the average across the three clustering quality measures: Silhouette score [49], Davies-Bouldin score [18] and Calinski-Harabasz score [7]. The quality scores are presented in the aforementioned order as squares on the left of the clustering results. The score value is encoded as opacity - higher quality ) vs. low quality: . Presenting various results of clustering and projection algorithms allows analysts to perform flexible exploration of the data for greater insight and intuition. 4.3 Clusters View (CV) The analyst can select a result in the CRV by clicking on it which shows the projection of the selected clustering result on a 2-Dimensional view in the Clusters View (CV). Every point is a day and the color encoding the cluster (Fig. 2 B), consistently throughout every other view of PLEADES. This view aims to give a view of days that are similar according to the clustering result, in addition of overlaps between different clusters (G1, T2). Using this view along with the CRV guides the analyst to dive deeper into specific clusters or days. The analyst can drill down on specific days by checking the “Select Days” (Fig. 2 B) box. They can then brush over the days of interest in the CV, which will show their details in the Cluster Details View (explained later) and highlight them in the Daily Values View (explained later). The analyst can also make a note by saving these days along with the participants associated with them, by giving their selection a name in a dialog that appears after the days are brushed (G5, T10). This enables analysts to apply their domain knowledge to understand whether certain days belong in a cluster, that have not been clustered by the automated approach. This also assists analysts in understanding the types of days that can be used for classification models. For example, they can assign meaningful semantic information, such as low stress and better sleep on days that are typically weekends and train and test a classifier on such days. 4.4 Users View (UV) The User’s View (UV) (Fig. 2 D) shows a list of the participants in the smartphonesensed symptoms studies. The colored bars within every participant’s row represent the distribution of that participant’s days across the different clusters (G3, T7). The user list can be sorted by the prevalence of days in a specific cluster, by clicking on that cluster’s respective color under “Sort users by cluster” (G3, T6) (Fig. 2 H). Hovering the mouse cursor over a user’s row highlights their days in the CV and the Daily Values View (explained later) while hiding others users’ days (G3, T7). The user can be filtered/selected for re-clustering by clicking on the checkboxes in their rows(G3, T6). 4.5 F-Stats View (FSV) The F-Stats View (FSV) (Fig. 2 F) displays the most important features for creating the clusters in the clustering chosen in CRV. We performed the Analysis Of Variance (ANOVA) test for every clustering result to obtain the f-statistic. We only show the  
   
  216  
   
  H. Mansoor et al.  
   
  features that have a statistically significant relationship i.e. p less than 0.05. We used a simple bar chart to show this ranked list of the most important features. This enables an analyst to reason about the chosen clustering result in terms of the proportion of importance given to each features and consequently the reasons for the separation between the different clusters (G2, T4). 4.6  
   
  Cluster Details View (CDV)  
   
  The Cluster Details View (CDV) (Fig. 2 C) visualizes aggregated data for the days in a cluster that are being hovered over in the CV. The CDV shows the information for the hovered over cluster against the averages across all clusters (G4, T8, T9). This bar represents the average across all days. The color of the fill with the gray stroke inside the gray bar represents the cluster of the day being hovered over in the CV. If the . The aggregated analyst has brushed over specific days in the CV, the fill color is: information depends on the daily measures that were contained in the dataset and can include comparisons such as the average incidence of occurrence of weekends in that cluster/ selected days, the average distance travelled and average sleep quality reported. 4.7  
   
  Daily Values View (DVV)  
   
  The Daily Values View (DVV) contains a parallel coordinates plot where every polyline represents a day and the Y-axes represent feature values (Fig. 2 G). The Y-axes are brushable and allow analysts to filter days based on specific ranges of features values. The lines are color-coded consistently to represent the cluster they belong to. This view lets analysts filter down on specific features and assign semantic meaning to clusters (G4, T8). 4.8  
   
  Feature Distribution Heatmap (FDH)  
   
  The Feature Distribution Heatmap (FDH) (Fig. 2 E) visualizes the average values of the features across the clusters with a gradient of dark blue to dark red, representing low and high values respectively (G2, T5). The features are shown ordered from top to bottom based on their importance (shown in FSV). This view is important as it shows the distributions of feature values across all clusters to help analysts compare between clusters and understand the representative characteristics of the days within clusters. This helps the analyst in quickly assigning semantic meaning to different clusters. 4.9  
   
  Saved Results View (SRV)  
   
  The Saved Results View (SRV) lets the analysts save the results of data exploration sessions. Clustering complex data is computationally time-consuming and intense. Constantly re-running clustering algorithms may not be scalable or reproducible and may disadvantage researchers with limited access to the latest computing hardware [9]. The analyst can save results from a data analysis session they performed by clicking on the “Save New Result” button in the SRV (Fig. 2 J). The clustering results along with all  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  217  
   
  C B  
   
  A  
   
  D  
   
  E  
   
  Fig. 3. (Image from our earlier paper [39]) kMeans clustering of every day across every participant based on the similarity of their geo-location features. The results are then projected using t-SNE. A) A clustering result with k=3 and high quality (Davies-Bouldin score) and the associated Feature Distribution Heatmap. B) Selecting a result with k=5. Cluster details are shown for the five clusters. The yellow cluster has high presence in “Res Grad night” and “Res Grad day”, where as the green and purple clusters have high values for presence in the “Res Undergrad day, evening and night”, possibly indicating two different student populations i.e. graduate students and undergraduate students. E) Brushing over “Res Grad night” shows no purple or green lines. (Color figure online)  
   
  other associated data will be stored for quick future access. The analyst can provide a descriptive name for the session which will then be displayed in the list every time PLEADES is started and can be selected to show the earlier session (G5, T10).  
   
  5 Evaluation with Use Cases We introduce Nina, a PhD candidate in computational psychology. Nina wants to build machine learning models that can accurately identify health symptoms like bad sleep and stress levels using objective smartphone-sensed data, with some human labels. Nina has two datasets (ReadiSens and StudentLife [59])and she would like to perform exploratory data analysis on them using PLEADES, before she start building her classifiers. She wants to get a high level understanding of relationships between symptoms and sensor data. 5.1 StudentLife (Dataset 1) The first dataset was collected by the StudentLife [59] project. This project collected data for 49 Dartmouth University (USA) students throughout a 10-week term. The students were required to install a smartphone application which passively gathered sensor data including:  
   
  218  
   
  H. Mansoor et al.  
   
  C  
   
  A  
   
  B  
   
  Fig. 4. (Image from our earlier paper [39]) After clustering potential grad and undergrad students, there are four clusters of undergraduate students. This can help insights by observing how undergraduates differ in their behaviors and how their smartphone labelled symptoms manifest in objective sensor data.  
   
  – – – – – –  
   
  Screen interactions Light Conversations (automatically detected) Activities (stationary, walking, running and unknown) On-campus location (detected through WiFi) GPS locations.  
   
  The buildings are binned into categories such as undergraduate-residential, graduate-residential, dining, academic and admin-services. The application also gathered GPS coordinates that we clustered using DBSCAN [20]. Unlike clustering approaches like k-means, DBSCAN does not need a predefined number of clusters, which is useful in our use cases as individuals can have varying travel patterns. DBSCAN is a density-based method with two hyperparameters: min points and min distance. A point x is a core point if there are at least min points number of points - set as five - in a distance of min distance from a point - set as 300m. We used the primary and secondary location only, which are defined as the geo-cluster where the participants spent most and the second most time in. Students were asked to respond to daily surveys about their sleep quality, hours of sleep and stress levels. The data was divided into days for every participant and analyst-specified features are also calculated per day. We calculated mobility statistics for the days in every cluster in the clustering result including the average distance travelled and the location entropy i.e. how many different geo-clusters were visited. These features are linked to symptoms [22]. We also visualize the proportion of weekends in every cluster in addition to the proportion of days in the “midterm”. This refers to a time period in the study that was academically demanding.  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  219  
   
  B A  
   
  C  
   
  Fig. 5. (Image from our earlier paper [39]) Days in the selected clump have little presence on campus and are more likely to be weekends, along with higher than average distance being travelled.  
   
  5.2 Use Case 1: Gaining a Quick Overview of the Data To get a high level view of the data, Nina first selects all of the sensor and chooses across 3 epochs (day, evening and night) (Fig. 2 I). These features are used to cluster and project using agglomerative clustering (k = 6 clusters) and t-SNE respectively (G1, T1). The clustering results with three and four clusters (second and third highest quality) (G1, T3) are selected. Nina interacts with them in the CV and finds nothing particularly interesting in the CDV. She then selects fourth highest quality clustering result (k = 5) (Fig. 2 A), that has good quality according to the Davies-Bouldin score (G1, T3). She sees in the FSV that on-campus building presence related features (Res Undergrad, Common Space, Dining) are very important for this clustering result (G2, T4). She hovers over the mouse over the clusters in the CV to view their details in the CDV. She sees that days in cluster (Fig. 2 B), have a higher amount of deadlines, higher stress levels, poorer sleep and generally fall on weekdays (Fig. 2 C) (G4, T8, T9). This makes intuitive sense as deadlines cause behavioral changes. In contrast to this cluster, has more days that are weekends, slightly better sleep duration and quality, fewer deadlines, and more distance traversed. Such contrasts clusters makes Nina curious and she decides to save this clustering session using the SRV and name it “More deadlines with less sleep” (G5, T10).  
   
  220  
   
  5.3  
   
  H. Mansoor et al.  
   
  Use Case 2: Examining Student Characteristics that Can Be Helpful for Insightful Comparisons  
   
  Nina realizes that features for presence in on campus residences were important across several clustering results. For the next clustering results, she only selects the location features (on-campus buildings and GPS locations) (Fig. 2 H) and re-computes the clustering results using K-Means (k = 6) and t-SNE across all participants. She selects a clustering result with k = 4 (Fig. 3 A). She sees in the FDH that there is one cluster with high presence in Res Grad and two clusters for which the constituent days had high levels of presence in Res Undergrad. She wants to see if she can find clusters with (Fig. 3 clearer feature distribution and selects a result with k = 5 (Fig. 3 B). Cluster B) has higher incidence of being in Res Undergrad across all times of day (Fig. 3 C) and has a higher value of being in other on-campus buildings such as Libraries, cluster Academic etc. (G2, T5). The days in cluster have few to no incidence of being in Res Undergrad. There are also few incidences of being in other on-campus building, with the exception being Res Graduate (Fig. 3 C). Sorting the UV (Fig. 3 D) using these three clusters and brushing on the “Res Grad night” axis in the DVV reveals that there and cluster (G3, T6). are no users who have days in both clusters This seems to be an indication that there are two sub-populations (G3, T7) in this dataset. Nina is aware that the StudentLife [59] study included both undergraduate and and are undergraduate students. Nina thinks that the users with days in clusters graduates whereas the users with days in are graduate students. This is an important finding for her as one of the goals at the start of the analysis session that she had for was to compare the behavior patterns and symptoms between different populations. Undergraduate and graduate students generally differ in several life circumstances such as their ages along with course-loads, familial commitments etc. She examines the details for these three clusters in the CDV to see if there are any significant difference. She sees (Fig. 3 B), the students reported slightly slightly more stress and slightly that for worse sleep along with more deadlines (G4, T8). Interestingly, for cluster (Fig. 3 B), the students reported average sleep quality, slightly lower stress levels and fewer than did not report any concerning average deadlines. The students in with days in cluster symptoms (Fig. 3 B). Nina wants ti discuss these findings with other students in her lab and proceeds to save the results from this analysis session in the SRV , calling it “Geo analysis of StudentLife dataset” (G5, T10). Nina found the overlaying of semantically understandable information like the different kinds of on-campus buildings with the objective sensor-data features to be useful. She liked having the ability to sort users by clustering prevalence that made it easy for her to discover these two populations of students. She also notices in the FDV that there are fewer days of presence in any on-campus building (Fig. 3 C) for the days in cluster . She hovers the mouse over a day in that cluster and notices in the CDV that the students travelled much more distance (Fig. 3 B) than usual for these days. Interestingly, these days were much more likely to be weekends, which makes intuitive sense to Nina (Fig. 5 A) and proceeds to select those days (G4, T9). Nina notices a odd shape in by clicking on “Select Days” (Fig. 2 B) and then brushing over the days in that clump.  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  221  
   
  B D A C  
   
  Fig. 6. (Image from our earlier paper [39]) Visualizing ReadiSens data (dataset 2). A) Clustering the geo-features of the ReadiSens data with kMeans and projecting it using Isomap. The yellow cluster has a much higher proportion of weekdays than other clusters along with higher levels of distance travelled, perhaps indicating work-life routine. The pink cluster has days that are more likely to be weekends and with lower sleep quality and little time spent in either the primary or the secondary locations. (Color figure online)  
   
  She sees in the CDV (Fig. 5 B) that these days are much more likely to be weekends when compared with the overall cluster , in addition to having much higher distance travelled. She also notices (Fig. 5 C) that there is little to no presence for all these days in on-campus buildings in the DVV. She is quite confident that the students travelled for these days. Additionally, she sees more hours of sleep, slightly better sleep quality and fewer deadlines. This is interesting for Nina as she is now able to assign semantic context to objective sensor data. She also plans to build classification models using these days, which can then enable her to find similar days in other clusters. She makes a note for future analysis by labeling these days “Travelling off campus” (G5, T10). 5.4 Use Case 3: Analyzing Graduate Vs. Undergraduate Students Nina wants to analyze the two distinguishable sub-populations and compare them to each other. She selects the students that she is reasonably confident belong to either group (15 undergraduates and 8 graduates) in the UV and re-runs the clustering and projection (same algorithms) for their data and k = 6 (G3, T6, T7). She chooses a result with k = 6 and looks at the FDH to gain a similar understanding to the last use case. She is particularly interested in viewing the distribution of incidences of presence in oncampus buildings. She notices four clusters  
   
  where the participants had a  
   
  with higher high incidence of being present in Res Undergrad and a single cluster Res Graduate (Fig. 4 C). Hovering the mouse over shows more than average number of days in the midterm with poorer sleep quality (Fig. 4 A). Interestingly, the students reported less stress and sociability issues (G4, T8) for . For , Nina noticed higher than average distance travelled with slightly worse stress and sleep quality levels (Fig. 4 B). She was able to gain a finer level of view for a population she identified earlier using PLEADES. She save the results to show her analysis to her research partners (G5, T10). 5.5 ReadiSens (Dataset 2) The second dataset that Nina has access to is called ReadiSens, which was provided to our team by a third party data collector. It contains data for 76 participants in a large  
   
  222  
   
  H. Mansoor et al.  
   
  A  
   
  B  
   
  C  
   
  Fig. 7. (Image from our earlier paper [39]) A) More likely to be weekends and the sleep quality is poorer. B) Average weekends with average sleep quality and little travel. C) The clump on the right has poorer sleep quality and more travelling. In addition the days in this clump are much more likely to be weekends.  
   
  scale smartphone sensed data study. The dataset also contains reported symptoms such as sleep duration and quality. The participants installed the ReadiSens data collection application that ran passively in the background and administered surveys daily and weekly to collect health and wellness symptom reports. The participants were completely anonymized. The sensors include: – – – –  
   
  GPS locations (anonymized while still keeping spatial relevance) Activity levels Screen interactions Sound levels.  
   
  We derived the mobility features from the GPS location data was using the same methodology as the previous dataset. We also used the same methodology (DBSCAN clustering) to derive participant’s primary and secondary locations. The participants provided daily answers about sleep quality and duration through an app administered survey. There was variance in the level of compliance for providing labels. The sensor data was divided per day across all users and we calculated the same contextual and mobility features (e.g. proportion of weekends, distance travelled etc.) as the previous dataset. 5.6  
   
  Use Case 4: Patterns of Presence in Primary Vs. Secondary Location  
   
  Nina selects all the data features across all epochs. She chooses kMeans and Isomap and selects k = 6. She goes through a few clustering results but does not seem to find  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  223  
   
  insight that stands out (G1, T2, T3). She decides to specify a single feature type (geolocation) and drill down on it (G1, T1). Nina is aware of research that shows that GPS detected location patterns can accurately identify semantically important location like “home” [22, 50] vs. “work” which in turn have important health ramifications [26, 48]. She selects primary and secondary geo-clusters across the 3 epochs and clusters the data using the same algorithms as before with k = 6. Nina selects a clustering result with k = 6 and interacts with some clusters (Fig. 6 , the participants typically stayed in their A) and sees that for the days in cluster primary location for all 3 epochs. These days were more likely to be weekends, with more than average sleep hours and less distance travelled (G4, T8, T9). Nina thinks that represents times where the participant stayed “home”. Having multiple connected visualizations make this intuition easier since she does not have access to actual GPS location for which she can get a human understandable location type from services like [46]. She assigns the days in this cluster with a label for this semantic information. Next she hovers over the cluster . These days are less likely to be weekends (Fig. 6 A). In addition, the participants are typically in their secondary location more during the evening and the day with much less presence in their secondary location at night (Fig. 6 B). Participants are also typically not at their primary location during the day and are a slightly more present there during evening. However, they are usually there for the night (Fig. 6 B). These days also had higher distance travelled reports of fewer than average sleep hours. Nina can confidently guess that these days are part of a home vs. work routine. She saves this clustering session in the SRV at “Home vs. work”. This is important for her as shift work behavior has long term health ramifications [48, 55]. Additionally, this is an ongoing project and the machine learning classifiers that Nina builds using these days can be deployed to identify future work patterns. and notices that these days are much more likely Nina hovers over the cluster to be weekends. She sees in the FDH that there are few instances of participants being present in their primary location or secondary location for  
   
  (Fig. 6 B). There is also  
   
  is a drop in the quality of sleep (G4, T8). This along with the fact that this cluster relatively small leads Nina to think that this cluster has days where participants travelled and stayed away from either home or work. However, the clustering result is of poor quality (Fig. 6 A) and the projection on the plane is scattered. Nina is unable to see a and decides to tweak the clustering parameters. clear spatial grouping of She selects location features and selects t-SNE, kMeans and k = 6. A result (k = 5) has a better quality than the previous selection (G1, T2, T3) (Fig. 7). The cluster (Fig. 7 A) has a higher number of weekend days and interestingly lower sleep quality. and selects those days separately. She can see that days in Nina sees two clumps of first clump (Fig. 7 B) are more likely to be weekends, with fewer hours of sleep, little movement across geo-locations and average sleep quality(G4, T8, T9). Nina sees that (Fig. 7 C) are more likely to be weekends, have higher distance travelled and also have poorer quality of sleep (G4, T8, T9). She is now confident that these days represent travel. Nina to wants to make classifiers that can detect this behavior in data that may not contain any human provided labels. She makes a note for these days as “Travel-  
   
  224  
   
  H. Mansoor et al.  
   
  ling with poor sleep scores” in the dialog that appears after the their selection the CV (G5, T10).  
   
  C A  
   
  Fig. 8. The participants with days in  
   
  B  
   
  cluster appear to stay at primary location most of the  
   
  time, whereas the the participants with days in later hours of the day.  
   
  5.7  
   
  tend to stay at at secondary location during  
   
  Use Case 5: Understanding Variances in Behaviors of Potentially Different Shift Workers  
   
  After use case 4, Nina is interested to see if she can find different kinds of shift workers in the anonymized Readisens data. Nina is aware of research that shows strong correlations between shift work and mental health [17, 55]. The location data has been anonymized which means she is unable to rely on services like Foursquare [46] to find out what the human understandable type of location. She chooses k = 6, location features, t-SNE and kMeans for dimension reduction and clustering algorithms respectively (G1, T2, T3). She selects a clustering result with average scores. She notices in cluster, the participants tend to stay at primary location CDV that for the days in ((Fig. 8 C)) with little distance travelled and roughly the average amount of days in being weekends (Fig. 8 A) (G4, T8, T9). In contrast, the days in the  
   
  cluster tend to  
   
  days have slightly much more likely to be weekdays (Fig. 8 B). Nina notices that fewer sleep hours. She also sort by both the clusters in the UV and notices that there is a and . Nina wonders whether these small number of participants with days in both days are representative of shift work (particularly later in the evening) vs. stay-at-home careers. She is interested in looking further into these days and the participants with days in these clusters, to see if she can find further differences in health markers. She makes a note of the days in both the clusters (G5, T10).  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  225  
   
  6 Evaluation with Experts To evaluate our interactive visual analytics approach, we invited three experts in implementing health predictive models using machine and deep learning using smartphonesensed data. They were all PhD candidates in data science. We also invited one expert in interactive data visualizations, who had a master’s degree in computer science and extensive experience in designing and implementing data visualizations. Due to COVID-19 restrictions, we held the evaluation session using video screen-sharing to demonstrate PLEADES. The experts were asked to feel free to contribute any feedback at any point of the evaluation session. After a brief walk-through tutorial, the experts were shown the same use cases as Nina. They were all very familiar with the concept of unsupervised clustering and projection as a useful way for performing exploratory data analysis. They found the ability to select sensor features and along with epochs to be quite useful. They mentioned that during early stage exploration of such data, they would require several iterations of parameters and algorithms before discovering interesting results. They also like the ability to save results from previous analysis sessions they were well aware that clustering and projection are computationally intensive tasks and that having the ability to save results would enable them to revisit prior insights in addition to fostering collaboration since they can share the results with their colleagues. One expert (in smartphone-sensed data model building) found our approach of separating out the raw sensor-level data from contextual data such as the proportion of days in weekends for specific clusters, the average amount of sleep for days in the cluster etc. in the CDV as “it shows two types of information like features that are maybe more granular and only smartphone detectable and then you have this contextual information that adds more semantic meaning.” The experts suggested interesting potential groupings of the participants that we had not considered, while walking through the use cases. For the ReadiSens dataset (Sect. 5.6), one expert wanted to compare regular travellers with stay-at-home people as both these have been shown to be predictive and (Figure 6 D). After seeof health issues [64]. He ordered the users by ing the ordered lists, he wanted to select a sub-sample of the participants in the two extremes of movements patterns and then compute a machine learning classification model to analyze those users and whether they can be clearly separated. For the StudentLife dataset, the experts found an interesting grouping when going through use case cluster, there was very little presence 2 (Sect. 5.3). They noticed that for the days in in on-campus residences but there was more presence in other on-campus buildings. There was also relatively larger presence in the primary location. The experts made an informed guess that the students with days in this cluster may have off-campus residences. One expert suggested that they would like to filter and group on-campus and off-campus students and using PLEADES, analyze the clustering and projection results of that data to observe changes in symptoms. On the whole, the experts found our approach to be valid and PLEADES useful. They demonstrated interest in using PLEADES to perform exploratory data analysis on smartphone sensed health and wellness data and to assign understandable semantic labels to objective sensor data.  
   
  226  
   
  H. Mansoor et al.  
   
  7 Discussion, Limitations and Future Directions An important aspect that the evaluators with research experience in machine learning model building, brought up was the learning curve behind using such a tool. They were all familiar utilizing data visualizations in other projects but felt that it would be very helpful to create a clear guide on how the views are connected and what specifically is being visualized by every pane.  
   
  Secondary Location During Day  
   
  Sorted by feat value  
   
  Primary Location During Day  
   
  A  
   
  C Sorted by clusters  
   
  Secondary Location During Day Primary Location During Day Secondary Location at Night Primary Location at Night Activity levels During Day … … ...  
   
  Nov 2019  
   
  Dec 2019  
   
  B  
   
  D Jan 2020  
   
  Feb 2020  
   
  Mar 2020  
   
  Apr 2020  
   
  May 2020  
   
  Fig. 9. A) Clustering participants to form cliques of users. B) Understanding the feature characteristics of the group C) Multiple sort-able axes to understand feature value distributions across different groups. D) Select-able time windows for seasonal analyses.  
   
  In addition, research in smartphone-sensed health monitoring is focusing more on longer term deployment for capturing data passively with minimal human labelling burden for health labels. Interactive visual analytics for health analyses of large populations challenging especially for a complex domain like smartphone-sensed health. Day level data analysis (which we have performed so far) of thousands of users is not scalable. In addition, external factors cause a large amount of diversity in populations: E.g.: comparing sleep patterns between night vs. day various shift workers, or comparing activity levels between laborers vs. desk workers etc. There also needs to be taken into account Intra-group differences as well. In addition, large scale changes in behaviors such as seasonal changes around winter breaks or large scale societal changes like COVID-19 may also be important to account for in any population level analysis. We will be investigating visual analytics approaches like multi-level visual aggregation, grouping and clustering to present such data for future work (Fig. 9). We performed a preliminary goal and task analysis for visual analytics-enabled population-level analysis of smartphone-sensor data. The first goal that we came up with was to allow analysts to define groups and make comparisons between those groups based on different health behaviors and data feature values. To enable this, the  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  227  
   
  first task is to cluster smartphone users into visual groups or cliques that can be color coded (Fig. 9 A and C). Such groupings of users can differentiate between laborers vs. desk workers vs. stay at home people etc. and can enable analysts to compare individuals more meaningfully. The second task is to enable Semantic Feature Definition for e.g. defining active screen use as not sleeping, different locations as home etc. based on temporal patterns of stays. Such Human-understandable features rather than sensorvalues can enable health experts to make sense of such data on a larger scale. The second goal is to determine distinct smartphone-sensed group signature and phenotype. To achieve that, we propose presenting feature distributions for color coded cliques (Fig. 9 C). Showing feature distributions across user cliques can facilitate the discovery of various inter and intra-clique correlations. The third goal is to facilitate interactive temporal analysis of health behaviors across different cliques (Fig. 9 D). We propose adding Interactive timeline window selection to calculate the data features over. This can enable analysts to take into account the various large level changes that can be expected at different times of the year. In addition to exploratory data analysis, data visualizations have been shown to be effective in fostering Explainable Machine Learning [13]. An important future direction for this research is to integrate machine learning pipelines for such exploratory data analyses that can enable analysts to run classification models. Analysts may plug and play different machine learning architectures, algorithms and parameters to optimize classification accuracy and computation expense [9]. For example, selecting days in a cluster with higher than average stress or poor sleep labels and using a visual interface to specify the machine learning algorithm, parameters and architecture to train a classifier using data from those days. The interface can show various metrics from the classification results such as accuracy, loss etc. and allow the analyst to tweak various parameters to see if the results can be improved incrementally.  
   
  8 Conclusion We researched, designed and implemented PLEADES, an interactive visual analytics tool to help analysts perform exploratory data analysis of smartphone-sensed data along with reported health and wellness measures. PLEADES helps analysts contextualize the factors that lead to the manifestation of smartphone detected and reported health and wellness symptoms. Our approach enables the analysts to select clustering parameters including the number of clusters desired, along with the ability to specify multiple clustering and dimension reduction techniques. PLEADES used multiple intuitively linked panes with interactive visualizations like heatmaps, bar charts and brushable parallel coordinated plots to help analysts understand different clustering results. This allows analysts to make important semantic links between human reported data and objective smartphone sensed-data. We validated our approach with intuitive use cases that visualized two real world datasets along with feedback from experts in smartphone-health detection and data visualizations.  
   
  228  
   
  H. Mansoor et al.  
   
  References 1. Abdullah, S., Murnane, E.L., Matthews, M., Choudhury, T.: Circadian computing: sensing, modeling, and maintaining biological rhythms. In: Rehg, J.M., Murphy, S.A., Kumar, S. (eds.) Mobile Health, pp. 35–58. Springer, Cham (2017). https://doi.org/10.1007/978-3-31951394-2 3 2. Andrienko, G., Andrienko, N., Rinzivillo, S., Nanni, M., Pedreschi, D., Giannotti, F.: Interactive visual clustering of large collections of trajectories. In: 2009 IEEE Symposium on Visual Analytics Science and Technology, pp. 3–10. IEEE (2009) 3. van Berkel, N., Goncalves, J., Wac, K., Hosio, S., Cox, A.L.: Human accuracy in mobile data collection (2020) 4. Boudjeloud-Assala, L., Pinheiro, P., Blansch´e, A., Tamisier, T., Otjacques, B.: Interactive and iterative visual clustering. Inf. Vis. 15(3), 181–197 (2016) 5. Boukhechba, M., Chow, P., Fua, K., Teachman, B.A., Barnes, L.E.: Predicting social anxiety from global positioning system traces of college students: feasibility study. JMIR Mental Health 5(3), e10101 (2018) 6. Boukhechba, M., Daros, A.R., Fua, K., Chow, P.I., Teachman, B.A., Barnes, L.E.: Demonicsalmon: Monitoring mental health and social interactions of college students using smartphones. Smart Health 9, 192–203 (2018) 7. Cali´nski, T., Harabasz, J.: A dendrite method for cluster analysis. Commun. Stat. Theory Methods 3(1), 1–27 (1974) 8. Cao, N., Lin, Y.R., Gotz, D., Du, F.: Z-glyph: Visualizing outliers in multivariate data. Inf. Vis. 17(1), 22–40 (2018). https://doi.org/10.1177/1473871616686635 9. Cashman, D., Perer, A., Chang, R., Strobelt, H.: Ablate, variate, and contemplate: Visual analytics for discovering neural architectures. IEEE Trans. Visual Comput. Graphics 26(1), 863–873 (2019) 10. Cavallo, M., Demiralp, C ¸ .: Clustrophile 2: Guided visual clustering analysis. IEEE Trans. Visual Comput. Graphics 25(1), 267–276 (2018) 11. Cavallo, M., Demiralp, C ¸ .: A visual interaction framework for dimensionality reduction based data exploration. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pp. 1–13 (2018) 12. Chatzimparmpas, A., Martins, R.M., Kerren, A.: t-visne: Interactive assessment and interpretation of t-sne projections. IEEE Trans. Visual Comput. Graphics 26(8), 2696–2714 (2020) 13. Chatzimparmpas, A., Martins, R.M., Jusufi, I., Kerren, A.: A survey of surveys on the use of visualization for interpreting machine learning models. Inf. Vis. 19(3), 207–233 (2020) 14. Chen, C., Wu, R., Khan, H., Truong, K., Chevalier, F.: Vidde: Visualizations for helping people with copd interpret dyspnea during exercise. In: The 23rd International ACM SIGACCESS Conference on Computers and Accessibility, pp. 1–14 (2021) 15. Choe, E.K., Lee, B., Kay, M., Pratt, W., Kientz, J.A.: Sleeptight: low-burden, self-monitoring technology for capturing and reflecting on sleep behaviors. In: Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pp. 121–132 (2015) 16. Choe, E.K., Lee, B., Zhu, H., Riche, N.H., Baur, D.: Understanding self-reflection: how people reflect on personal data through visual data exploration. In: Proceedings of the 11th EAI International Conference on Pervasive Computing Technologies for Healthcare, pp. 173–182 (2017) 17. Costa, G.: Shift work and health: current problems and preventive actions. Saf. Health Work 1(2), 112–123 (2010) 18. Davies, D., Bouldin, D.: A cluster separation measure. IEEE Trans. Patter Anal. Mach. Intell. (1979)  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  229  
   
  19. Demiralp, C ¸ .: Clustrophile: A tool for visual clustering analysis. arXiv preprint arXiv:1710.02173 (2017) 20. Ester, M., Kriegel, H.P., Sander, J., Xu, X., et al.: A density-based algorithm for discovering clusters in large spatial databases with noise. In: Kdd, vol. 96, pp. 226–231 (1996) 21. Fujiwara, T., Kwon, O.H., Ma, K.L.: Supporting analysis of dimensionality reduction results with contrastive learning. IEEE Trans. Visual Comput. Graphics 26(1), 45–55 (2019) 22. Gerych, W., Agu, E., Rundensteiner, E.: Classifying depression in imbalanced datasets using an autoencoder-based anomaly detection approach. In: 2019 IEEE 13th International Conference on Semantic Computing (ICSC), pp. 124–127. IEEE (2019) 23. Guo, P., Xiao, H., Wang, Z., Yuan, X.: Interactive local clustering operations for high dimensional data in parallel coordinates. In: 2010 IEEE Pacific Visualization Symposium (PacificVis), pp. 97–104. IEEE (2010) 24. Guo, R., et al.: Comparative visual analytics for assessing medical records with sequence embedding. Visual Informat. 4(2), 72–85 (2020) 25. Gupta, A., Tong, X., Shaw, C., Li, L., Feehan, L.: FitViz: a personal informatics tool for selfmanagement of rheumatoid arthritis. In: Stephanidis, C. (ed.) HCI 2017. CCIS, vol. 714, pp. 232–240. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-58753-0 35 26. Harrington, J.M.: Health effects of shift work and extended hours of work. Occup. Environ. Med. 58(1), 68–72 (2001) 27. Harrower, M., Brewer, C.A.: Colorbrewer. org: an online tool for selecting colour schemes for maps. Cartographic J. 40(1), 27–37 (2003). https://doi.org/10.1179/ 000870403235002042 28. Heng, T.B., Gupta, A., Shaw, C.: Fitviz-ad: A non-intrusive reminder to encourage nonsedentary behaviour. Electron. Imaging 2018(1), 1–332 (2018) 29. Krueger, R., et al.: Birds-eye - large-scale visual analytics of city dynamics using social location data. Comput, Graphics Forum 38(3), 595–607 (2019). https://doi.org/10.1111/cgf. 13713 30. Kwon, B.C., et al.: Clustervision: Visual supervision of unsupervised clustering. IEEE Trans. Visual Comput. Graphics 24(1), 142–151 (2017) 31. Li, J.K., et al.: A visual analytics framework for analyzing parallel and distributed computing applications. In: 2019 IEEE Visualization in Data Science (VDS), pp. 1–9. IEEE (2019) 32. Li, Y., Fujiwara, T., Choi, Y.K., Kim, K.K., Ma, K.L.: A visual analytics system for multimodel comparison on clinical data predictions. Visual Informat. 4(2), 122–131 (2020) 33. Liang, Y., Zheng, X., Zeng, D.D.: A survey on big data-driven digital phenotyping of mental health. Inf. Fusion 52, 290–307 (2019) 34. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. J. Mach. Learn. Res. 9, 2579–2605 (2008) 35. Madan, A., Cebrian, M., Moturu, S., Farrahi, K., et al.: Sensing the “health state” of a community. IEEE Pervasive Comput. 11(4), 36–45 (2011) 36. Mansoor, H., Gerych, W., Buquicchio, L., Chandrasekaran, K., Agu, E., Rundensteiner, E.: Comex: Identifying mislabeled human behavioral context data using visual analytics. In: 2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC), vol. 2 (2019). https://doi.org/10.1109/COMPSAC.2019.10212 37. Mansoor, H., Gerych, W., Buquicchio, L., Chandrasekaran, K., Agu, E., Rundensteiner, E.: Delfi: Mislabelled human context detection using multi-feature similarity linking. In: 2019 IEEE Visualization in Data Science (VDS) (2019). https://doi.org/10.1109/VDS48975.2019. 8973382 38. Mansoor, H., et al.: Argus: Interactive visual analysis of disruptions in smartphone-detected bio-behavioral rhythms. Visual Informat. 5(3), 39–53 (2021)  
   
  230  
   
  H. Mansoor et al.  
   
  39. Mansoor., H., et al.: Pleades: Population level observation of smartphone sensed symptoms for in-the-wild data using clustering. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - IVAPP: IVAPP, vol. 3, pp. 64–75. INSTICC, SciTePress (2021). https://doi.org/10.5220/ 0010204300640075 40. Mansoor, H., et al.: Visual analytics of smartphone-sensed human behavior and health. IEEE Comput. Graphics Appl. 41(3), 96–104 (2021) 41. Mead, A.: Review of the development of multidimensional scaling methods. J. Royal Stat. Soc. Ser. D (The Statistician) 41(1), 27–39 (1992) 42. Melcher, J., Hays, R., Torous, J.: Digital phenotyping for mental health of college students: a clinical review. Evid. Based Ment. Health 23(4), 161–166 (2020) 43. Mendes, E., Saad, L., McGeeny, K.: (2012). https://news.gallup.com/poll/154685/stayhome-moms-report-depression-sadness-anger.aspx 44. Mohr, D.C., Zhang, M., Schueller, S.M.: Personal sensing: understanding mental health using ubiquitous sensors and machine learning. Annu. Rev. Clin. Psychol. 13, 23–47 (2017) 45. M¨uller, S.R., Peters, H., Matz, S.C., Wang, W., Harari, G.M.: Investigating the relationships between mobility behaviours and indicators of subjective well-being using smartphone-based experience sampling and gps tracking. Eur. J. Pers. 34(5), 714–732 (2020) 46. NPR: https://developer.foursquare.com/ 47. Pu, J., Xu, P., Qu, H., Cui, W., Liu, S., Ni, L.: Visual analysis of people’s mobility pattern from mobile phone data. In: Proceedings of the 2011 Visual Information CommunicationInternational Symposium, p. 13. ACM (2011) 48. Ravesloot, C., et al.: Why stay home? temporal association of pain, fatigue and depression with being at home. Disabil. Health J. 9(2), 218–225 (2016) 49. Rousseeuw, P.J.: Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. J. Comput. Appl. Math. 20, 53–65 (1987) 50. Saeb, S., et al.: Mobile phone sensor correlates of depressive symptom severity in daily-life behavior: an exploratory study. J. Med. Internet Res. 17(7), e175 (2015) 51. Senaratne, H., et al.: Urban mobility analysis with mobile network data: a visual analytics approach. IEEE Trans. Intell. Transp. Syst. 19(5), 1537–1546 (2017) 52. Shen, Z., Ma, K.L.: Mobivis: A visualization system for exploring mobile data. In: 2008 IEEE Pacific Visualization Symposium, pp. 175–182. IEEE (2008) 53. Shneiderman, B.: The eyes have it: A task by data type taxonomy for information visualizations. In: Proceedings of the IEEE Symposium on Visual Languages, pp. 336–343. IEEE (1996) 54. Tenenbaum, J.B., De Silva, V., Langford, J.C.: A global geometric framework for nonlinear dimensionality reduction. Science 290(5500), 2319–2323 (2000) 55. Torquati, L., Mielke, G.I., Brown, W.J., Burton, N.W., Kolbe-Alexander, T.L.: Shift work and poor mental health: a meta-analysis of longitudinal studies. Am. J. Public Health 109(11), e13–e20 (2019) 56. Vaizman, Y., Ellis, K., Lanckriet, G., Weibel, N.: Extrasensory app: Data collection in-thewild with rich user interface to self-report behavior. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pp. 1–12 (2018) 57. Van Berkel, N., Ferreira, D., Kostakos, V.: The experience sampling method on mobile devices. ACM Comput. Surv. (CSUR) 50(6), 1–40 (2017) 58. Vetter, C.: Circadian disruption: What do we actually mean? Euro. J. Neurosc.(2018) 59. Wang, R., et al.: Studentlife: assessing mental health, academic performance and behavioral trends of college students using smartphones. In: Proceedings of the 2014 ACM International Joint Conference On Pervasive And Ubiquitous Computing, pp. 3–14 (2014)  
   
  Exploratory Data Analysis of Population Level Smartphone-Sensed Data  
   
  231  
   
  60. Wang, W., et al.: Social sensing: Assessing social functioning of patients living with schizophrenia using mobile phone sensing. In: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–15 (2020) 61. Wenskovitch, J., Dowling, M., North, C.: With respect to what? simultaneous interaction with dimension reduction and clustering projections. In: Proceedings of the 25th International Conference on Intelligent User Interfaces, pp. 177–188 (2020) 62. Wenskovitch, J., North, C.: Pollux: Interactive cluster-first projections of high-dimensional data. In: 2019 IEEE Visualization in Data Science (VDS), pp. 38–47. IEEE (2019) 63. Wenskovitch Jr., J.E.: Dimension Reduction and Clustering for Interactive Visual Analytics. Ph.D. thesis, Virginia Tech (2019) 64. Weston, G., Zilanawala, A., Webb, E., Carvalho, L.A., McMunn, A.: Long work hours, weekend working and depressive symptoms in men and women: findings from a uk populationbased study. J. Epidemiol. Community Health 73(5), 465–474 (2019) 65. Zhao, Y., et al.: Visual analytics for health monitoring and risk management in CARRE. In: El Rhalibi, A., Tian, F., Pan, Z., Liu, B. (eds.) Edutainment 2016. LNCS, vol. 9654, pp. 380–391. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-40259-8 33  
   
  Towards Interactive Geovisualization Authoring Toolkit for Industry Use Cases Jiˇr´ı Hynek1(B)  
   
  and V´ıt Rusˇ na´k2  
   
  1  
   
  2  
   
  Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic [email protected]  Institute of Computer Science, Masaryk University, Brno, Czech Republic [email protected]   
   
  Abstract. Interactive visualizations of geospatial data are commonplace in various applications and tools. The visual complexity of these visualizations ranges from simple point markers placed on the cartographic maps through visualizing connections, heatmaps, or choropleths to their combination. Designing proper visualizations of geospatial data is often tricky, and the existing approaches either provide only limited support based on pre-deﬁned templates or require extensive programming skills. In our previous work, we introduced the Geovisto toolkit – a novel approach that blends between template editing and programmatic approaches providing tools for authoring reusable multilayered map widgets even for nonprogrammers. In this paper, we extend our previous work focusing on Geovisto’s application in the industry. Based on the critical assessment of two existing usage scenarios, we summarize the necessary design changes and their impact on the toolkit’s architecture and implementation. We further present a case study where Geovisto was used in the production-ready application for IoT sensor monitoring developed by Logimic, a Czech-US startup company. We conclude by discussing the advantages and limitations of our approach and outlining the future work.  
   
  Keywords: Geospatial data tools  
   
  1  
   
  · Geovisualizations · Visual Authoring  
   
  Introduction  
   
  Interactive geovisualizations are used in various use cases, ranging from simple choropleths in newspaper articles to specialized analytical applications for disaster management [9] or ornitology [19]. The underlying geospatial data can combine location information, descriptive attributes (categorical, numerical, or even textual), and optionally also temporal dimensions (e.g., timestamp, duration). The interactive geovisualizations can display data in multiple layers and enable users to explore them at diﬀerent detail levels through zooming and panning. The right choice of visual geospatial data representation is not always straightforward. When done wrong, it might lead to obscuring data perspectives that are essential c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 232–256, 2023. https://doi.org/10.1007/978-3-031-25477-2_11  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  233  
   
  for the users [3]. Moreover, creating eﬃcient interactive geovisualizations usually requires programming skills. In the last decade, we can observe growing eﬀorts toward developing visualization authoring systems enabling the creation of such interactive visualizations for non-programmers [7,13]. Shortcomings of these tools include the focus on 2D charts, only a few types of available geovisualizations, and limited capabilities in terms of input data or support for multi-layered geovisualization interaction. Our work aims to provide the geovisualization authoring toolkit, which has an extensible API for programmers and allows authoring various use cases through a user-friendly interface for users without programming skills. In this paper, we: a) summarize the feedback acquired from the two usage scenarios in which the early Geovisto prototype introduced in [10]; b) introduce the architectural and implementation changes of the redesigned version; c) present a novel use case of Geovisto in the production-ready application for IoT sensor monitoring. The paper is structured as follows. Section 2 overviews the related work and summarizes the shortcomings of current geovisualization authoring approaches. Section 3 summarizes the Geovisto prototype and the two use cases described in detail in our previous work [10]. Section 4 presents the design requirements revisions and proposed changes to Geovisto’s architecture and implementation. Sections 5 and 6 show the novel architecture of the toolkit and the implementation respectively. The case study of Geovisto’s application in the context of IoT sensor monitoring is in Sect. 7. Finally, Sect. 8 discusses the advantages and disadvantages of our approach, and Sect. 9 summarizes the paper and outlines future work.  
   
  2  
   
  Related Work  
   
  In this section, we ﬁrst overview several widely-used 2D geovisualization types. Next, we present the visualization authoring methods with a particular focus on authoring tools and their limitations. 2.1  
   
  Geovisualization Types  
   
  Geovisualizations often take advantage of combining multiple layers where each layer presents only a subset of data. The typical base layer is a cartographic map that provides a spatial context for visualized data. The geographic information systems (e.g., QGIS1 or ArcGIS2 and the web mapping applications available to the general public (e.g., Google Maps, Bing Maps, Open Street Maps) provide the cartographic layers through public APIs, which can be used in the 3rd party applications. Further, we list the most common geospatial data visualization types. Point distribution maps represent the simplest geovisualizations used for visualizing datasets of elements containing only the location information (1 a)). If the 1 2  
   
  https://qgis.org/en/site/. https://www.arcgis.com/index.html.  
   
  234  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Fig. 1. Common geovisualization types: a) point distribution maps; b) proportional symbol map; c) choropleth; d) spider map; e) ﬂow map; f) heatmap. Source: Tableau Software LLC.  
   
  elements contain the location and one descriptive numerical value, they could be plotted as (proportional) symbol maps (1 b)). The symbol can be a circle or a glyph whose physical size determines the value. Symbol maps are also useful when the element has two or three descriptive attributes since we can distinguish their size, shape, and color. More advanced modiﬁcations enable us to show even more data, e.g., when the glyph symbol is replaced by small 2D charts (e.g., pie/donut/bar charts). When the dataset contains information, not for single locations but the whole regions, choropleth maps (or ﬁlled maps) are the best option. The color ﬁll of the region represents value (1 c)). Another type of spatial visualization is heatmaps (or density maps) that are common, e.g., from weather maps showing the measured temperature or precipitation (1 f)). A particular category is geovisualizations showing routes and paths. Well-known from navigations, the basic path maps show direction between two points. However, in the case of ﬂight monitoring websites or computer network visualizations, we can naturally extend the point-to-point to multipoint connections, also known as spider maps (1 d)). Finally, by adding the temporal dimension, we can visualize flow maps, enabling traﬃc visualization on the edges (1 e)). 2.2  
   
  Geovisualization Authoring Approaches  
   
  The interactive visualizations can be authored in three general ways: by programming, template editing, or authoring tools and applications. Programmatic approaches are the most demanding in terms of users’ skills and learning curve but oﬀer the most versatility for ﬁne-tuning of visual appearance and interaction capabilities. The frameworks for developing interactive visualizations are usually designed for web applications. D3 [2] is one of the most  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  235  
   
  popular imperative frameworks nowadays, and many other libraries use it. It allows mapping the input data to a Document Object Model (DOM) and transforms it via data-driven operations. ProtoVis [1] toolkit, also leveraging the imperative paradigm, is based on the idea of decomposing visualizations into hierarchies of visual primitives whose visual properties are data functions. The declarative paradigm frameworks represent Vega [17] and Vega-lite [16]. They both provide a set of building blocks for interactive visualization designs. They diﬀer in the level of abstraction and primary use cases. Vega-lite is a high-level grammar built on top of Vega and was designed for prototyping standard chart types rapidly. Backward compatibility allows programmers to implement more advanced use cases in Vega. There are also several widely-used geovisualization libraries for the web development such as Mapbox GLJS3 , OpenLayers4 or Leaﬂet5 . The latter one we utilize in Geovisto. Template editing is the exact opposite of programmatic approaches. It is a well-established way to create simple charts in spreadsheet applications like Microsoft Excel or Apache OpenOﬃce. The main characteristics are limited functionality in terms of interaction and the ability to visualize tabular-based data in a pre-deﬁned set of charts (e.g., pie charts, bar charts, or choropleths). Users can modify only a basic set of parameters such as color, font, chart shape, or legend position. Template editing is also available in dashboard platforms like Grafana [6]), data analysis tools such as Tableau [18], or analytical frameworks such as ElasticSearch in the form of extension library [4]. They allow the users to connect their dataset through API. On the other hand, their main disadvantage is that they centralize their platform’s visualization with limited support for their export. The authoring tools build on the advantages of the former two. We can imagine them as advanced graphics software focusing on designing interactive charts. They allow users to create visualizations from basic building blocks that can be widely customized in visual appearance and interaction capabilities through GUI. The output visualizations can be exported as web components and published still without programming skills. The visualization community introduced several projects in the last couple of years. Lyra [15], Data Illustrator [12], or Charticulator [14] represents such tools or systems. However, their primary focus is on authoring 2D charts, and geospatial data visualization is often limited to data presentation in a single layer. Another downside is that the tools require speciﬁc visual design knowledge, limiting some users. There are also examples of domain-speciﬁc visualization authoring applications. For example, NewsViews [5] targets data journalists to help them create interactive geovisualizations for online news articles. GeoDa Web [11] platform leverages the cloud storage and computing capabilities and enables data analysts to visualize and publish maps and plots to social media in a user-friendly way. Unlike the general visualization authoring systems, the domain-speciﬁc ones are simpler and reduce the need for speciﬁc visual design knowledge. 3 4 5  
   
  https://www.mapbox.com/mapbox-gljs. https://openlayers.org. https://leaﬂetjs.com.  
   
  236  
   
  2.3  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Limitations of Current Authoring Tools  
   
  We aim to generalize the geovisualization authoring tool while focusing on ease of use for professional and novice users. In general, we identiﬁed three limitations of the current tools that we address in our work. Tabular data as the primary input format. Most of the tools expect the data in a tabular format (e.g., CSV), where columns are attributes (or domains) of elements in rows. However, many of the recent data sets are in hierarchical object formats such as JSON or NoSQL databases. For these, additional data transformation or preprocessing is necessary before their use in visualizations. Our goal is to allow users to upload arbitrary geospatial data in an objectoriented format and select the visualization attributes. Limited number of configuration options. Since the existing tools focus mainly on general 2D chart visualizations, the list of available geovisualization types and their conﬁguration options are narrow. The most frequent are choropleths, heatmaps, or spider maps. As a result, the user can often display only a few data attributes. Our goal is to enable a combination of visualization types in multiple layers and let users decide which suits their needs. Limited interaction capabilities. Finally, current tools provide only limited interaction capabilities with visualized geospatial data such as their ﬁltering or region-based selection. Our goal is to let users conﬁgure the output geovisualization in line with the expected usage and allow them to set multi-layer interaction capabilities and cross-layer data linkage. We propose Geovisto – the geovisualization authoring toolkit, which enables conﬁguring geospatial data visualizations for use in web-based dashboard applications or as a part of visual analytics workﬂows. In the reminder, we present its design and prototype implementation. Two usage scenarios demonstrate its applicability.  
   
  3  
   
  Geovisto Prototype  
   
  In this section, we outline the Geovisto prototype and two usage scenarios described in detail in our earlier paper [10]. The usage scenarios that were used to demonstrate Geovisto’s features also served for further revision of the requirements. Geovisto prototype is a standalone ReactJS6 component, using Leaﬂet and D3.js JavaScript libraries. Thus, it can be shared and included as a widget in third-party web applications. The implementation is based on the four design requirements: a) it has a component-based architecture; b) the input data are transformed to a ﬂat data model ; c) its user interface enable user-deﬁned data mapping to multiple conﬁgurable layers; d) users can export and import map configurations. Geovisto renders the base map with one or more predeﬁned layers when loaded. Users can specify their dataset and import or export map conﬁgurations. Available layers are choropleth, marker, and connection layer. The layers 6  
   
  https://reactjs.org/.  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  237  
   
  Fig. 2. An example of the Geovisto prototype map widget. It contains the sidebar (on the left) used for the conﬁguration of layers, the deﬁnition of ﬁlter rules, and the map’s general setting. The map contains the choropleth, marker, and connection layers. The example shows the conﬁguration of the choropleth layer. It links the ‘from’ data domain with the ‘country’ visual dimension, the ‘value’ data domain with the ‘value’ visual dimension, and uses the ‘sum’ function to aggregate the values.  
   
  can be managed from the user interface (e.g., show/hide the layer, deﬁne the data visualized in each layer, apply basic ﬁltering on the visualized data). The Geovisto prototype also provides functions for projecting geographic features onto the map and interaction capabilities with the base map. It handles events invoked by the map layers (user interaction) and ships them to other layers, which can process them (since the map layers are independent). Figure 2 shows an example Geovisto prototype map widget. We further demonstrated the Geovisto prototype on two distinct usage scenarios: Covid-19 pandemic open data, and DDoS attack analysis. Covid-19 Pandemic Open Data: We used the map to visualize the worldwide spread of the COVID-19 disease to demonstrate the widget’s general applicability. We used the data from the rapidapi.com7 service and converted them to the JSON format to import them into the map widget. Since the map allows the users to change the data domains, the users can compare the countries from different aspects (sum of conﬁrmed cases, numbers of recoveries, and deaths). They can also combine these views using the choropleth and marker layers. Figure 3 shows an example of the use case.  
   
  7  
   
  https://rapidapi.com/.  
   
  238  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Fig. 3. Covid-19 pandemic data. The choropleth layer compares the number of conﬁrmed cases with the disease. The marker layer shows the number of deaths caused by the disease.  
   
  Fig. 4. DDoS attack analysis usage scenario. The example shows the share of mitigated and ﬁnished attacks for selected countries.  
   
  DDoS Attack Analysis: The scenario was performed in cooperation with Flowmon Networks a.s.8 , a company providing complex tools for automated monitoring, analysis, and network traﬃc protection. Figure 4 shows a DDoS monitoring component. It provides an overview of all DDoS attacks and the source and destination countries. The connections between the countries visualize the relations of the traﬃc ﬂows. A country’s details show speciﬁc aspects of the attacks, such as the state of the attacks (active, mitigated, ﬁnished) and their numbers (in pop-up windows). The multilayer map meets the requirements. It contains either the information about the source of DDoS attacks or their destinations. Then, the connection layer can display the relations between the countries. The users can ﬁlter the data to display only a speciﬁc subset. Finally, they can select a particular country in the choropleth and highlight all the related data presented in the same or other map layers. While the former usage scenario focused on demonstrating Geovisto’s features and general applicability, the latter demonstrated its utilization in the real-world example from the cybersecurity domain. We further analyzed and evaluated the usage scenarios to revise the design requirements and identify the shortcomings limiting Geovisto’s applications in the industry.  
   
  4  
   
  Design Requirements Revision  
   
  The main area of Geovisto’s deployment is industrial applications. In order to meet the requirements of the industry, we reformulated the initial design requirements based on the Geovisto prototype assessment in the presented scenarios. The revised requirements focus on ﬁve main aspects: 8  
   
  https://www.ﬂowmon.com/en.  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  239  
   
  – Usability : Geovisto should utilize the concept of authoring systems and provide map layers representing ready-to-use thematic maps to decrease the eﬀort to prototype visual projections of geographical data quickly; – Modularity : Geovisto should not be served as a monolith but in the form of independent modules which would provide particular functionality such as map layers and map controls; these modules could be used only when needed in order to decrease the size of the result; – Configurability : Geovisto’s basemap and its layers and controls might be customized with generic datasets, geographical data, appearance, and behavior; users might capture the current map state and reload this state later; – Extensibility : Geovisto should provide a core with programming API to customize the map programmatically, allow further extensions of the existing modules of the library, or implement new own modules concerning the current requirements; – Accessibility : the library (and modules of the library) should be available through a known package manager software to support versioning and improve integration into its own system infrastructure, build and deliver the solution as a part of its system. The following subsections list these aspects reﬂecting the initial prototype and identify required modiﬁcations to the original prototype design that have driven the re-implementation of the Geovisto toolkit. 4.1  
   
  Usability  
   
  One of the crucial problems during the design and implementation of user interface and visualization is to overcome the communication barrier between customers providing requirements and programmers delivering the ﬁnal product for them. Usually, salespeople, UX designers, or data analysts try to break this barrier. However, their lower knowledge of the system architecture and underlying data models might skew the description of the requirements to the programmers. These speciﬁcations are usually vague, and they do not consider actual data representation (e.g., data types and relations between data entities) and eﬀort (e.g., complexity and price of database queries) to map the data into the UI components. The main goal of the Geovisto toolkit is to blend the programmatic and template editing approaches known from contemporary mapping libraries to improve user experience during the prototyping phase. The idea of Geovisto is to provide a UI layer composed of ready-to-use map layers and controls, which would allow the UI designer to project the actual data and prototype the map views corresponding to the end-user requirements. Then, a snapshot of the map state could be exported and shared in a serialized format. Such a conﬁguration might help the programmer who implements a new map widget into the production version of the application or service used by the end-users, as illustrated in Fig. 5.  
   
  240  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Fig. 5. Geovisto’s authoring and conﬁguration sharing workﬂow. First, the UI designer creates a data projection into the map layers and exports the conﬁguration into a JSON ﬁle. The conﬁguration can be shared with end-users or programmers developing the web front-end.  
   
  The authoring tools could have been used while clarifying the user requirements and possible use cases of the Flowmon UX team. Since Geovisto can work with custom datasets, the UX team members used the prototype independently, generating multiple map conﬁgurations providing perspectives of their custom data without any coding knowledge. This approach improved mutual communication between them and programmers and rapidly increased the ability to generate new geovisualization use cases. While the Geovisto prototype provided decent authoring tools to prepare the map drafts, export, and import the map state, the programmatic deﬁnition of the state as properties of map and map layers was rather cumbersome. Since the Geovisto prototype was implemented in JavaScript, the property types were unclear, leading to occasional crashes and debugging requests. One of the most needed requirements was to re-implement the project into TypeScript9 , which only emphasizes the importance of statically typed languages in the industry and even the front-end and data visualization. 4.2  
   
  Modularity  
   
  When implementing the prototype, our initial goal was to design a clear code structure and decompose the library into the so-called Geovisto tools representing particular map layers and controls and the map core handling communication between the tools. Although we fulﬁlled the modular approach, the main drawback stood for how the library was delivered – in the form of one JavaScript repository. Thus, programmers using Geovisto needed to load the whole library,  
   
  9  
   
  https://www.typescriptlang.org.  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  241  
   
  although they wanted to use only a subset of Geovisto’s tools. To fully accomplish the proposed modularity, it was necessary to break apart the repository into standalone packages to be included as project dependencies when needed. The crucial task to handle this was to solve the problem of possible dependencies between the tools (e.g., one tool needed to know the type of event object sent by another tool). 4.3  
   
  Configurability  
   
  The Geovisto prototype worked with the following types of inputs: – Geographical Data: the speciﬁcation of polygons and their centroids represented in GeoJSON format. The prototype used the speciﬁcation of world countries published by J. Sundstrom10 but it should be replaceable with generic speciﬁcations. The only requirement was that every GeoJSON feature had to contain a polygon identiﬁer (e.g., country code), which is needed to connect the geographical data with the dataset. – Datasets: the values stored in a serialized format (JSON). The data needs to contain at least one data domain representing an identiﬁer of the geographical feature (e.g., country code). Since there are various models representing the data, it was essential to create a mechanism for processing such models – choosing the data domains and binding them to the map layers’ visual dimensions. Hence, every map layer provides a list of visual dimensions, which can be associated with data domains. The users can select the data domains manually using the layer settings provided by the map sidebar. In contrast to existing authoring tools, this approach focuses only on geospatial data. Users can work with multiple data domains representing geographic location formats (such as the ISO 3166 country codes) and use them in various use cases. Since the dataset can represent non-tabular data structures (e.g., JSON or XML formats), recursive data preprocessing was needed to construct a valid data model representing data domains. Only then, the list of the data domains was served to the UI. Figure 6 shows an example demonstrating the principle. The Geovisto prototype provides a basic ﬂattening algorithm that expands all nested arrays into a combination of ﬂat data visual projections and aggregations.  
   
  10  
   
  https://github.com/johan/world.geo.json.  
   
  242  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Fig. 6. An example of data composed of two records stored in the pseudo-JSON format. Since the records contain nested lists, they need to be preprocessed ﬁrst. They are expanded into four records represented by all combinations of the values. This representation is characterized by data domains that can be mapped into visual dimensions. The ﬁgure shows two diﬀerent projections and aggregation of data.  
   
  Another requirement from the Flowmon evaluation was to implement diﬀerent ﬂattening strategies. For instance, some of the nested lists might represent speciﬁc qualitative characteristics of network traﬃc which should not be preprocessed as described in the example of Fig. 6. For this purpose, the Geovisto prototype lacked a solid mechanism to deliver custom data preprocessing algorithms. Similarly, overriding the default Geovisto behavior and settings was cumbersome and required decent knowledge of the library architecture. For instance, Flowmon needed to integrate the map instances into its own environment, which is characterized by speciﬁc appearance (e.g., types of controls familiar to the company’s customers). Hence, the requirement to completely redesign the map layers and controls has a high priority for future deployment of the map instances to corporate environments. 4.4  
   
  Extensibility  
   
  Another requirement from the Flowmon evaluation was to implement diﬀerent ﬂattening strategies. For instance, some of the nested lists might represent speciﬁc qualitative characteristics of network traﬃc which should not be preprocessed as described in the example of Fig. 6. For this purpose, the Geovisto prototype lacked a solid mechanism to deliver custom data preprocessing mechanisms. Similarly, overriding the default Geovisto behavior and settings was cumbersome and required decent knowledge of the library architecture. For instance, Flowmon needed to integrate the map instances into its environment, characterized by speciﬁc appearance (e.g., types of controls familiar to its customers). Hence, the requirement to completely redesign the map layers and controls has  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  243  
   
  a high priority for future deployment of the map instances to corporate environments. 4.5  
   
  Accessibility  
   
  Since our goal is to deliver Geovisto in compact modules, it is essential to maintain modules versioning. These modules should be available in package management systems, such as npm (Node.js Package Manager), and the modules users (i.e., programmers) should decide on their own when to switch to a higher version. It is another essential requirement in the industry when delivering new product releases. In order to deliver a stable product to its customers, the dependent libraries must be reliable. Flowmon develops its user interfaces in the React.js library. It helps organize the user interface into logical parts (React components) and manage its rendering lifecycle and UI events. The Geovisto prototype was wrapped in the React component, but we decided to leave this abstraction in future versions and provide the React extension as a standalone package. We kept Geovisto as a Leaﬂet-based TypeScript library that can be integrated into any web application by the companies which might use diﬀerent UI frameworks (such as Angular or Vue.js).  
   
  5  
   
  Architecture  
   
  We updated the Geovisto prototype’s architecture concerning the listed requirements, which decomposes the library into the map core and map layers. We designed a revised architecture reﬂecting the new aspects of Geovisto. The idea of new Geovisto architecture is similar to the old one. However, it generalizes the Geovisto modules as so-called Geovisto Tools, representing map layers, map controls (e.g., sidebar), or utilities (e.g., ﬁlters and themes). Map layers are a particular type of tool (Fig. 7). The reason behind the decomposition was to provide standalone npm packages that can be included in a project when needed. Every Geovisto Tool is a TypeScript project with a package.json ﬁle containing the npm metadata11 . It contains two peer dependencies – Geovisto Core and Leaﬂet – which force the programmer to include Geovisto Core and Leaﬂet library in own project. Hence, the built packages of Geovisto Tools do not include the code of these libraries in order to minimize the size of the packages and prevent conﬂicts in dependencies. 5.1  
   
  Core  
   
  The management of the Geovisto Tools’ life cycle, inputs, and map state is provided by the Geovisto Core. It is an npm package12 which represents an abstraction of the Leaﬂet library, and it needs to be (together with Leaﬂet) included in every project which utilizes Geovisto Tools. 11 12  
   
  Metadata required by the Node.js Package Manager when resolving the tree of package dependencies, running, building, and publishing the package. https://www.npmjs.com/package/geovisto.  
   
  244  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Fig. 7. Geovisto architecture overview. The map component takes props and conﬁg and renders a Leaﬂet-based map composed of map tools – usually SVG elements generated via the Leaﬂet API or D3.js library. The map tools are independent of each other and communicate via events. They represent map layers, map controls, and utilities.  
   
  Fig. 8. Map core life-cycle. First, the map state is initialized by default values of the Geovisto toolkit and props given by the programmer. Then, the user can override the state using conﬁg (such as data mapping or appearance). The map and tools are rendered based on the combination of values given by Leaﬂet, programmer, and user. Finally, the user can use the map, change the state and export the conﬁg.  
   
  Figure 8 describes the workﬂow of the Geovisto Core. First, it processes the inputs and initializes the state of the map and instances of the required Geovisto  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  245  
   
  Tools. Then, based on the initial state, it creates and renders the Leaﬂet map and instances of Geovisto Tools. The phase of state initialization is crucial, and it determines the data projection and appearance of the map and tools. It is based on the following inputs: – Default Values (defaults) – the state deﬁned by Geovisto: the implicit values and functions representing the map’s default behavior and appearance. They make the initial state of the map and the tools. – Properties (props) – the state deﬁned by the programmer: the default state can be overridden programmatically by using props. The programmer might inﬂuence either the map’s appearance (static values) or override the default behavior (functions). Some of the props are optional (e.g., initial map zoom); however, there are several important mandatory props that are essential to render the map: • Geo-data manager providing geographical data (available GeoJSON definitions), which can be used across the tools. • Data manager providing dataset and strategy for data preprocessing into a suitable list data records which can be used across the tools. • Tool manager providing tool instances that should extend the Geovisto Core. Every tool might accept its own props and conﬁg to override its default state. – Configuration (config ) – the state deﬁned by the user: the ability of the user to override default values and properties deﬁned by the programmer. Conﬁg is deﬁned by serialized format data (e.g., JSON) and manager, which processes this format and transforms it to the Geovisto Core model and tools. It can also be exported during the map runtime to capture the snapshot of the current map state. In contrast to the props, it represents only the static characteristics of the map, not the procedural characteristics of the map. Conﬁg usually deﬁnes map appearance and data mapping (projections of data domains to the dimensions of map layers). All of the inputs are wrapped in so-called input managers, which process the inputs and transform them into Geovisto models. This approach gives the programmer the opportunity to work on their own input format and provide their own strategies to process these formats. It makes the library more generic and applicable in diﬀerent environments. 5.2  
   
  API  
   
  In contrast to the Geovisto prototype, the new Geovisto version (Core and selected tools) was reimplemented using the TypeScript language. Using static types allowed us to describe the exact model of all map objects. The code structure was split into two parts representing: – Types and Interfaces: declaration of all Geovisto objects, their properties, and methods (e.g., map, tool, layer, state, defaults, props, conﬁg, events, geo-data, data domains, layer dimensions, aggregation functions, ﬁlters, etc.)  
   
  246  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  – Classes: internal default deﬁnitions of map objects which provide implicit behavior, ready to use Both the declarations and deﬁnitions of Geovisto objects are exported using ES6 module exports, so they can be used to design new Geovisto objects or extend the existing ones. In order to allow comfortable overriding of implicit map objects, the Factory design pattern was applied. Besides that, the programmer can approach low-level Leaﬂet objects and utilize all capabilities of this library. The architecture of Geovisto Core is used in Geovisto Tools implementing the interfaces and types and extend the classes of Geovisto Core API. Since the tools might also be extended or used by each other, they export their own declarations and deﬁnitions of tool objects, similarly to the Geovisto Core. Then, the tools might communicate with each other using events and the Observer design pattern and the types of event objects are be known by the observers (e.g., the change of preferred style in the themes tool). The tools are represented as standalone npm packages. In order to avoid direct dependencies between the npm packages, only the types and interfaces can be imported, and so-called devDependecies13 are used.  
   
  6  
   
  Implementation  
   
  The Geovisto Core is distributed along with eight already published Geovisto Tools in the npm repository14 under the MIT license. The source codes are available on Github15 . Several other tools are in the development. Further, we present the published ones. 6.1  
   
  Layers  
   
  Layer tools represent thematic map layers. Besides Tile layer, each of the following layers allows deﬁning a GeoJSON describing geographical objects of the layer (e.g., polygons, or points based on the type of thematic map). In contrast to the prototype, Geovisto accepts multiple deﬁnitions of GeoJSON directly in the props. Then, a data mapping needs to be set to connect the data domains with the layer’s dimensions (e.g., geographical dimension, value dimension, or aggregation function) as illustrated in Fig. 2. The following map layer tools are already available in the npm repository. – Tile Layer Tool represents the base map layer which utilizes Leaﬂet Tile layer API to show underlying maps of existing tile providers16 . This might be required when the data needs to be connected with real geographical places. – Choropleth Layer Tool allows to use GeoJSON speciﬁcations of polygons representing geographic regions and link them with the data. Unlike basic choropleth widgets, our implementation can process custom deﬁnitions of 13 14 15 16  
   
  https://nodejs.dev/learn/npm-dependencies-and-devdependencies. https://www.npmjs.com/search?q=geovisto. https://github.com/geovisto. https://github.com/leaﬂet-extras/leaﬂet-providers.  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  247  
   
  geographic areas. Primarily, we work with the speciﬁcation of world countries. However, diﬀerent GeoJSON ﬁles can be used, as described in Sect. 5. The advantage of this approach is the higher scalability of the layer. We can use the layer in diﬀerent situations and detail (e.g., countries, districts, custom areas). We can also adjust it according to the foreign policy of speciﬁc countries (e.g., visualization of disputed territories). – Marker Layer Tool works with GeoJSON speciﬁcation of points and visualizes the data related to exact geographic locations via markers. Similar to the choropleth polygons, every marker has a unique identiﬁer and geographic position (e.g., country code and country centroid). Since marker visualization could be problematic when many are close to each other (clutter of markers), we use Leaﬂet.markercluster plugin17 to overcome this issue by clustering the close markers into groups and aggregating the values. – Connection Layer Tool visualizes relations between geospatial locations in the form of edges. The layer enables the user to select two required dimensions: from and to, representing nodes of the rendered edges (by default, we work with the country centroids identiﬁed by country codes). Optionally, the user can set the value, which aﬀects the strength of the lines. A common problem of connection maps is their complexity and poor edge placement. Holten and Van Wijk [8] proposed a force-directed edge bundling rendering technique that signiﬁcantly reduces the clutter of edges. S. Engle demonstrated its application18 on a ﬂight map in the US. The example implements the technique using the d3-force19 module of the D3.js library, which “implements a velocity Verlet numerical integrator for simulating physical forces on particles.” In Geovisto, we implemented an SVG overlay layer using the Leaﬂet API and rendered the SVG elements representing edges using the D3.js library and the d3-force module according to Engle’s approach. It was necessary to implement correct projections of the SVG elements into the Leaﬂet map concerning the map’s zoom and current position. The result provides a comprehensive view of edges that can be zoomed in/out. 6.2  
   
  Controls and Utilities  
   
  The second type of tool adds additional controls and functionality to the map layers. Currently, Geovisto provides the support for adding custom UI controls in the sidebar, ﬁltering and selection of data, and changing style themes: – Sidebar Tool provides a collapsible sidebar control and the API allowing other tools to add custom sidebar tabs or sidebar tab fragments. The map layer tools utilize this to provide controls for their customization and changing data mapping. – Filters Tool provides either UI controls to ﬁlter visualized data records and the API to deﬁne custom advanced ﬁlter operations. The users specify ﬁlter rules as conditional expressions evaluating selected data domains’ values. 17 18 19  
   
  https://github.com/Leaﬂet/Leaﬂet.markercluster. https://bl.ocks.org/sjengle/2e58e83685f6d854aa40c7bc546aeb24. https://github.com/d3/d3-force.  
   
  248  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  – Selection Tool provides a mechanism for connecting map layers with selected map layer geographical objects. The communication between the layers is implemented via the observer design pattern. Every event passed to the layers contains information about the source element selected by the user. It consists of the identiﬁer of the geographic element (e.g., country code) and the layer. The identiﬁers of geographic elements can be used in more than one layer (e.g., choropleth country, country marker, connection node). Then, the ﬁltering is based on the search of these identiﬁers through the map layer elements. The search algorithm avoids the cyclic event invocation. The elements found on the map are highlighted (Fig. 9). – Themes Tool provides a set of predeﬁned styles (e.g., colors), which are delivered to other tools via events and API, which allows deﬁning own custom style themes.  
   
  Fig. 9. Geographic element selection. The selection of Suriname in the choropleth layer invokes an event that is passed to other map layers. The connection layer handles the event, ﬁnds, and highlights all the edges which connect Suriname with other countries. This selection invokes another event which contains all the countries connected with Suriname. It aﬀects the choropleth and marker layer, which highlights appropriate countries. Further invocations of events are stopped.  
   
  7  
   
  Case Study: Logimic  
   
  We demonstrated Geovisto’s applicability in a case study that was done in cooperation with Logimic – the Czech-US startup company that brings innovative IoT solutions to industry.20 20  
   
  Logimics’ products include smart city dashboards for monitoring billions of sensors, street lighting control systems, indoor monitoring of temperature and humidity with small battery-operated wireless sensors, wireless control of industrial heaters, and many others (https://www.logimic.com/).  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  249  
   
  One of Logimic’s products is a user-friendly web application implemented in TypeScript and Angular framework. It processes and aggregates large-scale cloud data gathered from devices and provides monitoring and analytic tools to end-users (e.g., administrators of smart devices, service workers, or inhabitants of smart cities). Its main strength is the simplicity of data presentation delivered in the form of several dashboards, systematically organized in diﬀerent levels of detail. For instance, basic users can monitor devices using high-level views with simple indicators presenting key performance indicators (KPIs). On the other hand, analysts could utilize drill-down actions to watch KPI alerts, detect device problems and analyze the reasons behind these problems to create service requests for service workers. Due to devices’ geographic locations, KPI alerts and related device problems are problematic. Since many devices might be distributed in the city, such information is crucial to navigating the service worker to the device. It should be comprehensively displayed when an alert is focused. Geovisto was used for this purpose. Figure 10 shows an example of a device KPI alert selection and map focus.  
   
  Fig. 10. Device alerts view. The user can click on an alert in the list (on the left) and the corresponding device is zoomed in on the map with description popup. Then, the user can create a service request.  
   
  In order to include the Geovisto map into the Logimic application, it was necessary to create an Angular component that serves as a wrapper for the Logimic map. The principle was similar to the React component created for Flowmon (Sect. 3). Since the Geovisto Core library is delivered as an npm package and it was reimplemented to TypeScript, it was easy to set this library as the npm dependency and import the library in the Angular component written in  
   
  250  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  TypeScript. Geovisto creation and API functions calls were integrated into the Angular lifecycle callbacks, similarly as it can be done by using React lifecycle hooks. We expect the library to be used either in pure JavaScript/Typescript projects or in various advanced UI frameworks (such as Vue.js or Svelte). For the purposes of Logimic, we tested the following existing tools, currently provided by Geovisto: – Tile layer tool displays a real-world map which is vital to locate the devices. Also, the satellite view is beneﬁcial when the service worker is in the ﬁeld and needs to quickly locate the devices (e.g., actual positions of lamps, entrance to buildings) as shown in Fig. 11. The tool itself does not provide a map tiles server. It only provides the ability to connect the tool with a chosen mapping provider service21 using the Leaﬂet API. Logimic purchases API keys concerning the policy of the chosen map providers.  
   
  Fig. 11. Satellite perspective using Google maps. It can provide the service worker with a detailed view of the focused neighborhood and help associate the device marker with the actual location.  
   
  – Filters tool allows to select only the devices that might be important either for the desktop user to analyze a map region or the service worker for navigation across the series of same devices (e.g., the distribution of lamps in the street – Fig. 12). 21  
   
  https://github.com/leaﬂet-extras/leaﬂet-providers.  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  251  
   
  Fig. 12. Device ﬁltering. In this example, service workers can only see one device type (lamps) to unclutter the view and remove unneeded markers.  
   
  – Themes tool enables the widget appearance customization, so it ﬁts the surrounding environment. – Sidebar tool allows to add new controls for customization of mentioned tools. Currently, the controls are displayed as a part of the map widget, but with the planned redesign of the application, it might be required to move these controls outside the map widget and include them in the global menu. This was also one of Flowmon’s requirements. Since the Geovisto Core and tools provide API for their customization, it is possible to control them externally. Regarding the visualization of device markers, we integrated the Geovisto toolkit as part of the Logimic environment to demonstrate the possibility of extending Geovisto externally. The tool is similar to the Marker layer provided by Geovisto, but it adds some new Logimic-speciﬁc functionality: – The tool is connected to the list of device alerts and listens to the selection changes in order to focus the device on the map when an alert is clicked. – It extends marker popups for additional device metadata and allows to invoke service requests dialogs directly. This tool can be published as an npm package; however, it represents an internal extension applicable in the Logimic application.  
   
  8  
   
  Discussion  
   
  As conﬁrmed by the evaluation with Logimic, Geovisto’s attributes (usability, modularity, conﬁgurability, extensibility, and accessibility) improved vastly. The  
   
  252  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  multilayered map appeared to be an excellent way to display the geographic locations of IoT devices distributed in the cities. Satellite view, ﬁlter, and focus tools help locate the devices that alert KPI problems and navigate service workers to these devices. We expect that Geovisto might be used with diﬀerent use cases of diﬀerent industrial systems. Further, we present Geovisto’s advantages and disadvantages. 8.1  
   
  Advantages  
   
  One of Geovisto’s main advantages is that it provides UI tools to prototype map instances without implementing much code. Then, the state of a map instance can be serialized and exported. Even though the prototyping possibilities are limited, this functionality might be beneﬁcial for a programmer trying to ﬁnd the best map conﬁguration. Also, it might improve the communication between the UX team, which can ﬁnd an appropriate map conﬁguration, and the programmers, which can use this conﬁguration to implement the production version of the map. Geovisto can work with generic datasets and project them onto custom geographical objects (concerning the capabilities of chosen map layers). Users can select various data domains, which allows them to explore the information further. Showing more layers at the same time helps the users to see the data in context. The interactive data ﬁltering emphasizes the relations between geographical locations. The selection of map layer objects can be propagated to other layers, focusing the important details better with a combination of highlighting tools. When the programmers need to create an unusual data projection, they can either choose from the existing Geovisto Tools or implement their own using the Geovisto Core API. The second version of the library was rewritten in the statically typed TypeScript language, improving code readability and decreasing the number of runtime errors caused by the wrong API application. The library does have a modular architecture. It provides a thin core layer delivered as an npm package providing the core API. Then, additional Geovisto Tools extend the Core and provide particular map layers, controls, and utilities. Programmers can import additional npm packages in their projects only when required, which decreases the size of the result. 8.2  
   
  Limitations  
   
  Geovisto is an open-source library that is still under development. The usage scenarios showed only a fragment of geospatial data types that could be visualized. More usage scenarios and case studies are needed to validate the generalization of our approach. We should focus more on processing the large-scale data and the performance and proﬁling of the library. Another limitation relates to data preprocessing that has to be done to gain a ﬂat data structure. Our implicit approach causes enlargement and redundancy of the data. Thanks to that, the data can be processed quickly. However, it  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  253  
   
  would be helpful to design an algorithm that can work with the data without preprocessing and more eﬃciently. The new version of Geovisto oﬀers the possibility to override these algorithms. However, a more extensive set of implicit data processing strategies would improve its usability. Last but not least, the lack of proper usability evaluation is another drawback of our work. For instance, selecting colors and combining several map layers is limited by people’s comprehensibility of the widget. The users might be overwhelmed by the data, mainly when chosen inappropriate color combinations. The z-index of the layers is hardcoded, and users cannot change it. Usually, the users should not need to change the default settings (e.g., the marker and connection layers are above the choropleth), but they should control their ordering in the future with more layers added.  
   
  9  
   
  Conclusion and Future Work  
   
  There are three general approaches to authoring interactive geovisualizations: programmatic, template editing, and authoring tools. The existing authoring tools, however, are mainly focusing on 2D charts and diagrams, and their capabilities for authoring geovisualizations are limited. In our work, we focus on designing and implementing an authoring tool speciﬁcally focused on delivering interactive geovisualizations. Earlier, we implemented a prototype version of the Geovisto toolkit based on JavaScript and Leaﬂet and demonstrated it on two usage scenarios (DDoS Attack Analysis and Covid 19 Pandemic open data) in [10]. In this paper, we followed up with an analysis of usage scenarios, identiﬁed prototype limitations, and revised the design requirements for the new version in terms of usability, modularity, configurability, extensibility, and accessibility. Modular architecture allows to include only necessary parts of the library and decrease the size of the product. Conﬁgurability allows customization of included parts to integrate the map into the company’s environment. Improved extensibility oﬀers the creation of new map tools speciﬁc to companies. We reimplemented the Geovisto toolkit in TypeScript – a statically-typed language – that improves further development and decreases runtime errors. The library has been published in the form of npm several packages22 . The source codes are available on Github under MIT license23 . The requirements reﬂected the intended industrial use and invoked changes in Geovisto’s architecture and implementation. We presented the case study where Geovisto was used in the production-ready application for visualizing IoT sensor devices on the map developed by Logimic startup company. The case study conﬁrmed that the Geovisto toolkit fulﬁlls our goal of creating a programmatic mapping library that provides template editing known from mapping authoring systems. 22 23  
   
  https://www.npmjs.com/search?q=geovisto. https://github.com/geovisto.  
   
  254  
   
  9.1  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  Future Work  
   
  There are also several sub-projects we are currently working on. One of them is a web service enabling non-programmers to prototype map instances using the Geovisto toolkit and include them as widgets on their websites. We are developing an infrastructure that will manage map conﬁgurations, datasets, and GeoJSONs. The system will provide the front-end application wrapper, including the UI tools to manage user-deﬁned maps. The back-end will provide a conﬁguration database and API for remotely fetching the conﬁgurations. We also plan to support loading data from third-party relational and non-relational database systems. The second area deals with layer improvements and creating new ones. We have already implemented the bubble map, spike map, and heatmap layers, which play an essential role in the comprehensive data distribution visualization. Another almost ﬁnished tool provides animated geospatial data visualization with the time dimensions (i.e., timestamps). It enables the animation of spatiotemporal data domains and seeks them to the deﬁned time frame. As a result, it will allow the user to see the evolution of values in individual geographic regions in time. Also, we plan to improve the visual appearance of the layers and add interactive map legends. Since the users can show multiple layers in the map simultaneously, we will pay closer attention to the color pallets used in the layers (e.g., suﬃcient color contrast, color-blind safe palette combinations). Lastly, we will add further visual dimensions to the layers and controls for manipulating the layers. The third area focuses on quality assurance. Since the project is getting larger and composed of several packages that might use the API of others, it is necessary to do proper testing before publishing new versions. We will also pay attention to the documentation to better describe the API, including usage examples. Acknowledgements. Jiˇr´ı Hynek was supported by The Ministry of Education, Youth and Sports from the National Programme of Sustainability (NPU II) project “IT4Innovations excellence in science – LQ1602”. V´ıt Rusˇ n´ ak was supported by ERDF “CyberSecurity, CyberCrime and Critical Information Infrastructures Center of Excellence” (No. CZ.02.1.01/0.0/0.0/16 019/0000822) project. We also thank Progress Flowmon and Logimic, which provided usage scenarios and cooperated during the evaluation of the Geovisto toolkit.  
   
  References 1. Bostock, M., Heer, J.: Protovis: a graphical toolkit for visualization. IEEE Trans. Vis. Comput. Graph. 15(6), 1121–1128 (2009) 2. Bostock, M., Ogievetsky, V., Heer, J.: D3 data-driven documents. IEEE Trans. Vis. Comput. Graph. 17(12), 2301–2309 (2011). https://doi.org/10.1109/TVCG. 2011.185  
   
  Geovisualization Authoring Toolkit for Industry Use Cases  
   
  255  
   
  3. Degbelo, A., Kauppinen, T.: Increasing transparency through web maps. In: Companion Proceedings of the The Web Conference 2018, WWW 2018, pp. 899–904.International World Wide Web Conferences Steering Committee, Geneva (2018). https://doi.org/10.1145/3184558.3191515 4. Elasticsearch, B.: Maps for Geospatial Analysis (2020). https://www.elastic.co/ maps, Accessed 10 Feb 2020 5. Gao, T., Hullman, J.R., Adar, E., Hecht, B., Diakopoulos, N.: NewsViews: an automated pipeline for creating custom geovisualizations for news. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI 2014, pp. 3005–3014. Association for Computing Machinery, New York (2014). https:// doi.org/10.1145/2556288.2557228 6. Grafana Labs: Grafana: The Open Observability Platform (2020). https://grafana. com/, Accessed 10 June 2020 7. Grammel, L., Bennett, C., Tory, M., Storey, M.A.: A survey of visualization construction user interfaces. In: Hlawitschka, M., Weinkauf, T. (eds.) EuroVis Short Papers. The Eurographics Association (2013). https://doi.org/10.2312/PE. EuroVisShort.EuroVisShort2013.019-023 8. Holten, D., Van Wijk, J.J.: Force-directed edge bundling for graph visualization. Comput. Graph. Forum 28(3), 983–990 (2009). https://doi.org/10.1111/j.14678659.2009.01450.x 9. Huang, Q., Cervone, G., Jing, D., Chang, C.: DisasterMapper: a CyberGIS framework for disaster management using social media data. In: Proceedings of the 4th International ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data, BigSpatial 2015, pp. 1–6. Association for Computing Machinery, New York (2015). https://doi.org/10.1145/2835185.2835189 10. Hynek, J., Kachl´ık, J., Rusˇ n´ ak, V.: Geovisto: a toolkit for generic geospatial data visualization. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. SCITEPRESS - Science and Technology Publications (2021). https://doi.org/10. 5220/0010260401010111 11. Li, X., Anselin, L., Koschinsky, J.: GeoDa web: enhancing web-based mapping with spatial analytics. In: Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2015. Association for Computing Machinery, New York (2015). https://doi.org/10.1145/ 2820783.2820792 12. Liu, Z., Thompson, J., et al.: Data illustrator: augmenting vector design tools with lazy data binding for expressive visualization authoring. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI 2018, pp. 1–13. Association for Computing Machinery, New York (2018). https://doi.org/ 10.1145/3173574.3173697 13. Mei, H., Ma, Y., Wei, Y., Chen, W.: The design space of construction tools for information visualization: a survey. J. Visual Lang. Comput. 44, 120–132 (2018). https://doi.org/10.1016/j.jvlc.2017.10.001 14. Ren, D., Lee, B., Brehmer, M.: Charticulator: interactive construction of bespoke chart layouts. IEEE Trans. Vis. Comput. Graph. 25(1), 789–799 (2019) 15. Satyanarayan, A., Heer, J.: Lyra: an interactive visualization design environment. In: Proceedings of the 16th Eurographics Conference on Visualization, EuroVis 2014, pp. 351–360. Eurographics Association, Goslar, DEU (2014) 16. Satyanarayan, A., Moritz, D., Wongsuphasawat, K., Heer, J.: Vega-Lite: a grammar of interactive graphics. IEEE Trans. Vis. Comput. Graph. 23(1), 341–350 (2017)  
   
  256  
   
  J. Hynek and V. Rusˇ n´ ak  
   
  17. Satyanarayan, A., Russell, R., Hoﬀswell, J., Heer, J.: Reactive vega: a streaming dataﬂow architecture for declarative interactive visualization. IEEE Trans. Vis. Comput. Graph. 22(1), 659–668 (2015) 18. Tableau Software, LLC.: Mapping Concepts in Tableau (2020). https://help. tableau.com/current/pro/desktop/en-us/maps build.htm, Accessed 10 Feb 2020 19. Xavier, G., Dodge, S.: An exploratory visualization tool for mapping the relationships between animal movement and the environment. In: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Interacting with Maps, MapInteract 2014, pp. 36–42. Association for Computing Machinery, New York (2014). https://doi.org/10.1145/2677068.2677071  
   
  Computer Vision Theory and Applications  
   
  Global-first Training Strategy with Convolutional Neural Networks to Improve Scale Invariance Dinesh Kumar1(B)  
   
  and Dharmendra Sharma2  
   
  1  
   
  2  
   
  School of Information Technology, Engineering, Mathematics and Physics, University of the South Pacific, Suva, Fiji [email protected]  Faculty of Science and Technology, University of Canberra, ACT, Canberra, Australia [email protected]  Abstract. Modelled closely on the feedforward conical structure of the primate vision system - Convolutional Neural Networks (CNNs) learn by adopting a local to global feature extraction strategy. This makes them view-specific models and results in poor invariance encoding within its learnt weights to adequately identify objects whose appearance is altered by various transformations such as rotations, translations, and scale. Recent physiological studies reveal the visual system first views the scene globally for subsequent processing in its ventral stream leading to a global-first response strategy in its recognition function. Conventional CNNs generally use small filters, thus losing the global view of the image. A trainable module proposed by Kumar & Sharma [24] called Stacked Filters Convolution (SFC) models this approach by using a pyramid of large multi-scale filters to extract features from wider areas of the image, which is then trained by a normal CNN. The end-to-end model is referred to as Stacked Filter CNN (SFCNN). In addition to improved test results, SFCNN showed promising results on scale invariance classification. The experiments, however, were performed on small resolution datasets and small CNN as backbone. In this paper, we extend this work and test SFC integrated with the VGG16 network on larger resolution datasets for scale invariance classification. Our results confirm the integration of SFC, and standard CNN also shows promising results on scale invariance on large resolution datasets. Keywords: Convolutional neural network · Feature map · Filter pyramid · Global feature · Scale invariance · Visual system  
   
  1 Introduction Convolutional Neural Networks (CNNs) are modelled on the physiological understanding of the biological visual system of primates and has its basis firmly grounded to the idea that the visual system extracts and uses local features for classification and recognition of visual objects [25]. Such models have achieved great success in various computer vision tasks such as in image classification, object detection, visual concept discovery, semantic segmentation and boundary detection. Algorithms purely c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 259–278, 2023. https://doi.org/10.1007/978-3-031-25477-2_12  
   
  260  
   
  D. Kumar and D. Sharma  
   
  based on CNNs or used as a basis of other complicated algorithms such as R-CNN [9], ResNet [12], DenseNet [13], SegNets [2] etc. are applied in various practical application domains such as in self-driving cars, facial recognition authentication systems such as in mobile phones, no-checkout shopping such as Amazon Go store, fashion and textile, medical image processing and quality assurance in manufacturing industries [21]. One of the reasons for the success of CNNs is its ability to extract features automatically from image and video data, thus eliminating the need for requiring prior hand-crafted features which is needed in the case of traditional Machine Learning (ML) methods. This makes CNNs excellent view-specific evaluators, meaning they are able to generalise well to objects drawn from the train data distribution. Though they successfully extract local features for object detection, they are not able to learn invariant features. This means if the same objects are presented to the CNN for classification for example, in a different size, CNNs do not perform well [15, 17, 27]. Invariance refers to the ability of recognising objects even when the appearance varies in some ways as a result of transformations such as translations, scaling, rotation or reflection. Recent neuronal and physiological studies have provided new insights into the workings of the brain and Visual Information Processing System (VIPS) in general. These studies are showing the importance of global features towards classification and recognition tasks and suggest the visual system uses both local and global features in its recognition function. More importantly, there are suggestions of a global-first response strategy of the visual system to speed-up recognition [11, 14, 44] whereby cells tuned to global features respond to visual stimuli prior to cells tuned on local features. This theory provides the potential for using global features combined with local features to solve transformation invariance problems in CNNs. By design, CNNs employ a localto-global feature extraction strategy using local patch-wise convolution operation using mostly small kernels. This allows the model view localised patches of the image, but unable to consider global views of the image, thus losing out on spatial relationships between features which may be relevant to improving invariance property in the model architecture. There are some studies that show a combined use of global features with CNN features such as in [22, 23, 55], however, the models are designed to use global features in parallel with CNN features and therefore do not present the CNN networks to train using global features of the images. To extract a global-first view of the image to train a CNN is studied in the work of [24]. The authors developed a technique called Stacked Filters Convolution (SFC) to extract features from wider areas of the image by using a pyramid of large multi-scale filters in parallel. The extracted features are then further upscaled in order to a) enhance the extracted features, and b) make the feature map size equivalent to the input image size. The resultant features are then combined and trained on a normal CNN. This end-to-end model is referred to as Stacked Filter CNN. The experiments were performed on small resolution datasets (CIFAR10 [20] and FashionMNIST (FMNIST) [52]) and using small CNN (LeNet [25]) as backbone. Their results on small resolution datasets showed effectiveness of the integration of SFC module with the CNN backbone on classification of scaled images sampled from the validation sets of the same datasets. The model however, lacked validation on larger resolution datasets.  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  261  
   
  In this paper, we extend the work of [24] and test SFC integrated with the pretrained VGG16 network on larger resolution datasets for scale invariance classification. We select PASCAL VOC [7] and ImageHoof [42] datasets for this purpose. We use large multi-scale filters to first extract global features from input images which are upscaled and then combined (similar to the process defined in [24]). However, before training on the VGG16 network, we use a convolutional operation to downsample the combined features depth-wise so that the feature maps have the same dimensions as the input image. Our results confirm the integration of SFC, with CNN also shows promising results on scale invariance on large resolution datasets. The main contributions of this paper are to extend the work of [24] by (a) demonstrating the applicability of training CNN’s with global features instead of using larger resolution images directly as input; (b) showing the use of large multi-view filters as an effective way to extract global spatial information from larger resolution images and to serve as global features for learning by CNNs; and (c) showing the effectiveness of the integration of the SFC module with CNNs on larger resolution images. The rest of the paper is organised as follows: Sect. 2 reviews related work while Sect. 3 describes the SFC module architecture. Section 4 describes our experiment design and results are presented in Sect. 5. We summarise and point to future directions in Sect. 6.  
   
  2 Background Given this paper is related to topics on global feature extraction using CNNs, use of large kernels to extract multi-scale features from input images and pyramid based methods in CNNs for scale invariant classification, they are reviewed briefly in the following subsections. 2.1 Global Features in Computer Vision The term global features in computer vision refers to describing an image as a whole [30]. They are used to generalise the distribution of the visual information in the object through various statistics that represent information on contours, shape and texture in the image and are useful for tasks such as object detection and classification. Local features are used to describe image patches (key points in the image) of an object [4]. These features, represented as lines and curves are basic building blocks of object shapes and are useful for tasks such as object recognition. While local features are effectively extracted in CNNs using small filters performing a patch-wise operation with the target image, extracting global features requires studying the whole image or spatially larger areas of the target image. Several work has been proposed to extract global features from images. This paper considers two categories of global feature extractor methods and their application with CNNs. These categories are: (a) global feature descriptors; and (b) large kernels (filters) in CNNs.  
   
  262  
   
  D. Kumar and D. Sharma  
   
  Global Feature Descriptors. Several studies report use of global feature descriptors to solve image classification and object detection problems such as HOG [5, 34], invariant moments [51, 54], uniform local-binary patterns (LBP) and discrete cosine transform (DCT) [33], discrete fourier transform (DFT) [44], color and entropy statistics [4], Gabor filters for texture analysis [4], and shape index [30] but mostly without combining with CNNs. In one study, Scale-Invariant Feature Transform (SIFT) is combined with CNN [57] but we note SIFT is classified as a local feature descriptor [49]. A neural network model is proposed by Zhang et al. [55] called Histogram of Gradients (HOG) improved CNN (HCNN) that combines texture features from traditional CNNs and global structural features from HOG [5] to cover for the shortness of CNNs in recognising fooling images (where some local features are chaotically distributed). Fusion of global with local features made their network become more sensitive to fooling images despite recording a slight decrease in classification accuracy. The MNIST [26] and a subset of the ImageNet [6] datasets were used to evaluate the performance of HCNN. Figure 1 describes the architecture of HCNN. The model however, was not tested on scale invariance classification. A similar approach is followed in the work of [23] where the authors use a color histogram in addition to HOG as global feature extractors. Their end-to-end network called Global features improved CNN (GCNN) was tested on classification of scaled images. The results showed improved performance of GCNN over well established VGG16 and LeNet5 CNNs on classification of scaled images from the Tiny Imagenet [28] and FMNIST datasets. The GCNN model showed the potential of using global features from input images for training with CNNs to improve scale invariance. However, the global feature extractors HOG and color histogram are non-trainable thus, non-updatable when used with a CNN during the training phase. Furthermore, the models present the fusion of global and local features in a parallel pipeline structure rather than using global features as a priori which is the goal of the SFCNN model.  
   
  Fig. 1. The HCNN architecture [55]. The HCNN classifier processes concatenated CNN and HOG features (image reproduced from [21]).  
   
  Large Kernels (filters) in CNNs. The use of large kernels to extract features from spatially broader areas of the target image have been studied in some work. In the area of semantic segmentation, Peng et al. [36] proposed a Global Convolutional Network (GCN) in which they studied the use of large kernels. Two important design principles were followed for GCN. Firstly, to retain localisation performance, the model structure comprised of only fully convolutional and deconvolutional layers. Secondly, to  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  263  
   
  enable densely connections between feature maps and per-pixel classifiers, large kernel size was adopted in the network architecture. This enhanced the networks capability to handle different transformations. They conducted their experiments on PASCAL VOC dataset and concluded that large kernels play an important role in both classification and localisation tasks. In another piece of work Park and Lee [35] inform extracting information from a large area surrounding the target pixel is essential to extract for example texture information. The use of large kernels in CNNs also reduces the number of convolutions required to scan the entire image, and thus, requiring less computations. However, with increasing kernel sizes increases the number of parameters (weights) in the network that will require gradient updates. This will potentially slow the back propagation process. Moreso, given additional parameters to process may require additional training cycles to allow the model to reach convergence. These problems can be mitigated by carefully choosing kernel sizes in relation to the input image size and choosing a higher learning rate at the beginning of model training to speed-up convergence. Also, given the improvements in processing capabilities of modern computers and the availability of GPUs provide an opportunity to use large kernels in CNNs. The SFC module in SFCNN uses large kernels to extract features from effectively larger areas of the image. 2.2 Pyramid Based Methods in CNNs for Scale-Invariant Classification Pyramid based methods have been used to address scale invariance in CNNs to some extent but have been limited to either generating image pyramids, feature map pyramids or filter pyramids. Image Pyramids. In image pyramid based CNN, copies of the input image are generated at multiple scales forming a pyramid of images. Each scaled image is then processed by a series of convolution and pooling layers independently. These series form a columnar architecture dedicated to processing the original image at multiple resolutions. The final output is computed as an ensemble of outputs from all scale columns [50]. This process is described in Fig. 2. Image pyramid based technique has been practised in the works of [5, 8, 16, 31, 32, 39]. An obvious advantage of image pyramid based methods is the input image is seen by the model at multi-scale levels; hence, allowing the model to develop a representation for the given scale levels. However, scaling the target image to generate image pyramids is similar to applying scale augmentation. In our work, we present no augmentation of the input images. Feature Pyramids. In feature pyramids methods, some or all levels of feature maps are used towards generating a prediction. The basic feature pyramid based model, referred to as pyramidal feature hierarchy, uses all feature maps; hence, all scales for prediction. The final prediction is computed as an ensemble of all feature maps. Figure 3 illustrates this technique. Whilst pyramidal feature hierarchy methods allow use of features from different scales towards prediction, the flow of signals from one feature layer to another remain sequential and one-directional.  
   
  264  
   
  D. Kumar and D. Sharma  
   
  Fig. 2. Image pyramid CNN architecture [50] (image reproduced from [21]).  
   
  Fig. 3. Pyramidal Feature Hierarchy architecture [19].  
   
  For example, [29] exploit the pyramidal hierarchy of feature maps in deep convolutional networks by developing lateral connections from each feature map in the pyramid to build high-level semantic feature maps at all scales. They argue the feature maps of different spatial resolutions produced in the standard CNN introduces semantic gaps caused by different depths. In their work the higher-resolution maps were shown to be important for detecting smaller objects. In the end, they proposed a CNN architecture called Feature Pyramid Network (FPN) in which they developed lateral connections from each feature map in the standard CNN feature pyramid via a top-down pathway. They argued that by developing such lateral connections between feature maps makes CNN scale-invariant, as a change in an object’s scale is offset by shifting its level in the pyramid. The FPN architecture allowed them to combine rich semantics from all levels of the feature hierarchy by reusing the original feature maps. Since the feature maps are reused, FPN does not incur much computational overhead. Similar architectures are proposed in works of [18, 19, 56]. Feature pyramids capitalise on the feature maps of the CNN; hence, come at no extra computation overhead and have been argued to be useful for detecting small objects.  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  265  
   
  However, the limitation of feature pyramid is that it is built on inherent multi-scale, pyramidal structure of the base CNN networks, which are originally designed for classification tasks [56]. Hence, the features are only limited to the task the base CNN was designed for and may not be suitable for other image processing tasks. Filter Pyramids. The works of [40, 41] claim that visual processing is hierarchical and that along the hierarchy, receptive field sizes of the neurons increases. This structure of the vision system is claimed to build invariance - first to position and scale and then to viewport and other transformations. Inspired by these models the approach of using differently sized convolutional filters in parallel to capture more context is increasingly being explored by researchers [10, 22, 53]. Google’s INCEPTION family of models uses this approach [45–47]. The design of the INCEPTION module [46] is based heavily on the intuition that visual information should be processed at various scales and then aggregated so that subsequent levels can extract abstract features from different scales. Similarly, based on the concept of large kernels and pyramid based methods, [22] propose a distributed information integration CNN model called D-Net by combining local and global features from images. To extract global features, they developed a trainable layer called Filter Pyramid Convolutions (FPC). In FPC layer, various scale filters (from small to large filters) are applied to progressively cover broader areas of an image. The features extracted are then pooled, resulting compact sized feature maps as output in terms of its spatial dimension. This design however, limited the output of the FPC layer to be used as input in subsequent convolution layers. The design of the FPC layer is adopted in the SFC module. However, to overcome the problem of small scaled output feature maps from FPC layer, the feature maps in SFC module are instead upscaled. The upscaled feature maps allows SFC module output to be used as input in subsequent feature extraction layers of a CNN. The advantage of using filter pyramids or multi-scale filters such as in the INCEPTION module is that it allows the network to make larger spatial views of the image or feature maps. This allows the network to draw in more semantic information and discriminative features from the input than the conventional small filter sizes in standard CNNs. The limitations of this approach are that larger filter sizes mean more parameters for the network to update, and thus, take longer for network to converge.  
   
  3 Stacked Filter CNN (SFCNN) In this section we describe the trainable module called Stacked Filters Convolution (SFC) proposed in [24] that allows a CNN network take advantage of large kernels to extract global features to improve classification of scaled images. The design of SFC module is inspired by the work of [14] who show that biological visual system utilizes global features prior to local features in detection and recognition. The integration of SFC module with an existing CNN is referred to as Stacked Filter CNN (SFCNN). The SFCNN model is shown in Fig. 4. The core part of SFCNN - SFC is described in the following sub-section.  
   
  266  
   
  3.1  
   
  D. Kumar and D. Sharma  
   
  Stacked Filters Convolution (SFC) Module  
   
  The SFC module contains a collection of filters of varying sizes. This is unlike a traditional CNN where normally a small filter is applied for each convolution in a convolution layer. The multi-scale filters form a pyramidal structure of stacked filters and operate on the target image using the same standard sliding window convolution technique (Fig. 4(a)).  
   
  Fig. 4. Architecture of SFCNN with k = 3 filters in SFC module (adapted from [24]). In SFC module, multi-scale filters (a) generate multi-scale feature pyramids (b) which are upscaled using bilinear upscaling method. These feature maps are upscaled to a the same resolution (h, w) as the input image, concatenated (d) and then passed to the CNN network for further feature extraction.  
   
  The main aim of each filter with dimensions (kh , kw ) in the filter-stack is to produce a feature map as output with dimensions (fh , fw ) which can be upscaled (in its height and width dimensions only using a non-recurring scale factor (u)), to same size as the input image I; that is, (f(h∗u) , f(w∗u) ) = (Ih , Iw ). The value of hyper parameters stride and padding are also considered for selection of the dimensions of each filter. Subsequently, sizes of other filters are identified using a similar process. Since each filter produces multi-scale feature maps (Fig. 4(b)), these maps are then normalised to produce a uniform size final feature maps equivalent to the size of the input image (Fig. 4(c)). Here, the technique of bilinear upscaling method is used. Finally all upsampled feature maps are concatenated and passed to the next layer (Fig. 4(d)). Since the SFC module is the only layer that gets to inspect the target image, [24] proposes the inclusion of a small filter in the filter-stack. This is to allow extracting local features from the target image which would otherwise be lost. In this way, the SFC module also allows local features to be collected and packed together with global features for onward processing. In our implementation of the SFC module, we further downsample the concatenated features (in the depth dimension) using a convolution operation so that the final output feature maps are the same size as the input image. 3.2  
   
  Forward and Backward Propagation in SFC Module  
   
  A forward pass is achieved by passing an input image to the SFC module which applies convolutions with each filter from a filter stack and outputs a stack of feature maps as  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  267  
   
  a result. This process repeats for all stacks of filters in the layer resulting in stacks of output feature maps of different scales accordingly. The stack of output feature maps are then upsampled to generate uniform-sized outputs in terms of height and width of the feature maps in all stacks. The shape of each stack of upscaled features is saved for use in backward propagation. The upscaled stack of features are finally concatenated for forward propagation into the traditional CNN network (described in [25]). The backward function in SFC module receives gradients from the network. The gradients are then unstacked or slices in the exact same dimensions and shape of the individual stack of feature maps that were concatenated during the forward pass. This results in stacks of gradients maps corresponding to each stacked upscaled feature maps (during forward pass). Each stack of gradient is max pooled by the same factor that was used to upscale the feature maps initially during the forward pass. Using chain rule derivative algorithm these gradients are then used to update the weights of filters in the corresponding filter stacks.  
   
  4 Experiments In Kumar and Sharma [24], experiments to test the efficacy of SFC integrated with a CNN is conducted using CIFAR10 and FMNIST datasets trained with LeNet5 as the CNN module. Their experiments however, were performed on small resolution datasets and using small CNN. In this paper we extend their work by testing SFCNN on larger resolution datasets. This section describes the selected large resolution datasets, the VGG16 architecture and SFC parameters. 4.1 Datasets ImageHoof. A subset of the ImageNet dataset, ImageHoof contains images of all dog breeds. The dataset is freely available from fastai DL library [42]. This dataset consists of 12,954 training images and 3929 test images in color. There is no validation set. The images are divided into 10 mutually exclusive classes representing names of ten dog breeds. The original ImageHoof dataset is highly imbalanced and contains images of various resolutions. This dataset is therefore preprocessed prior to usage. First, ratio of train and test images is adjusted to 80%/20% respectively from the initial 67%/43%. This was done to shift more images to the training batch. Secondly, images from the training set were transferred to the test set for classes which had fewer images in population. Post preprocessing, the ImageHoof dataset contained a total of 13500 images consisting of 10800 and 2700 in the train and test set respectively. PASCAL VOC. The PASCAL Visual Object Classes (VOC) 2012 dataset is widely used as a benchmark dataset for object detection, semantic segmentation, and classification tasks [7]. The images are divided into 20 mutually exclusive classes representing names aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. The dataset is split into three subsets containing pixel-level segmentation annotations, bounding box annotations and object class annotations. For the purpose of this research we combine images from all three subsets resulting in 13351 for training and 3477 for testing.  
   
  268  
   
  4.2  
   
  D. Kumar and D. Sharma  
   
  VGG16 Network  
   
  Since our goal is to test large resolution datasets on the SFC module and train on a CNN network, we select the VGG16 network for this purpose. Here, the VGG16 network is utilised for benchmarking and also for integrating global features extracted through SFC to a CNN network. Proposed by Simonyan & Zisserman [43], VGG16 is a popular CNN model used by researchers in the computer vision field for image classification and segmentation tasks. It was originally trained on ImageNet dataset that contains over 14 million images categorised into 1000 classes. Several configurations of the VGG CNN exist, ranging from 11, 13, 16 and 19 weight layers. These configurations are labelled A-E and differ only in the depth. In this work, configuration D is used that contains 16 weight layers comprising of 13 convolution and 3 hidden layers in the fully connected part of the network. All convolution layers are configured with 3 × 3 filter sizes. The network also uses maxpooling layers and is used in several work such as in [1, 37, 38]. In this work, VGG16 network is trained on PASCAL VOC and ImageHoof datasets. The architecture of VGG16 is described in Table 1. This includes information on network structure and hyper-parameter settings. Since the VGG16 architecture is unchanged in our implementation provided us with the opportunity to use transfer learning, where we use the pretrained weights of the VGG16 implementation trained on the ImageNet dataset from PyTorch v1.2.0 deep learning library. Table 1. VGG16 - Configuration D - CNN architecture. VGG16 - Configuration D - 16 weight layers Input image size: 224 × 224 RGB image Layer Layer type  
   
  # of filters/neurons Filter/pool size Padding stride  
   
  1  
   
  convolution  
   
  64  
   
  3×3  
   
  1  
   
  2  
   
  convolution  
   
  64  
   
  3×3  
   
  1  
   
  2×2  
   
  maxpool  
   
  1 2  
   
  3  
   
  convolution  
   
  128  
   
  3×3  
   
  1  
   
  4  
   
  convolution  
   
  128  
   
  3×3  
   
  1  
   
  2×2  
   
  maxpool  
   
  1  
   
  1 1 2  
   
  5  
   
  convolution  
   
  256  
   
  3×3  
   
  1  
   
  1  
   
  6  
   
  convolution  
   
  256  
   
  3×3  
   
  1  
   
  1  
   
  7  
   
  convolution  
   
  256  
   
  3×3  
   
  1  
   
  2×2  
   
  maxpool  
   
  1 2  
   
  8  
   
  convolution  
   
  512  
   
  3×3  
   
  1  
   
  1  
   
  9  
   
  convolution  
   
  512  
   
  3×3  
   
  1  
   
  1  
   
  10  
   
  convolution  
   
  512  
   
  3×3  
   
  1  
   
  2×2  
   
  maxpool  
   
  1 2  
   
  11  
   
  convolution  
   
  512  
   
  3×3  
   
  1  
   
  1  
   
  12  
   
  convolution  
   
  512  
   
  3×3  
   
  1  
   
  1  
   
  13  
   
  convolution  
   
  512  
   
  3×3  
   
  1  
   
  maxpool 14  
   
  fully connected 4096  
   
  15  
   
  fully connected 4096  
   
  16  
   
  fully connected 1000 softmax  
   
  2×2  
   
  1 2  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  269  
   
  4.3 SFC Hyper-Parameters We follow a similar approach for selection of the hyper-parameters for the SFC module as described in [24]. For training on ImageHoof and PASCAL VOC datasets, the SFC module in our implementation was constructed with 4 stacks of filters (k_stacks = 34) having filters of sizes (3 × 3), (25 × 25), (65 × 65) and (113 × 113) respectively. The filter sizes are selected so that using appropriate upscaling factor, the resultant feature maps can be upscaled to (224 × 224) which is the input image size for the VGG16 network. Each stack is initialised with 16 filters (n_f ilters = 16). We set stride = 1 for all stacks, padding = 1 for stack with 3 × 3 filters, padding = 0 for the rest of the stacks and upscaling factors 1, 2, 1.4, 1.12 respectively for each filter stack. The final shape of the concatenated stacks of feature maps on the datasets is (64 × 224 × 224) where 64 = k_stacks × n_f ilters. Table 2 describes the hyper-parameters in detail. Table 2. Description of hyper-parameters and calculation of final output feature map sizes in the SFC module. The output size of the intermediate feature map in column (E) is dependent on the filter size (column (B)), stride and padding and must satisfy the condition that multiplied with a non-recurring upscaling factor must equal to the image size (column G) where (Ih , Iw ) = (Oh , Ow ). The same SFC module parameters are applied to both ImageHoof and PASCAL VOC datasets. (A) image size (Ih , Iw )  
   
  (B) filter size (C) stride s kh , kw  
   
  (D) padding p  
   
  (E) feature map output size (fh , fw ) (fh = (Ih + 2p) − kh + 1)/s (fw = (Iw + 2p) − kw + 1)/s  
   
  224×224  
   
  113×113  
   
  224×224  
   
  65×65  
   
  1  
   
  0  
   
  112×112  
   
  2  
   
  224×224  
   
  1  
   
  0  
   
  160×160  
   
  1.4  
   
  224×224  
   
  224×224  
   
  25×25  
   
  1  
   
  0  
   
  200×200  
   
  1.12  
   
  224×224  
   
  224×224  
   
  3×3  
   
  1  
   
  1  
   
  224×224  
   
  1  
   
  224×224  
   
  (F) up-scaling factor  
   
  (G) final output map size (Oh , Ow )  
   
  4.4 The SFCNN Network The complete SFCNN network used for training and evaluation on the task of scale invariance classification in this paper is given in Fig. 5. The network architecture is similar to the SFCNN architecture proposed in [24]. However, batch normalisation is applied to the output of SFC module in order to reduce generalisation error and to speed up training, without increasing overfitting [3]. Before training on the pretrained VGG16 network, the SFC module output is further downsampled in the depth dimension using a convolution operation. This was done to make the resolution of the SFC feature maps equivalent to a standard image resolution so that it can be accepted by the VGG16 network as input. This step was required in order to use transfer learning on the SFC output. 4.5 Training Process First we establish benchmark results by training the pretrained VGG16 network on ImageHoof and PASCAL VOC datasets separately. This establishes our benchmark  
   
  270  
   
  D. Kumar and D. Sharma  
   
  Fig. 5. Architecture of the SFCNN network used on ImageHoof and PASCAL VOC datasets.  
   
  results against which we compare results of the respective SFCNN networks. Then we integrate SFC module with the VGG16 network as shown in Fig. 5 and retrain on ImageHoof and PASCAL VOC datasets respectively. Hence, we obtain a total of four trained models for comparison (two models per dataset). The training parameters for our models are described in Table 3. We abbreviate the VGG16 trained networks on ImageHoof and PASCAL VOC datasets as VGG16-Imh and VGG16-voc respectively, and SFCNN-Imh and SFCNN-voc for SFCNN trained networks. Table 3. Summary of training parameters for the benchmark CNNs and SFCNN models. Dataset ImageHoof  
   
  PASCAL VOC  
   
  Network name  
   
  VGG16-Imh  
   
  Learning rate  
   
  0.001 (epochs 1–20) 0.001 (epoch 1) 0.0001 (epochs 2–20)  
   
  SFCNN-Imh  
   
  Training epochs  
   
  20  
   
  VGG16-voc SFCNN-voc 0.0001 - epochs 1–20  
   
  CNN activation function relu FC activation function  
   
  sigmoid  
   
  Transfer learning  
   
  yes  
   
  Momentum  
   
  0.9  
   
  Weight decay  
   
  0.0001  
   
  Optimiser  
   
  Stochastic gradient decent  
   
  Loss function  
   
  Cross-entropy  
   
  Train batch size  
   
  16  
   
  Test batch size  
   
  1  
   
  DL library  
   
  PyTorch v1.2.0  
   
  Hardware  
   
  Dell Precision T7910 64 GB RAM Nvidia Geforce Titan X 12 GB GPU  
   
  4.6  
   
  Scaled Images for Testing  
   
  Scaled images are prepared from the test set of each dataset for testing respective VGG16 and SFCNN netwirks. This process is described in detail in [24]. 7 scale  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  271  
   
  categories - [150, 140, 120, 100, 80, 60, 50] are established to test the models. Numbers > 100 indicate enlargement while < 100 indicate reduction in the size of an image. 50 test images per class from ImageHoof and PASCAL VOC datasets, are selected at random. Each image is then scaled according to the scale percentage defined in the scale category list, resulting in an additional 6 scaled images per class in addition to the original image (of scale 100%). In this way, for each class in each dataset, 7 scale category folders are created and respective scaled images stored in them accordingly. Using this process, 350 scaled images per class are sampled from each dataset. Furthermore, an ensemble dataset is created by combining all scaled images from all classes for each dataset. This resulted in a total of 3500 scaled images for testing on ImageHoof dataset (10 classes ×350) and 7000 scaled images on PASCAL VOC dataset (20 classes ×350) respectively. Figure 6 shows an example image from each dataset and its corresponding scaled versions for testing. The models are analysed on scaled images from each of these scale categories independently as well as on the ensemble dataset.  
   
  Fig. 6. An example of scaled test images from datasets PASCAL VOC - aeroplane (bottom) and ImageHoof - n02086240 (Shih-Tzu) (top). The numbers indicate percentage image is scaled to. 100 indicates no scaling.  
   
  4.7 Evaluation Metrics The accuracy metric is used to evaluate the trained models on scale categories. Accuracy is an intuitive performance measure to simply evaluate the generalisation capability the models by finding out the total number of scaled images that were correctly classified in the respective scale categories. The top-1 and top-5 accuracy of each network on the scale categories is evaluated.  
   
  5 Results and Discussion In this section we discuss the results of SFCNN in two specific areas. First, we compare the train and test statistics of SFCNN with the benchmark VGG16 trained models. Second, the models trained are evaluated on scaled images.  
   
  272  
   
  5.1  
   
  D. Kumar and D. Sharma  
   
  Comparing Train and Test Statistics on Non-scaled Images  
   
  The convergence of deep learning models are usually influenced by the choice of hyperparameters and the number of trainable parameters. Networks with a large parameter base usually converge slowly and take a longer time to train. In addition to the inclusion of a benchmark CNN within the SFCNN network, the SFC module contains components that directly increase the size of the network parameters. This mainly refers to the large multi-scale filters introduced in SFC. It is therefore expected that the SFCNN network would require more epochs to train to reach the level of results obtained by the VGG16 network given similar training environment. Despite this challenge, we attempted to train the models for the same number of epochs by carefully choosing the learning rates. Table 4 compares the train and test statistics (accuracy and loss) for all the networks. These statistics are evaluations on the raw train and test images without any form of scale jittering. Whilst the pretrained VGG16-Imh model overfits the ImageHoof dataset, SFCNN-Imh reveals a stable train and test results despite the increase in parameters in the network and when trained for the same number of epochs. While SFCNN-Imh train losses are higher, the network achieves the same test result of 92.1% as the VGG16-Imh network using pretrained weights. On the other hand, significant results are obtained on PASCAL VOC dataset where SFCNN-voc outperforms the VGG16-voc network on train and test statistics. Here, SFCNN-voc achieved a difference in accuracy of 2.5% higher than the VGG16-voc network indicating that SFCNN network performs better on datasets that are non-ImageNet based. These baseline results provide some evidence that training CNN networks on features extracted from larger areas of the input image using large kernels is useful in improving the generalisation capability of the models studied in this paper. Table 4. Train and test statistics of SFCNN and benchmark CNNs. Model  
   
  Train acc Train loss Test acc Difference (loss) Difference (test acc)  
   
  ImageHoof VGG16-Imh 1.000  
   
  0.001  
   
  0.921  
   
  SFCNN-Imh 0.996  
   
  0.015  
   
  0.921  
   
  VGG16-voc 0.665  
   
  0.616  
   
  0.572  
   
  SFCNN-voc 0.665  
   
  0.547  
   
  0.597  
   
  0.014  
   
  0.0%  
   
  –0.069  
   
  +2.5%  
   
  PASCAL VOC  
   
  5.2  
   
  Evaluation of SFCNN on Scaled Images  
   
  The classification results of the models on different scale categories and on different datasets can be viewed in Table 5. The column hit rate indicates the number of scale categories SFCNN outperformed the benchmark. Similar to [24], for this study, hit rate of >= 50% is desirable, that is, SFCNN should at least perform better on 50% of the scale categories compared to the benchmark VGG16 only networks. Since the ensemble test dataset combines all scaled images in one batch it is excluded from this ratio.  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  273  
   
  Classifications accuracies are obtained by testing the studied models on scaled images from each scale category. Compared with the benchmark CNNs, the performance of SFCNN is evaluated on scaled images in two categories as follows: (i) Generalisation on larger resolution datasets, and (ii) Effect on enlarged and reduced image scales Table 5. Performance summarisation (top-1 and top-5 accuracy) of SFCNN and benchmark CNNs on all the scale categories. The difference in accuracy between SFCNN and benchmark CNNs are shown as percentages in brackets. A positive figure indicates higher accuracy of SFCNN over the benchmark CNN on the respective scale category. Scale categories (scale size in (%)) Model  
   
  Metric  
   
  Enseble  
   
  150  
   
  140  
   
  120  
   
  100  
   
  80  
   
  60  
   
  50  
   
  hit rate  
   
  0.880 0.892 (+1.2%)  
   
  0.900 0.914 (+1.4%)  
   
  0.926 0.924 (–0.2%)  
   
  0.926 0.928 (+0.2%)  
   
  0.892 0.904 (+1.2%)  
   
  0.872 0.844 (–2.8%)  
   
  0.820 0.760 (–6.0%)  
   
  0.571 (4/7)  
   
  0.984 0.986 (+0.2%)  
   
  0.992 0.994 (+0.2%)  
   
  0.988 0.994 (+0.6%)  
   
  0.992 0.992 (0.0%)  
   
  0.994 0.986 (–0.8%)  
   
  0.992 0.978 (–1.4%)  
   
  0.988 0.980 (-0.8%)  
   
  0.571 (4/7)  
   
  0.513 0.566 (+5.3%)  
   
  0.513 0.588 (+7.5%)  
   
  0.519 0.587 (+6.8%)  
   
  0.543 0.585 (+4.2%)  
   
  0.406 0.323 0.551 0.334 (+14.5%) (+1.1%)  
   
  0.270 0.240 (–3.0%)  
   
  0.857 (6/7)  
   
  0.875 0.908 (+3.3%)  
   
  0.883 0.917 (+3.4%)  
   
  0.902 0.929 (+2.7%)  
   
  0.900 0.926 (+2.6%)  
   
  0.797 0.896 (+9.9%)  
   
  0.662 0.666 (+0.4%)  
   
  1.000 (7/7)  
   
  ImageHoof VGG16-Imh Top-1 acc 0.889 SFCNN-Imh 0.871 (–1.8%) ImageHoof VGG16-Imh Top-5 acc 0.991 SFCNN-Imh 0.987 (–0.4%) PASCAL VOC VGG16-voc Top-1 acc 0.440 SFCNN-voc 0.497 (+5.7%) PASCAL VOC VGG16-voc Top-5 acc 0.825 SFCNN-voc 0.858 (+3.3%)  
   
  0.736 0.757 (+2.1%)  
   
  Generalisation on Larger Resolution Datasets. The performance of SFCNN on small resolution datasets - CIFAR10 and FMNIST has been studied in the work of [24]. When tested on scaled images on various scale categories, their results revealed that the SFCNN network performed better on color images (CIFAR10) than on grey-scale images (FMNIST). They concluded that spatial features extracted from larger areas of the target image during training helped improve scale invariance in CNN networks, in particular for color images. This research has aimed to further extend the work of [24] by testing SFCNN on larger resolution datasets. ImageHoof and PASCAL VOC were selected for this purpose. Here, the hit rates of SFCNN on both datasets are > 50% indicating higher accuracy on majority of the scale categories than the benchmark VGG16 only networks. Significant results are obtained on PASCAL VOC dataset where the SFCNN-voc network outperforms the VGG16-voc network on 6 out of the 7 scale categories (top-1 accuracy) and on all scale categories considering top-5 accuracy (hite rate = 100%). Images from the various scale categories are combined in the ensemble test set. The average of the test accuracies on all scale categories determines the accuracy of the ensemble category. The higher classification accuracy score (both top-1 (49.7%) and  
   
  274  
   
  D. Kumar and D. Sharma  
   
  top-5 (85.8%)) on the PASCAL VOC ensemble test set is directly contributed by the individual scale category accuracies. For example, the top-1 accuracy score of SFCNNvoc network is higher than the benchmark VGG16-voc network by 5.7% which amounts to 399 more scaled images classified correctly. Furthermore, SFCNN top-5 accuracies indicate the model is able to predict the correct class of the scaled image in its top 5 predictions. For ImageHoof dataset, whilst the SFCNN-Imh network produces better results on majority of the scale categories, its performance on the ensemble dataset is lower than the benchmark VGG16-Imh networks. This means that the difference in accuracy (both top-1 and top-5) of the SFCNN-Imh networks on reduced scaled images was much lower than the benchmark VGG16-Imh networks. From the above results we observe that pretrained networks such as the VGG16 would produce competitive results when trained on datasets sampled from the ImageNet distribution such as the ImageHoof dataset. However, when non-ImageNet based datasets are used, SFCNN networks perform better. Our results also indicate that regardless of the source of data, training CNNs with spatial global features improves network generalisation and its ability to classify scaled images. Effect on Enlarged and Reduced Image Scales. Accuracy scores of SFCNN are compared with VGG16 networks on scaled up images (categories [150, 140, 120]). For these categories the best performing SFCNN is on PASCAL VOC dataset (SFCNNvoc) recording an average accuracy of +6.3% more than the benchmark VGG16-voc network (top-1 accuracy). A possible reason presented to explain this is the process of upscaling feature maps in the SFC module blurs the features thus smoothing edges and removing noise from an image in the process, thus improving classification accuracy. Though SFCNN-Imh records an average top-1 accuracy of +1.2% on ImageHoof dataset, it underperforms on scale category [120]. Upon investigation, the lower accuracies recorded on ImageHoof dataset were found to be attributed to a possible overfitting of the VGG16-voc and SFCNN-voc model given that the dataset is already seen by the pretrained VGG16 CNN. Comparing accuracy scores on scaled down images (categories [80, 60, 50]), promising performance of SFCNN is noted over VGG16 network trained on PASCAL VOC dataset. Here, average accuracy score is higher by 4.2% considering top-1 accuracy. However, there is poor performance of SFCNN on reduced scaled down Imagehoof images. Relating the feature map upscaling method with inter-class similarities of images of dogs revealed that whilst blurring helped smooth edges and remove noise, it also resulted in low-variation in feature maps as a result of similarities that existed in the original Imagehoof dataset such as color of dogs. Scale category [100] is where images are in their original state (unscaled). In this category test accuracy of SFCNN surpasses benchmark CNNs by 0.2% on ImageHoof dataset and 4.2% on PASCAL VOC dataset respectively.  
   
  6 Conclusion In this paper, we extend the work of [24] and test SFC module integrated with the pretrained VGG16 network on larger resolution datasets for scale invariance classification.  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  275  
   
  The SFC module is developed to model the global-first feature extraction strategy of the primate vision system proposed by [14]. In SFC module, global spatial features are extracted from the target image using large multi-scale kernels resulting in multi-scale feature maps. The multi-scale feature maps are upscaled to a uniform resolution and then combined. A convolution operation downsamples the feature maps which is then trained using a pretrained VGG16 network. The end-to-end network is referred to as SFCNN. We study the effects of global-first feature extraction on scale invariance by evaluating SFCNN’s ability to classify test images subjected to scale transformations and compare with established benchmarks results obtained in this research. Our results confirm SFCNN also generalises well on larger resolution datasets in addition to improving the network’s ability to classify scaled images. From our experimental results we conclude that spatial features extracted from larger areas of the target image during training help in improving the scale invariance capability of CNN based networks. The advantage of SFCNN lies in its ability to extract features from spatially larger areas of the input image using larger kernels. This overcomes the shortcomings of standard CNNs that usually apply smaller kernels which are more effective in extracting local features. The introduction of large multi-scale kernels in SFC module however, increases the network parameters. Furthermore, the SFC module generates larger feature maps as a result of upscaling which in turn requires more convolution operations in subsequent layers in the network. The concatenation of upscaled features further increases the volume of the feature maps. These limitations ultimately lead to increased computation time for SFCNN. Problems and opportunities that require further investigations are to a) test SFCNN on the ImageNet dataset, b) test on image resolutions larger than 224 × 224, c) trial other upscaling methods to upscale feature maps in SFC module, d) test SFCNN on other forms of transformations such as rotations and translations, and e) integrate SFC module with other benchmark CNNs such as ResNet and EfficientNet [48].  
   
  References 1. Alippi, C., Disabato, S., Roveri, M.: Moving convolutional neural networks to embedded systems: the alexnet and vgg-16 case. In: 2018 17th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), pp. 212–223. IEEE (2018) 2. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 39(12), 2481– 2495 (2017) 3. Bjorck, N., Gomes, C.P., Selman, B., Weinberger, K.Q.: Understanding batch normalization. In: Advances in Neural Information Processing Systems 31 (2018) 4. Bosch, M., Zhu, F., Khanna, N., Boushey, C.J., Delp, E.J.: Combining global and local features for food identification in dietary assessment. In: 2011 18th IEEE International Conference on Image Processing, pp. 1789–1792. IEEE (2011) 5. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005), pp. 886–893 (2005) 6. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale Hierarchical Image Database. In: CVPR 2009 (2009)  
   
  276  
   
  D. Kumar and D. Sharma  
   
  7. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. Int. J. Comput. Vis. 88(2), 303–338 (2010) 8. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach. Intell. 32(9), 1627–1645 (2009) 9. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 580–587 (2014) 10. Gong, Y., Wang, L., Guo, R., Lazebnik, S.: Multi-scale orderless pooling of deep convolutional activation features. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8695, pp. 392–407. Springer, Cham (2014). https://doi.org/10.1007/9783-319-10584-0_26 11. Han, Y., Roig, G., Geiger, G., Poggio, T.: Is the human visual system invariant to translation and scale? In: 2017 AAAI Spring Symposium Series (2017) 12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 770– 778 (2016) 13. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 4700–4708 (2017) 14. Huang, J., et al.: Rapid processing of a global feature in the on visual pathways of behaving monkeys. Front. Neurosci. 11, 474 (2017). https://doi.org/10.3389/fnins.2017.00474 15. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer networks. In: Advances in Neural Information Processing Systems 28, pp. 2017–2025. Curran Associates, Inc. (2015) 16. Kanazawa, A., Sharma, A., Jacobs, D.W.: Locally scale-invariant convolutional neural networks. CoRR abs/ arXiv: 1412.5104 (2014) 17. Kauderer-Abrams, E.: Quantifying translation-invariance in convolutional neural networks. arXiv preprint arXiv:1801.01450 (2017) 18. Kim, S.-W., Kook, H.-K., Sun, J.-Y., Kang, M.-C., Ko, S.-J.: Parallel feature pyramid network for object detection. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11209, pp. 239–256. Springer, Cham (2018). https://doi.org/10. 1007/978-3-030-01228-1_15 19. Kong, T., Sun, F., Huang, W., Liu, H.: Deep feature pyramid reconfiguration for object detection. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11209, pp. 172–188. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-012281_11 20. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images. Tech. rep, Citeseer (2009) 21. Kumar, D.: Multi-modal Information Extraction and Fusion with Convolutional Neural Networks for Classification of Scaled Images. Ph.D. thesis, University of Canberra, Canberra, Australia (2020) 22. Kumar, D., Sharma, D.: Distributed information integration in convolutional neural networks. In: Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - VISAPP, vol. 5, pp. 491–498. SciTePress (2020). https://doi.org/10.5220/0009150404910498 23. Kumar, D., Sharma, D.: Multi-modal information extraction and fusion with convolutional neural networks. In: 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–9. IEEE World Congress on Computational Intelligence (IEEE WCCI) (2020). https://doi. org/10.1109/IJCNN48605.2020.9206803  
   
  Global-first Training Strategy with Convolutional Neural Networks  
   
  277  
   
  24. Kumar, D., Sharma, D.: Feature map upscaling to improve scale invariance in convolutional neural networks. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, vol. 5, pp. 113–122 (Febuary 2021). https://doi.org/10.5220/0010246001130122 25. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et al.: Gradient-based learning applied to document recognition. Proc. IEEE 86(11), 2278–2324 (1998) 26. LeCun, Y., Cortes, C., Burges, C.J.: The mnist database of handwritten digits, vol. 10(34), p. 14 (1998). http://yann.lecun.com/exdb/mnist/ 27. Lenc, K., Vedaldi, A.: Understanding image representations by measuring their equivariance and equivalence. In: CVPR (2015) 28. Li, F.F., Karpathy, A., Johnson, J.: Tiny ImageNet Visual Recognition Challenge (2019). https://tiny-imagenet.herokuapp.com/. (Accessed 30-Dec-2019) 29. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 2117–2125 (2017) 30. Lisin, D.A., Mattar, M.A., Blaschko, M.B., Learned-Miller, E.G., Benfield, M.C.: Combining local and global image features for object class recognition. In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)-Workshops, p. 47. IEEE (2005) 31. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision 60(2), 91–110 (2004) 32. Marcos, D., Kellenberger, B., Lobry, S., Tuia, D.: Scale equivariance in cnns with vector fields. arXiv preprint arXiv:1807.11783 (2018) 33. Margae, S., Ait Kerroum, M., Fakhri, Y.: Fusion of local and global feature extraction based on uniform lbp and dct for traffic sign recognition. In: International Review on Computers and Software (IRECOS) vol. 10 (January 2015). https://doi.org/10.15866/irecos.v10i1.5051 34. Nguyen, T.K., Coustaty, M., Guillaume, J.L.: A combination of histogram of oriented gradients and color features to cooperate with louvain method based image segmentation. In: VISIGRAPP 2019 (2019) 35. Park, H., Lee, K.M.: Look wider to match image patches with convolutional neural networks. IEEE Signal Process. Lett. 24(12), 1788–1792 (2016) 36. Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters-improve semantic segmentation by global convolutional network. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 4353–4361 (2017) 37. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing Systems, pp. 91– 99 (2015) 38. Saqib, M., Khan, S.D., Sharma, N., Blumenstein, M.: A study on detecting drones using deep convolutional neural networks. In: 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pp. 1–5. IEEE (2017) 39. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229 (2013) 40. Serre, T.: Hierarchical models of the visual system. In: Jaeger, D., Jung, R. (eds.) Encyclopedia of Computational Neuroscience, pp. 1–12. Springer, New York (2013). https://doi.org/ 10.1007/978-1-4614-6675-8_345 41. Serre, T., Wolf, L., Bileschi, S., Riesenhuber, M., Poggio, T.: Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell. 29(3), 411–426 (2007). https://doi.org/10.1109/TPAMI.2007.56 42. Shaw, A.: Imagehoof dataset (2019). https://github.com/fastai/imagenette/blob/master/ README.md. (Accessed 10-Dec-2019)  
   
  278  
   
  D. Kumar and D. Sharma  
   
  43. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 44. Su, Y., Shan, S., Chen, X., Gao, W.: Hierarchical ensemble of global and local classifiers for face recognition. IEEE Trans. Image Process. 18(8), 1885–1896 (2009) 45. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: Thirty-First AAAI Conference on Artificial Intelligence (2017) 46. Szegedy, C., et al.: Going deeper with convolutions. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 1–9 (2015) 47. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 2818–2826 (2016) 48. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 (2019) 49. ping Tian, D., et al.: A review on image feature extraction and representation techniques. Int. J. Multimedia Ubiquitous Eng. 8(4), 385–396 (2013) 50. Wang, H., Kembhavi, A., Farhadi, A., Yuille, A.L., Rastegari, M.: Elastic: Improving cnns with dynamic scaling policies. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2258–2267 (2019) 51. Wu, J., Qiu, S., Kong, Y., Chen, Y., Senhadji, L., Shu, H.: Momentsnet: a simple learning-free method for binary image recognition. In: IEEE International Conference on Image Processing (ICIP), pp. 2667–2671. IEEE (2017) 52. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. Tech. rep., arXiv (2017) 53. Zagoruyko, S., Komodakis, N.: Learning to compare image patches via convolutional neural networks. In: Proceedings of the IEEE Conference On Computer Vision And Pattern Recognition, pp. 4353–4361 (2015) 54. Zekovich, S., Tuba, M.: Hu moments based handwritten digits recognition algorithm. In: Recent Advances in Knowledge Engineering and Systems Science (2013) 55. Zhang, T., Zeng, Y., Xu, B.: Hcnn: A neural network model for combining local and global features towards human-like classification. Int. J. Pattern Recognit Artif Intell. 30(01), 1655004 (2016) 56. Zhao, Q., et al.: M2det: A single-shot object detector based on multi-level feature pyramid network. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 9259–9266 (2019) 57. Zheng, L., Yang, Y., Tian, Q.: Sift meets cnn: A decade survey of instance retrieval. IEEE Trans. Pattern Anal. Mach. Intell. 40(5), 1224–1244 (2017)  
   
  Spline-Based Dense Medial Descriptors for Image Simplification Using Saliency Maps Jieying Wang4(B) , Leonardo de Melo2 , Alexandre X. Falc˜ao2 , Jiˇr´ı Kosinka1 , and Alexandru Telea3 1  
   
  4  
   
  Bernoulli Institute, University of Groningen, 9747 AG Groningen, The Netherlands [email protected]  2 Department of Information Systems, Institute of Computing, University of Campinas, S˜ao Paulo 13083-852, Brazil [email protected]  3 Department of Information and Computing Sciences, Utrecht University, 3584 CC Utrecht, The Netherlands [email protected]  Bernoulli Institute, Shandong University of Science and Technology, 579 Qianwangang Road, Huangdao District, Qingdao 266590, Shandong Province, P.R. China [email protected]  Abstract. Medial descriptors have attracted increasing interest in image representation, simplification, and compression. Recently, such descriptors have been separately used to (a) increase the local quality of representing salient features in an image and (b) globally compress an entire image via a B-spline encoding. To date, the two desiderates, (a) high local quality and (b) high overall compression of images, have not been addressed by a single medial method. We achieve this integration by presenting Spatial Saliency Spline Dense Medial Descriptors (3SDMD) for saliency-aware image simplification-and-compression. Our method significantly improves the trade-off between compression and image quality of earlier medial-based methods while keeping perceptually salient features. We also demonstrate the added-value of user-designed, as compared to automaticallycomputed, saliency maps. We show that our method achieves both higher compression and better quality than JPEG for a broad range of images and, for specific image types, yields higher compression and similar quality than JPEG 2000. Keywords: Medial descriptors · Saliency maps · B-splines · Image simplification · Image compression  
   
  1 Introduction Image simplification and compression are essential in many applications in science, engineering, and consumer contexts. Compression methods, such as the well-known JPEG [44] and the newer JPEG 2000 [39] and BPG [5] efficiently reduce the cost of storing and/or transmitting an image, typically in a lossy manner, by discarding certain image features or details. Simplification keeps image structures deemed important while eliminating less-important ones, to ease the analysis and processing of the former structures. c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 279–302, 2023. https://doi.org/10.1007/978-3-031-25477-2_13  
   
  280  
   
  J. Wang et al.  
   
  Fig. 1. Examples of the DSR saliency estimator failing to detect salient objects: (a3–d3) show the SSDMD results using the saliency maps (a2–d2) for images (a1–d1).  
   
  A particular class of simplification-and-compression methods models images as a set of luminance threshold-sets [58] and encodes these by their Medial Axis Transforms (MATs). Wang et al. [50] followed this approach to propose Dense Medial Descriptors (DMD), a lossy image compression method. While DMD showed promising quantitative and qualitative results, it cannot yet compare in both visual quality and compression ratio (CR) with state-of-the-art compression methods like JPEG or similar. Two lines of research tried to address this issue. Improving Quality: DMD simplifies an image globally, making it hard to preserve fine details in some areas while strongly simplifying the image in other areas. The SSDMD method [49] addressed this by adding a saliency map to DMD, allowing users to specify different spatial simplification levels over an image. SSDMD delivers higher local quality than DMD (as specified by the saliency map) but has two key limitations. First, it only marginally improves CR when compared to DMD, since highly-salient image areas actually increase the MAT information needed to be stored. Secondly, SSDMD uses automatically computed saliency maps to control simplification. Such maps can significantly fail to capture what users perceive as salient (thus, to be preserved) vs nonsalient (thus, to be simplified). Figure 1 outlines this problem for four images (a1–d1) with saliency maps (a2–d2; bright=salient; dark=non-salient) automatically computed by the DSR method [25]. SSDMD compression results (a3–d3) arguably lose details that humans would find salient, such as blurred faces (a3, b3, c3) and nearly complete loss of the leopard skin texture (d3). Improving Compression: DMD stores the MATs of an image’s threshold sets as pixel chains, which is exact, but inefficient storage-wise. The Spline Dense Medial Descriptors (SDMD) method [47] improved CR by representing MATs with accurate and compact-storage B-spline descriptors for each threshold set [48]. Yet, just as DMD,  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  281  
   
  Fig. 2. Spline-based dense medial descriptors pipeline with free parameters in green. Elements added by 3S-DMD method proposed in this paper are marked in red. (Color figure online)  
   
  SDMD simplifies images only globally, thus increasing CR but achieving limited visual quality. Our Contributions: We jointly address the visual quality and CR goals of all above earlier MAT-based image compression methods by a single method: – We combine the strengths of SSDMD [49] (spatial control of image simplification) with SDMD [47] (compact encoding of MATs with B-splines); – We allow users to interactively tune parameters of the joint method, including full control over the saliency map design; – We evaluate our proposal on additional image types and compare it, with favorable results, with state-of-the-art methods (JPEG and JPEG 2000). We organize this paper as follows. Section 2 presents related work on medial descriptors for image compression. Section 3 details our Spatial Saliency Spline-based Dense Medial Descriptor (3S-DMD) method. Section 4 evaluates our results. Section 5 discusses 3S-DMD. Section 6 concludes the paper.  
   
  2 Related Work Our 3S-DMD method (Fig. 2) adapts SDMD to use saliency maps to further simplify less important regions while preserving salient ones (new contributions marked red in the figure). We next discuss related work: dense medial descriptors (Sect. 2.1), splinebased DMD (Sect. 2.2), and saliency maps (Sect. 2.3). 2.1 Dense Medial Descriptors (DMD) The key idea of DMD [50] is to use medial axes to efficiently encode luminance threshold-sets of an image. Let I : R2 → [0, 255] be the 8-bit Y channel in the YUV space of a color image. DMD splits I in n = 256 threshold sets or layers   Ti = x ∈ R2 | I(x) ≥ i , 0 ≤ i < n. (1)  
   
  282  
   
  J. Wang et al.  
   
  Since adjacent layers often contain highly similar information, DMD uses only a subset D ⊂ {Ti } of L = |D| < 256 layers to encode I (Fig. 2, step 1). Here and next, | · | denotes set size. In some layers, small-size islands (connected components in the foreground Ti or background T i ) can appear, due to small local intensity variations. DMD fills in, respectively removes, islands smaller than a fraction ε of |Ti |, respectively |T i |, which contribute little to the image I (Fig. 2, step 2). Next, DMD extracts medial axis transforms (STi , DTTi ) from these L layers (Fig. 2, step 3), where DTTi (x) = min x − y y∈∂Ti  
   
  (2)  
   
  is the distance transform [15] of the boundary ∂Ti of layer Ti , and STi = {x ∈ Ti |∃f1 ∈ ∂Ti , f2 ∈ ∂Ti , f1 = f2 : x − f1  = x − f2  = DTTi (x)} (3) is the medial axis, or skeleton, of Ti . In Eq. 3, f1 and f2 are called feature points [19] of skeletal point x. Computing MATs of binary images is a well-studied topic, described in detail in classical work [21, 23, 31, 35, 36, 38]. The medial axes STi contain many so-called spurious branches caused by small perturbations along ∂Ti . Storing such branches takes significant space but contributes little to the reconstruction quality. To address this, DMD uses the salient-skeleton metric [40] defined as ρ(x) , (4) σ(x) = DTTi (x) where ρ(x) is the fraction of the boundary ∂Ti that the skeletal point x encodes [42]. Saliency-based regularization removes all pixels x ∈ STi where σ(x) is below a userspecified threshold δ > 0, yielding a simplified skeleton ST i and corresponding distance transform DTT i (Fig. 2, step 4). From the regularized MAT (ST i , DTT i ), one can reconstruct a simplified version Ti of each layer Ti as the union ∪x∈ST B(x, DTT i (x)) of i discs B centered at pixels x ∈ ST i and with radii given by DTT i (x). An approximation I of the input image I is finally obtained by drawing all reconstructed layers Ti atop each other in increasing order of luminance i (Fig. 2, step 7). DMD uses the fast GPU implementation of [7], which is pixel-exact and linear in the number of pixels in Ti [19, 28]. Full implementation details are available at [50]. DMD provides an accurate encoding of grayscale and color images. However, DMD stores the MATs (ST i , DTT i ) using pixel chain delta encoding, which is inefficient and leads to a poor compression ratio. 2.2  
   
  Spline-Based Medial Descriptors (SDMD)  
   
  Compactly encoding MATs has attracted interest in binary shape representation [22, 45]. B-splines were found effective for this as they store fewer (control) points than all pixels in an MAT. Zhu et al. [57] and Yushkevich et al. [55] accurately modeled MATs of 2D binary shapes with multiple cubic B-splines. Yet, they require vector representations of the input shape and its MAT and also require a Voronoi-based MAT method [4] which is slow for complex shapes.  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  283  
   
  Spline-based Medial Axis Transform (SMAT) [48] extended the above B-spline idea to use raster representations for Ti , STi , and DTTi , to directly handle any binary raster image. In detail, MAT branches (ST i , DTT i ), seen as 3D pixel curves, and fitted with 3D B-splines. Each control point cj = (pj , DTTi (pj )) ∈ R3 consists of a 2D position pj and its corresponding DT value. Fitting uses the least-squares method [13] aiming to get (1) a minimal number of control points and (2) an approximation error γ between the MATs and B-splines below a user-given value γ (Fig. 2, step 5). Reconstruction first rasterizes the B-splines using de Casteljau’s algorithm [34] and next creates the layers Ti by the disc-union method outlined in Sect. 2.1 (Fig. 2, step 6). Wang et al. [47] proposed Spline Dense Medial Descriptors (SDMD) that uses the SMAT method to encode color images. SDMD applies SMAT to all luminance threshold sets of an image but also proposes three improvements to increase CR and quality: adaptively encoding upper or lower threshold-sets, treating chrominance and luminance separately, and removing Y-structures from the skeletons. For details, we refer to [47]. SDMD achieves much higher compression ratios at similar or even better quality to JPEG. Compared with DMD, which proved to faithfully represent an image, SDMD encodes images both faithfully and compactly. Yet, both DMD and SDMD work globally: High simplification easily removes small, but visually important, details, leading to poor quality. Conversely, low simplification allocates storage to unimportant image areas, leading to poor compression. 2.3 Saliency Maps Saliency maps encode the relative importance of various parts of an image for a given task or perceptual standpoint. A saliency map μ : R2 → [0, 1] gives, for each image pixel x, its importance or saliency, between totally irrelevant (μ = 0) and maximal importance (μ = 1). Such maps have been used for image quality assessment [27], content-based image retrieval [8], context-aware image resizing [18], and saliencybased image compression [3, 59]. Saliency maps can be created either in supervised mode—by users via manual annotation (see next Sect. 3.1)—or in unsupervised mode, automatically computed from images. Supervised methods use ground-truth images to learn discriminant features of salient objects [29]. The most accurate supervised methods use deep-learning [6, 51] and typically outperform unsupervised methods. Ywt, they need large amounts of human-annotated training data, and the generalization of training models across image domains usually requires adaptation and retraining [29]. Unsupervised methods use prior knowledge about salient objects and local image characteristics. Most methods start by finding image regions (e.g. superpixels) with high color contrast relative to neighbors [20, 25, 56]. Besides contrast, objects in focus [20], near the image center [10], or having red and yellow tones, important for human vision [33], are all considered as salient factors. Conversely, regions similar to the boundary will have low saliency as most image boundaries are background in natural images [10, 20, 25, 56]. In our work, we use the DSR [25] unsupervised bottom-up saliency estimation method which provides reliable saliency maps without requiring parameter tuning and is fast. Any other saliency estimators can be directly used instead as long as users find the produced maps suitable for their tasks at hand.  
   
  284  
   
  J. Wang et al.  
   
  3 Proposed 3S-DMD Method As stated above, an important limitation of SDMD is that it simplifies an image globally. Our earlier work, Spatial Saliency DMD (SSDMD) [49], addressed this by simplifying the DMD MAT’s using a spatial saliency map. We next present both SSDMD and our new method, 3S-DMD, which improves SSDMD in several respects. Section 3.1 shows how 3S-DMD benefits from manually-designed saliency maps via an interactive application. Section 3.2 presents SSDMD’s saliency-map-based simplification of the MAT and how we improved this by saliency-based spline fitting. Finally, Sect. 3.3 shows how we measure the quality of the results of our new 3S-DMD method. 3.1  
   
  User-Driven Saliency Map Generation  
   
  Section 2.3 reviewed a variety of techniques to automatically compute saliency maps from an image. As mentioned in Sect. 1, such automatically-computed maps may not fully meet user needs (cf. Fig. 1 (a2, b2)) or even fail to detect salient objects (cf. Fig. 1 (c2, d2)). Even when such maps fit with what users expect, the simplification they induce can lead to unwanted results due to the hard-to-predict shapes that skeletons have. To handle all such issues, we developed an interactive application that allows users to create their custom saliency maps or adjust maps created by automatic methods. Figure 3a shows the user interface, in which one can draw the saliency map using tools listed in the toolbar, tune all the method’s parameters, run the end-to-end pipeline, and check the obtained results. A video of our tool is provided in the supplementary material [46]. We provide three ways for users to manually design saliency maps, as follows.  
   
  Fig. 3. Directly saliency drawing. (a) Interface with a loaded image. (b) User drawing to specify the saliency. (c) Computed saliency map. (d–f) Generated saliency maps.  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  285  
   
  Fig. 4. SSIM-guided user-specified saliency map generation.  
   
  Direct Drawing: Users can directly paint a saliency map with various brushes, whose brightness gives the desired saliency (black=0, white=1). Figure 3b shows the drawing of a map for the car image in Fig. 3a. The user marked the car area as highly salient (white ellipse, region J) and background areas farther from the car as zero salient (black scribbled bands, region H). Figure 3c shows the computed saliency map μ. Regions where the user painted saliency are taken over from the drawing (H’ and J’ are copies of H and J, respectively). Unpainted areas carry no hints that the user found them important or not (Fig. 3b, region I). We set here the saliency to the average value μ = 0.5 (Fig. 3b, region I’). Adjust Precomputed Saliency Map: Fully painting a custom saliency map can be cumbersome, especially when one wants to use multiple saliency values. We support this use-case by allowing users to draw to modify a precomputed saliency map. Figures 3d–f show three such precomputed maps obtained with the DSR method [25], structured matrix decomposition (SMD) method [33], and the recent ITerative Saliency Estimator fLexible Framework (ITSELF) [29] method. SSIM-guided User-specified Saliency: Users may be unfamiliar with, or unable to run, existing saliency estimation methods. Also, they may not know how to tweak saliency to get the best quality-compression balance. We address these issues by computing the saliency map in a corrective way, i.e., by comparing the compression method’s output with its input. Figure 4 shows how this works. Given an input image (a), we first run SDMD without a saliency map. Next, we evaluate the quality of the output (b) by the Structural SIMilarity (SSIM) metric [52]. The generated SSIM map (c) shows the per-pixel structural similarity between the original (a) and the output (b), with darker pixels indicating less similar regions. Figure 4 (c) shows that SDMD yields poorer quality over several car details, especially its two wheels. Having this insight, we scribble bright colors on the two wheels to tell their importance (Fig. 4 (d)). We now use this quite simple saliency map (e) to run 3S-DMD to generate a new result (f). As visible in the last image, the quality of the left front wheel has improved.  
   
  286  
   
  J. Wang et al.  
   
  Fig. 5. Pigeon image (a1) encoded with SDMD (a2) and 3S-DMD (a3) with saliency map (c) computed by the DSR [25] method. Images (b1–b3) show details in layer 127.  
   
  3.2  
   
  Saliency-Based Parameter Control  
   
  We next show how to use the saliency maps created by the various methods in the denoising (step 2), regularization (step 4), and spline fitting (step 5) of our end-to-end pipeline (Fig. 2). Salient Islands Detection: As explained in Sect. 2.1, (S)DMD only keeps islands, or connected components Ci , which meet the condition |Ci | ≥ ε|Ti |. This can remove small but salient features (see Fig. 5): For ε = 0.04, the pigeon’s eyes, visible in the original image (a1), are removed (a2). We confirm this by verifying that the small  (b2). Lowering ε can islands in region A in the threshold-set T127 (b1) get lost in T127 alleviate this, but this allocates more information to encode the less important background, thereby increasing image size. To address this, we use the saliency map μ to compute a saliency-aware metric  2μ(x)−1 Ciμ = k1 , (5) x∈Ci  
   
  and next remove only islands for which Ciμ < ε. The factor k1 in Eq. 5 controls how much μ affects island removal. For k1 = 1, Ciμ = |Ci |, so our method behaves like the original (S)DMD. In practice, we set k1 = 5, which means that the most salient pixels (μ(x) = 1) are given five times their original unit weight; the least important pixels (μ(x) = 0), in contrast, get one-fifth of their original unit weight. This keeps small-size, but salient, details in the compressed image. Figure 5 (b3) shows this for a saliency map computed with the DSR method [25]. Islands in region A, while small, have a high μ, so they are retained. In contrast, although large, the island in region B has a low saliency, so it is removed. This ends up with a smaller size, but perceptually better, result (a3).  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  287  
   
  Fig. 6. Salient skeleton detection on a squirrel image (a) with SDMD (b, e) and 3S-DMD (c, f) using a manually-designed saliency map (d).  
   
  Saliency-aware Skeleton Simplification: As Sect. 2.1 outlined, (S)DMD regularizes skeletons ST i by keeping only pixels x ∈ STi where σ(x) exceeds a user-set threshold δ (Fig. 2, step 4). The SSDMD method [49] further simplifies ST i by removing points x ∈ ST i whose saliency μ(x) is smaller than a new threshold. This not only increases the number of thresholds users have to deal with but also yields poor quality as low-saliency areas get completely removed. SSDMD alleviates this by using various heuristics such as selective layer keeping and interpolation tricks. However, this makes the end-to-end method quite complex. In contrast to SSDMD, we blend σ with the saliency map μ by computing (μ(x)−1)  
   
  σ  (x) = σ(x) · k2  
   
  (6)  
   
  and then obtain ST i by upper-thresholding σ  (x) with the user-set value δ, i.e., ST i = {x ∈ STi |σ  (x) > δ}.  
   
  (7)  
   
  The value k2 in Eq. 6 controls how much μ affects the skeleton simplification. For k2 = 1, our new metric σ  equals the original σ from (S)DMD. In practice, we set k2 = 2. Hence, the salient-skeleton values σ  (x) of the least important pixels (μ(x) = 0) become half of their original σ(x) values; in contrast, the σ values of the most important pixels (μ(x) = 1) stay unchanged. Figure 6 shows the improvement given by our new metric σ  . Images (e, f) show the regularized skeletons ST 43 of one layer, T43 , computed with SDMD’s σ metric and 3S-DMD’s σ  metric, for the same user-set δ = 0.6, and a simple manually-designed saliency map, for illustration purposes (image d). We see how 3S-DMD (image f) simplifies skeletons in the image background more, since the saliency is low there, than SDMD (image e), which has no notion of a low-importance background. In contrast, in the foreground image areas (white areas in the saliency map μ), the 3S-DMD and SDMD skeletons are identical. As a result, 3S-DMD yields the same image quality as SDMD, but with about 10% extra compression.  
   
  288  
   
  J. Wang et al.  
   
  Fig. 7. DMD compression has artifacts (a) found as low-SSIM regions (b). SSDMD (c) removes these but marks subtle background differences important for quality (d). Image taken from [49].  
   
  Saliency-Based Spline Fitting: Section 2.2 stated that SDMD finds the minimal number of B-spline control points needed to reach a user-given fitting error γ between a skeleton branch Bi and the B-spline Ci . This error is given by the Hausdorff distance H(Bi , Ci ) computed over all pixels x ∈ Bi . We modify the fixed user-set threshold γ to involve the saliency map μ by    
   
  γ =γ  
   
  (1−μ(x))  
   
  k3 |Bi |  
   
  x∈Bi  
   
  ,  
   
  (8)  
   
  where k3 controls how much μ influences the spline fitting. For k3 = 1, γ  equals the original γ. We set k3 = 2 in practice. Hence, when a branch is fully within a zerosaliency region (μ(x) = 0), γ  = 2γ, i.e., we allow a double fitting error as compared to the original SDMD. For branches located in a maximum saliency regions (μ(x) = 1), the fitting error stays the same, i.e., γ  = γ. 3.3  
   
  Saliency-Aware Quality Metric  
   
  ˜ ∈ R+ measure how close a compressed image I˜ is to the origQuality metrics Q(I, I) inal image I. Such metrics include the mean squared error (MSE) and peak signal-tonoise ratio (PSNR). While simple to compute and with clear physical meanings, these metrics do not match well perceived visual quality [53]. The SSIM index [52] alleviates this by measuring, pixel-wise, how perceptually similar two images are. Wang et al. [54] proposed Multiscale SSIM (MS-SSIM), which is an advanced top-down interpretation of how the human visual system comprehends images considering variations of image resolution and viewing conditions. While MS-SSIM models human perception well, it handles focus (high μ(x)) and context (low μ(x)) areas identically. Figure 7 illustrates this: Image (a) shows the DMD result of a car image and (b) shows the SSIM map. Image (a) shows some artifacts on the car roof, also visible as dark areas in the SSIM map (b). Image (c) shows the SSDMD result [49] of the car image, with strong background simplification and detail retention in the focus (car) area. The car-roof artifacts are removed, so (c) matches better the original image than (a). Yet, the MS-SSIM score of (c) is much lower than for DMD (0.9088 vs 0.9527). The large dark regions in the SSIM map background (d) explain this: While the saliency map μ clearly says that background is unimportant, MS-SSIM considers it equally important to foreground. Given the above, saliency data should be considered by a perception-aware quality metric. This is also reflected by saliency-based objective metrics [2, 14, 24, 26, 27]  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  289  
   
  Fig. 8. Comparison of DMD (a1–c1) with SSDMD (a2–c2) for three focus-and-context images. For each image, we show the standard MS-SSIM quality Q, and spatial-saliency-aware MS-SSIM Qµ . Image taken from [49].  
   
  which integrate a visual saliency map into the quality metric as a weighting map, thereby improving image-quality prediction performance. We follow the same idea by integrating the saliency map μ into the MS-SSIM [54] pooling function, as follows. Take the MS-SSIM metric for a reference image I and a distorted image I˜ −1   βM M βj ˜ = SSIM(I, I) ˜ ˜ cj (I, I) Q(I, I) ,  
   
  (9)  
   
  j=1  
   
  ˜ iteratively downsampled by a factor of 2 on scale where cj is the contrast map c(I, I) ˜ 1 ≤ j ≤ M ; SSIM(I, I) is the structural similarity of I and I˜ on scale M [52]; and the factor βj models the relative importance of different scales. We weigh Q by the saliency map μ yielding the saliency-aware quality metric  
   
   βM M βj −1  x∈I μ(x)SSIM(x) x∈I μj (x)cj (x)   Qμ= , x∈I μ(x) x∈I μj (x) j=1  
   
  (10)  
   
  where μj is the saliency map at scale j. For notation brevity, the arguments I and I˜ are omitted in Eq. 10. Using Qμ instead of Q allows in-focus values (high μ(x)) to contribute more to similarity than context values (low μ(x)). Figure 8 compares the results of DMD (a1–c1) and SSDMD (a2–c2) for three focusand-context images, using the standard MS-SSIM quality Q and our spatial-saliencyaware quality Qμ . The Q values for SSDMD are lower than those for DMD, which suggests that SSDMD has a poorer quality than DMD. Yet, we see that SSDMD creates images that are visually almost the same as DMD, in line with the almost identical Qμ values for SSDMD and DMD. Thus, we argue that Qμ is a better quality measure for focus-and-context simplification than Q. We consider Qμ next for evaluating the image quality.  
   
  290  
   
  J. Wang et al.  
   
  4 Results Section 3 proposed 3S-DMD, a method that incorporates three schemes for users to create a spatial saliency map, three ways for adjusting the original SDMD with these maps, and a saliency-aware quality metric Qμ to measure how well the reconstructed image I˜ captures the input image I. We next evaluate 3S-DMD’s results in detail, as follows. – – – – 4.1  
   
  First, we describe our evaluation methodology (Sect. 4.1). We show how 3S-DMD depends on its free parameters (Sect. 4.2). We compare 3S-DMD with DMD [50] and SDMD [47] (Sect. 4.3). Atop [49], we also compare with the JPEG and JPEG 2000 methods (Sect. 4.4). Evaluation Methodology  
   
  The 3S-DMD encoding consists of a tuple (w, h, {li }), i.e., the pixel width w and height h of the input image I, and the L selected layers li . A layer li = (i, f, {bki }) has an intensity value i, a flag f that tells if it uses upper- or lower-thresholding (for details, see [47]), and a B-spline set {bki } encoding its MAT. Each B-spline bki = (dki , {cj }) has a degree dki ∈ N and control points cj ∈ R3 (see Sect. 2.2). Sizes of the images I˜ and I are typically measured by bits per pixel (bpp), i.e., the number of bits used to encode a pixel’s grayscale or color value [12]. Yet, in an encoding context, we want to compare the sizes of I˜ and I, rather than measure their  Here, |3SDM D(I)|  is absolute sizes. For this, we define CR = |I|/|3SDM D(I)|. the byte-size of the 3S-DMD storage scheme outlined above, while |I| is the size (in bytes) of the original image I. The quality Qμ (Sect. 3.3) and compression ratio CR of 3S-DMD depend on four parameters (Fig. 2): the number of selected threshold-sets L, the size of removed islands ε, the skeletal saliency threshold δ, and the spline fitting tolerance γ. We establish ranges for these parameters based on results of previous work [47, 49, 50], as follows: L ∈ [1, 60], ε ∈ [0.001, 0.1], δ ∈ [0.01, 3], and γ ∈ [0.001, 0.005]. We further sample these ranges by the following representative values: L ∈ {15, 25}, ε = 0.02, δ ∈ {0.3, 0.8}, and γ = 0.0015. We use these values to compare DMD, SDMD, and 3SDMD (Sect. 4.3) and 3S-DMD with JPEG and JPEG 2000 (Sect. 4.4). We test all these methods on a 50-image database, which is selected randomly from the MSRA10K [9], SOD [30], and ECSSD [37] benchmarks. In addition to these real-world pictures, we also tested 3S-DMD on several artificially-designed images (Sect. 4.4). All test images have a resolution between 10002 to 20002 pixels. 4.2  
   
  Effect of Parameters  
   
  To intuitively illustrate how 3S-DMD performs for different parameter values, we first group these into weighting factors and user thresholds, and show the effect of these for a specific image.  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  291  
   
  Fig. 9. Progressive simplification of a flower image (a) using a saliency map (b) for different weight values k1 , k2 , and k3 (c1–c4).  
   
  Weighting Factors Effect: As explained in Sect. 3.2, the k1 , k2 , and k3 factors control how much the saliency map μ affects the island detection, skeleton simplification, and spline fitting, respectively. We call these weighting factors—in contrast to the user parameters discussed next—since they are more technical parameters, which do not arguably need to be exposed to end users. Secondly, their effect is strongly related to the way 3S-DMD treats image areas of different saliency. Let ‘foreground’ and ‘background’ describe areas of high, respectively, low saliency map μ values. Simply put, increasing all (or any) of these three weighting factors progressively simplifies the image background, similarly to a (soft) blurring effect, but keeps the image foreground relatively untouched. Figure 9 shows this for a flower image under different values for k1 , k2 , and k3 . The user parameters are all fixed to the default values L = 25, ε = 0.02, δ = 0.3, γ = 0.0015. The setting k1 = k2 = k3 = 1, shown in Fig 9 (c1), corresponds to the original SDMD method since, for this setting, μ has no effect on island detection, skeleton regularization, and spline fitting (see Eqs. 5, 6, and 8). As we increase k1 , k2 , and k3 , the image background gets progressively more simplified; see Figs. 9 (c2–c4). However, the flower in the foreground stays roughly the same in all images. The CR and Qμ values shown below the images match the above observations: as the weights increase, Qμ drops only slightly decreases, but CR increases strongly. In practice, as stated in Sect. 3.2, we found k1 = 5, k2 = 2, and k3 = 2 to be a good default for balancing CR and Qμ . User Thresholds Effect: 3S-DMD depends on four thresholds, as follows: – L controls how smoothly the simplified image captures color gradients; larger values yield smoother gradients;  
   
  292  
   
  J. Wang et al.  
   
  Fig. 10. 3S-DMD results for a flower image (a) using the saliency map of Fig. 9(b) for different combinations of parameters L, ε, δ, and γ.  
   
  – ε gives the scale of details that are kept in the image; larger values remove larger details; – δ controls the scale of corners that are kept in the image; larger values round off larger corners; – γ tells how accurately B-splines fit skeleton branches; larger values yield more distorted results. In contrast to the weighting factors discussed earlier, these four thresholds significantly influence the ‘style’ of the simplified image. Hence, we believe they are best left under the direct control of the end users. Figure 10 shows the effect of the thresholds L, ε, δ, and γ by showing the 3S-DMD results on the same flower image, using the same saliency map, as in Fig. 9. Image (a2) shows the results of 3S-DMD when setting user thresholds. The remaining images (b1–b4) are each the effect of a single user threshold change (red in the legend). If we decrease L( image (b1)), even if we select onlyL = 15 layers, we still get a visually convincing result. Yet, the stamens in region B and the flowers in regions A and C look duller than in image (a). Image (b2) uses a higher ε value, which removes many large islands in the image background, e.g., the one corresponding to the yellow flower in region A. Image (b3) uses a higher δ, which rounds off corners of background shapes, e.g. the flowers in regions A and C. Finally, image (b4) uses a higher γ, which distorts the boundaries of the flower in region A and creates subtly false colors in region D. 4.3  
   
  Comparison with DMD and SDMD  
   
  Figure 11 compares the Qμ and CR values of DMD (blue markers), SDMD (red markers), and our proposed 3S-DMD (green markers), for the four user-parameter settings  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  293  
   
  DMD method SDMD method 3S-DMD method Individual result Average result  
   
  CR  
   
  Fig. 11. Star plots of DMD (blue markers), SDMD (red markers), and 3S-DMD (green markers) for 50 real-world images. The actual image data (smaller dots) are connected to the corresponding average value (star center markers) for each method. Marker shapes indicate the four parameter settings being used. (Color figure online)  
   
  listed in Sect. 4.1, using a star plot. Small dots indicate metric values for a run involving a method-and-parameter-setting on a single image. Markers at the ‘star centers’ show average values for all runs over the 50 images in the benchmark for one parameter setting and one method. For each method (color), there are four stars, one for each of the four parameter-settings used, as indicated by the four glyph types in the figure’s legend. The star center triples depicted using the same glyph show runs that use the same parameter settings. We fixed ε = 0.02 and γ = 0.0015 so these user parameters are not listed in the figure’s legend. Figure 11 offers several insights. Small stars show little variance in CR and Qμ from the average for a given method-and-parameter-set. Large stars indicate more variance as a function of the actual images. The sizes and shapes of the stars in the figure are quite similar. Hence, DMD, SDMD, and 3S-DMD show a similar dependency of CR and Qμ on the real-world image type. This is due to the fact that SDMD and 3S-DMD inherit the thresholding and skeletonization used by DMD. Yet, the green stars are slightly larger and more spread horizontally, indicating that 3S-DMD can produce greater changes in CR for similar Qμ . For each color (method), its four stars show an inverse correlation of CR with Qμ . Indeed, more layers and smaller δ yield higher quality but less compression; conversely, fewer layers and larger δ slightly reduce quality, but strongly increase compression. The axes ranges show this too: CR varies roughly from 50 to 700, while quality varies between 0.91 and 0.98. The three large dots of the same glyph types let us compare the DMD, SDMD, and 3S-DMD methods under the same parameter setting. We see a clear inverse correlation pattern going from high Qμ and low CR (DMD, blue dots) to average Qμ and CR (SDMD, red dots) and then to lower Qμ and highest CR (3SDMD, green dots). Hence, 3S-DMD always gets higher CR than DMD and SDMD  
   
  294  
   
  J. Wang et al.  
   
  for only a small quality loss. On average, 3S-DMD increases CR by 234.2% relative to DMD, while Qμ drops by only 0.014. Compared with SDMD, 3S-DMD increases CR on average by 53.8%, while Qμ drops by a tiny 0.009. More importantly, when we compare CR and Qμ for different parameter settings, e.g., comparing the large round green marker with the star-shaped blue marker and square red marker, 3S-DMD not only yields a higher CR but also better quality.  
   
  Fig. 12. Comparison of 3S-DMD (a3–f3) with DMD (a1–f1) and SDMD (a2–f2) for six focusand-context images. For each result, we show the saliency-aware MS-SSIM Qµ and CR. The rightmost column shows the manually-modified DSR saliency map.  
   
  Figure 12 further compares the three methods for six focus-and-context images of insects, birds, animals, and plants from the MSRA10K benchmarks [9]. More results  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  295  
   
  are given in the supplementary material [46]. The zoomed-in areas show that, compared with DMD (a1–f1) and SDMD (a2–f2), 3S-DMD (a3–f3) preserves well important features marked as such by the saliency maps, like highlights (a, d), animals’ eyes (b, c, f), and the flower stamen (e). For background areas, all three methods perform visually roughly the same. The quality values Qμ are also similar for the three methods, with 3S-DMD scoring twice as best, three times as second-best, and once in the third place. CR values show that 3S-DMD achieves (significantly) higher compression than DMD and SDMD, except for image (b), where it scores slightly below SDMD. On the other hand, 3S-DMD retains for this example more details than SDMD for the foreground area, such as the bird’s eye, as also reflected by its higher quality score. 4.4 Comparison with JPEG and JPEG 2000 Tens of image compression exist, see e.g. [1, 11, 43] and methods cited therein. Comparing 3S-DMD with all of them is not feasible in the scope of this work. However, we provide a comparison with JPEG [44] and JPEG 2000 (J2K) [39] which are arguably among the most well-known, frequently-used, and generic, image compressors. Comparison with JPEG: Figure 13 compares 3S-DMD with JPEG on our image benchmark. The parameter setting of 3S-DMD (green dots) follows Sect. 4.3. JPEG (blue dots) is run under five quality settings: 10%, 30%, 50%, 70%, and 90%. As in Fig. 11, we use star plots for both 3S-DMD and JPEG: small dots are individual runs and large dots are averages. We see that 3S-DMD cannot reach the same Qμ values as when JPEG uses its 90% quality setting: the topmost blue dot is above the topmost green dot. However, the vertical spread of the blue vs green dots shows that the difference in quality (Qμ ) is small, about 4% on average. If we accept this small quality loss, 3S-DMD always gets higher compression rates than JPEG. In the limit, compared to JPEG with a quality of 10% (point A), 3S-DMD (point B) gets both higher CR and better quality. Figure 14 refines the above insights by showing six real-world images (building, plant, animal, natural scene, man-made structure, and people), compressed by 3S-DMD (a1–f1), JPEG (a2–f2), and J2K (a3–f3). We see that JPEG with a 10% quality creates obvious artifacts: checkerboarding (b2, c2, e2, f2), banding (a2, c2), and color faking (d2). 3S-DMD yields better quality (Qμ ) and does not exhibit such artifacts. Yet, 3SDMD loses small-scale, faint, details in the background, like the gravel in the sea (c1) and the red color of the traffic sign (f1). We argue that these are acceptable losses since these details are located in low-saliency areas. Separately, 3S-DMD always achieves higher CR than JPEG. Comparison with J2K: Figure 13 shows J2K (red dots) run under five fixed compression ratios: 100, 200, 300, 400, and 500. As CR increases, J2K has only a slightly quality loss and performs practically always better than JPEG. Figure 14 also verifies this: J2K’s quality Qμ is always higher than 0.99 and the compressed results are indistinguishable from the originals. 3S-DMD cannot (yet) achieve such quality. However, 3S-DMD can obtain comparable, and sometimes higher, CR values. We further refine the comparison with J2K by considering a narrower class of artificially made images,  
   
  296  
   
  J. Wang et al.  
   
  Fig. 13. Comparison of JPEG (blue dots), J2K (red dots), and 3S-DMD (green dots) for 50 images. The actual image data (smaller dots) are connected to the corresponding average value (larger dots) for each parameter setting of the three methods. (Color figure online)  
   
  such as graphics art (logos, graphics design), scientific visualization images, synthesized images using graphics rendering and vectorization methods [32], and cartoon images. For such images, 3S-DMD produces both higher CR and quality than J2K and JPEG. Figure 15 shows four representative images, one from each of the above four categories, compressed with 3S-DMD (a1–d1), JPEG (a2–d2), and J2K (a3–d3). As in earlier cases, JPEG with a quality of 10% generates obvious artifacts such as blocking (a2, b2, c2, d2), banding (c2), and color faking (c2), and has a CR well below the other two methods. When compared with J2K, our method yields similar Qμ values. We show some zoomed-in areas to expose a few subtle differences: For the graphics design example (a), 3S-DMD achieves visually much better results, without the checkerboarding and blur artifacts of J2K. This is also seen in the first image in Fig. 15 where 3SDMD got a higher Qμ than J2K. For the second image (b) in Fig. 15, 3S-DMD captures the smooth luminance gradient in the shadow area quite well. In contrast, J2K causes a slight amount of false color artifacts. For the strong-contrast images (c) and (d), J2K creates some small-scale blur artifacts. 3S-SDMD does not have such problems but suffers from a slight color change issue due to its selection of threshold-sets to be encoded. Most importantly, with a similar or better quality, 3S-DMD always yields higher compression than J2K for such synthetic images. We conclude that, for real-world images, 3S-DMD gets both higher CR and quality than JPEG but cannot match J2K’s quality at the same CR. Yet, for synthetic images, 3S-DMD gets both much higher CR and quality than JPEG, and also achieves higher CR at similar quality but with fewer artifacts than J2K.  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  297  
   
  Fig. 14. Comparison of 3S-DMD (a1–f1) with JPEG -10% (a2–f2) and J2K (a3–f3) for six realworld images. For each image, we show the saliency-aware metric Qµ and CR.  
   
  298  
   
  J. Wang et al.  
   
  5 Discussion We now discuss several aspects of our 3S-DMD image compression method. Genericity and Ease of Use: 3S-DMD is a general-purpose compression method for generic grayscale and color images. It relies on well-tested and robust algorithms such as the skeletonization method in [17, 42] and the least-squares B-spline fitting algorithm [13]. In contrast to segmentation tasks [16], 3S-DMD does not require precise saliency maps. Any saliency map that encodes which image areas are more important and which less for an application at hand can be used. 3S-DMD has four user parameters: the number of selected layers L, island size ε, skeleton saliency threshold δ, and spline fitting error γ. These parameters have intuitive effects and default values, as detailed in Sect. 4.2.  
   
  Fig. 15. Comparison of 3S-DMD (a1–d1) with JPEG-10% (a2–d2) and J2K (a3–d3) for four synthetic images. For each image, we show the saliency-aware Qµ and CR. The leftmost column shows the saliency map obtained by directly scribbling on the input image. The rightmost three columns show zoomed-in areas for 3S-DMD, JPEG, and J2K for detailed comparison.  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  299  
   
  Speed: We compute the most complex step in 3S-DMD, skeletonization, on the GPU [7, 41]. On a Linux PC with an Nvidia RTX 2060, this takes a few hundred milliseconds for images up to 10242 pixels. Spline fitting uses about 1 s per color channel, yielding a total of about 3 to 4 s for the compression. Replicability: We provide our full C++ source code and data for replication purposes, as well as a demo video and additional comparisons with DMD and SDMD, in the supplementary material [46]. Limitations: Image layer components that are one or two pixels thin cannot be encoded by MATs, so 3S-DMD cannot deal optimally with images with many thin-and-long details, such as animal fur, fine textures, and greenery. Figure 16 shows this for two such images. For smooth regions in the background (red boxes), 3S-DMD yields results that are indistinguishable from the originals. However, 3S-DMD cannot capture all the finegrained details present in the foreground (green boxes). One way to handle such cases is to artificially upscale the images, leading to fine details thicker than a few pixels, which next can be skeletonized with no problems. Studying how to perform this efficiently and with good CR values is an interesting topic for future work.  
   
  Fig. 16. Poor performance for 3S-DMD when handling images with many small-scale details, such as animal furs (a) and fine textures (b).  
   
  6 Conclusion We have presented 3S-DMD, a method for saliency-aware image simplification and compression. 3S-DMD combines the strengths of two of its precursors: SSDMD [49]  
   
  300  
   
  J. Wang et al.  
   
  that allows spatial control of image simplification, and SDMD [47] that compactly encodes MATs with B-splines. We have developed an interactive application for users to set parameters and customize saliency maps in three ways. We have illustrated how saliency maps involved in the SDMD pipeline offer spatially-dependent simplification. We have shown graphically and intuitively how 3S-DMD performs under different parameter combinations. To study the effectiveness of 3S-DMD, we have considered a database of 50 real-world images. Quantitative evaluation showed that 3S-DMD greatly improves the compression of SSDMD and SDMD at only a small quality loss. Our method delivers both higher CR and quality than JPEG. While we cannot reach the same high quality at the same CR values as J2K, our method yields similar quality, higher CR, and fewer artifacts for a wide class of synthetic images. We next aim to consider more extensive comparisons with additional compression techniques, e.g., deep neural network methods. Separately, we aim to extend 3S-DMD beyond grayscale or color image simplifications to simplify 3D scalar fields in scientific visualization, weighted with uncertainty-encoding maps. Acknowledgments. The first author acknowledges the China Scholarship Council (Grant number: 201806320354) for financial support.  
   
  References 1. Agustsson, E., Tschannen, M., Mentzer, F., Timofte, R., Van Gool, L.: Generative adversarial networks for extreme learned image compression. In: ICCV, pp. 221–231 (2019) 2. Alaei, A., Raveaux, R., Conte, D.: Image quality assessment based on regions of interest. Signal Image Video Process. 11, 673–680 (2017) 3. Andrushia, A., Thangarjan, R.: Saliency-based image compression using Walsh and Hadamard transform. In: Lect Notes Comp Vision Biomech, pp. 21–42 (2018) 4. Attali, D., Montanvert, A.: Computing and simplifying 2D and 3D continuous skeletons. Comput. Vision Image Understand. 67(3), 261–273 (1997) 5. Ballard, F.: Better portable graphics (2018). https://bellard.org/bpg 6. Borji, A., Cheng, M., Jiang, H., Li, J.: Salient object detection: a benchmark. IEEE TIP 24(12), 5706–22 (2015) 7. Cao, T.T., Tang, K., Mohamed, A., Tan, T.S.: Parallel banding algorithm to compute exact distance transform with the GPU. In: Proceedings ACM I3D, pp. 83–90 (2010) 8. Chen, T., Cheng, M., Tan, P., Shamir, A., Hu, S.: Sketch2photo: Internet image montage. ACM TOG 28(5) (2009) 9. Cheng, M.: MSRA10K salient object database (2014). mmcheng.net/msra10k 10. Cheng, M., Mitra, N.J., Huang, X., Torr, P.H., Hu, S.: Global contrast based salient region detection. IEEE TPAMI 37(3), 569–582 (2014) 11. Choi, Y., El-Khamy, M., Lee, J.: Variable rate deep image compression with a conditional autoencoder. ICCV pp. 3146–3154 (2019) 12. Daintith, J., Wright, E.: A Dictionary of Computing. Oxford Univ, Press (2008) 13. Eberly, D.: Least-squares fitting of data with B-spline curves (2014). geometric Tools. www.geometrictools.com/Documentation/BSplineCurveLeastSquaresFit.pdf 14. Engelke, U., Le Callet, P.: Perceived interest and overt visual attention in natural images. Image Commun. 39(PB), 386–404 (2015) 15. Fabbri, R., Costa, L.D.F., Torelli, J.C., Bruno, O.M.: 2D Euclidean distance transform algorithms: a comparative survey. ACM Comput Surv 40(1), 1–44 (2008)  
   
  Spline-Based Dense Medial Descriptors for Image Simplification  
   
  301  
   
  16. Falc˜ao, A., Bragantini, J.: The role of optimum connectivity in image segmentation: can the algorithm learn object information during the process? In: Couprie, M., Cousty, J., Kenmochi, Y., Mustafa, N. (eds.) DGCI 2019. LNCS, vol. 11414, pp. 180–194. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-14085-4 15 17. Falc˜ao, A., Stolfi, J., Lotufo, R.: The image foresting transform: theory, algorithms, and applications. IEEE TPAMI 26, 19–29 (2004) 18. Goferman, S., Zelnik, L., Tal, A.: Context-aware saliency detection. IEEE TPAMI 34(10), 1915–1926 (2011) 19. Hesselink, W.H., Roerdink, J.B.T.M.: Euclidean skeletons of digital image and volume data in linear time by the integer medial axis transform. IEEE TPAMI 30(12), 2204–2217 (2008) 20. Jiang, P., Ling, H., Yu, J., Peng, J.: Salient region detection by UFO: Uniqueness, focusness and objectness. In: Proceedings of the ICCV, pp. 1976–1983 (2013) 21. Kimmel, R., Shaked, D., Kiryati, N., Bruckstein, A.M.: Skeletonization via distance maps and level sets. CVIU 62(3), 382–391 (1995) 22. Kresch, R., Malah, D.: Skeleton-Based morphological coding of binary images. IEEE TIP 7(10), 1387–1399 (1998) 23. Lam, L., Lee, S., Suen, C.Y.: Thinning methodologies - a comprehensive survey. IEEE TPAMI 14(9), 869–885 (1992) 24. Le Callet, P., Niebur, E.: Visual attention and applications in multimedia technologies. Proc. IEEE 101(9), 2058–2067 (2013) 25. Li, X., Lu, H., Zhang, L., Ruan, X., Yang, M.: Saliency detection via dense and sparse reconstruction. In: Proceedings of the ICCV, pp. 2976–2983 (2013) 26. Liu, H., Engelke, U., Wang, J., Callet, Le, P., Heynderickx, I.: How does image content affect the added value of visual attention in objective image quality assessment? IEEE Signal Proc. Let. 20, 355–358 (2013) 27. Liu, H., Heynderickx, I.: Visual attention in objective image quality assessment: Based on eye-tracking data. IEEE TCSVT 21(7), 971–982 (2011) 28. Meijster, A., Roerdink, J., Hesselink, W.: A general algorithm for computing distance transforms in linear time. In: Proceedings ISMM, pp. 331–340 (2002) 29. de Melo Joao, L., de Castro Belem, F., Falcao, A.X.: Itself: Iterative saliency estimation flexible framework. Available at https://arxiv.org/abs/2006.16956 (2020) 30. Movahedi, V., Elder, J.: Design and perceptual validation of performance measures for salient object segmentation. In: IEEE Computer Society Conference (2010) 31. Ogniewicz, R., K¨ubler, O.: Hierarchical voronoi skeletons. Patt. Recogn. 28(3), 343–359 (1995) 32. Orzan, A., Bousseau, A., Barla, P., Winnem¨oller, H., Thollot, J., Salesin, D.: Diffusion curves: a vector representation for smooth-shaded images. Commun. ACM 56(7), 101–108 (2013) 33. Peng, H., Li, B., Ling, H., Hu, W., Xiong, W., Maybank, S.J.: Salient object detection via structured matrix decomposition. IEEE TPAMI 39(4), 818–832 (2016) 34. Piegl, L., Tiller, W.: The NURBS Book (2nd Ed.). Springer-Verlag (1997). https://doi.org/ 10.1007/978-3-642-59223-2 35. Pizer, S., Siddiqi, K., Sz´ekely, G., Damon, J., Zucker, S.: Multiscale medial loci and their properties. IJCV 55, 155–179 (2003) 36. Saha, P.K., Borgefors, G., Sanniti di Baja, G.: A survey on skeletonization algorithms and their applications. Patt. Recogn. Lett. 76, 3–12 (2016) 37. Shi, J., Yan, Q., Xu, L., Jia, J.: Hierarchical image saliency detection on extended CSSD. IEEE TPAMI 38(4D) (2016) 38. Siddiqi, K., Pizer, S.: Medial representations: mathematics, algorithms and applications (1nd Ed.). Springer (2008). https://doi.org/10.1007/978-1-4020-8658-8  
   
  302  
   
  J. Wang et al.  
   
  39. Taubman, D.S., Marcellin, M.W.: JPEG 2000: Image compression fundamentals, standards and practice. Kluwer Academic Publishers (2001) 40. Telea, A.: Feature preserving smoothing of shapes using saliency skeletons. In: Proc. VMLS, pp. 153–170 (2012) 41. Telea, A.: CUDASkel: real-time computation of exact Euclidean multiscale skeletons on CUDA (2019). webspace.science.uu.nl/∼telea001/Shapes/CUDASkel 42. Telea, A., Wijk, van, J.: An augmented fast marching method for computing skeletons and centerlines. In: Eurographics, pp. 251–259 (2002) 43. Toderici, G., et al.: Variable rate image compression with recurrent neural networks. In: 4th ICLR (2016) 44. Wallace, G.K.: The JPEG still picture compression standard. IEEE TCE 38(1), xviii-xxxiv (1992) 45. Wang, H., Schuster, G.M., Katsaggelos, A.K., Pappas, T.N.: An efficient rate-distortion optimal shape coding approach utilizing a skeleton-based decomposition. IEEE TIP 12(10), 1181–1193 (2003) 46. Wang, J.: 3S-DMD supplementary material (2021). https://github.com/WangJieying/3SDMD-resources 47. Wang, J., Kosinka, J., Telea, A.: Spline-based dense medial descriptors for lossy image compression. J. Imag. 7(8), 153 (2021) 48. Wang, J., Kosinka, J., Telea, A.: Spline-based medial axis transform representation of binary images. Comput. Graph. 98, 165–176 (2021) 49. Wang, J., de Melo Joao, L., Falc˜ao, A., Kosinka, J., Telea, A.: Focus-and-context skeletonbased image simplification using saliency maps. In: Proceedings of the VISAPP, pp. 45–55. SciTePress (2021) 50. Wang, J., Terpstra, M., Kosinka, J., Telea, A.: Quantitative evaluation of dense skeletons for image compression. Information 11(5), 274 (2020) 51. Wang, W., Lai, Q., Fu, H., Shen, J., Ling, H.: Salient object detection in the deep learning era: An in-depth survey. IEEE TPAMI PP (2021) 52. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from error visibility to structural similarity. IEEE TIP 13, 600–612 (2004) 53. Wang, Z., Bovik, A.: Mean squared error: Love it or leave it? a new look at signal fidelity measures. IEEE Signal Proc. Mag. 26, 98–117 (2009) 54. Wang, Z., Simoncelli, E., Bovik, A.: Multiscale structural similarity for image quality assessment. In: ACSSC, pp. 1398–1402 (2003) 55. Yushkevich, P., Thomas Fletcher, P., Joshi, S., Thall, A., Pizer, S.M.: Continuous medial representations for geometric object modeling in 2D and 3D. Image Vision Comput. 21(1), 17–27 (2003) 56. Zhang, J., et al.: Hypergraph optimization for salient region detection based on foreground and background queries. IEEE Access 6, 26729–267241 (2018) 57. Zhu, Y., Sun, F., Choi, Y.K., J¨uttler, B., Wang, W.: Computing a compact spline representation of the medial axis transform of a 2D shape. Graphical Models 76(5), 252–262 (2014) 58. Zwan, M.V.D., Meiburg, Y., Telea, A.: A dense medial descriptor for image analysis. In: Proceedings of the VISAPP, pp. 285–293 (2013) 59. Z¨und, F., Pritch, Y., Sorkine-Hornung, A., Mangold, S., Gross, T.: Content-aware compression using saliency-driven image retargeting. In: IEEE ICIP, pp. 1845–1849 (2013)  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder for Multi-modal Neuroimaging Analysis Refka Hanachi1(B) , Akrem Sellami2 , and Imed Riadh Farah1,3  
   
  3  
   
  1 RIADI Laboratory, ENSI, University of Manouba, Manouba 2010, Tunisia [email protected]  , [email protected]  2 CRIStAL Laboratory, University of Lille, 59655 Villeneuve-d’Ascq, France [email protected]  ITI Department, IMT Atlantique, 655 Avenue du Technopˆole, 29280 Plouzan´e, France  
   
  Abstract. The obsession with how the brain and behavior are related is a challenge for cognitive neuroscience research, for which functional magnetic resonance imaging (fMRI) has significantly improved our understanding of brain functions and dysfunctions. In this paper, we propose a novel multi-modal spatial cerebral graph based on an attention mechanism called MSCGATE that combines both fMRI modalities: task-, and rest-fMRI based on spatial and cerebral features to preserve the rich complex structure between brain voxels. Moreover, it attempts to project the structural-functional brain connections into a new multi-modal latent representation space, which will subsequently be inputted to our trace regression predictive model to output each subject’s behavioral score. Experiments on the InterTVA dataset reveal that our proposed approach outperforms other graph representation learning-based models, in terms of effectiveness and performance. Keywords: Spatial-cerebral features · Graph deep representation learning · Multi-modal MRI · Regression  
   
  1 Introduction In cognitive neuroscience, analyzing and characterizing the complex human brain activity has gained considerable interest over the past few decades, whether that it is driven experimentally, or clinically, and whose main debate consists on how to effectively make such a relationship between a brain characteristic and a behavioral score, reflecting its performance for instance, in an intelligence task or assessing the magnitude of the disease. This leads them to subsequently be able to interpret the brain regions responsible for these cognitive functions. Hence, the advent of powerful new brain imaging has provided valuable information and has brought us insights into the dynamic neural features. Magnetic Resonance Imaging (MRI) has recently proved to be useful and tends to be the most suitable method, providing, in particular, a true structural and functional c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 303–327, 2023. https://doi.org/10.1007/978-3-031-25477-2_14  
   
  304  
   
  R. Hanachi et al.  
   
  mapping of the brain, such that both its anatomy (structural MRI (sMRI)) and its functions (functional MRI (fMRI)) can be observed. More crucially, fMRI has been used to visualize brain activity by detecting changes in cerebral hemodynamics. Moreover, it has expanded our understanding of healthy humans’ brain organization by mapping the brain activity induced by a set of tasks that the participant performs in a controlled manner in the scanner (task-fMRI) as well as the mapping of functional connectivity from fMRI recordings of resting participants (rest-fMRI). Therefore, examining each voxel’s time series and detecting whether the oxygen change in blood flow known as BloodOxygen-Level Dependent (BOLD) signal, is altered when responding to a given stimulus is the general aim of the fMRI method to infer the human brain’s neuronal activity. In this context, several studies have been conducted for which standard approaches used by neuroscientists [1, 2], are focused on the univariate correlational analysis between a selected cerebral characteristic or such a single MRI modality and a behavioral score to determine individual differences. However, this method is very limiting in terms of future observations, since the study of each voxel is carried out in isolation. Therefore, no single modality has yet become the preferred alternative to address all brain function neuroscience issues. In fact, collecting multi-modal brain MRI from the same subject can effectively capitalize on the intensity of each imaging modality, and provide a comprehensive perspective into the brain [3, 4], for which fMRI has enabled a wide-ranging analysis in numerous application areas including reading exception words [5], face selectivity [6], and substantial clinical initiative to classify individual subjects either as patients or as controls [7]. Nevertheless, the study of multi-modal brain images is highly complex, involving the use of advanced experimental methods from raw data acquisition to image processing and statistical analysis in order to get the finished output presented typically as statistical maps showing which regions of the brain have reacted to any specific cognitive functions. The General Linear Model (GLM), for instance, is the most commonly used method in fMRI data analysis as part of the Statistical Parametric Mapping (SPM) analysis pipeline. In addition, as a single acquisition of fMRI results in a huge amount of noisy data measured in voxels per session, numerous computational method challenges are required. To approach this high dimensionality problem, dedicated methods to increase comprehensibility and improve the model’s performance were emphasized including three key groups: Feature Selection (FS) by disposing of uninformative and irrelevant features, Feature Extraction (FE) to create new features based on the existing ones, and Representation Learning (RL) by learning a function for a better data representation. Various methods like Principal Component Analysis (PCA), Independant Component Analysis (ICA) were applied in the literature, operating on regular data in a grid-sampled structure. However, by considering the brain spatial information between voxels (sMRI), one delicate issue links the nature of these data accentuating the intricate graph structure for which several challenges have been raised in handling the irregular data, and extending deep learning approaches to the graph domain. These considerations were addressed in our previous work [8], where we proposed a multi-modal graph deep learning method that fuses both fMRI modalities based on Brain Adjacency Graph (BAG) generated from sMRI to quantify each subject’s performance in voice recognition and identification.  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  305  
   
  In the present work, an in-depth study of related multi-view representation learning models based on regular data has been carried out. Furthemore, the method was enhanced by developing one extra cerebral graph. In fact, in addition to retaining the spatial information between the brain voxels, we developed a spatial-cerebral graph that aims to handle both fMRI cerebral features estimated at each vertex of the brain and its adjacent neighbors. Accordingly, a new multi-modal graph autoencoder network structure is proposed. Three main contributions are defined: 1. An adaptive dimensionality reduction of two fMRI modalities by using both the 3D mesh spatial information and the cerebral brain features to preserve the related information using a Graph Autoencoder (GAE) model which allows to project each feature vector of each modality, associated with each vertex of the spatial-cerebral graph in a new representation space. 2. A fusion of the complementarity resulting from two previous feature vectors. It will thus be performed by the multi-modal GAE model receiving the two graph input pairs of two reduced feature vectors using different fusion operators in the fusion layer. 3. Once we get fused the compressed representation Z of both fMRI modalities, a predictive trace regression model to output the scalar score of each subject is designed. Besides that, experiments were also improved in which unimodal and multi-modal comparative studies between the different representation learning methods, including PCA, ICA, and AE were conducted. The rest of this paper is organized as follows. Section 2 describes related multimodal graph deep learning models. Section 3 reveals in detail our proposed method based on the multi-modal spatial-cerebral graph autoencoder and the predictive trace regression model. In Sect. 4, our experimental results over the InterTVA dataset are presented. Finally, in Sect. 5, a conclusion is provided.  
   
  2 Existing Work In this section, we briefly review some of the numerous models dedicated to studying multi-modal representation learning relying both on regular and irregular data, and explore their mathematical concepts. 2.1 Feature Extraction (FE) Experimentally determined, fMRI is often arranged in 3-D, and each its application results in a huge amount of data measured in voxels (3-D pixels). Each of which contains one value that represents the mean signal (BOLD) computed at a given location. Therefore, learning this series of brain data poses various challenges to accurate analysis and interpretation in order to achieve the best results, one of which is the extremely high dimensional features. In this regard, numerous studies rely on FE methods that refer to the mapping process of an n-dimensional point into a lower p-dimensional space to better fit a predictive model and improve learning performance. Typically, it  
   
  306  
   
  R. Hanachi et al.  
   
  seeks to extract relevant features denoted by Z ∈ Rn×p from a high dimensional MRI data X ∈ Rn×D , where n is the number of pixels, D is the number of initial features, and p is the number of extracted features. Formally, the main goal of the FE is given as follows: Z = f (X) (1) where f is an extraction function that can be either linear or non-linear through PCA [9], locality-preserving projection (LPP) [10], Locally Linear Embedding (LLE) [11], Laplacian Feature Mapping (LFM) [12], etc. Years later, neural network research has had significant success in data representation, to which many state-of-the-art models have contributed by learning a function that simplifies the extraction of relevant features known as RL or Feature Learning. Hence we discern two major groups as shown in Fig. 1: one based on Euclidean data, where the datatypes are in a regularly structured 1–2 dimensional domain and the other based on non-Euclidean data with irregular structure, commonly called graphs. The group on the left which focuses on structured data requires three key analytical approaches: AutoEncoder (AE), Convolutional Neural Network (CNN), and Canonical Correlation Analysis (CCA). For the other group, various approaches have been proposed in the literature, which maps nodes into a latent representation space in which such p-dimensional space is considered to be sufficiently informative to preserve the original network structure. To do so, some of them use random walks [13–15] to directly obtain the embeddings for each node, while others are defined under the Graph Neural Network (GNN) model which addresses the network embedding problem based on adjacency matrix computation, through the Graph AE (GAE) model [16] as well as GraphSage [17] : a Convolutional Graph Neural Networks (ConvGNNs) spatial-based model.  
   
  Fig. 1. A taxonomy of representation learning based on Euclidean and non-Euclidean data.  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  307  
   
  2.2 Muti-view Representation Learning It is a critical research topic that combines the information acquired from particular unimodal data into a single compact representation. Several approaches have been used to justify the use of complementarity in existing data, highlighting significant dependencies that are difficult to track with a single modality. In this regard, CNN, CCA, and AE go for the most dedicated shared representation methods. Multi-view Representation Learning Based on CNN. CNN is a specialized Deep Neural Network (DNN) that has shown successful results for computer vision and image processing, which includes the feature extractor in the learning process to automatically learn high level features through backpropagation by using multiple learning blocks. Unlike unimodal CNN, multi-view CNN is dedicated to learning features over multiple modalities, allowing separate representation learning for each view and then mapping them into a shared space where mid-level fusion (features level) is involved (at the convolution layer) to learn multi-view correspondence feature maps [19]. Given two modalities a and b, each associated with a learned feature map X a and X b . Basically multi-view feature fusion at convolution layer includes Sum, Max and Concatenation operations, where – Sum Operation: computes the sum of the two feature maps X a + X b ,  – Max Operation: takes the maximum of the two feature map max X a , X b , – Concatenation Operation: that concatenates the two set of feature maps [X a , X b ]. Multi-view Representation Learning Based on CCA. CCA was proposed in 1936 and it is one of the most popular multi-view representation learning techniques of connecting two sets of features by finding a linear combination among these sets that are maximally correlated. Comparing to PCA, CCA describes the coordinate system that describes the maximum cross-covariance between two data sets. Formally explained, given a pair of observations, denoted from two points of view (x1 , y1 )..(xn , yn ), each view of which is associated with a matrix respectively X ∈ Rdx and Y ∈ Rdy , CCA aims to find direction vectors uj , wj , with j ∈ {1, ..., ..., k} that maximize the correlation between the projections uTj X and wjT Y in the projected space: (2) uj wj = argu,w maxcorr(uTj XwjT Y ) A few years later, the success of CCA and DNN in representation learning has motivated many researchers to develop a model that benefits from their complementarity and allows CCA to be expanded to take into account the non-linearity of data, for which, Deep Canonical Correlation Analysis (DCCA) [20] has gained more attention. It’s a non-linear extension of the linear method CCA which requires multiple views of data. Moreover, it consists of multiple stacked layers of two DNN f and g to compute representations and extract non linear features for each view, where the canonical correlation between the extracted features f (X) and g(Y ) is maximized: maxWf ,Wg ,U,V =  
   
  1 tr(U T f (X)g(Y )T V ) N  
   
  (3)  
   
  308  
   
  R. Hanachi et al.  
   
  s.t.  
   
  1 f (X)f (X)T + rx I)U = I, N 1 V T ( g(Y )g(Y )T + ry I)V = I, N uTi f (X)g(Y )T vj = 0, f or i = j, UT (  
   
  where U = [u1 , ... , uL ] and V = [v1 , ... , vL ] presents the CCA directions, f and g are non-linear transformations of the two DNNs and (rx , ry ) are regularization parameters for sample auto-covariance matrices. Multi-view Representation Learning Based on AE. It is an unsupervised neural network algorithm which learns latent representation through input reconstruction by reducing the reconstruction error. It consists of two networks : the first is called encoder: AEEnc(X) that automatically extracts useful features from an input X ∈ Rn×D and aims to map them into a latent space representation Z ∈ Rn×p . The second is called ˆ based on a decoder: AEDec(Z) which reconstructs and recovers the original data X learning function essentially used Mean Squared Error (MSE) : N 1  (Xi − Xˆi ) N i=1  
   
  (4)  
   
  ˆ are ideally identical (X ≈ X) ˆ and X > Z. Then, to learn features over where X and X multiple modalities, Split AE, and multi-modal Deep Autoencoder (MDAE) have been proposed [18]. Split AE attempts to extract the common representation from a single view which can be used to reconstruct all views where it is presumed that the reconstruction networks are distinct for each view. MDAE [21], unlike Split AE, it requires multiple modalities as inputs to find a shared representation of them via training a bimodal deep AE. It consists of two separate inputs and outputs, where each view is allocated separate hidden layers and then uses the concatenated final hidden layer of both views as input and maps them to a common embedding layer. The loss function on input pairs X, Y , is defined as follows : L(xi , yi ; θ) = LI (xi , yi ; θ) + LT (xi , yi ; θ)  
   
  (5)  
   
  where LI , LT are the generally perceived squared error loss caused by data reconstruction for the given inputs X and Y : LI (xi , yi ; θ) = xi − xi 22 LT (xi , yi ; θ) = yi − yi 22 Furthermore, and to sum up, while previous models are effective to capture hidden patterns of Euclidean data, there has been an increasing interest in learning about nonEuclidean data with undefined structures and unknown properties. As a result, they are unable to handle the complicated structure of graphs, posing various challenges in applying deep learning approaches to graph data. Hence, graph RL has attracted considerable research attention over the past few years.  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  309  
   
  2.3 Graph Representation Learning Two major groups can be defined as mentioned previously: Random walks, and GNN based approaches. We present a complete description of these methods in the following sections. Random Walk Based Approaches. The key idea behind these approaches is to optimize node embedding by quantifying similarity between nodes by their co-occurrence over the graph on short, random walks [22]. The three popular methods are: DeepWalk [13]: It is based on two major steps. The first addresses the neighborhood relations by randomly selecting the first node and traverses then the network to identify its related nodes. The second step uses a SkipGram algorithm [23] to update and learn node representations by optimizing node similarities that share the same information. Node2vec [15]: Representation Learning based on CNN an advanced version of DeepWalk, that considers two biased random walks p and q to identify the neighborhood of nodes. p controls the likelihood of immediately revisiting a node in the walk [15] and q controls the likelihood of exposed parts of the graph is not explored. Metapath2vec [14]: Representation Learning based on CNN it was proposed to handle the network’s heterogeneity by maximizing its probability. It uses a meta-path random walk that determines the node type order within which the random walker traverses the graph to ensure that the semantic relationships between nodes type are incorporated into SkipGram. GNN Based Approaches. Both surveys [16] and [24] define various models based on GNN such as ConvGNNs and GAE, etc. ConvGNNs: It was proposed to manage convolution operations on graph domains in generating a node v’s representation by aggregating neighbors’ features xu with its own features xv , where u ∈ N (v) [16]. It covers two main approaches: spectral-based in which, the convolution operation is defined over the entire graph, and spatial-based that defines convolution by taking each node into account, and aggregates neighborhood information. One of the most applied spatial-based approaches is namely, GraphSage (SAmple and aggreGatE) [17]. It first defines the set of the neighborhood for each node by fixing a parameter k ∈ {1, ..., K} that controls the neighborhood depth, then, it trains a set of aggregator functions to learn the node’s representation given its feature and local neighborhood: for each node, it generates a neighborhood representation with an aggregator function and concatenates it to the current node representation through which a fully connected layer is fed with a nonlinear activation function [17]. GAE: It encodes nodes/graphs into a latent vector space and reconstructs graph data from the encoded information [16]. Its architecture consists of two networks: an encoder enc() to extract a node’s feature information by using graph convolutional layers and a decoder dec() to reconstruct the graph adjacency matrix Aˆ while preserving the graph topological information [16] based on a learning function which computes the distance between a node’s inputs and its reconstructed inputs.  
   
  310  
   
  R. Hanachi et al.  
   
  3 Proposed Approach In this section, we present our proposed method including the multi-modal graph autoencoder based on attention mechanism (MSCGATE), and the predictive model. Figure 2 reports the general overview of the proposed methodology.  
   
  Fig. 2. General overview of the proposed multi-modal graph deep learning method.  
   
  Three key phases are defined: A) Multi-modal Brain Graphs Preprocessing: the goal is to use common pipelines to analyze both task-, and rest-fMRI fMRI modalities, that provide 3-D brain scans with 20–40 thousand voxels in a three-dimensional rectangular cuboid, whose dimensions are in the range of millimeters, each of which has an associated timeseries of as many time-points as volumes acquired per session. Each brain slice has been processed using SPM12 for slice-timing correction and motion’s correction of both data. Then, statistical analysis based on GLM has been performed on all voxels to identify activated brain areas as well as Region Of Interest (ROI) definitions. (a) task-fMRI preprocessing: the estimation of the parameters of the GLM model results in a set of features which consist of the pattern of β-values computed on each pixel per subject in a cognitive task, that allows then, constructing the activation matrix Xt ∈ Rn×Dt where n the number of pixels i ∈ {1, 2, ..., n} and D the initial dimension of features. (b) rest-fMRI preprocessing: it requires the participants to remain quiet while lying in the scanner to evaluate functional connectivity. It was performed using FreeSurfer to identify the set of pixels whose time series correlated with the time series of each ROIs in the regions of the Destrieux atlas, i.e. how much the pixel Xi relates the region ROIj . Therefore, we compute the average matrix of All ROIs, and the correlation between the pixel and the mean of each ROI using a correlation coefficient such as Pearson coefficient : P(Xi , ROIj ) =  
   
  cov(Xi , ROIj ) σXi σROIj  
   
  (6)  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  311  
   
  These correlations constitute then the feature vector Xr ∈ RDr >100f eatures estimated on each pixel per subject. Moreover, two cerebral graphs Gtask cerebral and Grest cerebral are defined from these two matrices, each of which takes into account the cerebral related task-, and rest fMRI features estimated at each vertex of the brain with a feature vector (Xti ∈ RDt and Xri ∈ RDr ) respectively, where Dt and Dr > 100 features. Similarly, for the sMRI modality, we extract the cortical surface by obtaining the 3-D mesh incorporating the voxels. From this 3-D mesh, we therefore create the two spatial graphs Gtask spatial and Grest spatial in which, each voxel specifies its adjacent neighbors. Furthermore, we obtain then two spatial-cerebral graphs Gt and Gr by combining the two previous graphs for each fMRI modality to better handle the related cerebral features and the spatial connectivity. B) Multi-modal SCG Feature Learning and Fusion: it consists of building an MSCGATE model, which takes as input two spatial-cerebral graphs, i.e., Gt and Gr . The goal is to learn locally a latent representation of the multi-modal information by considering the neighborhood information between voxels. C) Behavior Score Interpretation: it involves solving the regression problem, that is, predicting the behavioral score measuring each subject’s performance in a cognitive task using the latent representation Z with a trace regression model operating at the subject level. 3.1 Multi-modal Brain Graphs (BGs) Construction We now explore the Brain Graphs (BGs) construction from both the triangulated mesh 3-D and the activation, and correlation matrices as illustrated in Fig. 3. Brain Spatial Graph (BSG). sMRI was used to present the connections of each brain vertex from which we obtain a triangulated 3-D mesh representing the cortex surface denoted G(V, E) where V refers to the set of vertices {v1 , ..., vn }, E represents its connectivity with respect to the edges of the graph {e1 , ..., eE } (ei ∈ V × V). Motivated by the need for a structural representation of the basic topological information provided by G to traverse the triangulation, an efficient approach is to store the set of edges E in an adjacency matrix Aspatial ∈ Rn×n where n the number of pixels. Therefore, it is generated using the following formula :  1, if v ∈ ei (7) Aspatial (v, ei ) = 0, otherwise Additionally, Aspatial allows each connection between pixels to be projected in a 2-D structure and where each pixel specifies its five vertices from V for a current neighborhood size k = 1 and six vertices with the added self-loops. Therefore, increasing the neighborhood size can introduce unnecessary sharing of information between pixels. Moreover, getting too big k can also trigger all pixels to have the same representation.  
   
  312  
   
  R. Hanachi et al.  
   
  Fig. 3. Multi-modal Brain Graphs (BGs) construction.  
   
  More formally, Algorithm 1 provides a computational demonstration from the 3-D triangulated mesh to the resulted of two BSGs, i.e., Gtask spatial and Grest spatial . Input: 3-D mesh M , scalar k, activation matrix Xt , and correlation matrix Xr , /* Generate Aspatial from the mesh */ initialization; Aspatial = 0, k = 1 ; for i ← 1 to n do for j ← 1 to n do if Mi,j ∈ Nk (M ) are connected then Aspatial (i, j) ← 1; end end end /* Generate Gtask spatial , Grest spatial using Aspatial , Xt and Xr */ Gtask spatial ← (V, E,Xt ) ; Grest spatial ← (V, E,Xr ) Algorithm 1. BSG construction Algorithm. Brain Cerebral Graph (BCG). The aim here is to integrate the pixels’ neighboring cerebral features. Therefore, we construct from the two activation, and correlation matrices, two graphs Gtask cerebral = (V, E,Wtask ) and Grest cerebral = (V, E,Wrest ),  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  313  
   
  respectively. V is a set of vertices, E are the edges of the corresponding graph, Wtask cerebral , and Wrest cerebral are the weighted matrices. Moreover, the weight wi,j for the edge ei,j is computed as follows : Wcerebral (ei ) =  
   
  1 λ + dist(vi , vj )  
   
  (8)  
   
  2  
   
  where λ is a heat kernel, dist(vi , vj ) = vi − vj 2 is the euclidean distance, vi and vj are the activation, or/and the correlation coordinates with respect to the corresponding matrix. In order to take the nearest neighbors to vi , we controlled the neighborhood size k based on a well defined threshold s. Then, the adjacency matrix Acerebral , i.e., Atask cerebral , and Arest cerebral is obtained:  1, if wi,j < s Acerebral (v, ei ) = (9) 0, otherwise Brain Spatial-Cerebral Graph (BSCG). To reveal the intrinsic relations between voxels of the brain, we mix here the two constructed graphs, i.e., the spatial and cerebral graphs into a fused graph brain for each fMRI modality to be inputted then into the designed GAE for training. Therefore, we obtain Gt = (V, E,Wt ), and Gr = (V, E,Wr ). The weighted matrix W of each corresponding graph: Wt , and Wr is therefore defined as follows: (10) W(ei ) = Wcerebral (ei ) + (1 − )Wspatial (ei ) where  defines a compromise between spatial and cerebral fMRI features of each voxel at each graph location. Moreover, the adjacency matrix A, i.e., At , or/and Ar is generated :  1, if W(ei ) > 0 A(v, ei ) = (11) 0, otherwise 3.2 Multi-modal Graph Auto-Encoder Based on the Attention Mechanism (MSCGATE) Once the BSCG is constructed, our aim is to learn locally a multi-modal latent representation from both modalities task-, and rest-fMRI by aggregating for each voxel the information of its neighborhood. Therefore, we firstly, present the SCGATE feature extractor model. Then, we describe the multi-modal SCGATE (MSCGATE) with different fusion layers. Feature Learning with SCGATE Model. The main analysis keys, in our approach, target the noisy nature and vast amount of multi-modal imaging data with regard to the number of features per voxel from the pre-processed brain scans in each subject which greatly surpassed the number of training samples. While, it is now crucial to admit only the relevant features contributing to a better data interpretability, hence we choose to  
   
  314  
   
  R. Hanachi et al.  
   
  design a graph representation learning network based on AE and the attention mechanism as a feature extractor method simply called SCGATE. The key reason behind it, is the projection of a graph into a latent representation space based on encoding-decoding networks in which such low-dimensional space is considered to be sufficiently informative to preserve the original graph structure. Its main architecture includes two networks: Graph Encoder GEnc() and Graph Decoder GDec() for each modality. Graph Encoder Network GEnc() : It seeks to build new latent representations of vertices by taking the graph structure into account through stacked layers. Each single graph encoder layer attempts to aggregate the information from the neighboring vertices of a target vertex yielding a richer node representations according to their relevance. To allocate learnable weights to the aggregation, an attention mechanism is implemented. The weights can therefore be directly expressed by attention coefficients between nodes and provide interpretability. Formally, a single graph layer of GEnc() based on the attention mechanism can be defined as follows  (l) (l−1) hli = σ(( αij W (l) hj )) (12) j∈Ni  
   
  where hli is the new representation of vertex i in the l − th layer. Ni is the set of vertex i’s neighbors. αij is the aggregation weight, which measures how important vertex j to vertex i, and σ denotes the activation function. In our case, we use the attention mechanism in order to compute aggregation weight, i.e., to measure the relevance between vertices and their neighbors. Formally, it can be expressed as: αij =   
   
  exp(σ(aT [W hi ||W hj ])) exp(σ(aT [W hi ||W hk ]))  
   
  (13)  
   
  k∈N  
   
  where a denotes the weigh vector of the mechanism attention, and || is the concatenation operation. Graph Decoder Network GDec() : It allows to reconstruct and recover the input data, ˆ = GDec (X, (A)). Each graph decoder layer seeks to reconstruct the node represenX tations by considering the representations of their neighbors according to their importance and relevance, which allows capturing the hidden representation of vertices containing the rich features. As GEnc() , the GDec() specifies the same number of layers in which each graph decoder layer seeks to reverse the process of its corresponding graph encoder layer. Formally, a single graph layer of GDec() based on the attention mechanism can be defined as follows:  (l) ˆ l = σ(( ˆ (l) h(l−1) )) αij W (14) h i j j∈Ni  
   
  Loss function L: Node features and graph structure are included in the graph-structured data for which both should be encoded by high-quality node representations. Hence,  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  315  
   
  we use the MSE loss function to minimize the reconstruction error of node features as follows: N  ||Xi − Xˆi ||2 (15) L= i=1  
   
  MSCGATE with Fusion Layer. In order to learn a better representation from multiple input modalities, i.e., both fMRI modalities based on BSCG, an MSCGATE is designed which shares with the SCGATE model, the two networks: GEnc() and GDec() per modality, i.e., SCGAT Et and SCGAT Er . In fact, they take as inputs their correspond BSCG, i.e., Gt = (Xt , At ), and Gr = (Xr , At ) respectively. The two SCGATEs: SCGAT Et and SCGAT Er transform the multi-modal inputs into a basically lower-dimensional representation (every cortical locations in both fMRI modalities) Xt ∈ Rn×Dt and Xr ∈ Rn×Dr and project them into a latent space representation Zt ∈ Rn×pt and Zr ∈ Rn×pr . Moreover, both latent representations Zt and Zr will be fused in order to find a common shared space. In this context, various types of fusion operations can be used to get compressed latent representations of both input modalities. These operations include max-pooling() which takes the maximum of Zt and Zr , mean-pooling() which is the average between Zt and Zr , concat() which concatenates Zt and Zr , and inner-product() that is a generalization of the dot product operation between samples in both Zt and Zr . The MSCGATE seeks then to reconstruct each ˆ t = M SCGAT EDec (Z), modality using the common latent representation Z, i.e., X ˆ and Xr = M SCGAT EDec (Z). Figure 4 reports the main architecture of the proposed MSCGATE.  
   
  Fig. 4. MSCGATE architecture that learns a better representation from the fused multiple input views Z.  
   
  The MSCGATE is trained using MSE loss function which is defined as follows, taking into account for example, the concat operator to get fused both latent represen-  
   
  316  
   
  R. Hanachi et al.  
   
  tations : L(Xt , Xr ; θ) = Lt (Xt , Xr ; θ) + Lr (Xt , Xr ; θ)  
   
  (16)  
   
  where Lt (Xt , Xr ; θ) = Lr (Xt , Xr ; θ) =  
   
  1 N 1 N  
   
  ˆt 2 M SCGAT EDect (concat(M SCGAT EEnct (Xt ), M SCGAT EEncr (Xr ))) − X 2  
   
  M SCGAT EDecr (concat(M SCGAT EEnct (Xt ), M SCGAT EEncr (Xr ))) − Xˆr   
   
  More formally, Algorithm 2 summarizes the entire generation of multi-modal representation Z ∈ Rn×p from the combination of both fMRI modalities based on BSCG. Input: Pair of Gt (At , Xt ), Gr (Ar , Xr ) where Xt ∈ Rn×Dt , Xr ∈ Rn×Dr , At ∈ Rn×n , and Ar ∈ Rn×n . for enc ← 2 to 100 do /* Learn both latent representations from (At , Xt ) and (Ar , Xr ) Zt = M SCGAT EEnct (At ,Xt ) ; Zr = M SCGAT EEncr (Ar ,Xr ) ; /* Fusion of Zt and Zr Z ∈ Rn×p = concat(Zt , Zr ); /* From Z, reconstruct the original input pairs (Atˆ, Xt ) = M SCGAT EDect (Z); (Arˆ, Xr ) = M SCGAT EDecr (Z); end Algorithm 2. MSCGATE pipeline.  
   
  3.3  
   
  */ */ */  
   
  Trace Regression Predictive Model  
   
  After applying the MSCGATE model upon both fMRI modalities based on BSCG, the objective now is to predict the behavioral score of each subject reflecting its performance in a cognitive task (in a voice recognition task) using the fused latent representation Z ∈ Rn×p . This prediction is carried out to solve the regression problem basically performed with the well-known linear regression model to predict a scalar response from a vector-valued input, which can be defined as follows : yi = β T Zi + i ,  
   
  i = 1, ..., N  
   
  (17)  
   
  where y is the predicted variable, β refers to the regression coefficients, Z is the independent variable, is a vector of values i that add noise to the linear y −Z relation, and β T Zi is the inner product between Zi and β. Although this approach was a reasonable compromise when predicting a scalar behavioral score from a vector-valued fMRI data input, it is nevertheless necessary, in our case, to learn a model capable of handling the explanatory variables of the matrix provided by the fused latent representation Z for which the trace regression model has gained rising interest. It is a generalization of the  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  317  
   
  linear regression model that operates on matrix-valued input and attempts to project it into real-valued outputs [25], defined as follows [8] y = tr(βˆT Z) +  
   
  (18)  
   
  where tr(.) is the trace and βˆ is the matrix of regression coefficients. Numerous studies [26, 27] opted for the regularized least squares to determine an estimation of βˆ as follows :   n  T 2 ˆ (yi − tr(β zi )) + λ(β) (19) β = argminβ i=1  
   
  ˆ In our where λ(β) is a matrix regularizer to explore the low-rank structure of β. case, by considering the 3-D mesh, we use a manifold regularization based on Graph Laplacian G. To empower the nodes with the same importance as their neighbors, we use two regularization terms where the first is defined based on the Laplace matrix L of G (20) λ1 (β) = ηtr(β T Lβ) where L=D−W  
   
  (21)  
   
  D is a diagonal matrix of node degrees, D = diag(d1 , ..., dn ), W is the weighted adjacency matrix of G defined as W = (wij )i,j=1,...,n with wij = wji ≥ 0, where wij = 0 refers that the vertices vi and vj are disconnected. The second lies on the group-sparsity regularization strategy which takes the form  λ2 (β) = α βj 2 (22) j  
   
  Hence, the predictive model is carried out to solve the trace regression problem with the two previous regularization terms  λ(β) = ηtr(β T Lβ)/2 + α βj 2 (23) j  
   
  4 Experimental Results This section discusses the experimental protocol of our proposed method to illustrate its efficiency in the clinical initiative for predicting individual differences in new subjects. It first presents the applied InterTVA dataset to address then the relative results of our predictive model providing both quantitative and qualitative evaluation. 4.1 InterTVA Data Description Our experiments were conducted on the InterTVA dataset1 , which aims at studying the inter-individual differences using multi-modal MRI data on 40 healthy subjects. 1  
   
  https://openneuro.org/datasets/ds001771.  
   
  318  
   
  R. Hanachi et al.  
   
  We used an event-related voice localizer in which participants were invited to close their eyes while passively listening to 72 vocal sounds and 72 non-vocal sounds of a fixed duration of 500 ms, with inter-stimulus intervals in the range of 4–5 s. For the rest-fMRI, subjects were asked to rest quit while lying in the scanner for a duration of 12mn. Moreover, anatomical scans (3D T1 images) were acquired for each subject. The main pipeline for analysis of both fMRI modalities (task- and rest-fMRI) includes slicetiming correction and motion’s correction using SPM12 (www.fil.ion.ucl.ac.uk/spm). Then, statistical analysis based on GLM has been performed on all voxels. For taskfMRI, the estimation of the parameters of the GLM model results in a set of features that consist of the pattern of β-values induced by hearing each of 144 sounds. This allows therefore, constructing the feature vector Xt ∈ RDt where Dt = 144. RestfMRI was performed using FreeSurfer to identify the set of voxels whose time series correlated with the time series of each ROIs. These correlations constitute therefore, the feature vector Xr ∈ RDr where Dr = 150. These two features are then used as inputs of our graph representation learning model trained with 36 × 20484 samples. Moreover, four functional runs were performed, each including 36 trials (12 words × 3 speakers) in which, subjects had to be familiar with three speakers by initially listening to their stories while memorizing the association between the voice and the speaker’s name printed on the screen (Anne, Betty and Chloe). The aim here, is not to pay attention to the content of the stories, but rather to try to memorize the voice of the speaker and the connection with his name by pressing one of the three keys on the keyboard built for the experiment to recognize the correct speaker. At the end, a behavioral score is measured which shows the average of the Percentage of correct responses (PC) answered during the 4 runs, in which a total of 144 identification trials were presented (36 trials × 4 runs): m P Ci (24) P C = i=1 m P C1 + ... + P Cm (25) = m where m is the number of runs. 4.2  
   
  Parameters Tuning  
   
  Both unimodal SCGATE and multi-modal SCGATE were implemented using the Keras framework and learned over 500 epochs with a batch size of 300 training samples. After several tests, we choose the Adam optimizer with a learning rate is equal to 10− 5. Moreover, each model was built using three hidden layers for each fMRI modality: [Dt , 130, 110, enc, 110, 130, Dt ] for task-fMRI and [Dr , 130, 110, enc, 110, 130, Dr ] for rest-fMRI in which ten dimensions of the latent representation enc have been developed from 2 to 100 features. In addition, we opted for (relu, linear) as an activation functions for the hidden layers and the output layer respectively. Furthermore, we split the whole samples into training and testing sets over 10-fold-cross-validation where each fold consists of 36 subjects for training (36 × 20484 samples), and 4 subjects for testing (4 × 20484 samples).  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  319  
   
  4.3 Performance Evaluation Metrics In order to evaluate our trace regression model, three performance metrics were computed, i.e., Mean Absolute Error (MAE), MSE, and R-squared score, i.e., R2 (coefficient of determination): – MAE seeks to measure the average magnitude of the errors in a series of predictions, without considering their direction. It is defined as follows: M AE =  
   
  N 1  |yi − yˆi | N i=1  
   
  (26)  
   
  – MSE basically measures the average squared error of our predictions which takes the form: N 1  M SE = (yi − yˆi )2 (27) N i=1 – R2 which is the percent of variance explained by the model. It is always going to be between −∞ and 1. Usually, it shows how closely the model estimations match the true values. N (yi − yˆi )2 M SE(model) = 1 − i=1 (28) R2 = 1 − N M SE(baseline) ¯i )2 i=1 (yi − y where N is the number of subjects, y is the true values (score of behavior), yˆ is the predicted values, and y¯ is the mean of the true values. 4.4 Prediction Performance In this section, we provide both quantitative and qualitative evaluation of the proposed predictive model. The quantitative reports the experimental results using unimodal and multi-modal fMRI data performed both with different representation learning models as well as graph representation learning models while the qualitative evaluation discusses the visual interpretation of our predictive model. Quantitative Evaluation. As mentioned previously, the standard methods used by neuroscientists are based on single fMRI modality studied independently, i.e., task-, or rest-fMRI which results in a univariate correlation analysis. Hence, we address firstly the prediction performances performed with different representation learning models using both unimodal and multi-modal fMRI data and evaluate them according to the best average MSE obtained across 10-fold cross-validation. Therefore, the same procedure will be carried out using different graph representation learning models in which we emphasize the effectiveness and the added value of fusing activation- and connectivity based information.  
   
  320  
   
  R. Hanachi et al.  
   
  1. Unimodal Representation Learning: the noisy nature of brain MRI data poses critical challenges to accurate analysis, mitigate the high dimensionality problem of the raw data, and apply different reduction methods to enhance data interpretability. These approaches are typically based on traditional PCA, and ICA, whereas, it has been now a growing interest in applying advanced deep learning techniques such as AE to improve comprehensibility and computation costs. Therefore, we compared these three techniques to explain the usefulness of the AE model for better-compressed representation learning data. The AE was trained using different pairs of activation functions for the hidden layers and output layer: (linear, linear), (linear, sigmoid), (relu, linear) and (relu, sigmoid). Therefore, Fig. 5, and 6 report the average MSE across encoding dimensions learned on task- and rest-fMRI data respectively. Hence, we can deduce that with few dimensions, all AE models display compromising results for both data, in which the best MSE value learned on task-fMRI data is reached using AE(relu, linear) on 10 encoding dimensions equal to 0.08, while the best MSE value on the rest-fMRI data goes for the AE (linear, linear) across 20 encoding dimensions equal to 0.34.  
   
  Fig. 5. Average MSE using representation learning methods on task-fMRI.  
   
  Fig. 6. Average MSE using representation learning methods on rest-fMRI.  
   
  2. Multi-modal Representation Learning: to ensure completeness, we also compared the performances of the models obtained using multi-modal fMRI data with two different architectures: concatenated inputs and concatenated latent representation. Hence, we report in Fig. 7, the best average MSE versus encoding dimensions across 10-fold cross validation learned on fused fMRI data per model using concatenated inputs and in Table 1, the best average MSE and R2 using concatenated latent representation. Therefore, we can deduce that, for the first architecture, the best results are obtained using the MDAE(relu, linear) with an MSE value equal to 0.08 learned on 10 features that are hardly better than those obtained from the unimodal model using task-fMRI. Moreover,  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  321  
   
  all MDAE models exceeds the unimodal ones which really emphasizes the completeness of information from two modalities and that the predictive model provides better efficiency by multi-modal data. In addition, we can also interpret that, with our multi-modal architecture, the best performances applies to all MDAE models and, in particular, the MDAE(relu, linear) model is most suitable for compressing the data and learning them a better representation with an MSE value equal to 0.062 and R2 = 0.282. As a result, we may deduce that an efficient representation learning scheme for the interpretation of human behavior is a multi-modal autoencoder that operates a concatenation of the latent representation compared to performances obtained with the first architecture.  
   
  Fig. 7. Average MSE using representation learning methods on multi-modal fMRI data. Table 1. Best average MSE, and R2 (± standard deviation) using concatenated latent representation estimated on trace regression model based on PCA, ICA, and different MDAE models. Zt + Zr Model  
   
  MSE  
   
  R2  
   
  PCA  
   
  N/A  
   
  N/A  
   
  ICA  
   
  N/A  
   
  N/A  
   
  MDAE (linear, linear)  
   
  0.065 (± 0.0058) 0.273 (± 0.0033)  
   
  MDAE (linear, sigmoid) 0.079 (± 0.0124) 0.249 (± 0.0204) MDAE(relu, linear)  
   
  0.062 (± 0.0095) 0.282 (± 0.0044)  
   
  MDAE (relu, sigmoid)  
   
  0.073 (± 0.0086) 0.254 (± 0.0051)  
   
  3. Unimodal Graph Representation Learning: in this section, our model SCGATE is compared to various graph representation learning models including Node2vec, GraphSage, DeepWalk, Metapath2vec, and our previous approach GATE [8] based on the  
   
  322  
   
  R. Hanachi et al.  
   
  reconstruction error MSE which computes the prediction error between the true behavioral score and the estimated one. Therefore, Figs. 8, and 9 report the obtained results where the MSE value varies according to different encoding dimensions using task-, and rest-fMRI based on the constructed BSCG respectively. Thus, we can deduce that when the cerebral similarities are added to the process of GAE, our proposed SCGATE remains the appropriate one for learning representation from both fMRI data which was to be expected as it makes full use of both spatial connectivity, and cerebral features between voxels to learn new latent feature space. Moreover, the obtained MSE is equal to 0.052 learned on 10 dimensions for task-fMRI and 0.1 for encoding dimension = 30 for rest-fMRI compared to our previous approach with = 0.07, and 0.12 on 10 and 30 features respectively. Furthermore, we can observe that all of the models using task-fMRI perform slightly better than those using rest-fMRI because it fits the task completed with the behavioral GVMT test compared to rest-fMRI with less information about the total brain functional connectivity. Consequently, the Graphsage model appears to have an MSE value of 0.085 on 50 features better than others, i.e., Node2vec, Metapath2vec and Deepwalk with 0.1, 0.11, and 0.12 on 20, 50, and 60 features respectively. 4. Multi-modal Graph Representation Learning: to enrich our experiments and assess the effectiveness of our proposed MSCGATE, another architecture was introduced which is based on the concatenation of inputs (Xt , Xr ) to be compared then with the performances obtained in the case of mid-level fusion with different operators such as AVG(), Max(), Product() and Concat() using three evaluation metrics MSE, MAE and R2 . Hence, we report in Table 2 and 3 the best average values versus encoding dimension using concatenated inputs and concatenated latent representation for each method respectively. Compared to our previous work, it can be seen that for the first architecture, an important gain of performances is achieved with our proposed MSCGATE for  
   
  Fig. 8. Average MSE versus encoding dimension across 10-fold cross validation using task-fMRI.  
   
  Fig. 9. Average MSE versus encoding dimension across 10-fold cross validation using rest-fMRI.  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  323  
   
  which the MSE, MAE, and R2 values climb from 0.057 to 0.055, 0.057 to 0.052, and 0.281 to 0.288, respectively. Besides that, we obtain also, with our second architecture, significant performance improvements with our MSCGATE with all fusion operators, where in particular the inner-product operator reached the best MSE, MAE, and R2 values equal to 0.047, 0.045, and 0.297, respectively out of 20 features of which 10 were extracted from task,- and 10 of rest-fMRI. Here, we can justify the effectiveness of the complementarity of the information offered by the two modalities based on BSCG constructed compared to the unimodal modality (M SEZt +Zr < M SEtask−f M RI ) and (M SEXt +Xr < M SErest−f M RI ) and that the mid-level fusion is more appropriate for achieving our contribution objective in predicting the behavioral score. In addition, and compared to the performances obtained from multi-modal fMRI data using the MDAE model in Table 1, the added importance of the integration of spatial and cerebral informations in the analysis of human behavior has been clearly demonstrated. Table 2. Best average MSE, MAE, and R2 (± standard deviation) using concatenated inputs estimated on trace regression model based on Node2vec, Graphsage, Deepwalk, Metapath2vec, MGATE, and MSCGATE models. Xt + Xr MAE  
   
  R2  
   
  Model  
   
  MSE  
   
  Node2vec  
   
  0.114 (± 0.026) 0.112 (± 0.019) 0.120 (± 0.014)  
   
  Graphsage  
   
  0.099 (± 0.010) 0.097 (± 0.021) 0.141 (± 0.017)  
   
  Deepwalk  
   
  0.122 (± 0.013) 0.119 (± 0.010) 0.117 (± 0.025)  
   
  Metapath2vec 0.103 (± 0.012) 0.099 (± 0.020) 0.142 (± 0.016) MGATE  
   
  0.057 (± 0.009) 0.057 (± 0.010) 0.281 (± 0.010)  
   
  MSCGATE  
   
  0.055 (± 0.013) 0.052 (± 0.021) 0.288 (± 0.032)  
   
  ˆ on the Qualitative Evaluation. The aim here is to project estimated beta maps beta white cortical mesh in order to get a visual interpretation. Therefore, Fig. 10 reports the obtained average beta maps estimated using MSCGATE and trace regression model. Actually, in order to extract significant regions, we use statistical tests including t−test and p − value where t = 1.985 and p < 0.005. The goal is to assess the evidence presented by the data against a statistical hypothesis, presenting two complementary hypothesis called the null hypothesis which defines no significant region. It will be rejected if it appears to be incompatible with the sample data, and the alternative hypothesis denoted by H0 , and H1 respectively. The t − test is a set of parametric statistical tests where the calculated test statistic follows a Student t distribution under the null hypothesis. It is used to assume if the mean of a population varies from the hypothesized value. For instance, giving a sample data {Z1 , ..., Zn }, the hypothesis  
   
  324  
   
  R. Hanachi et al.  
   
  Table 3. Best average MSE, MAE, and R2 (± standard deviation) using concatenated latent representation estimated on trace regression model based on Node2vec, Graphsage, Deepwalk, Metapath2vec, MGATE (Avg()), MGATE (Max()), MGATE (Concat()), MGATE (Product()), MSCGATE (Avg()), MSCGATE (Max()), MSCGATE (Concat()), and MSCGATE (Product()) models. Zt + Zr R2  
   
  Model  
   
  MSE  
   
  MAE  
   
  Node2vec  
   
  0.103 (± 0.032) 0.09 (± 0.081)  
   
  Graphsage  
   
  0.098 (± 0.042) 0.091(± 0.073) 0.145 (± 0.009)  
   
  0.122 (± 0.014)  
   
  Deepwalk  
   
  0.119 (± 0.023) 0.104 (± 0.154) 0.102 (± 0.013)  
   
  Metapath2vec  
   
  0.098 (± 0.010) 0.092(± 0.123) 0.136 (± 0.016)  
   
  MGATE (Avg())  
   
  0.056 (± 0.015) 0.052 (± 0.093) 0.284 (± 0.019)  
   
  MGATE (Product())  
   
  0.051 (± 0.009) 0.049 (± 0.008) 0.296 (± 0.008)  
   
  MGATE (Concat())  
   
  0.054 (± 0.009) 0.052 (± 0.010) 0.289 (± 0.009)  
   
  MGATE (Max())  
   
  0.061 (± 0.025) 0.058 (± 0.012) 0.274 (± 0.019)  
   
  MSCGATE (Avg())  
   
  0.055 (± 0.043) 0.051 (± 0.021) 0.290 (± 0.067)  
   
  MSCGATE (Product()) 0.047 (± 0.029) 0.045 (± 0.009) 0.297 (± 0.039) MSCGATE (Concat())  
   
  0.053 (± 0.012) 0.052 (± 0.066) 0.286 (± 0.041)  
   
  MSCGATE (Max())  
   
  0.058 (± 0.064) 0.056 (± 0.114) 0.284 (± 0.079)  
   
  H0 : μ = μ0  
   
  compared  
   
  to  
   
  H1 : μ = μ0  
   
  (29)  
   
  can be computed using the test statistic t: t=  
   
  Z¯ − μ0 √ s n  
   
  (30)  
   
  where Z¯ is the sample mean, n the sample size and s refers to the sample standard deviation. Once the t statistic is computed, a p−value can be determined from the values of a Student t distribution. It indicates the probability of obtaining a test statistic as extreme as the results actually reported if the null hypothesis is true, quantifying the power of the proof against it. Therefore, the statistical significance is deduced only when p − value is typically ≤ 0.05. By admitting this, we obtained then a satisfactory results, which confirm the performance of the proposed method. The maps in Fig. 10 describe brain activation through voxels whose t-values surpass a certain statistical threshold for significance. The assumption that the experimental task stimulates these voxels. Moreover, we can see that the MSCGATE model can provide several significant regions responsible for a substantially higher response to vocal than non-vocal sounds, distributed in both the temporal and frontal lobes. This could be induced by improved robustness of the information present in the latent representation of the fused task- and rest-fMRI data.  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  325  
   
  Fig. 10. Average weight maps βˆ estimated using best MSCGATE model, thresholded after a test for statistical significance (t > 1.985, p < 0.005). Significant regions appear in yellow color. (A) left hemisphere mesh (B) right hemisphere mesh.  
   
  5 Conclusion This work proposes a new multi-modal graph deep learning approach that leads to a better interpretation of human behavior from the integration of both fMRI modalities using the BSCG built on both spatial (sMRI) and cerebral fMRI features. Three major phases were defined, including our proposed MSCGATE model, which seeks to learn a fused representation estimated at the cortical location level, which is then used as input to our trace regression predictive model to quantify each subject’s behavioral score in the voice recognition and identification task. Aside from being innovative, this method was able to manage the irregular structure provided by neuroimaging data while retaining both the cerebral features and the connectivity information. Our experimental findings demonstrate that our model outperforms previous state-of-the-art deep, and graph representation learning techniques in terms of effectiveness and performance operating on both regular and irregular data.  
   
  References 1. Demirci, O., Clark, V.P., Magnotta, V.A., et al.: A review of challenges in the use of fMRI for disease classification/characterization and a projection pursuit application from a multi-site fMRI schizophrenia study. Brain Imaging Behav. 2(3), 207–226 (2008) 2. Mihalik, A., Ferreira, F.S., Rosa, M.J., et al.: Brain-behaviour modes of covariation in healthy and clinically depressed young people. Sci. Rep. 9, 1–11 (2019)  
   
  326  
   
  R. Hanachi et al.  
   
  3. Sui, J., Adali, T., Yu, Q., et al.: A review of multivariate methods for multimodal fusion of brain imaging data. J. Neurosci. Methods 204(1), 68–81 (2012) 4. Sui, J., Pearlson, G.D., et al.: In search of multimodal neuroimaging biomarkers of cognitive deficits in schizophrenia. Biol. Psychiat. 78(11), 794–804 (2015) 5. Blackmon, K., Barr, W.B., Kuzniecky, R., et al.: Phonetically irregular word pronunciation and cortical thickness in the adult brain. Neuroimage 51(4), 1453–1458 (2010) 6. Saygin, Z.M., Osher, D.E., et al.: Anatomical connectivity patterns predict face selectivity in the fusiform gyrus. Nat. Neurosci. 15(2), 321–327 (2012) 7. Du, W., Calhoun, V.D., et al.: High classification accuracy for schizophrenia with rest and task FMRI data. Front. Hum. Neurosci. 6(145) (2012) 8. Hanachi, R., Sellami, A., Farah, I.: Interpretation of human behavior from multi-modal brain mri images based on graph deep neural networks and attention mechanism. In: Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications 4: VISAPP, pp. 56–66 (2021) 9. Jiang, J., Ma, J., Chen, C., et al.: SuperPCA: A superpixelwise pca approach for unsupervised feature extraction of hyperspectral imagery. IEEE Trans. Geosci. Remote Sens. 56(8), 4581– 4593 (2018) 10. He, X., Cai, D., Yan, S., Zhang, H.-J.: Neighborhood preserving embedding. In: Tenth IEEE International Conference on Computer Vision (ICCV 2005), pp. 1208–1213 (2005) 11. Ma, L., Crawford, M.M., Tian, J.: Anomaly detection for hyperspectral images based on robust locally linear embedding. J. Infrared Milli Terahz Waves 31, 753–762 (2010) 12. Belkin, M., Niyogi, P.: Laplacian eigenmaps for dimensionality reduction and data representation. Neural Comput. 15(6), 1373–1396 (2003) 13. Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk. In: Proceedings of the 20th ACM SIGKDD International Conference On Knowledge Discovery and Data Mining - KDD 2014 (2014) 14. Dong, Y., Chawla, N.V., Swami, A.: Metapath2vec: Scalable representation learning for heterogeneous networks. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2017), pp. 135–144. Association for Computing Machinery, New York (2017) 15. Grover, A., Leskovec, J.: Node2vec: scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp, 855–864. Association for Computing Machinery (2016) 16. Wu, Z., Pan, S., Chen, F., Long, G., et al.: A comprehensive survey on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst. 32(1), 1–21 (2020) 17. Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large graphs. In: Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017), pp. 1025–1035. Curran Associates Inc., Red Hook (2017) 18. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep learning. In: ICML, pp. 689–696 (2011) 19. Alam, M.T., Kumar, V., Kumar, A.: A Multi-view convolutional neural network approach for image data classification. In: 2021 International Conference on Communication information and Computing Technology (ICCICT), pp. 1–6 (2021) 20. Sun, C., Yuan, Y.-H., Li, Y., Qiang, J., Zhu, Y., Shen, X.: Multi-view fractional deep canonical correlation analysis for subspace clustering. In: Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds.) ICONIP 2021. LNCS, vol. 13109, pp. 206–215. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-92270-2 18 21. Bhatt, G., Jha, P., Raman, B.: Representation learning using step-based deep multi-modal autoencoders. Pattern Recogn. 95, 12–23 (2019) 22. Khosla, M., Setty, V., Anand, A.: A comparative study for unsupervised network representation learning. IEEE Trans. Knowl. Data Eng. (2020)  
   
  BS-GAENets: Brain-Spatial Feature Learning Via a Graph Deep Autoencoder  
   
  327  
   
  23. Mikolov, T., Chen, K.: Greg Corrado. Efficient Estimation of Word Representations in Vector Space, Jeffrey Dean (2013) 24. Zhang, Z., Cui, P., Zhu, W.: Deep learning on graphs: A survey. CoRR (2018) 25. Kadri, H., Ayache, S., Huusari, R., Rakotomamonjy, A., Ralaivola, L.: Partial trace regression and low-rank kraus decomposition. In: International Conference on Machine Learning (2020) 26. Koltchinskii, V., Lounici, K., Tsybakov, A.B.: Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. Ann. Stat. 39(5), 2302–2329 (2011) 27. Fan, J., Gong, W., Zhu, Z.: Generalized high dimensional trace regression via nuclear norm regularization. J. Econom. 212(1), 177–202 (2019)  
   
  Enhancing Backlight and Spotlight Images by the Retinex-Inspired Bilateral Filter SuPeR-B Michela Lecca(B) Fondazione Bruno Kessler, Digital Industry Center, 38123 Trento, Italy [email protected]  https://tev.fbk.eu/people/profile/lecca Abstract. Backlight and spotlight images are pictures where the light sources generate very bright and very dark regions. The enhancement of such images has been poorly investigated and is particularly hard because it has to brighten the dark regions without over-enhance the bright ones. The solutions proposed till now generally perform multiple enhancements or segment the input image in dark and bright regions and enhance these latter with different functions. In both the cases, results are merged in a new image, that often must be smoothed to remove artifacts along the edges. This work describes SuPeR-B, a novel Retinex inspired image enhancer improving the quality of backligt and spotlight images without needing for multi-scale analysis, segmentation and smoothing. According to Retinex theory, SuPeR-B re-works the image channels separately and rescales the intensity of each pixel by a weighted average of intensities sampled from regular sub-windows. Since the rescaling factor depends both on spatial and intensity features, SuPeR-B acts like a bilateral filter. The experiments, carried out on public challenging data, demonstrate that SuPeR-B effectively improves the quality of backlight and spotlight images and also outperforms other state-of-the-art algorithms. Keywords: Image enhancement · Retinex theory · Bilateral filter  
   
  1 Introduction Despite the modern cameras enable the acquisition of images of increasing quality, algorithms for image enhancement are still necessary to remove undesired effects mainly due to wrong camera settings and to illumination issues. In fact, low exposure times, low-light, back-light, flashes, colored lights, multiple lights may strongly alter the image quality, producing noise, color distortion, strong shadows and hampering the visibility and readability of the details and content of the observed scene. The many enhancers proposed in the literature rely on a set of assumptions about the illumination of the scene, the reflectance of the materials composing the scene and some spectral properties of the acquisition device [1, 17, 35]. For instance, some methods assume that the light is uniform or varies slightly across the scene, that the objects in the scene are matte and/or that the gamut of the camera is known. While these hypotheses lead to fair mathematical models and solutions of the enhancement problem, they also circumscribe the applicability of the enhancer to specific contexts. In general, image enhancement is still an open problem and in particular, the improvement of backlight and spotlight images has been scarcely investigated. Backlight images are pictures where the c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 328–347, 2023. https://doi.org/10.1007/978-3-031-25477-2_15  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  329  
   
  light source is located behind the objects to be acquired so that these latter appear very dark while the background is very brilliant. Spotlight images are pictures of dark environments with very brilliant but not diffuse light sources. In both these images, there are dark regions with illegible content. High dynamic range (HDR) cameras attempt to improve the visibility of the dark region content by acquiring multiple shots of the scene at different exposure times and merging them in an unique image by tone mapping. Nevertheless, this hardware solution suffers of two main problems: first, tone mapping often introduces color distortions and thus is a topic under research, and second, HDR imaging cannot solve the problem of improving the quality of an existing image captured with standard devices. Different software solutions have been proposed. Multiresolution Retinex algorithms attempt to simulate the HDR imaging mechanism by processing the input image at multiple scales and averaging together the results [9]. These algorithms effectively brighten the dark regions but have long execution times and often generate halos and artifacts. Multiple enhancement is implemented also in [33], where logarithmic stretching and gamma correction are applied to the image brightness to over-enhance dark regions while contrast is adjusted separately. This method avoids artifacts and is computational inexpensive, but experiments presented in [19] show some cases with an unsatisfactory enhancement of the dark regions. Other solutions are proposed in [2, 21–23, 25, 30, 31, 34]. All these works partition the input image in dark and bright regions and process them differently in order to enhance the first ones without over-enhance the second ones. Borders between dark and bright regions are reworked separately to avoid artifacts on the image edges. The use of different enhancing functions on dark and bright regions generally provide good results, but these latter strongly depend on the accuracy of the segmentation. The work in [27] processes separately edges and flat regions and merges together the enhancement results. It performs well on images with slight illumination changes, while performs poorly on images with nearblack portions. In general, smoothing operations are applied to remove noise and color distortions, as e.g. guided filters in [21] and Laplacian operators in [27]. This work describes SuPeR-B, a novel Retinex inspired bilateral filter which enhances color images without needing for segmentation and smoothing. SuPeR-B is a variant of the algorithm SuPeR [20], that enhances real-world color images based on a pixelwise local spatial processing of the channel intensities in accordance with some principles of the Retinex theory [11]. Both SuPeR and SuPeR-B partition the support of the input image by regular, non overlapping tiles, extract from each tile the maximum values of its red, green and blue components, and use them to rescale the red, green and blue intensities of each pixel. SuPeR and SuPeR-B differ to each other in the way the rescaling factor - which varies from pixel to pixel - is computed. While in SuPeR this factor depends on the spatial distance of the tiles from the pixel to be enhanced, in SuPeR-B it also depends on the difference between the intensities of the pixel and the intensities extracted from the tiles. Thanks to this modification, SuPeR-B can tune the contributions from the tiles accounting both for spatial and intensity features. In this sense, SuPeR-B works as a bilateral filter. In particular, given a pixel x in a dark region, SuPeR-B improves it by weighting more the contributions from tiles close to x both in terms of intensity and spatial distance. Moreover, if the contribution of the intensity differences is switched off, SuPeR-B implements SuPeR.  
   
  330  
   
  M. Lecca  
   
  SuPeR-B was firstly introduced in the conference paper [19], where a set of preliminary experiments show that it effectively enhances backlight and spotlight images also in comparison with other state-of-the-art methods. This paper extends [19] by providing a more detailed description of SuPeR-B and a more comprehensive analysis, including both an objective and a subjective assessment of a set of visual features of the enhanced pictures. In particular, tests and comparisons are here performed on the large dataset TM-DIED [32] and on another dataset collected by this author and made available for free along with the enhancement results [18].  
   
  2 Background As mentioned in Sect. 1, SuPeR-B is derived from the algorithm SuPeR [20]. This latter is a real-world color image enhancer implementing some principles of the Retinex theory, which was developed by Land and McCann to understand how humans see colors [11, 12]. Specifically, Retinex is grounded on some experiments which demonstrated that, when looking at a same point, humans and cameras may report different colors [14]. This is because humans process the color of any observed point p based not only on the photometric properties of that point but also on the local spatial distribution of the colors surrounding that point. In particular, colors closer to p influence more the human color sensation of p than colors located farther. Moreover, the Retinex experiments reported that humans acquire and re-work the red, green and blue components of the light independently and implement the so-called color constancy, which is the capability to smooth or even remove eventual chromatic dominants of the light sources illuminating the scene. The Retinex algorithm proposes a computational routine to simulate this mechanism and to infer from any input digital color image the corresponding human color sensation. When used on a digital color image without implementing color space transformations related to the human vision system, the Retinex algorithm works as an image enhancer, i.e. it increases the brightness, the contrast and the dynamic range of the input image while decreases possible color casts due to the light. Due to the importance of image enhancement in many computer vision fields, the local spatial color processing proposed by Retinex has been widely investigated and many variants have been developed, including also multi-resolution implementations that - as mentioned in Sect. 1 - were used for backlight image enhancement, [5, 7, 8, 10, 24, 26, 29, 36, 37]. SuPeR is one of these variants. It belongs to the Milano Retinex family [16, 29], that is a set of algorithms widely used as enhancers and grounded on the Retinex evidences described above. These algorithms propose different models for the local spatial color distribution, reaching different levels of enhancement. The interest for SuPeR is justified by the good results it achieves both in terms of image enhancement and execution time. SuPeR enhances any input color image J by four steps:  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  331  
   
  Fig. 1. Workflow of SuPeR.  
   
  1. Global Image Processing: The spatial support of J, i.e. the rectangle of pixels defining the image, is partitioned by N regular, rectangular, non overlapping tiles T1 , . . . , TN ; 2. Channelwise Pre-processing: For each color channel I of J, the values of I are rescaled over [0,1]. Moreover, the null values of I are set to a small value close to zero. This operation is needed to enable the computation of the intensity ratios described in the next. In this phase, SuPeR computes the set TI = {(bi , mi ) : i = 1, . . . , N }, where for each i, bi is the barycenter of Ti and mi is the maximum value of I over Ti ; 3. Pixelwise Processing: For each channel I and for each pixel x, SuPeR computes the subset TI (x) ⊆ TI defined as: TI (x) = {(b, m) ∈ TI : m ≥ I(x)},  
   
  (1)  
   
  i.e. TI (x) contains the pairs (b, m) such that m exceeds I(x). Then, SuPeR maps the intensity I(x) onto a new value LI (x) defined as: ⎧ I(x) ⎨  (b,m)∈TI (x) (1−d(x,b)) m if TI (x) = ∅ (1−d(x,b)) (b,m)∈TI (x) (2) LI (x) = ⎩1 otherwise where d is the squared Euclidean distance between x and b, normalized by the square of the length D of the diagonal of the image support so that d ranges over [0, 1]:  x − b 2 . (3) D2 4. Merging operation: For each channel I, the values of LI are rescaled over {0, . . . , 255} and the obtained images LI s (one for each channel) are packed together into a new color image L, which is the enhanced version of J. d(x, b) =  
   
  332  
   
  M. Lecca  
   
  Fig. 2. Examples of image enhancement by SuPeR.  
   
  Equation (2) implements a spatial color processing in line with the Retinex principles. In fact, Equation (2) is applied channel by channel, in agreement with the fact that human vision system processes the long, medium and short wavelengths separately. Moreover, the computation of intensity ratios allows to smooth possible chromatic dominant of the light, since for the most devices, the change of the RGB triplet of any pixel due to a light variation is well approximated by a linear diagonal transform of that triplet [6, 13]. Dividing I(x) by greater values and remapping the values of LI over {0, . . . , 255} enable to increase the brightness and the contrast of the image and to stretch the distribution of the channel intensity. Finally, the maximum intensities of the tiles involved in Equation (2) are weighted by a function of the spatial distance of the tile barycenters from x so that the contributions of farther tiles are less important than the contributions of closer tiles. In this way, as Retinex, SuPeR processes the image colors based on their spatial position and so implements a Retinex inspired local spatial analysis of the colors. The name SuPeR summarizes the main characteristics of this algorithms, i.e. the use of tiles as Super Pixels and the implementation of some principles of the Retinex theory. Fig. 1 schematizes the workflow of SuPeR, while Fig. 2 displays some examples of image enhancement by SuPeR. In general, SuPeR provides good performance on realworld low-light color images and some examples are shown in Fig. 2. Nevertheless, the performance of SuPeR decreases when the images are captured under extreme light conditions. An example is given in Fig. 3 and discussed in the next Section.  
   
  3 SuPeR-B Figure 3 shows an example of an image with backlight, where the foreground object, a statue located close to a tree, is displayed against a brilliant background, i.e. the blue sky, and appears very dark. The enhancement provided by SuPeR, showed in the middle of Fig. 3, is quite unsatisfactory: while the bluish color of the light is removed and the overall image brightness is higher, the statue is still dark and most of its details are not  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  333  
   
  Fig. 3. An example of enhancement of a backlight image by SuPeR and SuPeR-B.  
   
  visible. The reason of this result is that the intensities of the dark pixels of the statue are divided by the much greater intensities values sampled from the sky. Despite penalized by the spatial distance weights, the sky intensities influence heavily the values of L on the statue region. As already mentioned in Sect. 1, to solve this issue, multiscale Retinex approaches, like for instance the original Retinex algorithm [12] or its variants in [29] applied at multiple scales, tune the spatial locality of the color processing as follows. The enhancement of any pixel x is performed by accounting for the spatial color distribution over neighborhoods of x with decreasing size. For the smallest size, the spatial color distribution around x is computed on a set of pixels very close to x and thus generally having intensities similar to x. In this case, the intensities of x are rescaled by similar values sampled from the computed spatial color distribution. These intensities are thus mapped on values close to 1, so that the enhanced region becomes very bright. This operation improves the dark regions, but tends to over-enhance already bright images. On the contrary, the enhancement computed over large neighborhoods of x does not improve the visibility of the dark regions, as observed for Fig. 3, while preserves the appearance of the bright regions. The final enhanced image is obtained by averaging the different enhancement results and this avoids over- and under-enhancement. Nevertheless, multi-scale approaches often present two main issues: they generate noise and artifacts along the edges and have a long execution time. The algorithm SuPeR-B proposes an alternative solution exploiting Retinex principles. As the multi-resolution approaches described above, SuPeR-B observes that any pixel x in a dark region can be better improved by weighting more the contribution of pixels close to x and with similar intensity. Based on this fact, SuPeR-B inherits from SuPeR the general computational workflow: it implements the global and the channelwise processing and the merging operation of SuPeR, while modifies the pixel-wise processing by replacing the spatial weighting function in Eq. (2) with a new one, indicated by f and accounting both for spatial and intensity features. Equation (2) is thus substituted by the following one: ⎧ I(x) ⎨  (b,m)∈TI (x) f (δI(x,b),d(x,b)) m if TI (x) = ∅ f (δI(x,b),d(x,b)) (b,m)∈TI (x) (4) LB (x) = ⎩1 otherwise where δI(x, b) = I(b) − I(x) and f is designed to weight more the intensities of TI (x) that are closer to x both in terms of spatial and intensity distance. In this way, SuPeR-B  
   
  334  
   
  M. Lecca  
   
  acts like a bilateral filter, working both on the spatial and intensity domains. The letter ’B’ in the name SuPeR-B has three meanings: it indicates that SuPeR-B is a second version of SuPeR, that it has been developed to enhance backlight images and that it is a bilateral filter. There is not an unique expression for f . The current implementation of SuPeR-B models f by a Coon patch, which is a compact surface whose boundaries are described by four paths c0 , c1 , d0 , d1 : [0, 1] → R2 intersecting two by two at four corners, i.e. c0 (0) = d0 (0), c0 (1) = d1 (0), c1 (0) = d0 (1) and c1 (1) = d1 (1). The equation of the Coon patch C limited by c0 , c1 , d0 , d1 is: C(s, t) = S(s, t) + T (s, t) − U (s, t)  
   
  (5)  
   
  where S(s, t) = (1 − t)c0 (s) + tc1 (s) T (s, t) = (1 − s)d0 (t) + sd1 (t) U (s, t) = c0 (0)(1 − s)(1 − t) + c0 (1)s(1 − t) + c1 (0)(1 − s)t + c1 (1)st. Pictorially, the surface described by Eq. (5) can be imagined to be formed by moving the endpoints of c0 along the paths d0 and d1 modifying the shape of c0 accordingly until c0 reaches (and coincides with) c1 . The same surface can be obtained by moving the endpoints of d0 along the paths c0 and c1 modifying the shape of d0 until d0 reaches (and coincides with) d1 . Changing the position of the corners and/or the equations of the paths enable to model many different Coon patches. The Coon surface used in the current implementation of SuPeR-B has a parametric expression and is bounded by four lines defined as follows: c0 (s) = s(α − 1) + 1 c1 (s) = s(b − a) + a  
   
  (6) (7)  
   
  d0 (t) = t(a − 1) + 1 d1 (t) = t(b − α) + α  
   
  (8) (9)  
   
  where α, b, a are real-world user parameters and s, t ∈ [0, 1]. Function f is thus defined as: f (s, t) = max(C(s, t), ε),  
   
  (10)  
   
  where ε is a strictly positive, real number close to zero introduced to guarantee that f is positive for any choice of the paths and to prevent division by zero in Eq. (4). In Eq. (10), the parameters s and t represent respectively the variation of intensity δI and the value of d between two image pixels. Moreover, the variability range of the parameters α, a and b must be chosen so that f satisfies two important requirements, i.e. for any x 1. f guarantees the respect of the Retinex principle stating that the pixels of TI (x) whose barycenters are spatially closer to x influence more the enhancement than the other pixels of TI (x);  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  335  
   
  2. f guarantees that the the pixels of TI (x) with barycenters closer to x and with intensity closer to I(x) are weighted more than the other pixels of TI (x). These requirements are satisfied when α ≤ 1, a < 1, b ≤ min{a, α}.  
   
  (11)  
   
  In fact, if a ≥ 1, the intensities of the pixels in TI (x) far from x become more relevant than those close to x, violating the requirement 1. Similarly, if α > 1, the pixels in TI (x) with intensity values much greater than I(x) contribute more to the value LB (x), leading to unsatisfactory results, as shown by the case displayed in Fig. 3. Finally, the value of b must be smaller than α and b to avoid that f increases when (s, t) approaches to 1, violating in this way one or both the requirements 1 and 2. Some examples of function f are shown in Fig. 4. The parameter a controls the spatial locality of the image processing. Decreasing a makes the higher the contribution of the pixels of TI (x) close to x to LB (x). The parameter α controls the contribution of the intensity differences δI and the lower α, the higher is the weight given to the intensities of TI (x) close to I(x). In particuar, when α = 1 the intensity variations are weighted equally, regardless of their amount. The parameter b controls the value of f when d and δI grow up: the lower b, the less important the effects of high values of d and δI on LB (x) are. Finally, it is to note that for α = 1, a = 0, b = 0, SuPeR-B exactly implements SuPeR. Therefore, SuPeR-B can be also considered as a generalized version of SuPeR.  
   
  Fig. 4. Examples of function f for different values of the parameters α, a, b.  
   
  4 Evaluation Evaluating the performance of an image enhancer is in general a hard task and there is not an agreed measure for this [3]. Therefore, the evaluation proposed by this work considers a set of objective measures that capture different features of image quality, i.e. the  
   
  336  
   
  M. Lecca  
   
  brightness, the contrast, and the color distribution, that are usually improved by enhancing. In addition, a subjective evaluation is performed: a set of volunteers observed the images enhanced by SuPeR-B with different parameters and chose the image that they consider to have the best quality in terms of visibility of content and details. This analysis enables to establish when humans consider SuPeR-B outperforming SuPeR, i.e. when tuning the spatial and /or intensity features through the parameters of SuPeR-B provides better results than the spatial tuning of SuPeR. Objective and subjective evaluations are carried out on a dataset of 55 real-world images extracted from a personal collection of pictures of the author and for this reason called Personal-DB. The images depict both indoor and outdoor environments captured under natural or artificial illuminations, including backlight, spotlight, low-light and/or colored lights. Personal-DB and all the enhancement results obtained by SuPeR-B and discussed here are freely available [18]. In addition, the objective evaluation has been carried out on the larger, publicly available dataset TM-DIED (The Most Difficult Image Enhancement Dataset) [32], which contains 222 real-world pictures considered highly challenging for the enhancement task and including low-light and backlight images as well. On TM-DIED, the enhancement by SuPeR-B has been performed by using the parameter triplets (α, a, b) which have been mostly selected in the subjective analysis performed on Personal-DB. Some examples of images from Personal-DB and TM-DIED are given in Fig. 5. The results obtained on both the datasets have been finally compared with the enhancement achieved by four algorithms used to improve the quality of backlight images, i.e. [21, 24, 27, 33].  
   
  Fig. 5. Some examples of images from Personal-DB (first row) and TM-DIED (second row).  
   
  4.1  
   
  Objective Evaluation  
   
  Given a color image J, the objective evaluation is performed by considering three features that are usually modified by enhancement and by comparing them on input and  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  337  
   
  enhanced images. Precisely, these features are the brightness, the contrast, the color distribution of an image. These features are computed on the gray-level image BJ obtained from J by averaging pixel by pixels its color intensities, i.e. for any pixel x of J, 3  
   
  BJ(x) =  
   
  1 Ij (x) 3 j=1  
   
  (12)  
   
  where Ij is the jth color channel of J. According to this notation, the features used for the objective evaluation are: 1. Mean brightness (m0 ): This is the mean value of the intensity values of BJ. 2. Mean multi-resolution contrast (m1 ): This is a measure of the local intensity variations, computed at different scales and averaged to get a measure of the image contrast. Proposed in [28], m1 is computed by considering k half-scaled versions BJ1 , . . . BJk of BJ; for each j = 1, . . . , k, a pixel-wise and a global contrasts are computed. Specifically, the pixel-wise contrast is computed at the pixel x of BJj and is defined as the average of the absolute differences between BJj (x) and the intensities of its 8 neighboring pixels. The global contrast of BJj is obtained by averaging its pixel-wise contrasts. Finally, the value of m1 is the mean value of the global contrasts of the BJj s. 3. Deviance from color distribution flatness (m2 ): this is a measure of the entropy of the distribution of the values of BJ and is defined as the L1 distance between the probability density function of the values of BJ and the uniform probability density function. The objective evaluation is performed by comparing the features mi s before and after enhancing. Precisely, the brightness and the contrast are expected to be increased by enhancing, while the color distribution flatness is expected to decrease. It is to note that the exact amount of the values mi s varies from image to image. In particular, images with an already clear content, i.e. images that do not need for enhancement, are expected to report slight variations of the mi s. On the contrary, images with an unreadable content are expected to be heavily modified by the enhancer and thus the variations of their mi s are expected to be noticeable. For a fair assessment, the measures mi s must be evaluated together. In fact, for instance, a high value of m0 may correspond to a saturated image, where the details are completely lost; in this case, checking the values m1 and m2 can provide additional information about edges and saturated colors. For the backlight images, the assessment of m1 must be treated with particular attention, since brightening the dark regions may decrease the global contrast of the image, especially along the edges with the brighter regions. To provide a more accurate evaluation, the measures mi s have been computed on the whole image as well as separately on the dark and on the bright regions. To this purpose, the image to be evaluated is segmented in two regions PB and PD by a threshold procedure such that:  
   
  where τ=  
   
  PB = {x ∈ S : BJ(x) > τ } PD = {x ∈ S : BJ(x) ≤ τ }  
   
  (13) (14)  
   
  maxx∈S BJ(x) − minx∈S BJ(x) . 2  
   
  (15)  
   
  338  
   
  M. Lecca  
   
  PB and PD correspond respectively to bright and dark regions. The values of the meaD sures mi s computed on PB and PD are indicated respectively with mB i s and mi s. Some examples of such an image segmentation are shown in Fig. 6. On Personal-DB the evaluation of the mi s is performed on the original images as well as on the images enhanced by SuPeR-B for different values of the parameters α, a and b, listed in Table 1. As already observed, the triplet (α=1, a=0, b=0) corresponds to SuPeR. On the images of TM-DIED the objective evaluation has been performed by considering the parameters of SuPeR-B mostly selected in the subjective analysis described in the next. Table 1. Parameters of SuPeR-B used for the objective and subjective evaluations on PersonalDB. Each column reports a triplet of parameters. α −0.5 −0.5 −1 −1 0 a −0.5 0  
   
  −1 0  
   
  0 0.5 1  
   
  −1 0 0  
   
  b −0.5 −0.5 −1 −1 −1 0 0  
   
  1  
   
  1  
   
  −1 0  
   
  0  
   
  −1 −1 0  
   
  Fig. 6. Some examples of image segmentation in dark and bright regions from Personal-DB. See text for more explanation.  
   
  4.2  
   
  Subjective Evaluation  
   
  In the subjective evaluation, 55 panels, each displaying an image J from PersonalDB and 11 versions of J enhanced by SuPeR-B with the parameters (α, a,b) listed in Table 1, were shown to a set of 10 volunteers (see Fig. 7 for an example). Each volunteer was independently asked to indicate the image that she/he considered having the higher quality in terms of visibility of content and details. This analysis aims at measuring the performance of SuPeR-B in a qualitative way, also in comparison with SuPeR, as well as at detecting whose parameters of SuPeR-B provide the best performance from human perspective. These parameters have been then used to enhance the images of TM-DIED.  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  339  
   
  Fig. 7. On left, an example of panel used for the subjective evaluation. The first image on top is the input image with a strong backlight, the last image on bottom is obtained by SuPeR. On right, a graphical legend specifying the parameters used by SuPeR-B for each enhancement. For example, the image at the position (2, b) has been enhanced by setting (α, a, b) = (0, –1, –1).  
   
  4.3 Comparison The performance of SuPeR-B has been compared with those of SuPeR and of other three algorithms specifically designed to improve the quality of backlight images. Precisely, these algorithms are the multi-scale version of the Retinex algorithm [24] (here MSR for short), the content-aware dark image enhancer based on channel division [27] (here CHANNEL-DIVISION for short), the fusion-based method for single backlight image enhancement [33] (here FUSION for short) and the learning-based restoration method for backlit images [21] (here L-RESTORATION for short). As already mentioned in Sect. 1, MSR implements the Retinex algorithms at multiple resolutions, i.e. it processes the channel intensities of each pixel by considering neighborhoods with decreasing size and averages the results. At the smallest resolution, i.e. when the size of the pixel neighborhood is the 3×3 window centered at the pixel, the image enhanced by MSR looks like a color gradient, where only the strongest edges are highly visible and colors of flat regions tend to the white color so that any possible chromatic dominant of the light is removed. Enlarging the size of the neighborhood leads to images with different levels of removal of the light effects. In principle, averaging these results enables to brighten the dark regions without over-enhancing the bright ones, but in practice MSR often produces artifacts along the strongest image edges, while chromatic dominants of the light are still visible. It is also to note that the MSR method considered here is based on the work [10], which models the spatial locality of Retinex by considering Gaussian functions. Precisely, at each scale, MSR divides the channel intensity I(x) of each pixel x (or the V component of the image at x, depending on the implementation) by (I  G)(x) where G is a Gaussian function and  denotes the convolution. The support of G varies with the scale allowing to consider different neighborhoods of x.  
   
  340  
   
  M. Lecca  
   
  To avoid artifacts along edges, CHANNEL-DIVISION represents the input image in the HVS color space and partitions its V component (i.e. the image intensity) in edges and textured regions based on local contrast. CHANNEL-DIVISION enhances these latter separately with ad-hoc functions and merges together the results boosting the details in dark areas while preserving the smoothness of the flat portions of the image. The enhanced version of V is then packed with the original H and S channels in a new image, which is converted back to the RGB color space. Since CHANNEL-DIVISION acts on the intensity only, possible chromatic dominants of the light are not removed. Generally, this algorithm provides good results for a wide range of images, but it poorly performs in extreme conditions, as for example when the image contains near-zero areas where the signal is very noisy and the contrast computation is less accurate. FUSION also represents the input image in the HSV color space. It computes from the channel V three new images, where the first one is a logarithmic stretch of V, the second one is a gamma-corrected version of V, and the third one is obtained by an unsharp masking algorithm applied on V. These operations aim at improving the detail visibility in dark areas while avoiding the over-enhancement of the bright portions. The three images obtained from V are combined into a new image W, which is an enhanced version of V. In this fusion process, the contributions of the three images are weighted by Gaussian functions computed pixel by pixel and then smoothed to reduce halos. The components H, S and W are then mapped back to the RGB color space to yield the final enhancement of the input color image. As CHANNEL-DIVISION, FUSION does not remove possible color casts due to the illumination. L-RESTORATION segments the input image in bright and dark regions and enhances them differently. Segmentation is based on the analysis of the peaks in the illumination spatial distribution and of some statistic differences between bright and dark regions (i.e. luminance values, skewness of the luminance distribution, luminance saturation). L-RESTORATION re-works each segmented region by a tone mapping function which stretches the Weber contrast of the region while controls possible tone distortions. Boundaries between bright and dark regions are enhanced by a mixture of the tone mapping functions applied to the dark and bright regions. L-RESTORATION provides good results, but these latter heavily depend on the segmentation accuracy. Moreover, since L-RESTORATION mainly works on the luminance channel, it neither smooths nor removes possible color casts due to the illumination. All these methods have been compared against SuPeR-B on Personal-DB because the smaller number of images and their smaller size with respect to TM-DIED enable a simpler qualitative analysis of the results and guarantee faster execution times of the codes of MSR, CHANNEL-DIVISION, FUSION and L-RESTORATION.  
   
  5 Results The code of SuPeR-B used in these experiments has been derived from the repository of SuPeR available on GiThub1 by replacing the SuPeR spatial weights with the new function f . For all the datasets, the number of tiles have been set to 100. For FUSION the 1  
   
  SuPeR-B code: https://github.com/StefanoMesselodi?tab=repositories,.  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  341  
   
  code used here is a version provided by the authors some years ago at a link specified in [33], while for L-RESTORATION the code is available online2 For CHANNELDIVISION an implementation provided by the authors has been employed. For the contrast computation, the number of rescaled images varies from image to image, precisely each image is rescaled kth times, until the area of the kth images is smaller than 256 pixels.  
   
  Fig. 8. Comparison among SuPeR-B with different parameters on three images from TM-DIED. 2  
   
  L-RESTORATION code: https://github.com/7thChord/backlit.  
   
  342  
   
  M. Lecca  
   
  Table 2 reports the mean values of the objective measures mi s computed on the dataset Personal-DB. For any choice of the triplets (α, a, b) in Table 1, SuPeR-B improves the brightness, the contrast and the entropy of the color distribution on the D whole image. In particular, the comparison between the mB i s and the mi s reveals that D dark regions are remarkably improved by SuPeR-B: in fact, the values of mD 0 and m1 D (m2 , resp.) after enhancing are higher (lower, resp.) than those measured on the images without enhancement. In general, the lower α, a and b, the higher the locality of the spatial and intensity processing is and the better the values of mD i s are. On the contrary, the enhancement of the bright regions makes them even brighter but decreases their contrast and worses their deviance from the histogram flatness. This is because, as SuPeR, SuPeR-B tends to remove slight edges that, according to Retinex theory, are considered to be irrelevant for understanding the main content of the image. As a consequence, the histogram of the image brightness becomes more peaked and the contrast decreases. Therefore, for low values of α, a and b, the details of dark regions become much more visible, but the risk here is to over-enhance the brightest ones, creating artifacts or removing important edges. The selection of the triplet (α, a, b) granting the best result is not trivial, since it depends on the image content as well as on the application for which the enhancement is performed. In this framework, the subjective assessment of the panels built up from the images of Personal-DB and their enhanced versions leads to some important outcomes. First, in general, given a panel, the volunteers express different preferences, meaning that the process of image quality assessment by humans is hard to be modeled. Second, on average, people consider the enhancement by SuPeR satisfactory for the 23.63% of the images, they prefer SuPeR-B for the 72.0% of the images, while for the remaining 0.04% of the images they judge the enhancement useless or pejorative. On average, humans consider SuPeR-B necessary and outperforming SuPeR when the input image has a strong backlight (see e.g. Figure 7), while SuPeR is judged to provide a similar or better result on images with spotlights (see e.g. panels 50 and 51), on low light images (see e.g. panels 10 and 41), and on images where illumination differences between dark and bright regions are moderate (see e.g. panels 9 and 17). Finally, the subjective evaluation reports that the parameters triplets of the most preferred enhanced versions are (α=-1, a=0, b=-1), (α=1, a=-1, b=-1), (α=0, a=-1, b=-1), (α=-0.5, a=0, b=-0.5) and (α=1, a=0, b=0). These parameter triplets have been employed in the experiments on the larger dataset TM-DIED and the results, shown in Table 3, have a trend similar to that reported on Personal-DB. Some examples of enhancement of TM-DIED pictures are shown in Fig. 8: on these cases, SuPeR-B with (α, a, b = 0) = (1, 0, 0) outperforms SuPeR. The comparison of SuPeR-B with SuPeR, MSR, CHANNEL-DIVISION, FUSION and L-RESTORATION has been performed on Personal-DB. Some examples of images processed by these algorithms are displayed in Fig. 9. From Table 1, MSR provides the best results in terms of brightness, contrast and deviance from color histograms both on the whole images and on its dark and bright regions. Nevertheless, a qualitative inspection of the results shows that the images enhanced by MSR look like cartoons, with very evident dark edges and not natural colors. Moreover, in some cases, the brightest regions are under-enhanced due to the Gaussian smoothing proposed in [10] (see for instance the sky in the third and fourth images of Fig. 9).  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  343  
   
  Fig. 9. Comparison among SuPeR-B and other enhancers on some images from Personal-DB. See text for more details.  
   
  In general, on the whole images as well as on their dark regions, CHANNELDIVISION, FUSION and L-RESTORATION perform similarly to SuPeR-B in terms of contrast, while they report worse values of brightness and - except for LRESTORATION - of deviance from color distribution flatness. Qualitatively, this means that in backlight and spotlight images, CHANNEL-DIVISION, FUSION and LRESTORATION still preserve a remarkable difference between the dark and the bright regions, so that the details of the first ones appear less readable than in the images enhanced by SuPeR-B. On the contrary, On the bright regions CHANNEL-DIVISION, FUSION and L-RESTORATION provide a higher contrast and a lower deviance from color distribution flatness. This in general indicates that SuPeR-B over-enhances the bright regions slightly more than the other algorithms. Among the algorithms compared with SuPeR-B, L-RESTORATION is that providing more similar results to SuPeR-B, especially on low-light images and on images with a moderate difference between dark and bright regions.  
   
  344  
   
  M. Lecca Table 2. Objective evaluation on personal-DB. m0  
   
  Algorithm  
   
  m1  
   
  58.78  
   
  None  
   
  m2  
   
  mB 0  
   
  mB 1  
   
  mB mD 2 0  
   
  5.88 4.47 189.89 14.74 4.95  
   
  31.61  
   
  mD 1  
   
  mD 2  
   
  5.08 5.22  
   
  SuPeR-B (α=–1, a=–1, b=–1)  
   
  136.13  
   
  9.54 2.84 219.89  
   
  7.77 5.86 118.83 10.32 3.12  
   
  SuPeR-B (α=–1, a=0, b=–1)  
   
  7.74 5.89 117.48 10.06 3.26  
   
  135.01  
   
  9.33 2.94 219.58  
   
  SuPeR-B (α=–0.5, a=–0.5, b=–0.5) 127.75  
   
  9.27 2.91 215.95  
   
  8.86 5.65 108.74  
   
  9.73 3.36  
   
  SuPeR-B (α=–0.5, a=0, b=–0.5)  
   
  127.13  
   
  9.16 2.99 215.88  
   
  8.83 5.66 107.93  
   
  9.59 3.45  
   
  SuPeR-B (α=0.5, a=0, b=0)  
   
  105.37  
   
  8.48 3.28 212.04 11.21 5.42  
   
  80.96  
   
  SuPeR-B (α=0, a=–1, b=–1)  
   
  118.54  
   
  9.16 2.99 213.73 10.36 5.48  
   
  97.11  
   
  9.34 3.56  
   
  SuPeR-B (α=0, a=0, b=–1)  
   
  118.65  
   
  8.97 3.06 213.83 10.10 5.52  
   
  97.19  
   
  9.15 3.62  
   
  SuPeR-B (α=0, a=0, b=0)  
   
  114.84  
   
  8.78 3.18 213.46 10.33 5.50  
   
  92.36  
   
  8.89 3.76  
   
  SuPeR-B (α=1, a=–1, b=–1)  
   
  100.32  
   
  8.43 3.31 210.93 11.94 5.34  
   
  75.18  
   
  8.29 3.94  
   
  SuPeR-B (α=1, a=0, b=–1)  
   
  101.88  
   
  8.37 3.32 211.20 11.65 5.38  
   
  76.94  
   
  8.26 3.95  
   
  99.98  
   
  8.24 3.39 211.00 11.71 5.37  
   
  74.66  
   
  8.10 4.03  
   
  SuPeR-B (α=1, a=0, b=0)  
   
  8.43 3.91  
   
  121.35 20.21 2.48 191.20 16.26 4.84 107.82 21.82 2.59  
   
  MSR FUSION  
   
  84.85  
   
  9.41 3.59 199.63 14.81 4.92  
   
  61.77  
   
  9.14 4.30  
   
  CHANNEL-DIVISION  
   
  75.54  
   
  7.58 4.15 229.94 13.42 6.02  
   
  43.25  
   
  7.54 4.45  
   
  8.60 2.50 200.09  
   
  83.03  
   
  9.15 2.80  
   
  101.51  
   
  L-RESTORATION  
   
  9.52 4.75  
   
  Table 3. Objective evaluation on TM-DIED. Algorithm None SuPeR-B (α = –1, a=0, b=–1)  
   
  m0 80.40  
   
  m1  
   
  m2  
   
  mB 0  
   
  mB 1  
   
  mB mD 2 0  
   
  8.81 3.58 187.64 14.65 4.50  
   
  139.22 12.02 2.67 213.14  
   
  SuPeR-B (α=–0.5, a=0, b=–0.5) 130.46 11.72 2.61 208.55  
   
  35.59  
   
  mD 1  
   
  mD 2  
   
  8.22 5.04  
   
  8.21 5.70 107.21 14.60 3.09 9.55 5.38  
   
  96.01 13.73 3.35  
   
  SuPeR-B (α=0, a=–1, b=–1)  
   
  120.60 11.43 2.66 206.05 11.48 5.20  
   
  82.73 12.74 3.59  
   
  SuPeR-B (α=1, a=–1, b=–1)  
   
  104.21 10.50 3.03 202.71 13.12 5.05  
   
  61.41 11.06 4.12  
   
  SuPeR-B (α=1, a=0, b=0)  
   
  104.04 10.40 3.06 202.70 12.98 5.07  
   
  61.06 10.94 4.16  
   
  6 Conclusions This work presented the Retinex inspired bilateral filter SuPeR-B, which modified the image enhancer SuPeR for better improving backlight and spotlight images. The experiments reported in the previous Section show that SuPeR-B effectively increases the visibility of content and details of such images, also outperforming other methods proposed in the literature for this task. The bilateral processing of spatial and intensity information, which is the core of SuPeR-B, is modeled by a Coon patch, whose parameters can be tuned by the users. In the current implementation, the Coon patch is basically a piece-wise planar patch whose slope depends on the parameters α, a and b and defines the weights of the spatial and intensity features involved in the input image enhancement. The choice of the values of α, a and b providing the best enhancement of a given image is a crucial point for SuPeR-B, because different parameters’ value lead  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  345  
   
  to different enhancement results. By this way, the objective and subjective evaluations performed in this study draw general guidelines to set these parameters according to the input image. Precisely, for backlight and spotlight images, low values of α, a and b are recommended, while higher values are suitable for images with global low light or with moderate difference between bright and dark regions. These outcomes suggest to automatize the parameter settings based on the shape of the channel distribution: in case of bi-modal distribution, α, a and b must be low, while in case of more uniform distributions higher values may be used. This general suggestion should be further investigated in future work, taking also into account the context in which image processing is done and the final application, like human inspection, e.g. for entertainment [4], or computer vision tasks, e.g. for image description and matching, where robustness to light and high quality of details are crucial [15]. Moreover, the experimental data made available for free would like to promote future comparison and analysis.  
   
  References 1. Ackar, H., Abd Almisreb, A., Saleh, M.A.: A review on image enhancement techniques. Southeast Europe J. Soft Comput. 8(1) (2019) 2. Akai, M., Ueda, Y., Koga, T., Suetake, N.: A single backlit image enhancement method for improvement of visibility of dark part. In: 2021 IEEE International Conference on Image Processing (ICIP), pp. 1659–1663 (2021). https://doi.org/10.1109/ICIP42928.2021.9506526 3. Barricelli, B.R., Casiraghi, E., Lecca, M., Plutino, A., Rizzi, A.: A cockpit of multiple measures for assessing film restoration quality. Patt. Recogn. Lett. 131, 178– 184 (2020). https://doi.org/10.1016/j.patrec.2020.01.009, https://linkinghub.elsevier.com/ retrieve/pii/S0167865520300076 4. Bellotti, S., Bottaro, G., Plutino, A., Valsesia, M.: Mathematically based algorithms for film digital Restoration. In: Imagine Math 7, pp. 89–104. Springer, Cham (2020). https://doi.org/ 10.1007/978-3-030-42653-8 6 5. Chang, H., Ng, M.K., Wang, W., Zeng, T.: Retinex image enhancement via a learned dictionary. Opt. Eng. 54(1), 013107 (2015) 6. Finlayson, G.D., Drew, M.S., Funt, B.V.: Color constancy: generalized diagonal transforms suffice. JOSA A 11(11), 3011–3019 (1994) 7. Fu, X., Sun, Y., LiWang, M., Huang, Y., Zhang, X.P., Ding, X.: A novel retinex based approach for image enhancement with illumination adjustment. In: 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1190–1194. IEEE (2014) 8. Jiang, Z., Li, H., Liu, L., Men, A., Wang, H.: A switched view of retinex: Deep selfregularized low-light image enhancement. Neurocomputing 454, 361–372 (2021) 9. Jobson, D.J., Rahman, Z., Woodell, G.A.: Properties and performance of a center/surround retinex. IEEE Trans. Image Process. 6(3), 451–462 (1997) 10. Jobson, D.J., Rahman, Z.u., Woodell, G.A.: A multiscale retinex for bridging the gap between color images and the human observation of scenes. IEEE Trans. Image Process. 6(7), 965–976 (1997) 11. Land, E.: The Retinex. Am. Sci. 52(2), 247–264 (1964) 12. Land, E.H., John, McCann. J.: Lightness and Retinex theory. Optical Soc. Am. 1, 1–11 (1971)  
   
  346  
   
  M. Lecca  
   
  13. Lecca, M.: On the von Kries model: estimation, dependence on light and device, and applications. In: Celebi, M.E., Smolka, B. (eds.) Advances in Low-Level Color Image Processing. LNCVB, vol. 11, pp. 95–135. Springer, Dordrecht (2014). https://doi.org/10.1007/978-94007-7584-8 4 14. Lecca, M.: Color vision is a spatial process: the retinex theory. In: Bianco, S., Schettini, R., Tr´emeau, A., Tominaga, S. (eds.) Computational Color Imaging, pp. 26–39. Springer International Publishing, Cham (2017) 15. Lecca, M.: Comprehensive evaluation of image enhancement for unsupervised image description and matching. IET Image Processing 14(10), 4329–4339 (December 2020). https://digital-library.theiet.org/content/journals/10.1049/iet-ipr.2020.1129 16. Lecca, M.: Generalized equation for real-world image enhancement by milano retinex family. J. Opt. Soc. Am. A 37(5), 849–858 (2020). https://doi.org/10.1364/JOSAA.384197. http://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-37-5-849 17. Lecca, M.: Machine colour constancy: a work in progress. Color. Technol. 137(1), 72–77 (2021) 18. Lecca, M.: Personal-DB (Dec 2021). https://tev.fbk.eu/resources/imageenhancement 19. Lecca, M.: A retinex inspired bilateral filter for enhancing images under difficult light conditions. In: VISIGRAPP (4: VISAPP), pp. 76–86 (2021) 20. Lecca, M., Messelodi, S.: SuPeR: Milano Retinex implementation exploiting a regular image grid. J. Opt. Soc. Am. A 36(8), 1423–1432 (Aug 2019). https://doi.org/10.1364/JOSAA.36. 001423, http://josaa.osa.org/abstract.cfm?URI=josaa-36-8-1423 21. Li, Z., Wu, X.: Learning-based restoration of backlit images. IEEE Trans. Image Process. 27(2), 976–986 (2018) 22. Li, Z., Cheng, K., Wu, X.: Soft binary segmentation-based backlit image enhancement. In: 2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP), pp. 1–5 (2015). https://doi.org/10.1109/MMSP.2015.7340808 23. Ma, C., Zeng, S., Li, D.: A new algorithm for backlight image enhancement. In: 2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS), pp. 840–844. IEEE (2020) 24. Morel, J.M., Petro, A.B., Sbert, C.: A PDE formalization of Retinex theory. IEEE Trans. Image Process. 19(11), 2825–2837 (2010) 25. Peicheng, Z., Bo, L.: Backlit image enhancement based on illumination-reflection imaging model. In: 2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE), pp. 438–443 (2021). https://doi.org/10.1109/CACRE52464.2021. 9501394 26. Petro, A.B., Sbert, C., Morel, J.M.: Multiscale retinex. Image Processing On Line pp. 71–88 (2014) 27. Ramirez Rivera, A., Byungyong Ryu, Chae, O.: Content-aware dark image enhancement through channel division. IEEE Trans. Image Process. 21(9), 3967–3980 (2012) 28. Rizzi, A., Algeri, T., Medeghini, G., Marini, D.: A proposal for contrast measure in digital images. In: CGIV 2004–2nd European Conference on Color in Graphics, Imaging, and Vision and 6th Int. Symposium on Multispectral Color Science, pp. 187–192. Aachen (2004) 29. Rizzi, A., Bonanomi, C.: Milano Retinex family. J. Electron. Imag. 26(3), 031207–031207 (2017) 30. Tsai, C.M., Yeh, Z.M.: Contrast compensation by fuzzy classification and image illumination analysis for back-lit and front-lit color face images. IEEE Trans. Consum. Electron. 56(3), 1570–1578 (2010) 31. Ueda, Y., Moriyama, D., Koga, T., Suetake, N.: Histogram specification-based image enhancement for backlit image. In: 2020 IEEE International Conference on Image Processing (ICIP), pp. 958–962. IEEE (2020)  
   
  Enhancing Backlight and Spotlight Images by SuPeR-B  
   
  347  
   
  32. Vonikakis, V.: Tm-died: The most difficult image enhancement dataset (Dec 2021). https:// sites.google.com/site/vonikakis/datasets 33. Wang, Q., Fu, X., Zhang, X., Ding, X.: A fusion-based method for single backlit image enhancement. In: 2016 IEEE International Conference on Image Processing (ICIP), pp. 4077–4081 (2016) 34. Wang, S., Zheng, J., Hu, H.M., Li, B.: Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE Trans. Image Process. 22(9), 3538–3548 (2013) 35. Wang, W., Wu, X., Yuan, X., Gao, Z.: An experiment-based review of low-light image enhancement methods. IEEE Access 8, 87884–87917 (2020) 36. Wei, C., Wang, W., Yang, W., Liu, J.: Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560 (2018) 37. Yang, W., Wang, W., Huang, H., Wang, S., Liu, J.: Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE Trans. Image Process. 30, 2072– 2086 (2021)  
   
  Rethinking RNN-Based Video Object Segmentation Fatemeh Azimi1,2(B) , Federico Raue2 , J¨orn Hees2 , and Andreas Dengel1,2 1  
   
  TU Kaiserslautern, Kaiserslautern, Germany German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany {fatemeh.azimi,federico.raue,jorn.hees,andreas.dengel}@dfki.de 2  
   
  Abstract. Video Object Segmentation is a fundamental task in computer vision that aims at pixel-wise tracking of one or multiple foreground objects within a video sequence. This task is challenging due to real-world requirements such as handling unconstrained object and camera motion, occlusion, fast motion, and motion blur. Recently, methods utilizing RNNs have been successful in accurately and efficiently segmenting the target objects as RNNs can effectively memorize the object of interest and compute the spatiotemporal features which are useful in processing the visual sequential data. However, they have limitations such as lower segmentation accuracy in longer sequences. In this paper, we expand our previous work to develop a hybrid architecture that successfully eliminates some of these challenges by employing additional correspondence matching information, followed by extensively exploring the impact of various architectural designs. Our experiment results on YouTubeVOS dataset confirm the efficacy of our proposed architecture by obtaining an improvement of about 12pp on YoutTubeVOS compared to RNN-based baselines without a considerable increase in the computational costs. Keywords: Video Object Segmentation · Recurrent neural networks · Correspondence matching  
   
  1 Introduction nOe-shot Video Object Segmentation (VOS) is the task of densely tracking the intended foreground objects in a video, given the first mask of the object’s appearance. VOS plays an important role in various applications such as video editing, autonomous driving, and robotics. During the last years, a wide variety of learning-based solutions have been proposed for VOS trying to maximize the segmentation accuracy via addressing different challenging scenarios such as tracking smaller objects, handling occlusion, fast motion, crowded scenes with similar object instances, etc. [1, 4, 45, 47, 49]. The suggested approaches in the literature can be roughly categorized to three main groups. The first category naively tries to extend an image segmentation model to video domain [9, 29]. During inference, these models try to adapt the trained network to the specific scene and foreground object. This is usually done via further finetuning the network using the single object mask provided for the first frame (this process is known c Springer Nature Switzerland AG 2023  A. A. de Sousa et al. (Eds.): VISIGRAPP 2021, CCIS 1691, pp. 348–365, 2023. https://doi.org/10.1007/978-3-031-25477-2_16  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  349  
   
  as online training). Therefore, these models are relatively slow and their performance is sub-optimal as training on a single image can result in overfitting behavior. The second class deploys a memory component for memorizing the object of interest and processing the motion information [1, 40, 47]. Although using memory is a natural choice for processing the sequential data and these methods can achieve a good performance without requiring online training, their accuracy is limited by the functioning of the memory module. For example, their performance considerably drops for longer sequences due to the limited memory capacity and error propagation. The third group is based on template matching [41, 45, 49]. These methods capture the target in each frame through finding the correspondences between the frame at hand and a reference frame (e.g. the given mask at t = 0). These approaches can also obtain a good performance with a fast run-time; however, their performance degrades in scenes with multiple similar object instances or when the object appearance changes drastically with respect to the reference frames. As it is expected, the model struggles to find the similarities in these scenario, resulting in low segmentation accuracy. In this paper, we extend our previous work [2] that builds on top of a sequence-tosequence (S2S) [47] baseline for VOS, due to its good performance and straightforward design and training procedure. The S2S architecture is an encoder-decoder network with an RNN module in the bottleneck which is responsible for processing the spatiotemporal features and tracking the target object. To improve the performance of this method in segmenting the longer sequences, we take inspiration from the matching-based algorithms [45]. We hypothesize that the matching-based methods complement S2S model by providing additional training signals that can enhance the segmentation accuracy and reduce the adverse effect of error propagation. Utilizing the reliable information in the reference frame can be especially useful in handling occluded scenes where the RNN may struggle to lose the target object after several time steps. To this end, we employ both RNN and matching branches and develop a fusion block to merge the RNN spatiotemporal features with the template matching and refer to our model as hybrid S2S (HS2S). Additional to [2], we experiment with two architecture variants of our model. In the first form, we explore the effectiveness of bidirectional design [33] where in addition to utilizing the information from the past time steps, we integrate the future frames via a bidirectional RNN network. In the second variation, we explore a multi-task training setup by joint training the VOS model together with the unsupervised optical flow objective. Our intuition is that since optical flow and VOS are well-aligned tasks (in both cases the model has to learn pixel motion between the consecutive frames), training the model with both objectives might bring additional benefits via utilizing the optical flowrelated constraints. We perform extensive experiments and ablations on YouTubeVOS dataset [47] to study the role of different components in our HS2S model. Our experimental results confirm the effectiveness of our hybrid design by obtaining an increase of about 12pp in the overall segmentation accuracy compared to the RNN-based baseline.  
   
  2 Related Work In this section, we provide a summary of the traditional variational methods based on energy minimization as well as more recent learning-based approaches proposed for solving the VOS task.  
   
  350  
   
  F. Azimi et al.  
   
  In [7, 16, 39], the authors attempt to solve foreground object estimation using supervoxels to capture similar regions across space and time. These methods cluster similar pixels across spatial dimensions into superpixel nodes and find the edges between these nodes across time and space by employing motion and appearance similarities to form the supervoxels. Accordingly, the object masks are obtained via processing and merging the connected supervoxels. In [8], Brox et al. develop a bottom-up approach for segmenting the foreground objects and utilize the optical flow motion information to enforce temporal consistency across a video shot. The main idea here is that pixels with similar motion patterns should belong to the same object. Similarly, Papazoglou et al. [25] propose a two-stage segmentation algorithm where in the first stage, the initial segmentation masks are obtained through processing the optical flow and motion boundaries. In the next stage, the masks are refined by applying two smoothness constraints. The first constraint enforces spatio-temporal consistency across video frames while the second implies that the foreground objects should only change smoothly over time. In [12], the authors suggest that optical flow only provides local information across neighboring frames which is not optimal. To address this limitation, they develop a model that integrates non-local information across space and time. With [11, 14, 20] and the release of specialized and large-scale VOS datasets [29, 30, 47], the learning-based solutions for VOS have replaced more traditional models during the last few years. In [9], the authors present a training strategy to extend a network designed for medical image segmentation [22] for VOS. Starting from VGG16 [34] weights pretrained on ImageNet [11], they further train the network with segmentation objective on a VOS-specific dataset. During the inference, they perform online training and additionally fine-tune the network to be specialized for capturing the object of interest within the test scene. Perazzi et al. [28] also only rely on static images; to track the target object, they guide the network by inputting the object mask from the previous time step to the network. This method also needs online training for achieving acceptable performance. To address this limitation, [48] employs a modulator network that generates the normalization values of the main segmentation network, specific to each video. Another line of work suggests utilizing RNNs for computing the spatio-temporal features, integrating the motion information, and memorizing the target object [1, 38, 47], [40]. These methods achieve good performance without online training, however, their segmentation accuracy worsens for longer sequences due to limited RNN memory and error propagation. This limitation is improved by incorporation of an external memory in [23]; however, this causes additional hardware memory constraints to the system. As a result, in practice, only a fraction of the frames can be stored in the memory which can be sub-optimal. Differently, Wug et al. suggest detecting the foreground object via finding the correspondences between each frame and reference frames using a Siamese architecture [45]. In [50], the authors propose a transductive approach that instead of only relying on a limited number of reference frames, additionally integrate the information from the past segmented frames while [6, 17], develop a system that learns the appearance model of the object of interest and using this model, it captures the target throughout the rest of the video. These methods are efficient with a good performance, but their accuracy  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  351  
   
  degrades in the presence of multiple similar objects as the model is confused by finding multiple correspondences. To improve this challenge, [49] additionally incorporates background correspondence matching. They demonstrate this design helps the model to better handle ambiguities in the correspondence search.  
   
  3 Method In this part, we describe our proposed architecture based on a hybrid propagation policy, referred to as HS2S. Our model builds on top of S2S [47], a sequence-to-sequence model for video object segmentation. Following a detailed study on the performance of the S2S model, we address multiple shortcomings of the S2S design by inserting additional information obtained from correspondence matching. The S2S model consists of an encoder-decoder network similar to U-Net [31] that learns the mapping between the RGB color space and the object segmentation mask. S2S uses a ConvLSTM [46] module between the encoder and the decoder, intended for processing the spatiotemporal features and tracking the target object. This memory component is accountable for maintaining the temporal coherency between the predicted segmentation masks across several video frames. To this end, S2S utilizes an initializer network that generates the initial ConvLSTM hidden states through processing the first RGB frame and foreground object mask. The S2S model can be summarized as follows [47]: h0 , c0 = Initializer(x0 , y0 )  
   
  (1)  
   
  x ˜t = Encoder(xt )  
   
  (2)  
   
  ht , ct = RNN(˜ xt , ht−1 , ct−1 )  
   
  (3)  
   
  y˜t = Decoder(ht )  
   
  (4)  
   
  where h and c are the hidden and cell states for the ConvLSTM, t is the time step, x is the RGB input, y is the ground-truth mask and y˜ is the predicted output. Having a closer look at the failure cases in the S2S model, we observed the model’s performance degrades for longer videos. There are multiple factors that can potentially contribute to this limitation. First, the limited memory of RNNs is an inherent challenge for RNN-based architectures. Due to this issue, the model struggles to fully capture the essential information in the scene as well as the evolution of the object’s appearance. Moreover, as the video sequences become longer, they tend to lose access to the information from the earlier time steps. This is particularly problematic for the occluded scenes; as, if the model forgets the occluded object, it will not be able to re-capture the object once it re-appears in the scene. Finally, due to the feedback connection in the RNN module, the erroneous predictions will flow to the future time steps. These results in drift and error propagation in the final results, exacerbating the model’s accuracy for segmenting the frames further in time.  
   
  352  
   
  F. Azimi et al.  
   
  3.1  
   
  Hybrid Sequence-to-Sequence VOS  
   
  We propose to supplement RNN module with correspondence matching for tracking the object of interest in a video. The benefits of matching-based solutions [45] for one-shot VOS gives the opportunity to solve some problems inherent in RNNs. For example, Oh et al. design a siamese architecture that obtains a good segmentation accuracy through matching with reference frames at t = 0 and the guidance information from the previous mask [45]. However, these models struggle in scenes with similar objects or when the object’s appearance changes drastically over time as the model cannot detect the object via matching to the reference frames anymore. We hypothesize that the pros and cons of the RNN-based and matching-based VOS solutions complement each other. The informative signals computed from template matching are crucial for better handling the occluded videos; no matter how long the occlusion duration, the model would still have the chance to re-capture the object through matching it with the reference frames at t = 0. Moreover, these additional training signals reduce the adverse effect of error propagation thus improving the overall segmentation quality. On the other hand, integrating the motion features and the spatiotemporal model learned by RNNs can serve as prior for the approximate object location at time step t. This combination helps the model to disambiguate the situations with similar objects by employing the location prior. S2S  
   
  HS2S t  
   
  0  
   
  Initializer  
   
  0  
   
  Encoder  
   
  t  
   
  t-1 Reference Encoder  
   
  Reference Encoder  
   
  x ˜t  
   
  x ˜t h0 , c 0  
   
  RNN  
   
  Encoder  
   
  x ˆt−1  
   
  x ˆ0  
   
  RNN ht Fusion  
   
  Weight sharing ht  
   
  x ˆ 0 , ht  
   
  x ˆt−1 , ht  
   
  GC  
   
  GC  
   
  RNN Decoder  
   
  Decoder  
   
  ConvLSTM GC  
   
  Fig. 1. The HS2S architecture combining the RNN-based and matching based features for VOS. We utilize a fusion block consisting of multiple Global Convolution (GC) [26] layers that merges the spatiotemporal features from RNN with the reference frames from the first and the previous time steps.  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  353  
   
  Our proposed architecture is shown in Fig. 1. We utilize one encoder for the input frames and another encoder for the reference frames and masks. As suggested by previous works [28, 45], we utilize the time steps 0 and t − 1 as our reference frames. Time step 0 is specifically important since the ground-truth segmentation mask for this frame is available at inference; therefore, the information from this step is highly reliable for the model. Moreover, integrating the information from t − 1 serves as an additional signal about the approximate object location. To reduce the model complexity, we replace the initializer network with a teacherforcing training strategy [28]. Having the initializer network removed, we simply initialize the memory hidden states with zero vectors. Next, we feed the segmentation mask from the previous time step as an additional input to the encoder [28]. By receiving the previous segmentation mask as input, the model is informed about the approximate location of the target object. In the next step, we merge the reference and the spatiotemporal RNN features through a nonlinear fusion function. To properly combine the information from these to branches, this module requires both local and global connections across the spatial feature dimensions. Accordingly, one could design this module using convolution layers with very big kernel sizes [26], or utilize attention mechanisms to span the height and width dimensions and incorporate all the features [43, 44]. Ablation on the impact of various designs for merge block architecture is provided in subsection 4.5. Finally, the output of the merge block is then passed through a decoder network and transformed into the segmentation masks through a stack of upsampling and convolution layers. With the same notation as in eqs. (1) to (2) amd (4), the overall steps can be summarized into the formulation below [2]: h0 , c0 = 0  
   
  (5)  
   
  x ˆ0 = Reference Encoder(x0 , y0 )  
   
  (6)  
   
  x ˆt−1 = Reference Encoder(xt−1 , yt−1 )  
   
  (7)  
   
  x ˜t = Encoder(xt , yt−1 )  
   
  (8)  
   
  ht , ct = RNN(˜ xt , ht−1 , ct−1 )  
   
  (9)  
   
  y˜ = Decoder(˜ x0 , x ˜t−1 , h)  
   
  (10)  
   
  As the architectures for encoder and reference encoder in Fig. 1 are identical, we experimented with weight-sharing between the two encoders; however, this architecture resulted in lower segmentation accuracy. This behavior can be due to two reasons: First, reduction in model’s expressive power, and second, the misalignment between the reference features and the teacher-forcing training strategy. As we can see in Fig. 1, the RGB frame and mask fed to the ref-encoder are from the same time step while the input to the encoder differs by 1 (t − 1 and t). Therefore, the functions learned by these two encoders are not the same and that weight sharing would not be applicable to this scenario.  
   
  354  
   
  F. Azimi et al.  
   
  Training Loss. We train our model with a combination of Balanced BCE loss and an additional auxiliary term of border classification [2]: Ltotal = λ Lseg + (1 − λ) Laux  
   
  (11)  
   
  The BCE term assigns either foreground or background label to each pixel in the image. As in the image, the portion of background pixels usually outweighs the foreground, the training will be biased towards paying higher attention to the background. This issue is addressed by multiplying the loss terms with a balancing factor [9]: Lseg (W) =  
   
  T  t=1  
   
  (−α  
   
    
   
  log P (yj = 1|X; W) − (1 − α)  
   
  j∈Y+  
   
    
   
  log P (yj = 0|X; W))  
   
  j∈Y−  
   
  (12) with α being the ratio of background to foreground pixels. The Border classification objective additionally classifies each pixel based on their relative distance to the object border. As a result, this term provides finer information about the pixel location and improves the quality of detected object edges. Further explanation and an in-depth analysis of this loss-term’s effects can be found in [1]. 3.2  
   
  Bidirectional Architecture  
   
  In HS2S architecture, the video frames are processed sequentially passing the information from the past to the future. Bidirectional sequence-to-sequence architectures enable the model to integrate information from the past as well as the future; they have been effective in improving the performance of sequential processing tasks such as Machine Translation [13, 33, 36, 38]. Therefore, it is natural to conjecture that integrating the information from the future frames might benefit the HS2S model. However, based on the task definition in VOS, we need the object mask in the last frame (t = T ) to process the video backward in time. Otherwise, the model will not recognize which object to track. To address this challenge, we design the bidirectional HS2S (Bi-HS2S) architecture shown in Fig. 2. As explained earlier, there are two different ways to inform the network about the object of interest. One is through using an initializer network that processes the first RGB and the mask frames and initializes the memory hidden states. The second way is by simply feeding the segmentation mask from the previous time step to the encoder network as a guidance signal. The second option does not fit the bidirectional design as in the backward processing of the video sequence, we do not have access to the initial object mask. As a result, we resort to the first alternative. As illustrated in Fig. 2, we initialize the memory hidden states with the initializer network in the forward path. For the backward processing, we simply initialize the backward memory with the last hidden state ht obtained from the forward path. Our intuition is that ht contains information about the target object at t = T and can serve as a reasonable initialization. Finally, we combine the information from the forward and backward paths together with the reference features via the fusion block and pass it to the decoder to predict the segmentation masks.  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  t  
   
  t-1 Reference Encoder Initializer  
   
  0  
   
  h0 , c0  
   
  T-1  
   
  x ˜t RNN A  
   
  ht,A ht,B x ˆt−1  
   
  x ˆt−1  
   
  Encoder x ˜T  
   
  ht,A  
   
  RNN B  
   
  x ˆt−1  
   
  T  
   
  Reference Encoder  
   
  Encoder  
   
  Fusion  
   
  355  
   
  RNN A hT ,B  
   
  x ˆT −1  
   
  RNN B  
   
  Fusion  
   
  Conv1×1 GC  
   
  GC  
   
  Decoder  
   
  Decoder  
   
  GC  
   
  Fig. 2. The bidirectional HS2S architecture. The hidden states from the forward and backward RNNs are combined using a convolution layer and then merged with the reference features and passed to the decoder.  
   
  3.3 Multi-task Training with Optical Flow Prediction In multi-task learning, several tasks are combined within a single problem formulation and network architecture. This approach has been shown to be a successful training technique when the combined tasks are aligned in the objective and can provide supplemental information to each other [32]. In this section, we take inspiration from RAFT [37], a recent state-of-the-art optical flow architecture and design an architecture that combines video object segmentation with optical flow prediction, referred to as RAFTHS2S. Our intuition is that VOS and optical flow objectives are similar as they both tend to learn the pixel movement from one frame to the next. Accordingly, we explore whether combining these two learning objectives brings additional information to the model and enhances the segmentation accuracy. RAFT model [37] receives two consecutive images (It and It−1 ) as input and generates the flow field capturing the pixel motion between the consecutive frames. It consists of two encoders with the same architecture but separate weights; The first encoder extracts the features ft and ft−1 while the second encoder only processes It−1 to provide additional context to the network. Inspired by traditional optical methods, RAFT iteratively refines the estimated flow utilizing a ConvGRU [35] that produces the flow delta at each time step.  
   
  356  
   
  F. Azimi et al.  
   
  0  
   
  t  
   
  t-1 Reference Encoder  
   
  Reference Encoder  
   
  Encoder x ˜t  
   
  x ˆ0  
   
  x ˆt−1  
   
  Fusion  
   
  [˜ xt , x ˜t−1 , x ˆt−1 ]  
   
  RNN ht  
   
  Weight sharing  
   
  t  
   
  t-1  
   
  L  
   
  L  
   
  GRU  
   
  GRU  
   
  RAFT Lphotometric  
   
  t-1  
   
  Context  
   
  Decoder  
   
  Inner product L Lookup operation  
   
  Fig. 3. The multi-task training setup in RAFT-HS2S, combining HS2S with an optical flow method named RAFT. RAFT module computes the correlation between frames at t and t − 1 using the inner product between the respective feature vectors and generates an initial estimate of the optical flow between these consecutive frames. Then, it iteratively refines the approximated flow using a ConvGRU module that performs lookup operations based on the correlation volume and a context feature vector computed from the frame at t − 1.  
   
  Motivated by the commonality in VOS and optical flow training objectives, we adapt HS2S to accommodate the RAFT components as depicted in Fig. 3. As can be seen in this plot, the reference encoder additionally takes the role of context encoding for the RAFT model, and the Encoder is employed for processing It and It−1 . For the optical flow loss which is added to the objective in Eq. 11, we use an unsupervised objective, namely photometric loss [18]:  Lphotometric = |I (1) − w(I (2) )| (13) Here Lphotometric is the photometric loss over all the pixels and w is the warping operation that warps I (2) to I (1) using the optical flow between these two frames. This term implies that having the precise motion, the pixel colors resulting from warping one image to the other should match.  
   
  4 Experiments 4.1  
   
  Implementation Details  
   
  In this section, we explain the implementation and the training details of our HS2S model explained in Subsect. 3.1. We use the same experimental setup and hyperparameters for the other architecture variants unless mentioned otherwise. Additional ablations are provided in Sects. 4.3 to 4.5.  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  357  
   
  The encoder backbone in Fig. 1 is based on ResNet50 [14] architecture pretrained on ImageNet [11], with the following modifications. We remove the final fully connected layer which generates the image classification output and add conv1×1 to reduce the number of channels in the bottleneck from 2048 to 1024. In the first layer, we add a convolution layer to process the segmentation mask and combine it with the RGB features. For the memory module, we use a ConvLSTM layer [46] with 3 × 3 filters as suggested in [47]. The merge module consists of a stack of two Global Convolution layers [26] with a kernel size of 7 × 7, following the setup in [45]. The decoder consists of 5 upsampling layers followed by convolution layers with 5 × 5 kernel sizes and 1024, 512, 256, 128, 64 number of channels respectively. The last layer activation function is a sigmoid nonlinearity that outputs the probability of each pixel belonging to the foreground or background. Moreover, we utilize skip connections [31] and skip-memory connections [1] for obtaining refined segmentation masks and better tracking the smaller objects. As mentioned in Subsect. 3.1, we use a teacher-forcing training strategy, feeding the ground-truth segmentation masks from time step t−1 as input in time t. Since we do not have access to the ground-truth labels during the inference, the masks predicted by the model are used instead. As pointed out by [3, 5], this training approach is problematic since the model predictions are often erroneous, and the accumulation of errors result in a gap between the training and testing phases. Following the recipe suggested in [5], we deploy a curriculum learning policy to address this issue. At the beginning of the training when the model does not generate high-quality masks, we use the groundtruth labels; once the training loss is stable, we follow a probabilistic scheme to decide whether to choose from the ground-truth or use the model prediction as input. The probability of selecting the model predictions is gradually increased from 0 to 0.5. We use a batch size of 16 and Adam [19] optimizer with a starting learning rate of 1e − 4. Once the training loss is stabilized, the learning rate is reduced every 5 epochs by a factor of 0.9. 4.2 Experimental Results We assess our method on YouTubeVOS [47], the largest dataset for VOS consisting of 3, 471 and 474 videos in training and validation sets respectively. We report F and J scores, the standard metric for evaluating VOS models [27]. YouTubeVOS evaluation additionally reports seen and unseen scores to separately measure the model’s accuracy for the objects that have been present or absent during the training. The unseen scores quantify the model generalization to new object types. In Table 1, we present the results obtained from HS2S model (plus its variants as explained in Sects. 3.1 to 3.2) as well as our baseline S2S, and the other state-of-theart models. The results provided in the upper half of the table are for the methods with additional online training. Using the first object mask, these approaches further train the network at test time; as a result, they often achieve a better accuracy but they are considerably slower. As can be seen from the results in Table 1, our HS2S method reaches a significant improvement in comparison with the S2S baseline and outperforms this approach even when it is fine-tuned by additional online training (S2S(OL)). Moreover,  
   
  358  
   
  F. Azimi et al.  
   
  we observe that utilizing the Bi-HS2S leads to further improvement of about 1pp while RAFT-HS2S achieves similar performance as HS2S. This implies that the information provided from the optical flow loss is already included in the VOS objective and combining these additional terms does not bring additional benefits to the model. Table 1. Comparison of the experimental results from the HS2S model [2] with state-of-the-art methods on YouTubeVOS dataset. OL refers to methods with additional Online Training. Method  
   
  OL Jseen Junseen Fseen Funseen Overall  
   
  OSVOS [21]  
   
    
   
  59.8  
   
  54.2  
   
  60.5  
   
  60.7  
   
  58.8  
   
  MaskTrack [28]  
   
    
   
  59.9  
   
  45.0  
   
  59.5  
   
  47.9  
   
  50.6  
   
  OnAVOS [42]  
   
    
   
  60.1  
   
  46.6  
   
  62.7  
   
  51.4  
   
  55.2  
   
  S2S(OL) [47]  
   
    
   
  71.0  
   
  55.5  
   
  70.0  
   
  61.2  
   
  64.4  
   
  OSMN [48]  
   
  ✗  
   
  60.0  
   
  40.6  
   
  60.1  
   
  44.0  
   
  51.2 53.8  
   
  RGMP [45]  
   
  ✗  
   
  59.5  
   
  45.2  
   
  –  
   
  –  
   
  RVOS [40]  
   
  ✗  
   
  63.6  
   
  45.5  
   
  67.2  
   
  51.0  
   
  56.8  
   
  A-GAME [17]  
   
  ✗  
   
  66.9  
   
  61.2  
   
  –  
   
  –  
   
  66.1  
   
  S2S(no-OL) [47] ✗  
   
  66.7  
   
  48.2  
   
  65.5  
   
  50.3  
   
  57.7  
   
  S2S++ [1]  
   
  ✗  
   
  68.7  
   
  48.9  
   
  72.0  
   
  54.4  
   
  61.0  
   
  STM- [23]  
   
  ✗  
   
  67.1  
   
  63  
   
  69.4  
   
  71.6  
   
  68.2  
   
  TVOS [50]  
   
  ✗  
   
  –  
   
  –  
   
  –  
   
  –  
   
  67.2  
   
  HS2S [2]  
   
  ✗  
   
  73.6  
   
  58.5  
   
  77.4  
   
  66.0  
   
  68.9  
   
  Bi-HS2S  
   
  ✗  
   
  74.9  
   
  59.6  
   
  78.0  
   
  66.7  
   
  69.8  
   
  RAFT-HS2S  
   
  ✗  
   
  73.2  
   
  58.7  
   
  77.3  
   
  66.1  
   
  68.8  
   
  In Fig. 4, we provide a qualitative comparison between the segmentation results from S2S [47] baseline and our HS2S model. As can be seen, our model can successfully maintain the segmentation accuracy for later time steps. Moreover, our method can handle scenes with multiple similar objects, which is challenging for matching-based algorithms. To quantify the impact of our hybrid architecture on the accuracy of longer videos, we select a subset of sequences from YouTubeVOS training set that consist of more than 20 frames for evaluation and use the rest for training. We resort to using this subset since the ground-truth masks for the YoutubeVOS validation set are not provided. Then, we calculate the segmentation accuracy for earlier frames (t < 10) and later frames (t > 10) independently. As can be seen in Table 2a, These two models have similar performance for earlier frames, while HS2S significantly outperforms the S2S model for frames in the further time steps. Additionally, we experiment with our HS2S model when using either 0th or (t − 1)th frame as a reference in order to assess the role of each one in the final performance. The results for this experiment are provided in Table 2b. 4.3  
   
  Ablation on the Impact of Encoder Architecture  
   
  The encoder networks in Fig. 1 are responsible for extracting descriptive features which will be then processed through the memory and decoded into a segmentation mask via  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  359  
   
  Time  
   
  Fig. 4. Visual comparison between the S2S and HS2S results in the upper and lower rows, respectively. We observe that our hybrid method can successfully maintain the segmentation accuracy at the later time steps.  
   
  the decoder network. Thus, improving the quality of the encoder network is expected to directly reflect on the segmentation quality. In this section, we study the behavior of HS2S model when employing various encoder architectures including VGG [34], ResNet50 [14], DeepLab [10], and Axial-DeepLab [43]. The DeepLab backbone is a modified ResNet50 with less pooling operations resulting in increased spatial feature dimension at the output (Higher spatial dimensions are presumably beneficial for dense prediction methods due to preserving fine local information). Furthermore, it consists of an Atrous Spatial Pyramid Pooling (ASPP) module composed of a stack of convolution layers with various dilation rates. We experiment with a DeepLab backbone pretrained on image segmentation, with and without the ASPP module. Different than CNN-based backbones, Axial-DeepLab [43] consists of fully-attentional blocks. In this model, the authors propose to break the attention into horizontal and vertical attentions to reduce the computational cost of the attention-based backbone (from quadratic to linear). However, this model still requires significantly  
   
  360  
   
  F. Azimi et al.  
   
  Table 2. (a) Comparison of our model with S2S baseline for shorter and longer sequences (S2S* is our implementation of this model with ResNet50 backbone which achieves about 1pp higher accuracy compared to [47]). We observe that our method significantly improves the performance for frames in later time steps. (b) The performance of HS2S model when either the first frame is used as reference (HS2S0,t−1 shows the accuracy of our model when either 0th or the t − 1th frames are used as reference). We observe that both frames contribute to boosting the segmentation accuracy and the best results are obtained when using both reference frames. (a) Performance comparison considering shorter (t < 10) and longer (t > 10) sequences [2].  
   
  (b) Performance of HS2S model when either the first or previous frame is used as reference [2].  
   
  Method  
   
  Fl20  
   
  Method  
   
  Jseen Junseen Fseen Funseen  
   
  S2S*  
   
  74.4  
   
  73.7  
   
  54.5  
   
  54.6  
   
  HS2S0  
   
  72.6 55.4  
   
  76.7 61.2  
   
  HS2S (ours) 77.1  
   
  76.3  
   
  65.5  
   
  64.2  
   
  HS2St−1 72.2 55.1  
   
  76.1 61.3  
   
  higher memory compared to the CNN-based backbones. Due to memory limitation, we experimented with small Axial-DeepLab architecture as elaborated in [43] Table 3. An ablation on the impact of backbone network. Backbone  
   
  Jseen Junseen Fseen Funseen overall  
   
  VGG16 [34]  
   
  71.4  
   
  56.0  
   
  74.9  
   
  64.8  
   
  66.8  
   
  ResNet-50 [14]  
   
  73.6  
   
  58.5  
   
  77.4  
   
  66.0  
   
  68.9  
   
  ResNet-101 [14]  
   
  73.9  
   
  58.5  
   
  77.3  
   
  66.2  
   
  69.0  
   
  ResNet-50-DeepLab (without ASPP) [10] 73.3  
   
  59.1  
   
  77.2  
   
  66.0  
   
  68.9  
   
  ResNet-50-DeepLab (w/ ASPP) [10]  
   
  72.3  
   
  56.9  
   
  76.5  
   
  64.2  
   
  67.4  
   
  Axial [43]  
   
  70.5  
   
  55.0  
   
  73.1  
   
  61.8  
   
  65.1  
   
  As can be seen in Table 3, we obtained the best results when applying the ResNetbased encoder. Surprisingly, integrating the additional ASPP module from the DeepLab architecture resulted in lower performance. This behavior can be due to the added complexity from combining the spatiotemporal RNN features with multi-scale processing in the ASPP module resulting in a more challenging optimization problem and suboptimal performance. 4.4  
   
  Ablation on the Impact of RNN Architecture  
   
  One of the main blocks in the HS2S architecture is the RNN block, accountable for memorizing the target object. In this section, we provide an ablation studying the HS2S performance when deploying three different RNN-based memories. The first variant is ConvLSTM [46]. This module is developed for processing sequential visual data by replacing the fully connected layers in LSTM with convolution layers, adjusting the LSTM layer for visual pattern recognition.  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  361  
   
  As the second model, we study DeepRNN [24]. In this model, the authors address the challenges in training deep RNN models. Although deeper networks are expected to learn better representations compared to their shallow counterparts in the case of CNNs, deep RNN architectures designed by simply stacking the RNN layers does not lead to considerable improvement in the model accuracy. In [24], Pang et al. suggest this behavior roots in the complex optimization operation when dealing with RNNs. Processing the entangled spatial and temporal information in sequential visual data leads to the optimization process becoming overly complex. This condition could become even more extreme for deeper RNNs, resulting in sub-optimal performance. To this end, they propose to disentangle the information related to the spatial flow from the temporal flow. They design a Context bridge module (CBM) which is composed of two computing blocks for processing the representation and the temporal flows. By enforcing these sources of information to flow independently, the optimization process could potentially be simplified. In our experiments, we deployed a stack of 5 RNN layers following the setup proposed in [24]. In the third variant, TensorLSTM [35], the authors attempt to improve the learning of long-term spatiotemporal correspondences for processing longer videos. They design a higher-order convolutional LSTM architecture named TensorLSTM that can better capture extended correlations. TensorLSTM consists of a preprocessing and a convolutional tensor-train module. The preprocessing module computes feature vectors from multiple overlapping sliding windows from the previous hidden states. These embeddings are then further processed through the convolutional tensor-train module and passed to the LSTM. Consequently, they are able to efficiently integrate the information from the previous hidden states and improve the capturing of long-term correlations. Table 4. An ablation on the choice of RNN module. Method  
   
  Jseen Junseen Fseen Funseen overall  
   
  ConvLSTM [46]  
   
  73.6  
   
  58.5  
   
  77.4  
   
  66.0  
   
  68.9  
   
  DeepRNN [24]  
   
  72.4  
   
  57.3  
   
  75.8  
   
  64.9  
   
  67.6  
   
  Tensor-TrainLSTM [35] 74.7  
   
  60.2  
   
  78.5  
   
  66.4  
   
  70.0  
   
  As it can be seen from the results in Table 4, TensorLSTM achieves a better segmentation accuracy compared to the other variants. This implies that in HS2S architecture, we do not require deeper RNNs to carry the information about the object of interest. However, accessing the information from multiple frames over an extended time period is beneficial for the model. In a way, TensorLSTM applies attention to a limited past context via the sliding-window mechanism in the preprocessing module. This observation is in line with employing the dual propagation strategy in Subsect. 3.1 where simply merging the information from the time-step t − 1 improves the segmentation results.  
   
  362  
   
  4.5  
   
  F. Azimi et al.  
   
  Ablation on Various Designs for the Fusion Block  
   
  In this section, we study the model’s performance when working with three different fusion block architectures in Table 5. Intuitively, the fusion block needs to provide global connections across the spatial dimensions as the object might be displaced to a further location compared to the reference frames. Additionally, it has to assign higher attention to the locations that belong to the foreground. In HS2Ssim , the spatiotemporal RNN features are merged with the reference features based on cosine similarity. HS2SGC merges these two branches using global convolution layers as suggested in [26] while HS2Sattn replaces this operation with an attention layer [15]. We obtained similar performance for different fusion architecture options, but the design using attention attained the highest accuracy. Table 5. An ablation on the impact of backbone network. Method  
   
  Jseen Junseen Fseen Funseen overall  
   
  HS2SGC  
   
  73.6  
   
  58.5  
   
  77.4  
   
  66  
   
  68.9  
   
  HS2Ssim 72.3  
   
  56.4  
   
  76.2  
   
  62.5  
   
  66.9  
   
  HS2Sattn 73.9  
   
  58.7  
   
  77.5  
   
  66.3  
   
  69.1  
   
  5 Conclusion In this paper, we expanded our previous HS2S approach [2] to generate a hybrid architecture for VOS that combines the merits of RNN-based and matching-based methods. As before, we find that all of our hybrid approaches are especially beneficial for the segmentation of objects that are occluded and re-appearing, as well as in cases where many similar objects need to be tracked and segmented. In this paper, we have investigated two derived architectures: a bi-directional and a multi-task (optical flow) extension of our previous approach. Further, we expanded our previous ablation study by investigating further segmentation backbones, RNN, and fusion blocks of the underlying architectures. Resulting from these investigations, we found that our bi-directional extension (Bi-HS2S) improves over our previous architectures by nearly 1 pp and more than 12pp when compared to other state-of-the-art RNN-based baselines (such as S2S(no-OL) [47]). To our surprise, our multi-task extension also taking optical flow into account (RAFT-HS2S) failed to improve over HS2S. In the expanded ablation study, we found that the ResNet-101 based backbone network, Tensor-TrainLSTM RNN architecture, and attention fusion blocks seemed to be the most beneficial design choices. For future work, we aim to investigate the potential benefit of utilizing the depth information either as input or as an unsupervised learning objective integrated into our HS2S model. Acknowledgement. This work was supported by the TU Kaiserslautern CS PhD scholarship program, the BMBF project ExplAINN (01IS19074), and the NVIDIA AI Lab (NVAIL) program. Further, we thank Christiano Gava, Stanislav Frolov, Tewodros Habtegebrial and Mohammad  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  363  
   
  Reza Yousefi for the many interesting discussions and proofreading of this paper. Finally, we thank all members of the Deep Learning Competence Center at the DFKI for their feedback and support.  
   
  References 1. Azimi, F., Bischke, B., Palacio, S., Raue, F., Hees, J., Dengel, A.: Revisiting sequence-tosequence video object segmentation with multi-task loss and skip-memory. In: 2020 25th International Conference on Pattern Recognition (ICPR), pp. 5376–5383. IEEE (2021). arXiv:2004.12170 2. Azimi, F., Frolov, S., Raue, F., Hees, J., Dengel, A.: Hybrid-s2s: Video object segmentation with recurrent networks and correspondence matching. In: VISAPP, pp. 182–192 (2021). arXiv:2010.05069 3. Azimi, F., Nies, J.F.J.N., Palacio, S., Raue, F., Hees, J., Dengel, A.: Spatial transformer networks for curriculum learning. arXiv preprint arXiv:2108.09696 (2021) 4. Azimi, F., Palacio, S., Raue, F., Hees, J., Bertinetto, L., Dengel, A.: Self-supervised test-time adaptation on video data. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3439–3448 (2022) 5. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for sequence prediction with recurrent neural networks. In: Advances in Neural Information Processing Systems, pp. 1171–1179 (2015) 6. Bhat, G., et al.: Learning what to learn for video object segmentation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12347, pp. 777–794. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-58536-5 46 7. Brendel, W., Amer, M., Todorovic, S.: Multiobject tracking as maximum weight independent set. In: CVPR 2011, pp. 1273–1280. IEEE (2011) 8. Brox, T., Malik, J.: Object segmentation by long term analysis of point trajectories. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6315, pp. 282–295. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15555-0 21 9. Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taix´e, L., Cremers, D., Van Gool, L.: Oneshot video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 221–230 (2017) 10. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017) 11. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. IEEE (2009) 12. Faktor, A., Irani, M.: Video segmentation by non-local consensus voting. In: BMVC, p. 8 (2014) 13. Graves, A., Fern´andez, S., Schmidhuber, J.: Bidirectional LSTM networks for improved phoneme classification and recognition. In: Duch, W., Kacprzyk, J., Oja, E., Zadro˙zny, S. (eds.) ICANN 2005. LNCS, vol. 3697, pp. 799–804. Springer, Heidelberg (2005). https:// doi.org/10.1007/11550907 126 14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016) 15. Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180 (2019)  
   
  364  
   
  F. Azimi et al.  
   
  16. Jain, S.D., Grauman, K.: Supervoxel-consistent foreground propagation in video. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8692, pp. 656–671. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10593-2 43 17. Johnander, J., Danelljan, M., Brissman, E., Khan, F.S., Felsberg, M.: A generative appearance model for end-to-end video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8953–8962 (2019) 18. Jonschkowski, R., Stone, A., Barron, J.T., Gordon, A., Konolige, K., Angelova, A.: What matters in unsupervised optical flow. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12347, pp. 557–572. Springer, Cham (2020). https://doi.org/ 10.1007/978-3-030-58536-5 33 19. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 20. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems, pp. 1097–1105 (2012) 21. Maninis, K.K., Caelles, S., Chen, Y., Pont-Tuset, J., Leal-Taix´e, L., Cremers, D., Van Gool, L.: Video object segmentation without temporal information. IEEE Trans. Patt. Anal. Mach. Intell. (TPAMI) 41(6), 1515–1530 (2018) 22. Maninis, K.-K., Pont-Tuset, J., Arbel´aez, P., Van Gool, L.: Deep retinal image understanding. In: Ourselin, S., Joskowicz, L., Sabuncu, M.R., Unal, G., Wells, W. (eds.) MICCAI 2016. LNCS, vol. 9901, pp. 140–148. Springer, Cham (2016). https://doi.org/10.1007/978-3-31946723-8 17 23. Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-time memory networks. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 9226–9235 (2019) 24. Pang, B., Zha, K., Cao, H., Shi, C., Lu, C.: Deep rnn framework for visual sequential applications. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 423–432 (2019) 25. Papazoglou, A., Ferrari, V.: Fast object segmentation in unconstrained video. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 1777–1784 (2013) 26. Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters-improve semantic segmentation by global convolutional network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4353–4361 (2017) 27. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: Computer Vision and Pattern Recognition (2016) 28. Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learning video object segmentation from static images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2663–2672 (2017) 29. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 724–732 (2016) 30. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel´aez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017) 31. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015). https://doi.org/10.1007/978-3-31924574-4 28 32. Ruder, S.: An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098 (2017)  
   
  Rethinking RNN-Based Video Object Segmentation  
   
  365  
   
  33. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE Trans. Signal Process. 45(11), 2673–2681 (1997) 34. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 35. Su, J., Byeon, W., Kossaifi, J., Huang, F., Kautz, J., Anandkumar, A.: Convolutional tensortrain lstm for spatio-temporal learning. arXiv preprint arXiv:2002.09131 (2020) 36. Sundermeyer, M., Alkhouli, T., Wuebker, J., Ney, H.: Translation modeling with bidirectional recurrent neural networks. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 14–25 (2014) 37. Teed, Z., Deng, J.: RAFT: recurrent all-pairs field transforms for optical flow. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12347, pp. 402–419. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-58536-5 24 38. Tokmakov, P., Alahari, K., Schmid, C.: Learning video object segmentation with visual memory. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4481– 4490 (2017) 39. Vazquez-Reina, A., Avidan, S., Pfister, H., Miller, E.: Multiple hypothesis video segmentation from superpixel flows. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010. LNCS, vol. 6315, pp. 268–281. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3642-15555-0 20 40. Ventura, C., Bellver, M., Girbau, A., Salvador, A., Marques, F., Giro-i Nieto, X.: Rvos: Endto-end recurrent network for video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5277–5286 (2019) 41. Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos: Fast endto-end embedding learning for video object segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9481–9490 (2019) 42. Voigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for video object segmentation. arXiv preprint arXiv:1706.09364 (2017) 43. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., Chen, L.-C.: Axial-DeepLab: stand-alone axial-attention for panoptic segmentation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) ECCV 2020. LNCS, vol. 12349, pp. 108–126. Springer, Cham (2020). https://doi. org/10.1007/978-3-030-58548-8 7 44. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7794–7803 (2018) 45. Wug Oh, S., Lee, J.Y., Sunkavalli, K., Joo Kim, S.: Fast video object segmentation by reference-guided mask propagation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7376–7385 (2018) 46. Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. In: Advances in Neural Information Processing Systems, pp. 802–810 (2015) 47. Xu, N., Yang, L., Fan, Y., Yue, D., Liang, Y., Yang, J., Huang, T.: Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327 (2018) 48. Yang, L., Wang, Y., Xiong, X., Yang, J., Katsaggelos, A.K.: Efficient video object segmentation via network modulation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6499–6507 (2018) 49. Yang, Z., Wei, Y., Yang, Y.: Collaborative video object segmentation by foregroundbackground integration. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12350, pp. 332–348. Springer, Cham (2020). https://doi.org/10.1007/9783-030-58558-7 20 50. Zhang, Y., Wu, Z., Peng, H., Lin, S.: A transductive approach for video object segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6949–6958 (2020)  
   
  Author Index  
   
  Agu, Emmanuel 206 Alajaji, Abdulaziz 206 Atzberger, Daniel 162 Azimi, Fatemeh 348  
   
  Kosinka, Jiˇrí 279 Kumar, Dinesh 259 Labedan, Patrice 112 Lamberti, Fabrizio 3 Lecca, Michela 328 Limberger, Daniel 162 Lucas, Daniel 183 Lupini, Gianmario 3  
   
  Buquicchio, Luke 206 Calandra, Davide 3 Cech, Tim 162 Chandrasekaran, Kavin Coe, Patrick 43  
   
  206  
   
  Dehais, Frédéric 112 Dengel, Andreas 348 Döllner, Jürgen 162 Domrös, Sören 183 Espadoto, Mateus 135 Evreinov, Grigori 43 Falcão, Alexandre X. 279 Farah, Imed Riadh 303 Gerych, Walter 206 Goebbels, Steffen 21 Hanachi, Refka 303 Harada, Munenori 69 Hees, Jörn 348 Hensel, Simon 21 Hirata Jr., Roberto 135 Hirata, Nina S. T. 135 Hosobe, Hiroshi 90 Hynek, Jiˇrí 232  
   
  Mansoor, Hamid 206 Melo, Leonardo de 279 Murakami, Mashiho 69 Nakahira, Katsuko T. 69 Nakamura, Yuya 90 Oliveira, Artur André A. M.  
   
  Peysakhovich, Vsevolod 112 Pratticò, Filippo Gabriele 3 Raisamo, Roope 43 Raue, Federico 348 Rundensteiner, Elke 206 Rusˇnák, Vít 232 Scheibel, Willy 162 Sellami, Akrem 303 Sharma, Dharmendra 259 Shino, Motoki 69 Telea, Alexandru C. 135 Telea, Alexandru 279 von Hanxleden, Reinhard  
   
  Jansen, Klaus  
   
  183 Wang, Jieying  
   
  Kada, Martin 21 Kitajima, Muneo 69  
   
  135  
   
  Ziat, Mounia  
   
  279 43  
   
  183  

 Report "Computer Vision, Imaging and Computer Graphics Theory and Applications. 16th International Joint Conference, VISIGRAPP 2021 Virtual Event, February 8–10, 2021 Revised Selected Papers 9783031254765, 9783031254772"  
 ×    

 --- Select Reason ---  Pornographic  Defamatory  Illegal/Unlawful  Spam  Other Terms Of Service Violation  File a copyright complaint     

 Close  Submit    

    Contact information  
 Michael Browner   
   [email protected]    
   
   Address:   
 1918 St.Regis, Dorval, Quebec, H9P 1H6, Canada.   
   
 Support & Legal  
  O nas 
  Skontaktuj się z nami 
  Prawo autorskie 
  Polityka prywatności 
  Warunki 
  FAQs 
  Cookie Policy 
    
 Subscribe to our newsletter  
  Be the first to receive exclusive offers and the latest news on our products and services directly in your inbox.  
   Subscribe     

 Copyright © 2024 DOKUMEN.PUB. All rights reserved.        

 Unsere Partner sammeln Daten und verwenden Cookies zur Personalisierung und Messung von Anzeigen. Erfahren Sie, wie wir und unser Anzeigenpartner Google Daten sammeln und verwenden  .   Cookies zulassen

100. VizSec_2 conference:
Please enable cookies.   
 Sorry, you have been blocked  
 You are unable to access  10times.com  

 Why have I been blocked?  
 This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.  
   
 What can I do to resolve this?  
 You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.  

 Cloudflare Ray ID: 8ea8d9e079081061   •  Your IP: Click to reveal  42.119.229.54  •   Performance & security by  Cloudflare

