<div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas="">
    
<div id="page-wrapper" class="page-wrapper">
  <div id="page">

          <header id="header" class="site-header" data-drupal-selector="site-header" role="banner">

                <div class="site-header__fixable" data-drupal-selector="site-header-fixable">
          <div class="site-header__initial">
            <button class="sticky-header-toggle" data-drupal-selector="sticky-header-toggle" role="switch" aria-controls="site-header__inner" aria-label="Sticky header" aria-checked="false">
              <span class="sticky-header-toggle__icon">
                <span></span>
                <span></span>
                <span></span>
              </span>
            </button>
          </div>

                    <div id="site-header__inner" class="site-header__inner" data-drupal-selector="site-header-inner">
            <div class="container site-header__inner__container">

              


<div id="block-olivero-site-branding" class="site-branding block block-system block-system-branding-block">
  
    
    <div class="site-branding__inner">
              <div class="site-branding__text">
        <div class="site-branding__name">
          <a href="/" title="Home" rel="home">CoNLL</a>
        </div>
      </div>
      </div>
</div>

<div class="header-nav-overlay" data-drupal-selector="header-nav-overlay"></div>


                              <div class="mobile-buttons" data-drupal-selector="mobile-buttons">
                  <button class="mobile-nav-button" data-drupal-selector="mobile-nav-button" aria-label="Main Menu" aria-controls="header-nav" aria-expanded="false">
                    <span class="mobile-nav-button__label">Menu</span>
                    <span class="mobile-nav-button__icon"></span>
                  </button>
                </div>

                <div id="header-nav" class="header-nav" data-drupal-selector="header-nav">
                  
<nav id="block-olivero-main-menu" class="primary-nav block block-menu navigation menu--main" aria-labelledby="block-olivero-main-menu-menu" role="navigation">
            
  <h2 class="visually-hidden block__title" id="block-olivero-main-menu-menu">Main navigation</h2>
  
        


          
        
    <ul class="menu primary-nav__menu primary-nav__menu--level-1" data-drupal-selector="primary-nav-menu--level-1">
            
                          
        
        
        <li class="primary-nav__menu-item primary-nav__menu-item--link primary-nav__menu-item--level-1">
                              
                      <a href="/" class="primary-nav__menu-link primary-nav__menu-link--link primary-nav__menu-link--level-1 is-active" data-drupal-selector="primary-nav-menu-link-has-children" data-drupal-link-system-path="<front>" aria-current="page">            <span class="primary-nav__menu-link-inner primary-nav__menu-link-inner--level-1">Home</span>
          </a>

            
                  </li>
      
                          
        
        
        <li class="primary-nav__menu-item primary-nav__menu-item--link primary-nav__menu-item--level-1">
                              
                      <a href="/previous-editions" class="primary-nav__menu-link primary-nav__menu-link--link primary-nav__menu-link--level-1" data-drupal-selector="primary-nav-menu-link-has-children" data-drupal-link-system-path="node/4">            <span class="primary-nav__menu-link-inner primary-nav__menu-link-inner--level-1">Previous editions</span>
          </a>

            
                  </li>
      
                          
        
        
        <li class="primary-nav__menu-item primary-nav__menu-item--link primary-nav__menu-item--level-1">
                              
                      <a href="/previous-tasks" class="primary-nav__menu-link primary-nav__menu-link--link primary-nav__menu-link--level-1" data-drupal-selector="primary-nav-menu-link-has-children" data-drupal-link-system-path="node/13">            <span class="primary-nav__menu-link-inner primary-nav__menu-link-inner--level-1">Previous tasks</span>
          </a>

            
                  </li>
      
                          
        
        
        <li class="primary-nav__menu-item primary-nav__menu-item--link primary-nav__menu-item--level-1">
                              
                      <a href="http://www.signll.org/" class="primary-nav__menu-link primary-nav__menu-link--link primary-nav__menu-link--level-1" data-drupal-selector="primary-nav-menu-link-has-children">            <span class="primary-nav__menu-link-inner primary-nav__menu-link-inner--level-1">SIGNLL</span>
          </a>

            
                  </li>
      
                          
        
        
        <li class="primary-nav__menu-item primary-nav__menu-item--link primary-nav__menu-item--level-1">
                              
                      <a href="/contact" class="primary-nav__menu-link primary-nav__menu-link--link primary-nav__menu-link--level-1" data-drupal-selector="primary-nav-menu-link-has-children" data-drupal-link-system-path="node/1">            <span class="primary-nav__menu-link-inner primary-nav__menu-link-inner--level-1">Contact</span>
          </a>

            
                  </li>
          </ul>
  


  </nav>


                  

  <div class="region region--secondary-menu">
    <nav id="block-olivero-account-menu" class="block block-menu navigation menu--account secondary-nav" aria-labelledby="block-olivero-account-menu-menu" role="navigation">
            
  <span class="visually-hidden" id="block-olivero-account-menu-menu">User account menu</span>
  
        


          <ul class="menu secondary-nav__menu secondary-nav__menu--level-1">
            
                          
        
        
        <li class="secondary-nav__menu-item secondary-nav__menu-item--link secondary-nav__menu-item--level-1">
          <a href="/user/login" class="secondary-nav__menu-link secondary-nav__menu-link--link secondary-nav__menu-link--level-1" data-drupal-link-system-path="user/login">Log in</a>

                  </li>
          </ul>
  


  </nav>

  </div>

                </div>
                          </div>
          </div>
        </div>
      </header>
    
    <div id="main-wrapper" class="layout-main-wrapper layout-container">
      <div id="main" class="layout-main">
        <div class="main-content">
          <a id="main-content" tabindex="-1"></a>
          
          <div class="main-content__container container">
            

  <div class="region region--highlighted grid-full layout--pass--content-medium">
    <div data-drupal-messages-fallback="" class="hidden messages-list"></div>

  </div>

            



                          <main role="main">
                

  <div class="region region--content-above grid-full layout--pass--content-medium">
    

<div id="block-olivero-page-title" class="block block-core block-page-title-block">
  
  

  <h1 class="title page-title">
<span>CoNLL 2024</span>
</h1>


  
</div>

  </div>

                

  <div class="region region--content grid-full layout--pass--content-medium" id="content">
    

<div id="block-corolla-system-main" class="block block-system block-system-main-block">
  
    
      <div class="block__content">
      

<article data-history-node-id="44" class="node node--type-page node--promoted node--view-mode-full">
  <header class="">
    
          
      </header>
  <div class="node__content">
        
            <div class="text-content clearfix field field--name-body field--type-text-with-summary field--label-hidden field__item"><p><em>May 24, 2024</em></p>
<p>CoNLL is a yearly conference organized by <a href="http://www.signll.org/">SIGNLL</a> (ACL's Special Interest Group on Natural Language Learning), focusing on theoretically, cognitively and scientifically motivated approaches to computational linguistics. This year, CoNLL will be colocated with <a href="https://2024.emnlp.org/">EMNLP 2024</a>. Registrations for CoNLL can be made through EMNLP (workshop 1).</p>
<p>Submission page available <a href="https://openreview.net/group?id=EMNLP/2024/Workshop/CoNLL#tab-recent-activity">here</a>.</p>
<p><strong>Papers that have received reviews in current or previous ARR cycles can be committed to CoNLL 2024 </strong><a href="https://openreview.net/group?id=EMNLP/2024/Workshop/CoNLL_ARR_Commitment"><strong>here</strong></a><strong> by August 30, 2024.&nbsp;</strong></p>
<h2><strong>Accepted papers</strong></h2>
<p>A list of papers that have been accepted for CoNLL 2024 is available <a href="https://conll.org/2024-accepted-papers">here</a>.</p>
<h2><strong>Program</strong></h2>
<p>The program for CoNLL 2024 is available <a href="https://conll.org/program_2024">here</a>.</p>
<h2>Call for Papers</h2>
<p>SIGNLL invites submissions to the 28th Conference on Computational Natural Language Learning (CoNLL 2024). The focus of CoNLL is on <strong>theoretically, cognitively and scientifically motivated approaches to computational linguistics, rather than on work driven by particular engineering applications</strong>. Such approaches include:</p>
<ul>
<li>Computational learning theory and other techniques for theoretical analysis of machine learning models for NLP</li>
<li>Models of first, second and bilingual language acquisition by humans</li>
<li>Models of sign language acquisition, understanding, and production</li>
<li>Models of language evolution and change</li>
<li>Computational simulation and analysis of findings from psycholinguistic and neurolinguistic experiments</li>
<li>Analysis and interpretation of NLP models, using methods inspired by cognitive science or linguistics or other methods</li>
<li>Data resources, techniques and tools for scientifically-oriented research in computational linguistics</li>
<li>Connections between computational models and formal languages or linguistic theories</li>
<li>Linguistic typology, translation, and other multilingual work</li>
<li>Theoretically, cognitively and scientifically motivated approaches to text generation</li>
</ul>
<p>We welcome work targeting any aspect of language, including:</p>
<ul>
<li>Speech and phonology</li>
<li>Syntax and morphology</li>
<li>Lexical, compositional and discourse semantics</li>
<li>Dialogue and interactive language use</li>
<li>Sociolinguistics</li>
<li>Multimodal and grounded language learning</li>
</ul>
<p>We do not restrict the topic of submissions to fall into this list. However, the submissions’ relevance to the conference’s focus on <strong>theoretically, cognitively and scientifically motivated approaches</strong> will play an important role in the review process.</p>
<p>Submitted papers must be <strong>anonymous</strong> and use the <a href="https://2024.emnlp.org/calls/main_conference_papers/#paper-submission-information">EMNLP 2024 template</a>. Submitted papers may consist of up to <strong>8 pages</strong> of content plus unlimited space for references. Authors of accepted papers will have an additional page to address reviewers’ comments in the camera-ready version (9 pages of content in total, excluding references). Optional anonymized supplementary materials and a <strong>PDF appendix</strong> are allowed. The appendix should be submitted as a separate PDF file (reviewers are not required to consider the materials in the appendix so it should not include any essential content to the understanding of the paper). Please refer to the <a href="https://2024.emnlp.org/calls/main_conference_papers/">EMNLP 2024 Call for Papers</a> for more details on the submission format. Note that, unlike EMNLP, we do not mandate that papers have a discussion section of the limitations of the work. However, we strongly encourage authors to have such a section in the appendix.</p>
<p>Please submit via <a href="https://openreview.net/group?id=EMNLP/2024/Workshop/CoNLL#tab-recent-activity">Open Review</a>. CoNLL 2024 will accept ARR submission depending on the full review to be completed by <strong>Jul 1, 2024</strong>. Please note that CoNLL 2024 is an in-person conference. We expect all accepted papers to be presented physically and presenting authors must register through EMNLP (workshop).&lt;/p&gt;</p>
<p><strong>Timeline</strong><br>(All deadlines are 11:59pm UTC-12h, AoE)<br>Submission deadline:<s> Monday July 1, 2024</s> (EXTENDED) Sunday, July 7, 2024<br>ARR Commitment deadline: Friday, August 30, 2024<br>Notification of acceptance: <s>Friday, September 20 </s>(DELAYED), Tuesday, September 24, 2024<br>Camera ready papers due: Friday, October 11, 2024<br>Conference: November 15 - 16, 2024</p>
<p><strong>Venue</strong><br>CoNLL 2024 will be held in-person, along with <a href="https://2024.emnlp.org/">EMNLP</a> in <strong>Miami, Florida</strong>.</p>
<p><strong>Multiple submission policy</strong><br>CoNLL 2024 will refuse papers that are currently under submission, or that will be submitted to other meetings or publications, including EMNLP. Papers submitted elsewhere and papers that overlap significantly in content or results with papers that will be (or have been) published elsewhere will be rejected. Authors submitting more than one paper to CoNLL 2024 must ensure that the submissions do not overlap significantly (&gt;25%) with each other in content or results.</p>
<p><strong>Information About Travel Visas</strong><br>If you will be requiring travel visas to Miami, Florida, please fill out this form: <a href="https://forms.office.com/r/rwq33DTqhL">Travel Visa Form</a></p>
<p>This has been prepared by the EMNLP organizers to facilitate the process of acquiring visas. If visas are needed, your information should be provided as early as possible. If you have more questions, please contact Mark Finlayson and Zoey Liu, who are the local chairs here: <a href="https://2024.emnlp.org/organization/">EMNLP Organizers</a></p>
<h2>CoNLL 2024 Chairs and Organizers</h2>
<p><strong>The conference's co-chairs are:</strong></p>
<p><a href="https://www.khoury.northeastern.edu/people/malihe-alikhani/">Malihe Alikhani</a> (Northeastern University, MA, USA)</p>
<p><img src="https://static.wixstatic.com/media/28d0b7_e36b7ba335ef438e944aa99410f0fa9e~mv2.jpg" width="261" height="253"></p>
<p><a href="https://libbybarak.wordpress.com/">Libby Barak</a> (Montclair State university, NJ, USA)</p>
<p><img src="https://i0.wp.com/libbybarak.wordpress.com/wp-content/uploads/2018/06/dsf4977-1.jpg" width="302" height="201"></p>
<hr>
<p><strong>Publication chairs:</strong></p>
<p><a href="https://merterm.github.io/">Mert Inan</a> (Northeastern University, MA, USA)</p>
<p><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&amp;user=ztpK4iwAAAAJ" width="213" height="235"><br><a href="https://www.juliareneewatson.com/">Julia Watson</a> (University of Toronto, ON, Canada)</p>
<p><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&amp;user=QNXA_tcAAAAJ" width="201" height="261"></p>
<p><strong>SIGNLL</strong></p>
<ul>
<li>SIGNLL President: <a href="https://en.cognitive.huji.ac.il/people/omri-abend">Omri Abend</a> (<span style="-webkit-text-stroke-width:0px;background-color:rgb(255, 255, 255);color:rgb(46, 46, 46);display:inline !important;float:none;font-family:&quot;Trebuchet MS&quot;, &quot;Helvetica Neue&quot;, Arial, Helvetica, sans-serif;font-size:15.008px;font-style:normal;font-variant-caps:normal;font-variant-ligatures:normal;font-weight:400;letter-spacing:normal;orphans:2;text-align:start;text-decoration-color:initial;text-decoration-style:initial;text-decoration-thickness:initial;text-indent:0px;text-transform:none;white-space:normal;widows:2;word-spacing:0px;">Hebrew University of Jerusalem, Israel)</span></li>
<li>SIGNLL Secretary: <a href="https://research.vu.nl/en/persons/antske-fokkens">Antske Fokkens</a> (<span style="-webkit-text-stroke-width:0px;background-color:rgb(255, 255, 255);color:rgb(46, 46, 46);display:inline !important;float:none;font-family:&quot;Trebuchet MS&quot;, &quot;Helvetica Neue&quot;, Arial, Helvetica, sans-serif;font-size:15.008px;font-style:normal;font-variant-caps:normal;font-variant-ligatures:normal;font-weight:400;letter-spacing:normal;orphans:2;text-align:start;text-decoration-color:initial;text-decoration-style:initial;text-decoration-thickness:initial;text-indent:0px;text-transform:none;white-space:normal;widows:2;word-spacing:0px;">Vrije Universiteit Amsterdam, Netherlands</span>)</li>
</ul>
<h2>Invited speakers</h2>
<p><img src="https://mbzuai.ac.ae/wp-content/uploads/2023/10/Thamar-487px.jpg" width="311" height="233"></p>
<p><a href="https://mbzuai.ac.ae/study/faculty/thamar-solorio/">Thamar Solorio</a> (Mohamed bin Zayed University of Artificial &nbsp;Intelligence, MBZUAI)</p>
<p><strong>Title: </strong>Towards AI models that can help us to become better global social beings</p>
<p style="-webkit-text-stroke-width:0px;caret-color:rgb(255, 255, 255);color:rgb(255, 255, 255);font-family:Helvetica;font-size:12px;font-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;line-height:1.38;margin-bottom:0pt;margin-top:0pt;orphans:auto;text-align:start;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;widows:auto;word-spacing:0px;" dir="ltr"><span style="background-color:transparent;color:rgb(0, 0, 0);font-family:Arial, sans-serif;font-size:11pt;font-variant-alternates:normal;font-variant-east-asian:normal;font-variant-numeric:normal;vertical-align:baseline;"><strong>Abstract:</strong> Cultural norms and values fundamentally shape our social interactions. Communication within any society reflects these cultural contexts. For example, while direct eye contact is often seen as a sign of confidence in many Western cultures, it may be viewed as disrespectful in other parts of the world. Moreover, human-human interactions include so much more than just the words we utter; nonverbal communication, including body language and other cues, provides rich signals to those around us.</span></p>
<p style="-webkit-text-stroke-width:0px;caret-color:rgb(255, 255, 255);color:rgb(255, 255, 255);font-family:Helvetica;font-size:12px;font-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;line-height:1.38;margin-bottom:0pt;margin-top:0pt;orphans:auto;text-align:start;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;widows:auto;word-spacing:0px;" dir="ltr"><span style="background-color:transparent;color:rgb(0, 0, 0);font-family:Arial, sans-serif;font-size:11pt;font-variant-alternates:normal;font-variant-east-asian:normal;font-variant-numeric:normal;vertical-align:baseline;">As vision language models (VLMs)&nbsp; are increasingly integrated into user-facing applications, it is becoming relevant to wonder if and to what extent this technology can robustly process these signals. My research group is interested in developing evaluation frameworks to assess the abilities of VLMs concerning interpreting social cues and in developing new approaches that can assist us and, perhaps, enhance our cross-cultural human-human interactions.&nbsp;</span></p>
<p style="-webkit-text-stroke-width:0px;caret-color:rgb(255, 255, 255);color:rgb(255, 255, 255);font-family:Helvetica;font-size:12px;font-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;line-height:1.38;margin-bottom:0pt;margin-top:0pt;orphans:auto;text-align:start;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;widows:auto;word-spacing:0px;" dir="ltr">&nbsp;</p>
<p style="-webkit-text-stroke-width:0px;caret-color:rgb(255, 255, 255);color:rgb(255, 255, 255);font-family:Helvetica;font-size:12px;font-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;line-height:1.38;margin-bottom:0pt;margin-top:0pt;orphans:auto;text-align:start;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;widows:auto;word-spacing:0px;" dir="ltr" id="m_8539696057909576474gmail-docs-internal-guid-230c488a-7fff-ddab-45e1-6d0595e192fe"><span style="background-color:transparent;color:rgb(0, 0, 0);font-family:Arial, sans-serif;font-size:11pt;font-variant-alternates:normal;font-variant-east-asian:normal;font-variant-numeric:normal;vertical-align:baseline;"><strong>Bio: </strong>Thamar Solorio is a professor in the NLP department at MBZUAI. She is also a tenured professor of Computer Science at the University of Houston. She is the director and founder of the RiTUAL Lab. Her research interests include NLP for low-resource settings and multilingual data, including code-switching and information extraction. More recently, she was moved towards language and vision problems, focusing on developing inclusive NLP. She received a National Science Foundation (NSF) CAREER award for her work on authorship attribution and was awarded the 2014 Emerging Leader ABIE Award in Honor of Denice Denton. She served two terms as an elected board member of the North American Chapter of the Association of Computational Linguistics (NAACL) and was PC co-chair for NAACL 2019. She is an Editor in Chief for the ACL Rolling Review (ARR) initiative and was a member of the advisory board for ARR. She serves as general chair for the 2024 Conference on Empirical Methods in Natural Language Processing.</span></p>
<p style="-webkit-text-stroke-width:0px;caret-color:rgb(255, 255, 255);color:rgb(255, 255, 255);font-family:Helvetica;font-size:12px;font-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;line-height:1.38;margin-bottom:0pt;margin-top:0pt;orphans:auto;text-align:start;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;widows:auto;word-spacing:0px;" dir="ltr">&nbsp;</p>
<p style="-webkit-text-stroke-width:0px;caret-color:rgb(255, 255, 255);color:rgb(255, 255, 255);font-family:Helvetica;font-size:12px;font-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;line-height:1.38;margin-bottom:0pt;margin-top:0pt;orphans:auto;text-align:start;text-decoration:none;text-indent:0px;text-transform:none;white-space:normal;widows:auto;word-spacing:0px;" dir="ltr"><span style="background-color:transparent;color:rgb(0, 0, 0);font-family:Arial, sans-serif;font-size:11pt;font-variant-alternates:normal;font-variant-east-asian:normal;font-variant-numeric:normal;vertical-align:baseline;">&nbsp;</span></p>
<p><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&amp;user=ZZvFoNsAAAAJ" width="217" height="283"><br><a href="https://vl2.gallaudet.edu/people/lorna-quandt">Lorna Quandt</a> (Gallaudet University)</p>
<p><strong>Title:</strong> Integrating AI-Driven Sign Language Technologies in Education: Recognition, Generation, and Interaction</p>
<p><strong>Abstract: </strong>This talk explores integrating AI-driven technologies in sign language research, covering the unique challenges of sign language recognition and generation. Dr. Quandt will explore these cutting-edge considerations through the lens of two research projects, ASL Champ! and BRIDGE. Both projects focus on sign language recognition and generation, which is crucial for advancing interaction in virtual and educational environments. ASL Champ! utilizes a dataset of 3D signs to enhance deep-learning-powered sign recognition in virtual reality. At the same time, BRIDGE extends this work by incorporating both recognition and generation of signs to create a more robust, interactive experience. This dual focus underscores the importance of pursuing recognition and generation in tandem rather than treating them as entirely distinct challenges. By leveraging advances in AI and natural language processing (NLP), we can create technologies that recognize and generate signs and facilitate deeper understanding and use of signed languages. These advancements hold great educational potential, particularly in providing more accessible tools for deaf students and enabling broader instruction in sign language. The talk will also address how these innovations can reshape the NLP field by widening the focus beyond spoken/written language and into multimodal, signed, and nonverbal aspects of language, which can inform all linguistic research.</p>
<p><strong>Bio:</strong> Dr. Lorna Quandt is the Action &amp; Brain Lab director at Gallaudet University in Washington, D.C. She serves as Co-Director of the VL2 Research Center alongside Melissa Malzkuhn. Dr. Quandt is an Associate Professor in the Ph.D. in Educational Neuroscience (PEN) program and the Science Director of the Motion Light Lab. Dr. Quandt founded the Action &amp; Brain lab in early 2016. Before that, Dr. Quandt obtained her BA in Psychology from Haverford College and a PhD in Psychology, specializing in Brain &amp; Cognitive Sciences, from Temple University. She completed a postdoctoral fellowship at the University of Pennsylvania, working with Dr. Anjan Chatterjee. Her research examines how knowledge of sign language changes perception, particularly visuospatial processing. Dr. Quandt is also pursuing the development of research-based educational technology to create new ways to learn signed languages in virtual reality.</p>
<p>&nbsp;</p>
<hr>
<p><strong>Areas and ACs</strong></p>
<ul>
<li>Computational Psycholinguistics, Cognition and Linguistics: Nathan Schneider</li>
<li>Computational Social Science: Kate Atwell</li>
<li>Interaction and Grounded Language Learning: Anthony Sicilia</li>
<li>Lexical, Compositional and Discourse Semantics: Shira Wein</li>
<li>Multilingual Work and Translation: Yuval Marton</li>
<li>Natural Language Generation: Tuhin Chakrabarty</li>
<li>Resources and Tools for Scientifically Motivated Research: Venkat</li>
<li>Speech and Phonology: Huteng Dai</li>
<li>Syntax and Morphology: Leshem Choshen</li>
<li>
<p>Theoretical Analysis and Interpretation of ML Models for NLP: Kevin Small</p>
<hr>
<p><strong>Sponsor</strong></p>
<p><img data-entity-uuid="79dcb8df-da74-49c1-9070-c15f574279f5" data-entity-type="file" src="/sites/conll.org/files/inline-images/image.png" width="293" height="68"><br>&nbsp;</p>
</li>
</ul>
</div>
      
  </div>
  </article>

    </div>
  </div>


<div id="block-corolla-block-1" class="block block-block-content block-block-contentcf253b90-7a86-4851-86b5-da6a6ae6fe71">
  
    
      <div class="block__content">
      
            <div class="text-content clearfix field field--name-body field--type-text-with-summary field--label-hidden field__item"><p>Webmaster: <a href="mailto:jens.lemmens@uantwerpen.be?subject=Concerning%20CoNLL%20Website">Jens Lemmens</a></p>
</div>
      
    </div>
  </div>

  </div>

              </main>
                        
          </div>
        </div>
        <div class="social-bar">
          
<div class="social-bar__inner fixable">
  <div class="rotate">
    

<div id="block-olivero-syndicate" role="complementary" class="block block-node block-node-syndicate-block">
  
    
      <div class="block__content">
      


<a href="/rss.xml" class="feed-icon">
  <span class="feed-icon__label">
    RSS feed
  </span>
  <span class="feed-icon__icon" aria-hidden="true">
    <svg xmlns="http://www.w3.org/2000/svg" width="14.2" height="14.2" viewBox="0 0 14.2 14.2">
  <path d="M4,12.2c0-2.5-3.9-2.4-3.9,0C0.1,14.7,4,14.6,4,12.2z M9.1,13.4C8.7,9,5.2,5.5,0.8,5.1c-1,0-1,2.7-0.1,2.7c3.1,0.3,5.5,2.7,5.8,5.8c0,0.7,2.1,0.7,2.5,0.3C9.1,13.7,9.1,13.6,9.1,13.4z M14.2,13.5c-0.1-3.5-1.6-6.9-4.1-9.3C7.6,1.7,4.3,0.2,0.8,0c-1,0-1,2.6-0.1,2.6c5.8,0.3,10.5,5,10.8,10.8C11.5,14.5,14.3,14.4,14.2,13.5z"></path>
</svg>
  </span>
</a>

    </div>
  </div>

  </div>
</div>

        </div>
      </div>
    </div>

    <footer class="site-footer">
      <div class="site-footer__inner container">
        
        

  <div class="region region--footer-bottom grid-full layout--pass--content-medium">
    

<div id="block-olivero-powered" class="block block-system block-system-powered-by-block">
  
    
    
  <span>
    Powered by    <a href="https://www.drupal.org">Drupal</a>
    <span class="drupal-logo" role="img" aria-label="Drupal Logo">
      <svg width="14" height="19" viewBox="0 0 42.15 55.08" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M29.75 11.73C25.87 7.86 22.18 4.16 21.08 0 20 4.16 16.28 7.86 12.4 11.73 6.59 17.54 0 24.12 0 34a21.08 21.08 0 1042.15 0c0-9.88-6.59-16.46-12.4-22.27zM10.84 35.92a14.13 14.13 0 00-1.65 2.62.54.54 0 01-.36.3h-.18c-.47 0-1-.92-1-.92-.14-.22-.27-.45-.4-.69l-.09-.19C5.94 34.25 7 30.28 7 30.28a17.42 17.42 0 012.52-5.41 31.53 31.53 0 012.28-3l1 1 4.72 4.82a.54.54 0 010 .72l-4.93 5.47zm10.48 13.81a7.29 7.29 0 01-5.4-12.14c1.54-1.83 3.42-3.63 5.46-6 2.42 2.58 4 4.35 5.55 6.29a3.08 3.08 0 01.32.48 7.15 7.15 0 011.3 4.12 7.23 7.23 0 01-7.23 7.25zM35 38.14a.84.84 0 01-.67.58h-.14a1.22 1.22 0 01-.68-.55 37.77 37.77 0 00-4.28-5.31l-1.93-2-6.41-6.65a54 54 0 01-3.84-3.94 1.3 1.3 0 00-.1-.15 3.84 3.84 0 01-.51-1v-.19a3.4 3.4 0 011-3c1.24-1.24 2.49-2.49 3.67-3.79 1.3 1.44 2.69 2.82 4.06 4.19a57.6 57.6 0 017.55 8.58A16 16 0 0135.65 34a14.55 14.55 0 01-.65 4.14z"></path>
</svg>
    </span>
  </span>
</div>

  </div>

      </div>
    </footer>

    <div class="overlay" data-drupal-selector="overlay"></div>

  </div>
</div>

  </div>